input_key,citation_key,header,name,title,link,full_bib
2404.17807,hendrickx2019semeval,"[\protect\citeauthoryear{Hendrickx \bgroup \em et al.\egroup }{2019}]{hendrickx2019semeval} Iris Hendrickx, Su~Nam Kim, et~al.",Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.,Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.,,"[\protect\citeauthoryear{Hendrickx \bgroup \em et al.\egroup }{2019}]{hendrickx2019semeval} Iris Hendrickx, Su~Nam Kim, et~al. 
 Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. 
 {\em arXiv preprint arXiv:1911.10422}, 2019."
2404.17807,jat2018improving,"[\protect\citeauthoryear{Jat \bgroup \em et al.\egroup }{2018}]{jat2018improving} Sharmistha Jat, Siddhesh Khandelwal, and Partha Talukdar.",Improving distantly supervised relation extraction using word and entity based attention.,Improving distantly supervised relation extraction using word and entity based attention.,,"[\protect\citeauthoryear{Jat \bgroup \em et al.\egroup }{2018}]{jat2018improving} Sharmistha Jat, Siddhesh Khandelwal, and Partha Talukdar. 
 Improving distantly supervised relation extraction using word and entity based attention. 
 {\em arXiv preprint arXiv:1804.06987}, 2018."
2404.17807,li2024unlocking,"[\protect\citeauthoryear{Li \bgroup \em et al.\egroup }{2024}]{li2024unlocking} Guozheng Li, Wenjun Ke, et~al.",Unlocking instructive in-context learning with tabular prompting for relational triple extraction.,Unlocking instructive in-context learning with tabular prompting for relational triple extraction.,,"[\protect\citeauthoryear{Li \bgroup \em et al.\egroup }{2024}]{li2024unlocking} Guozheng Li, Wenjun Ke, et~al. 
 Unlocking instructive in-context learning with tabular prompting for relational triple extraction. 
 {\em arXiv preprint arXiv:2402.13741}, 2024."
2404.17807,liu2019roberta,"[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup }{2019}]{liu2019roberta} Yinhan Liu, Myle Ott, et~al.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup }{2019}]{liu2019roberta} Yinhan Liu, Myle Ott, et~al. 
 Roberta: A robustly optimized bert pretraining approach. 
 {\em arXiv preprint arXiv:1907.11692}, 2019."
2404.17807,luan2018multi,"[\protect\citeauthoryear{Luan \bgroup \em et al.\egroup }{2018}]{luan2018multi} Yi~Luan, Luheng He, et~al.","Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction.","Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction.",,"[\protect\citeauthoryear{Luan \bgroup \em et al.\egroup }{2018}]{luan2018multi} Yi~Luan, Luheng He, et~al. 
 Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. 
 {\em arXiv preprint arXiv:1808.09602}, 2018."
2404.17807,touvron2023llama,"[\protect\citeauthoryear{Touvron \bgroup \em et al.\egroup }{2023}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[\protect\citeauthoryear{Touvron \bgroup \em et al.\egroup }{2023}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, et~al. 
 Llama: Open and efficient foundation language models. 
 {\em arXiv preprint arXiv:2302.13971}, 2023."
2404.17807,wang2023instructuie,"[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup }{2023c}]{wang2023instructuie} Xiao Wang, Weikang Zhou, et~al.",Instructuie: Multi-task instruction tuning for unified information extraction.,Instructuie: Multi-task instruction tuning for unified information extraction.,,"[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup }{2023c}]{wang2023instructuie} Xiao Wang, Weikang Zhou, et~al. 
 Instructuie: Multi-task instruction tuning for unified information extraction. 
 {\em arXiv preprint arXiv:2304.08085}, 2023."
2404.17807,zhang2015relation,[\protect\citeauthoryear{Zhang and Wang}{2015}]{zhang2015relation} Dongxu Zhang and Dong Wang.,Relation classification via recurrent neural network.,Relation classification via recurrent neural network.,,"[\protect\citeauthoryear{Zhang and Wang}{2015}]{zhang2015relation} Dongxu Zhang and Dong Wang. 
 Relation classification via recurrent neural network. 
 {\em arXiv preprint arXiv:1508.01006}, 2015."
2404.18466,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2404.18466,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2404.18466,dou2023loramoe,"[Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan, et~al.]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al.",Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,,"[Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan, et~al.]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al. 
 Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. 
 \emph{arXiv preprint arXiv:2312.09979}, 2023."
2404.18466,wu2024llama,"[Wu et~al.(2024{\natexlab{a}})Wu, Gan, Ge, Lu, Wang, Feng, Luo, and Shan]{wu2024llama} Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye~Feng, Ping Luo, and Ying Shan.",Llama pro: Progressive llama with block expansion.,Llama pro: Progressive llama with block expansion.,,"[Wu et~al.(2024{\natexlab{a}})Wu, Gan, Ge, Lu, Wang, Feng, Luo, and Shan]{wu2024llama} Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye~Feng, Ping Luo, and Ying Shan. 
 Llama pro: Progressive llama with block expansion. 
 \emph{arXiv preprint arXiv:2401.02415}, 2024{\natexlab{a}}."
2404.18466,zhang2023instruction,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al.",Instruction tuning for large language models: A survey.,Instruction tuning for large language models: A survey.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al. 
 Instruction tuning for large language models: A survey. 
 \emph{arXiv preprint arXiv:2308.10792}, 2023{\natexlab{a}}."
2404.18466,yu2023language,"[Yu et~al.(2023)Yu, Yu, Yu, Huang, and Li]{yu2023language} Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li.",Language models are super mario: Absorbing abilities from homologous models as a free lunch.,Language models are super mario: Absorbing abilities from homologous models as a free lunch.,,"[Yu et~al.(2023)Yu, Yu, Yu, Huang, and Li]{yu2023language} Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 
 Language models are super mario: Absorbing abilities from homologous models as a free lunch. 
 \emph{arXiv preprint arXiv:2311.03099}, 2023."
2404.18466,cui2023ultrafeedback,"[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback} Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun.",Ultrafeedback: Boosting language models with high-quality feedback.,Ultrafeedback: Boosting language models with high-quality feedback.,,"[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback} Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 
 Ultrafeedback: Boosting language models with high-quality feedback. 
 \emph{arXiv preprint arXiv:2310.01377}, 2023."
2404.18466,wang2023trace,"[Wang et~al.(2023{\natexlab{a}})Wang, Zhang, Chen, Gao, Jin, Yang, Xi, Zheng, Zou, Gui, et~al.]{wang2023trace} Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, et~al.",Trace: A comprehensive benchmark for continual learning in large language models.,Trace: A comprehensive benchmark for continual learning in large language models.,,"[Wang et~al.(2023{\natexlab{a}})Wang, Zhang, Chen, Gao, Jin, Yang, Xi, Zheng, Zou, Gui, et~al.]{wang2023trace} Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, et~al. 
 Trace: A comprehensive benchmark for continual learning in large language models. 
 \emph{arXiv preprint arXiv:2310.06762}, 2023{\natexlab{a}}."
2404.18466,ivison2023camels,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.",Camels in a changing climate: Enhancing lm adaptation with tulu 2.,Camels in a changing climate: Enhancing lm adaptation with tulu 2.,,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al. 
 Camels in a changing climate: Enhancing lm adaptation with tulu 2. 
 \emph{arXiv preprint arXiv:2311.10702}, 2023."
2404.18466,han2024parameter,"[Han et~al.(2024)Han, Gao, Liu, Zhang, et~al.]{han2024parameter} Zeyu Han, Chao Gao, Jinyang Liu, Sai~Qian Zhang, et~al.",Parameter-efficient fine-tuning for large models: A comprehensive survey.,Parameter-efficient fine-tuning for large models: A comprehensive survey.,,"[Han et~al.(2024)Han, Gao, Liu, Zhang, et~al.]{han2024parameter} Zeyu Han, Chao Gao, Jinyang Liu, Sai~Qian Zhang, et~al. 
 Parameter-efficient fine-tuning for large models: A comprehensive survey. 
 \emph{arXiv preprint arXiv:2403.14608}, 2024."
2404.18466,zhang2023adaptive,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Bukharin, He, Cheng, Chen, and Zhao]{zhang2023adaptive} Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu Chen, and Tuo Zhao.",Adaptive budget allocation for parameter-efficient fine-tuning.,Adaptive budget allocation for parameter-efficient fine-tuning.,,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Bukharin, He, Cheng, Chen, and Zhao]{zhang2023adaptive} Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu Chen, and Tuo Zhao. 
 Adaptive budget allocation for parameter-efficient fine-tuning. 
 \emph{arXiv preprint arXiv:2303.10512}, 2023{\natexlab{b}}."
2404.18466,zaken2021bitfit,"[Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg]{zaken2021bitfit} Elad~Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.",Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.,Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.,,"[Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg]{zaken2021bitfit} Elad~Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 
 Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. 
 \emph{arXiv preprint arXiv:2106.10199}, 2021."
2404.18466,xiao2023lm,"[Xiao et~al.(2023)Xiao, Liu, Zhang, and Xing]{xiao2023lm} Shitao Xiao, Zheng Liu, Peitian Zhang, and Xingrun Xing.",Lm-cocktail: Resilient tuning of language models via model merging.,Lm-cocktail: Resilient tuning of language models via model merging.,,"[Xiao et~al.(2023)Xiao, Liu, Zhang, and Xing]{xiao2023lm} Shitao Xiao, Zheng Liu, Peitian Zhang, and Xingrun Xing. 
 Lm-cocktail: Resilient tuning of language models via model merging. 
 \emph{arXiv preprint arXiv:2311.13534}, 2023."
2404.18466,luo2023empirical,"[Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.",An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,,"[Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 
 An empirical study of catastrophic forgetting in large language models during continual fine-tuning. 
 \emph{arXiv preprint arXiv:2308.08747}, 2023."
2404.18466,peng2024scalable,"[Peng et~al.(2024)Peng, Tian, Liu, Yang, and Jia]{peng2024scalable} Bohao Peng, Zhuotao Tian, Shu Liu, Mingchang Yang, and Jiaya Jia.",Scalable language model with generalized continual learning.,Scalable language model with generalized continual learning.,,"[Peng et~al.(2024)Peng, Tian, Liu, Yang, and Jia]{peng2024scalable} Bohao Peng, Zhuotao Tian, Shu Liu, Mingchang Yang, and Jiaya Jia. 
 Scalable language model with generalized continual learning. 
 \emph{arXiv preprint arXiv:2404.07470}, 2024."
2404.18466,razdaibiedina2023progressive,"[Razdaibiedina et~al.(2023)Razdaibiedina, Mao, Hou, Khabsa, Lewis, and Almahairi]{razdaibiedina2023progressive} Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi.",Progressive prompts: Continual learning for language models.,Progressive prompts: Continual learning for language models.,,"[Razdaibiedina et~al.(2023)Razdaibiedina, Mao, Hou, Khabsa, Lewis, and Almahairi]{razdaibiedina2023progressive} Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi. 
 Progressive prompts: Continual learning for language models. 
 \emph{arXiv preprint arXiv:2301.12314}, 2023."
2404.18466,wu2024continual,"[Wu et~al.(2024{\natexlab{b}})Wu, Luo, Li, Pan, Vu, and Haffari]{wu2024continual} Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari.",Continual learning for large language models: A survey.,Continual learning for large language models: A survey.,,"[Wu et~al.(2024{\natexlab{b}})Wu, Luo, Li, Pan, Vu, and Haffari]{wu2024continual} Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. 
 Continual learning for large language models: A survey. 
 \emph{arXiv preprint arXiv:2402.01364}, 2024{\natexlab{b}}."
2404.18466,xu2023baize,"[Xu et~al.(2023)Xu, Guo, Duan, and McAuley]{xu2023baize} Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.",Baize: An open-source chat model with parameter-efficient tuning on self-chat data.,Baize: An open-source chat model with parameter-efficient tuning on self-chat data.,,"[Xu et~al.(2023)Xu, Guo, Duan, and McAuley]{xu2023baize} Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 
 Baize: An open-source chat model with parameter-efficient tuning on self-chat data. 
 \emph{arXiv preprint arXiv:2304.01196}, 2023."
2404.18466,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2404.18466,chen2021evaluating,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}, 2021."
2404.18466,han2019episodic,"[Han et~al.(2019)Han, Kang, Jung, and Hwang]{han2019episodic} Moonsu Han, Minki Kang, Hyunwoo Jung, and Sung~Ju Hwang.",Episodic memory reader: Learning what to remember for question answering from streaming data.,Episodic memory reader: Learning what to remember for question answering from streaming data.,,"[Han et~al.(2019)Han, Kang, Jung, and Hwang]{han2019episodic} Moonsu Han, Minki Kang, Hyunwoo Jung, and Sung~Ju Hwang. 
 Episodic memory reader: Learning what to remember for question answering from streaming data. 
 \emph{arXiv preprint arXiv:1903.06164}, 2019."
2404.18466,yang2018hotpotqa,"[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa} Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W Cohen, Ruslan Salakhutdinov, and Christopher~D Manning.","Hotpotqa: A dataset for diverse, explainable multi-hop question answering.","Hotpotqa: A dataset for diverse, explainable multi-hop question answering.",,"[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa} Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W Cohen, Ruslan Salakhutdinov, and Christopher~D Manning. 
 Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 
 \emph{arXiv preprint arXiv:1809.09600}, 2018."
2404.19232,chern2023factool,"[{Chern et~al.(2023)Chern, Chern, Chen, Yuan, Feng, Zhou, He, Neubig, Liu et~al.}]{chern2023factool} I~Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et~al. 2023.",Factool: Factuality detection in generative ai--a tool augmented framework for multi-task and multi-domain scenarios.,Factool: Factuality detection in generative ai--a tool augmented framework for multi-task and multi-domain scenarios.,,"[{Chern et~al.(2023)Chern, Chern, Chen, Yuan, Feng, Zhou, He, Neubig, Liu et~al.}]{chern2023factool} I~Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et~al. 2023. 
 Factool: Factuality detection in generative ai--a tool augmented framework for multi-task and multi-domain scenarios. 
 \emph{arXiv preprint arXiv:2307.13528}."
2404.19232,min2023factscore,"[{Min et~al.(2023)Min, Krishna, Lyu, Lewis, Yih, Koh, Iyyer, Zettlemoyer, and Hajishirzi}]{min2023factscore} Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang~Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.",Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.,Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.,,"[{Min et~al.(2023)Min, Krishna, Lyu, Lewis, Yih, Koh, Iyyer, Zettlemoyer, and Hajishirzi}]{min2023factscore} Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang~Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. 
 Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. 
 \emph{arXiv preprint arXiv:2305.14251}."
2404.19232,santurkar2023whose,"[{Santurkar et~al.(2023)Santurkar, Durmus, Ladhak, Lee, Liang, and Hashimoto}]{santurkar2023whose} Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023.",Whose opinions do language models reflect?,Whose opinions do language models reflect?,,"[{Santurkar et~al.(2023)Santurkar, Durmus, Ladhak, Lee, Liang, and Hashimoto}]{santurkar2023whose} Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. 
 Whose opinions do language models reflect? 
 \emph{arXiv preprint arXiv:2303.17548}."
2404.19232,shen2023chatgpt,"[{Shen et~al.(2023)Shen, Chen, Backes, and Zhang}]{shen2023chatgpt} Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023.",In chatgpt we trust? measuring and characterizing the reliability of chatgpt.,In chatgpt we trust? measuring and characterizing the reliability of chatgpt.,,"[{Shen et~al.(2023)Shen, Chen, Backes, and Zhang}]{shen2023chatgpt} Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. 
 In chatgpt we trust? measuring and characterizing the reliability of chatgpt. 
 \emph{arXiv preprint arXiv:2304.08979}."
2404.19232,zhong2023agieval,"[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval} Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.",Agieval: A human-centric benchmark for evaluating foundation models.,Agieval: A human-centric benchmark for evaluating foundation models.,,"[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval} Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. 
 Agieval: A human-centric benchmark for evaluating foundation models. 
 \emph{arXiv preprint arXiv:2304.06364}."
2405.00622,abdali2024securing,"[Abdali et~al.(2024)Abdali, Anarfi, Barberan, and He]{abdali2024securing} Abdali, S., Anarfi, R., Barberan, C., and He, J.","Securing large language models: Threats, vulnerabilities and responsible practices.","Securing large language models: Threats, vulnerabilities and responsible practices.",,"[Abdali et~al.(2024)Abdali, Anarfi, Barberan, and He]{abdali2024securing} Abdali, S., Anarfi, R., Barberan, C., and He, J. 
 Securing large language models: Threats, vulnerabilities and responsible practices. 
 \emph{arXiv preprint arXiv:2403.12503}, 2024."
2405.00622,azerbayev2023llemma,"[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma} Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.~D., McAleer, S., Jiang, A.~Q., Deng, J., Biderman, S., and Welleck, S.",Llemma: An open language model for mathematics.,Llemma: An open language model for mathematics.,,"[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma} Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.~D., McAleer, S., Jiang, A.~Q., Deng, J., Biderman, S., and Welleck, S. 
 Llemma: An open language model for mathematics. 
 \emph{arXiv preprint arXiv:2310.10631}, 2023."
2405.00622,qwen2023qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen2023qwen} Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen2023qwen} Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2405.00622,baichuan2023baichuan2,[Baichuan(2023)]{baichuan2023baichuan2} Baichuan.,Baichuan 2: Open large-scale language models.,Baichuan 2: Open large-scale language models.,,"[Baichuan(2023)]{baichuan2023baichuan2} Baichuan. 
 Baichuan 2: Open large-scale language models. 
 \emph{arXiv preprint arXiv:2309.10305}, 2023."
2405.00622,ban2023query,"[Ban et~al.(2023)Ban, Chen, Wang, and Chen]{ban2023query} Ban, T., Chen, L., Wang, X., and Chen, H.",From query tools to causal architects: Harnessing large language models for advanced causal discovery from data.,From query tools to causal architects: Harnessing large language models for advanced causal discovery from data.,,"[Ban et~al.(2023)Ban, Chen, Wang, and Chen]{ban2023query} Ban, T., Chen, L., Wang, X., and Chen, H. 
 From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. 
 \emph{arXiv preprint arXiv:2306.16902}, 2023."
2405.00622,bommasani2021opportunities,"[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities} Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.",On the opportunities and risks of foundation models.,On the opportunities and risks of foundation models.,,"[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities} Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al. 
 On the opportunities and risks of foundation models. 
 \emph{arXiv preprint arXiv:2108.07258}, 2021."
2405.00622,bubeck2023sparks,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}, 2023."
2405.00622,chen2023videollm,"[Chen et~al.(2023{\natexlab{a}})Chen, Zheng, Wang, Xu, Huang, Pan, Wang, Wang, Qiao, Lu, et~al.]{chen2023videollm} Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et~al.",Videollm: Modeling video sequence with large language models.,Videollm: Modeling video sequence with large language models.,,"[Chen et~al.(2023{\natexlab{a}})Chen, Zheng, Wang, Xu, Huang, Pan, Wang, Wang, Qiao, Lu, et~al.]{chen2023videollm} Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et~al. 
 Videollm: Modeling video sequence with large language models. 
 \emph{arXiv preprint arXiv:2305.13292}, 2023{\natexlab{a}}."
2405.00622,chen2021evaluating,"[Chen et~al.(2021{\natexlab{b}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating} Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[Chen et~al.(2021{\natexlab{b}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating} Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}, 2021{\natexlab{b}}."
2405.00622,chen2023learning,"[Chen et~al.(2023{\natexlab{b}})Chen, Ma, Song, Cao, Zhang, and Li]{chen2023learning} Chen, M., Ma, Y., Song, K., Cao, Y., Zhang, Y., and Li, D.",Learning to teach large language models logical reasoning.,Learning to teach large language models logical reasoning.,,"[Chen et~al.(2023{\natexlab{b}})Chen, Ma, Song, Cao, Zhang, and Li]{chen2023learning} Chen, M., Ma, Y., Song, K., Cao, Y., Zhang, Y., and Li, D. 
 Learning to teach large language models logical reasoning. 
 \emph{arXiv preprint arXiv:2310.09158}, 2023{\natexlab{b}}."
2405.00622,chen2024quantifying,"[Chen et~al.(2024)Chen, Cao, Zhang, and Lu]{chen2024quantifying} Chen, M., Cao, Y., Zhang, Y., and Lu, C.",Quantifying and mitigating unimodal biases in multimodal large language models: A causal perspective.,Quantifying and mitigating unimodal biases in multimodal large language models: A causal perspective.,,"[Chen et~al.(2024)Chen, Cao, Zhang, and Lu]{chen2024quantifying} Chen, M., Cao, Y., Zhang, Y., and Lu, C. 
 Quantifying and mitigating unimodal biases in multimodal large language models: A causal perspective. 
 \emph{arXiv preprint arXiv:2403.18346}, 2024."
2405.00622,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2405.00622,dao2023investigating,"[Dao \& Le(2023)Dao and Le]{dao2023investigating} Dao, X.-Q. and Le, N.-B.",Investigating the effectiveness of chatgpt in mathematical reasoning and problem solving: Evidence from the vietnamese national high school graduation examination.,Investigating the effectiveness of chatgpt in mathematical reasoning and problem solving: Evidence from the vietnamese national high school graduation examination.,,"[Dao \& Le(2023)Dao and Le]{dao2023investigating} Dao, X.-Q. and Le, N.-B. 
 Investigating the effectiveness of chatgpt in mathematical reasoning and problem solving: Evidence from the vietnamese national high school graduation examination. 
 \emph{arXiv preprint arXiv:2306.06331}, 2023."
2405.00622,das2024security,"[Das et~al.(2024)Das, Amini, and Wu]{das2024security} Das, B.~C., Amini, M.~H., and Wu, Y.",Security and privacy challenges of large language models: A survey.,Security and privacy challenges of large language models: A survey.,,"[Das et~al.(2024)Das, Amini, and Wu]{das2024security} Das, B.~C., Amini, M.~H., and Wu, Y. 
 Security and privacy challenges of large language models: A survey. 
 \emph{arXiv preprint arXiv:2402.00888}, 2024."
2405.00622,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2405.00622,dong2022survey,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z.",A survey on in-context learning.,A survey on in-context learning.,,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. 
 A survey on in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}, 2022."
2405.00622,gallegos2023bias,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} Gallegos, I.~O., Rossi, R.~A., Barrow, J., Tanjim, M.~M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N.~K.",Bias and fairness in large language models: A survey.,Bias and fairness in large language models: A survey.,,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} Gallegos, I.~O., Rossi, R.~A., Barrow, J., Tanjim, M.~M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N.~K. 
 Bias and fairness in large language models: A survey. 
 \emph{arXiv preprint arXiv:2309.00770}, 2023."
2405.00622,gan2023giellm,"[Gan et~al.(2023)Gan, Zhang, and Mori]{gan2023giellm} Gan, C., Zhang, Q., and Mori, T.",Giellm: Japanese general information extraction large language model utilizing mutual reinforcement effect.,Giellm: Japanese general information extraction large language model utilizing mutual reinforcement effect.,,"[Gan et~al.(2023)Gan, Zhang, and Mori]{gan2023giellm} Gan, C., Zhang, Q., and Mori, T. 
 Giellm: Japanese general information extraction large language model utilizing mutual reinforcement effect. 
 \emph{arXiv preprint arXiv:2311.06838}, 2023."
2405.00622,gehman2020realtoxicityprompts,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A.",Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A. 
 Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 
 \emph{arXiv preprint arXiv:2009.11462}, 2020."
2405.00622,ji2023benchmarking,"[Ji et~al.(2023)Ji, Ma, Li, and Wang]{ji2023benchmarking} Ji, Z., Ma, P., Li, Z., and Wang, S.",Benchmarking and explaining large language model-based code generation: A causality-centric approach.,Benchmarking and explaining large language model-based code generation: A causality-centric approach.,,"[Ji et~al.(2023)Ji, Ma, Li, and Wang]{ji2023benchmarking} Ji, Z., Ma, P., Li, Z., and Wang, S. 
 Benchmarking and explaining large language model-based code generation: A causality-centric approach. 
 \emph{arXiv preprint arXiv:2310.06680}, 2023."
2405.00622,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2405.00622,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2405.00622,kiciman2023causal,"[K{\i}c{\i}man et~al.(2023)K{\i}c{\i}man, Ness, Sharma, and Tan]{kiciman2023causal} K{\i}c{\i}man, E., Ness, R., Sharma, A., and Tan, C.",Causal reasoning and large language models: Opening a new frontier for causality.,Causal reasoning and large language models: Opening a new frontier for causality.,,"[K{\i}c{\i}man et~al.(2023)K{\i}c{\i}man, Ness, Sharma, and Tan]{kiciman2023causal} K{\i}c{\i}man, E., Ness, R., Sharma, A., and Tan, C. 
 Causal reasoning and large language models: Opening a new frontier for causality. 
 \emph{arXiv preprint arXiv:2305.00050}, 2023."
2405.00622,lee2023github,"[Lee et~al.(2023{\natexlab{b}})Lee, Kang, Yoon, and Yoo]{lee2023github} Lee, J.~Y., Kang, S., Yoon, J., and Yoo, S.",The github recent bugs dataset for evaluating llm-based debugging applications.,The github recent bugs dataset for evaluating llm-based debugging applications.,,"[Lee et~al.(2023{\natexlab{b}})Lee, Kang, Yoon, and Yoo]{lee2023github} Lee, J.~Y., Kang, S., Yoon, J., and Yoo, S. 
 The github recent bugs dataset for evaluating llm-based debugging applications. 
 \emph{arXiv preprint arXiv:2310.13229}, 2023{\natexlab{b}}."
2405.00622,li2023emotionprompt,"[Li et~al.(2023{\natexlab{a}})Li, Wang, Zhu, Zhang, Hou, Lian, and Xie]{li2023emotionprompt} Li, C., Wang, J., Zhu, K., Zhang, Y., Hou, W., Lian, J., and Xie, X.",Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus.,Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus.,,"[Li et~al.(2023{\natexlab{a}})Li, Wang, Zhu, Zhang, Hou, Lian, and Xie]{li2023emotionprompt} Li, C., Wang, J., Zhu, K., Zhang, Y., Hou, W., Lian, J., and Xie, X. 
 Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus. 
 \emph{arXiv preprint arXiv:2307.11760}, 2023{\natexlab{a}}."
2405.00622,li2023blip,"[Li et~al.(2023{\natexlab{d}})Li, Li, Savarese, and Hoi]{li2023blip} Li, J., Li, D., Savarese, S., and Hoi, S.",Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,,"[Li et~al.(2023{\natexlab{d}})Li, Li, Savarese, and Hoi]{li2023blip} Li, J., Li, D., Savarese, S., and Hoi, S. 
 Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. 
 \emph{arXiv preprint arXiv:2301.12597}, 2023{\natexlab{d}}."
2405.00622,li2023videochat,"[Li et~al.(2023{\natexlab{e}})Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao]{li2023videochat} Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y.",Videochat: Chat-centric video understanding.,Videochat: Chat-centric video understanding.,,"[Li et~al.(2023{\natexlab{e}})Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao]{li2023videochat} Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. 
 Videochat: Chat-centric video understanding. 
 \emph{arXiv preprint arXiv:2305.06355}, 2023{\natexlab{e}}."
2405.00622,li2023starcoder,"[Li et~al.(2023{\natexlab{f}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al.",Starcoder: may the source be with you!,Starcoder: may the source be with you!,,"[Li et~al.(2023{\natexlab{f}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al. 
 Starcoder: may the source be with you! 
 \emph{arXiv preprint arXiv:2305.06161}, 2023{\natexlab{f}}."
2405.00622,li2023survey,"[Li et~al.(2023{\natexlab{g}})Li, Du, Song, Wang, and Wang]{li2023survey} Li, Y., Du, M., Song, R., Wang, X., and Wang, Y.",A survey on fairness in large language models.,A survey on fairness in large language models.,,"[Li et~al.(2023{\natexlab{g}})Li, Du, Song, Wang, and Wang]{li2023survey} Li, Y., Du, M., Song, R., Wang, X., and Wang, Y. 
 A survey on fairness in large language models. 
 \emph{arXiv preprint arXiv:2308.10149}, 2023{\natexlab{g}}."
2405.00622,liang2022holistic,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic} Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et~al.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic} Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et~al. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}, 2022."
2405.00622,liu2023evaluating,"[Liu et~al.(2023{\natexlab{a}})Liu, Ning, Teng, Liu, Zhou, and Zhang]{liu2023evaluating} Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y.",Evaluating the logical reasoning ability of chatgpt and gpt-4.,Evaluating the logical reasoning ability of chatgpt and gpt-4.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Ning, Teng, Liu, Zhou, and Zhang]{liu2023evaluating} Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. 
 Evaluating the logical reasoning ability of chatgpt and gpt-4. 
 \emph{arXiv preprint arXiv:2304.03439}, 2023{\natexlab{a}}."
2405.00622,lu2024gpt,"[Lu et~al.(2024)Lu, Qian, Zheng, Fan, Gao, Zhang, Shao, Deng, Fu, Huang, et~al.]{lu2024gpt} Lu, C., Qian, C., Zheng, G., Fan, H., Gao, H., Zhang, J., Shao, J., Deng, J., Fu, J., Huang, K., et~al.","From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities.","From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities.",,"[Lu et~al.(2024)Lu, Qian, Zheng, Fan, Gao, Zhang, Shao, Deng, Fu, Huang, et~al.]{lu2024gpt} Lu, C., Qian, C., Zheng, G., Fan, H., Gao, H., Zhang, J., Shao, J., Deng, J., Fu, J., Huang, K., et~al. 
 From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities. 
 \emph{arXiv preprint arXiv:2401.15071}, 2024."
2405.00622,paranjape2023art,"[Paranjape et~al.(2023)Paranjape, Lundberg, Singh, Hajishirzi, Zettlemoyer, and Ribeiro]{paranjape2023art} Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M.~T.",Art: Automatic multi-step reasoning and tool-use for large language models.,Art: Automatic multi-step reasoning and tool-use for large language models.,,"[Paranjape et~al.(2023)Paranjape, Lundberg, Singh, Hajishirzi, Zettlemoyer, and Ribeiro]{paranjape2023art} Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M.~T. 
 Art: Automatic multi-step reasoning and tool-use for large language models. 
 \emph{arXiv preprint arXiv:2303.09014}, 2023."
2405.00622,parisi2022talm,"[Parisi et~al.(2022)Parisi, Zhao, and Fiedel]{parisi2022talm} Parisi, A., Zhao, Y., and Fiedel, N.",Talm: Tool augmented language models.,Talm: Tool augmented language models.,,"[Parisi et~al.(2022)Parisi, Zhao, and Fiedel]{parisi2022talm} Parisi, A., Zhao, Y., and Fiedel, N. 
 Talm: Tool augmented language models. 
 \emph{arXiv preprint arXiv:2205.12255}, 2022."
2405.00622,roziere2023code,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code} Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.",Code llama: Open foundation models for code.,Code llama: Open foundation models for code.,,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code} Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al. 
 Code llama: Open foundation models for code. 
 \emph{arXiv preprint arXiv:2308.12950}, 2023."
2405.00622,shimoni2018benchmarking,"[Shimoni et~al.(2018)Shimoni, Yanover, Karavani, and Goldschmnidt]{shimoni2018benchmarking} Shimoni, Y., Yanover, C., Karavani, E., and Goldschmnidt, Y.",Benchmarking framework for performance-evaluation of causal inference analysis.,Benchmarking framework for performance-evaluation of causal inference analysis.,,"[Shimoni et~al.(2018)Shimoni, Yanover, Karavani, and Goldschmnidt]{shimoni2018benchmarking} Shimoni, Y., Yanover, C., Karavani, E., and Goldschmnidt, Y. 
 Benchmarking framework for performance-evaluation of causal inference analysis. 
 \emph{arXiv preprint arXiv:1802.05046}, 2018."
2405.00622,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2405.00622,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2405.00622,tu2023causal,"[Tu et~al.(2023)Tu, Ma, and Zhang]{tu2023causal} Tu, R., Ma, C., and Zhang, C.",Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis.,Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis.,,"[Tu et~al.(2023)Tu, Ma, and Zhang]{tu2023causal} Tu, R., Ma, C., and Zhang, C. 
 Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. 
 \emph{arXiv preprint arXiv:2301.13819}, 2023."
2405.00622,tufano2024autodev,"[Tufano et~al.(2024)Tufano, Agarwal, Jang, Moghaddam, and Sundaresan]{tufano2024autodev} Tufano, M., Agarwal, A., Jang, J., Moghaddam, R.~Z., and Sundaresan, N.",Autodev: Automated ai-driven development.,Autodev: Automated ai-driven development.,,"[Tufano et~al.(2024)Tufano, Agarwal, Jang, Moghaddam, and Sundaresan]{tufano2024autodev} Tufano, M., Agarwal, A., Jang, J., Moghaddam, R.~Z., and Sundaresan, N. 
 Autodev: Automated ai-driven development. 
 \emph{arXiv preprint arXiv:2403.08299}, 2024."
2405.00622,wang2020infobert,"[Wang et~al.(2020{\natexlab{a}})Wang, Wang, Cheng, Gan, Jia, Li, and Liu]{wang2020infobert} Wang, B., Wang, S., Cheng, Y., Gan, Z., Jia, R., Li, B., and Liu, J.",Infobert: Improving robustness of language models from an information theoretic perspective.,Infobert: Improving robustness of language models from an information theoretic perspective.,,"[Wang et~al.(2020{\natexlab{a}})Wang, Wang, Cheng, Gan, Jia, Li, and Liu]{wang2020infobert} Wang, B., Wang, S., Cheng, Y., Gan, Z., Jia, R., Li, B., and Liu, J. 
 Infobert: Improving robustness of language models from an information theoretic perspective. 
 \emph{arXiv preprint arXiv:2010.02329}, 2020{\natexlab{a}}."
2405.00622,wei2023cmath,"[Wei et~al.(2023{\natexlab{a}})Wei, Luan, Liu, Dong, and Wang]{wei2023cmath} Wei, T., Luan, J., Liu, W., Dong, S., and Wang, B.",Cmath: can your language model pass chinese elementary school math test?,Cmath: can your language model pass chinese elementary school math test?,,"[Wei et~al.(2023{\natexlab{a}})Wei, Luan, Liu, Dong, and Wang]{wei2023cmath} Wei, T., Luan, J., Liu, W., Dong, S., and Wang, B. 
 Cmath: can your language model pass chinese elementary school math test? 
 \emph{arXiv preprint arXiv:2306.16636}, 2023{\natexlab{a}}."
2405.00622,wu2023empirical,"[Wu et~al.(2023{\natexlab{a}})Wu, Jia, Zhang, Wu, Li, Zhu, Wang, Lee, Peng, and Wang]{wu2023empirical} Wu, Y., Jia, F., Zhang, S., Wu, Q., Li, H., Zhu, E., Wang, Y., Lee, Y.~T., Peng, R., and Wang, C.",An empirical study on challenging math problem solving with gpt-4.,An empirical study on challenging math problem solving with gpt-4.,,"[Wu et~al.(2023{\natexlab{a}})Wu, Jia, Zhang, Wu, Li, Zhu, Wang, Lee, Peng, and Wang]{wu2023empirical} Wu, Y., Jia, F., Zhang, S., Wu, Q., Li, H., Zhu, E., Wang, Y., Lee, Y.~T., Peng, R., and Wang, C. 
 An empirical study on challenging math problem solving with gpt-4. 
 \emph{arXiv preprint arXiv:2306.01337}, 2023{\natexlab{a}}."
2405.00622,xu2023cvalues,"[Xu et~al.(2023{\natexlab{a}})Xu, Liu, Yan, Xu, Si, Zhou, Yi, Gao, Sang, Zhang, et~al.]{xu2023cvalues} Xu, G., Liu, J., Yan, M., Xu, H., Si, J., Zhou, Z., Yi, P., Gao, X., Sang, J., Zhang, R., et~al.",Cvalues: Measuring the values of chinese large language models from safety to responsibility.,Cvalues: Measuring the values of chinese large language models from safety to responsibility.,,"[Xu et~al.(2023{\natexlab{a}})Xu, Liu, Yan, Xu, Si, Zhou, Yi, Gao, Sang, Zhang, et~al.]{xu2023cvalues} Xu, G., Liu, J., Yan, M., Xu, H., Si, J., Zhou, Z., Yi, P., Gao, X., Sang, J., Zhang, R., et~al. 
 Cvalues: Measuring the values of chinese large language models from safety to responsibility. 
 \emph{arXiv preprint arXiv:2307.09705}, 2023{\natexlab{a}}."
2405.00622,yuan2023evaluating,"[Yuan et~al.(2023{\natexlab{a}})Yuan, Liu, Zi, Liu, Peng, and Lou]{yuan2023evaluating} Yuan, Z., Liu, J., Zi, Q., Liu, M., Peng, X., and Lou, Y.",Evaluating instruction-tuned large language models on code comprehension and generation.,Evaluating instruction-tuned large language models on code comprehension and generation.,,"[Yuan et~al.(2023{\natexlab{a}})Yuan, Liu, Zi, Liu, Peng, and Lou]{yuan2023evaluating} Yuan, Z., Liu, J., Zi, Q., Liu, M., Peng, X., and Lou, Y. 
 Evaluating instruction-tuned large language models on code comprehension and generation. 
 \emph{arXiv preprint arXiv:2308.01240}, 2023{\natexlab{a}}."
2405.00622,yuan2023well,"[Yuan et~al.(2023{\natexlab{b}})Yuan, Yuan, Tan, Wang, and Huang]{yuan2023well} Yuan, Z., Yuan, H., Tan, C., Wang, W., and Huang, S.",How well do large language models perform in arithmetic tasks?,How well do large language models perform in arithmetic tasks?,,"[Yuan et~al.(2023{\natexlab{b}})Yuan, Yuan, Tan, Wang, and Huang]{yuan2023well} Yuan, Z., Yuan, H., Tan, C., Wang, W., and Huang, S. 
 How well do large language models perform in arithmetic tasks? 
 \emph{arXiv preprint arXiv:2304.02015}, 2023{\natexlab{b}}."
2405.00622,zevcevic2023causal,"[Ze{\v{c}}evi{\'c} et~al.(2023)Ze{\v{c}}evi{\'c}, Willig, Dhami, and Kersting]{zevcevic2023causal} Ze{\v{c}}evi{\'c}, M., Willig, M., Dhami, D.~S., and Kersting, K.",Causal parrots: Large language models may talk causality but are not causal.,Causal parrots: Large language models may talk causality but are not causal.,,"[Ze{\v{c}}evi{\'c} et~al.(2023)Ze{\v{c}}evi{\'c}, Willig, Dhami, and Kersting]{zevcevic2023causal} Ze{\v{c}}evi{\'c}, M., Willig, M., Dhami, D.~S., and Kersting, K. 
 Causal parrots: Large language models may talk causality but are not causal. 
 \emph{arXiv preprint arXiv:2308.13067}, 2023."
2405.00622,zhang2023understanding,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Bauer, Bennett, Gao, Gong, Hilmkil, Jennings, Ma, Minka, Pawlowski, et~al.]{zhang2023understanding} Zhang, C., Bauer, S., Bennett, P., Gao, J., Gong, W., Hilmkil, A., Jennings, J., Ma, C., Minka, T., Pawlowski, N., et~al.",Understanding causality with large language models: Feasibility and opportunities.,Understanding causality with large language models: Feasibility and opportunities.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Bauer, Bennett, Gao, Gong, Hilmkil, Jennings, Ma, Minka, Pawlowski, et~al.]{zhang2023understanding} Zhang, C., Bauer, S., Bennett, P., Gao, J., Gong, W., Hilmkil, A., Jennings, J., Ma, C., Minka, T., Pawlowski, N., et~al. 
 Understanding causality with large language models: Feasibility and opportunities. 
 \emph{arXiv preprint arXiv:2304.05524}, 2023{\natexlab{a}}."
2405.00622,zhang2023lora,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Shi, Chu, and Li]{zhang2023lora} Zhang, L., Zhang, L., Shi, S., Chu, X., and Li, B.",Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning.,Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning.,,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Shi, Chu, and Li]{zhang2023lora} Zhang, L., Zhang, L., Shi, S., Chu, X., and Li, B. 
 Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. 
 \emph{arXiv preprint arXiv:2308.03303}, 2023{\natexlab{c}}."
2405.00622,zhang2023safetybench,"[Zhang et~al.(2023{\natexlab{d}})Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench} Zhang, Z., Lei, L., Wu, L., Sun, R., Huang, Y., Long, C., Liu, X., Lei, X., Tang, J., and Huang, M.",Safetybench: Evaluating the safety of large language models with multiple choice questions.,Safetybench: Evaluating the safety of large language models with multiple choice questions.,,"[Zhang et~al.(2023{\natexlab{d}})Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench} Zhang, Z., Lei, L., Wu, L., Sun, R., Huang, Y., Long, C., Liu, X., Lei, X., Tang, J., and Huang, M. 
 Safetybench: Evaluating the safety of large language models with multiple choice questions. 
 \emph{arXiv preprint arXiv:2309.07045}, 2023{\natexlab{d}}."
2405.00622,zhao2023survey,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et~al.",A survey of large language models.,A survey of large language models.,,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et~al. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}, 2023."
2405.00622,zheng2023progressive,"[Zheng et~al.(2023{\natexlab{a}})Zheng, Liu, Xie, Li, and Li]{zheng2023progressive} Zheng, C., Liu, Z., Xie, E., Li, Z., and Li, Y.",Progressive-hint prompting improves reasoning in large language models.,Progressive-hint prompting improves reasoning in large language models.,,"[Zheng et~al.(2023{\natexlab{a}})Zheng, Liu, Xie, Li, and Li]{zheng2023progressive} Zheng, C., Liu, Z., Xie, E., Li, Z., and Li, Y. 
 Progressive-hint prompting improves reasoning in large language models. 
 \emph{arXiv preprint arXiv:2304.09797}, 2023{\natexlab{a}}."
2405.00622,zhong2023study,"[Zhong \& Wang(2023)Zhong and Wang]{zhong2023study} Zhong, L. and Wang, Z.",A study on robustness and reliability of large language model code generation.,A study on robustness and reliability of large language model code generation.,,"[Zhong \& Wang(2023)Zhong and Wang]{zhong2023study} Zhong, L. and Wang, Z. 
 A study on robustness and reliability of large language model code generation. 
 \emph{arXiv preprint arXiv:2308.10335}, 2023."
2405.00622,zhou2022least,"[Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, et~al.]{zhou2022least} Zhou, D., Sch{\""a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et~al.",Least-to-most prompting enables complex reasoning in large language models.,Least-to-most prompting enables complex reasoning in large language models.,,"[Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, et~al.]{zhou2022least} Zhou, D., Sch{\""a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et~al. 
 Least-to-most prompting enables complex reasoning in large language models. 
 \emph{arXiv preprint arXiv:2205.10625}, 2022."
2405.00622,zhu2023promptbench,"[Zhu et~al.(2023{\natexlab{a}})Zhu, Wang, Zhou, Wang, Chen, Wang, Yang, Ye, Gong, Zhang, et~al.]{zhu2023promptbench} Zhu, K., Wang, J., Zhou, J., Wang, Z., Chen, H., Wang, Y., Yang, L., Ye, W., Gong, N.~Z., Zhang, Y., et~al.",Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.,Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.,,"[Zhu et~al.(2023{\natexlab{a}})Zhu, Wang, Zhou, Wang, Chen, Wang, Yang, Ye, Gong, Zhang, et~al.]{zhu2023promptbench} Zhu, K., Wang, J., Zhou, J., Wang, Z., Chen, H., Wang, Y., Yang, L., Ye, W., Gong, N.~Z., Zhang, Y., et~al. 
 Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. 
 \emph{arXiv preprint arXiv:2306.04528}, 2023{\natexlab{a}}."
2405.00622,zhu2023multilingual,"[Zhu et~al.(2023{\natexlab{b}})Zhu, Liu, Dong, Xu, Kong, Chen, Li, and Huang]{zhu2023multilingual} Zhu, W., Liu, H., Dong, Q., Xu, J., Kong, L., Chen, J., Li, L., and Huang, S.",Multilingual machine translation with large language models: Empirical results and analysis.,Multilingual machine translation with large language models: Empirical results and analysis.,,"[Zhu et~al.(2023{\natexlab{b}})Zhu, Liu, Dong, Xu, Kong, Chen, Li, and Huang]{zhu2023multilingual} Zhu, W., Liu, H., Dong, Q., Xu, J., Kong, L., Chen, J., Li, L., and Huang, S. 
 Multilingual machine translation with large language models: Empirical results and analysis. 
 \emph{arXiv preprint arXiv:2304.04675}, 2023{\natexlab{b}}."
2405.02266,chowdhery2022palm,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.",Palm: Scaling language modeling with pathways.,Palm: Scaling language modeling with pathways.,,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 
 Palm: Scaling language modeling with pathways. 
 \emph{arXiv preprint arXiv:2204.02311}, 2022."
2405.02266,colombo2023transductive,"[Colombo et~al.(2023)Colombo, Pellegrain, Boudiaf, Storchan, Tami, Ayed, Hudelot, and Piantanida]{colombo2023transductive} Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Victor Storchan, Myriam Tami, Ismail~Ben Ayed, Celine Hudelot, and Pablo Piantanida.",Transductive learning for textual few-shot classification in api-based embedding models.,Transductive learning for textual few-shot classification in api-based embedding models.,,"[Colombo et~al.(2023)Colombo, Pellegrain, Boudiaf, Storchan, Tami, Ayed, Hudelot, and Piantanida]{colombo2023transductive} Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Victor Storchan, Myriam Tami, Ismail~Ben Ayed, Celine Hudelot, and Pablo Piantanida. 
 Transductive learning for textual few-shot classification in api-based embedding models. 
 \emph{arXiv preprint arXiv:2310.13998}, 2023."
2405.02266,variational,"[Derakhshani et~al.(2022)Derakhshani, Sanchez, Bulat, da~Costa, Snoek, Tzimiropoulos, and Martinez]{variational} Mohammad~Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme~Turrisi da Costa, Cees~GM Snoek, Georgios Tzimiropoulos, and Brais Martinez.",Variational prompt tuning improves generalization of vision-language models.,Variational prompt tuning improves generalization of vision-language models.,,"[Derakhshani et~al.(2022)Derakhshani, Sanchez, Bulat, da~Costa, Snoek, Tzimiropoulos, and Martinez]{variational} Mohammad~Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor Guilherme~Turrisi da Costa, Cees~GM Snoek, Georgios Tzimiropoulos, and Brais Martinez. 
 Variational prompt tuning improves generalization of vision-language models. 
 \emph{arXiv preprint arXiv:2210.02390}, 2022."
2405.02266,upl,"[Huang et~al.(2022)Huang, Chu, and Wei]{upl} Tony Huang, Jack Chu, and Fangyun Wei.",Unsupervised prompt learning for vision-language models.,Unsupervised prompt learning for vision-language models.,,"[Huang et~al.(2022)Huang, Chu, and Wei]{upl} Tony Huang, Jack Chu, and Fangyun Wei. 
 Unsupervised prompt learning for vision-language models. 
 \emph{arXiv preprint arXiv:2204.03649}, 2022."
2405.02266,prompt_tuning,"[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{prompt_tuning} Brian Lester, Rami Al-Rfou, and Noah Constant.",The power of scale for parameter-efficient prompt tuning.,The power of scale for parameter-efficient prompt tuning.,,"[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{prompt_tuning} Brian Lester, Rami Al-Rfou, and Noah Constant. 
 The power of scale for parameter-efficient prompt tuning. 
 \emph{arXiv preprint arXiv:2104.08691}, 2021."
2405.02266,liang2023comprehensive,"[Liang et~al.(2023)Liang, He, and Tan]{liang2023comprehensive} Jian Liang, Ran He, and Tieniu Tan.",A comprehensive survey on test-time adaptation under distribution shifts.,A comprehensive survey on test-time adaptation under distribution shifts.,,"[Liang et~al.(2023)Liang, He, and Tan]{liang2023comprehensive} Jian Liang, Ran He, and Tieniu Tan. 
 A comprehensive survey on test-time adaptation under distribution shifts. 
 \emph{arXiv preprint arXiv:2303.15361}, 2023."
2405.02266,aircraft,"[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{aircraft} Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.",Fine-grained visual classification of aircraft.,Fine-grained visual classification of aircraft.,,"[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{aircraft} Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. 
 Fine-grained visual classification of aircraft. 
 \emph{arXiv preprint arXiv:1306.5151}, 2013."
2405.02266,ucf101,"[Soomro et~al.(2012)Soomro, Zamir, and Shah]{ucf101} Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.",Ucf101: A dataset of 101 human actions classes from videos in the wild.,Ucf101: A dataset of 101 human actions classes from videos in the wild.,,"[Soomro et~al.(2012)Soomro, Zamir, and Shah]{ucf101} Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah. 
 Ucf101: A dataset of 101 human actions classes from videos in the wild. 
 \emph{arXiv preprint arXiv:1212.0402}, 2012."
2405.02266,filip,"[Yao et~al.(2021)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and Xu]{filip} Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.",Filip: Fine-grained interactive language-image pre-training.,Filip: Fine-grained interactive language-image pre-training.,,"[Yao et~al.(2021)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and Xu]{filip} Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 
 Filip: Fine-grained interactive language-image pre-training. 
 \emph{arXiv preprint arXiv:2111.07783}, 2021."
2405.02266,vlm_review,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Huang, Jin, and Lu]{vlm_review} Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.",Vision-language models for vision tasks: A survey.,Vision-language models for vision tasks: A survey.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Huang, Jin, and Lu]{vlm_review} Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 
 Vision-language models for vision tasks: A survey. 
 \emph{arXiv preprint arXiv:2304.00685}, 2023{\natexlab{a}}."
2405.03,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2405.03,ahmed2024meta,"[{Ahmed et~al.(2024)Ahmed, Niloy, Raychaudhuri, Oymak, and Roy-Chowdhury}]{ahmed2024meta} Sk~Miraj Ahmed, Fahim~Faisal Niloy, Dripta~S Raychaudhuri, Samet Oymak, and Amit~K Roy-Chowdhury. 2024.",Meta: Multi-source test time adaptation.,Meta: Multi-source test time adaptation.,,"[{Ahmed et~al.(2024)Ahmed, Niloy, Raychaudhuri, Oymak, and Roy-Chowdhury}]{ahmed2024meta} Sk~Miraj Ahmed, Fahim~Faisal Niloy, Dripta~S Raychaudhuri, Samet Oymak, and Amit~K Roy-Chowdhury. 2024. 
 Meta: Multi-source test time adaptation. 
 \emph{arXiv preprint arXiv:2401.02561}."
2405.03,beltagy2020longformer,"[{Beltagy et~al.(2020)Beltagy, Peters, and Cohan}]{beltagy2020longformer} Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 2020.",Longformer: The long-document transformer.,Longformer: The long-document transformer.,,"[{Beltagy et~al.(2020)Beltagy, Peters, and Cohan}]{beltagy2020longformer} Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 2020. 
 Longformer: The long-document transformer. 
 \emph{arXiv preprint arXiv:2004.05150}."
2405.03,bolton2024biomedlm,"[{Bolton et~al.(2024{\natexlab{a}})Bolton, Venigalla, Yasunaga, Hall, Xiong, Lee, Daneshjou, Frankle, Liang, Carbin et~al.}]{bolton2024biomedlm} Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et~al. 2024{\natexlab{a}}.",Biomedlm: A 2.7 b parameter language model trained on biomedical text.,Biomedlm: A 2.7 b parameter language model trained on biomedical text.,,"[{Bolton et~al.(2024{\natexlab{a}})Bolton, Venigalla, Yasunaga, Hall, Xiong, Lee, Daneshjou, Frankle, Liang, Carbin et~al.}]{bolton2024biomedlm} Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et~al. 2024{\natexlab{a}}. 
 Biomedlm: A 2.7 b parameter language model trained on biomedical text. 
 \emph{arXiv preprint arXiv:2403.18421}."
2405.03,bolton2024assessing,"[{Bolton et~al.(2024{\natexlab{b}})Bolton, Xiong, Muralidharan, Schamroth, Muralidharan, Manning, and Daneshjou}]{bolton2024assessing} Elliot Bolton, Betty Xiong, Vijaytha Muralidharan, Joel Schamroth, Vivek Muralidharan, Christopher~D Manning, and Roxana Daneshjou. 2024{\natexlab{b}}.",Assessing the potential of mid-sized language models for clinical qa.,Assessing the potential of mid-sized language models for clinical qa.,,"[{Bolton et~al.(2024{\natexlab{b}})Bolton, Xiong, Muralidharan, Schamroth, Muralidharan, Manning, and Daneshjou}]{bolton2024assessing} Elliot Bolton, Betty Xiong, Vijaytha Muralidharan, Joel Schamroth, Vivek Muralidharan, Christopher~D Manning, and Roxana Daneshjou. 2024{\natexlab{b}}. 
 Assessing the potential of mid-sized language models for clinical qa. 
 \emph{arXiv preprint arXiv:2404.15894}."
2405.03,chen2023large,"[{Chen et~al.(2023{\natexlab{a}})Chen, Du, Hu, Keloth, Peng, Raja, Zhang, Lu, and Xu}]{chen2023large} Qingyu Chen, Jingcheng Du, Yan Hu, Vipina~Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, and Hua Xu. 2023{\natexlab{a}}.","Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.","Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.",,"[{Chen et~al.(2023{\natexlab{a}})Chen, Du, Hu, Keloth, Peng, Raja, Zhang, Lu, and Xu}]{chen2023large} Qingyu Chen, Jingcheng Du, Yan Hu, Vipina~Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, and Hua Xu. 2023{\natexlab{a}}. 
 Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. 
 \emph{arXiv preprint arXiv:2305.16326}."
2405.03,chen2023meditron,"[{Chen et~al.(2023{\natexlab{b}})Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{chen2023meditron} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023{\natexlab{b}}.",Meditron-70b: Scaling medical pretraining for large language models.,Meditron-70b: Scaling medical pretraining for large language models.,,"[{Chen et~al.(2023{\natexlab{b}})Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{chen2023meditron} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023{\natexlab{b}}. 
 Meditron-70b: Scaling medical pretraining for large language models. 
 \emph{arXiv preprint arXiv:2311.16079}."
2405.03,frisoni2024generate,"[{Frisoni et~al.(2024)Frisoni, Cocchieri, Presepi, Moro, and Meng}]{frisoni2024generate} Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro, and Zaiqiao Meng. 2024.",To generate or to retrieve? on the effectiveness of artificial contexts for medical open-domain question answering.,To generate or to retrieve? on the effectiveness of artificial contexts for medical open-domain question answering.,,"[{Frisoni et~al.(2024)Frisoni, Cocchieri, Presepi, Moro, and Meng}]{frisoni2024generate} Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro, and Zaiqiao Meng. 2024. 
 To generate or to retrieve? on the effectiveness of artificial contexts for medical open-domain question answering. 
 \emph{arXiv preprint arXiv:2403.01924}."
2405.03,gema2023parameter,"[{Gema et~al.(2023)Gema, Daines, Minervini, and Alex}]{gema2023parameter} Aryo Gema, Luke Daines, Pasquale Minervini, and Beatrice Alex. 2023.",Parameter-efficient fine-tuning of llama for the clinical domain.,Parameter-efficient fine-tuning of llama for the clinical domain.,,"[{Gema et~al.(2023)Gema, Daines, Minervini, and Alex}]{gema2023parameter} Aryo Gema, Luke Daines, Pasquale Minervini, and Beatrice Alex. 2023. 
 Parameter-efficient fine-tuning of llama for the clinical domain. 
 \emph{arXiv preprint arXiv:2307.03042}."
2405.03,han2023medalpaca,"[{Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem}]{han2023medalpaca} Tianyu Han, Lisa~C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L{\""o}ser, Daniel Truhn, and Keno~K Bressem. 2023.",Medalpaca--an open-source collection of medical conversational ai models and training data.,Medalpaca--an open-source collection of medical conversational ai models and training data.,,"[{Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem}]{han2023medalpaca} Tianyu Han, Lisa~C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L{\""o}ser, Daniel Truhn, and Keno~K Bressem. 2023. 
 Medalpaca--an open-source collection of medical conversational ai models and training data. 
 \emph{arXiv preprint arXiv:2304.08247}."
2405.03,hendrycks2020measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2405.03,huang2023k,"[{Huang et~al.(2023)Huang, Liu, Zhong, Shi, and Lee}]{huang2023k} Yangsibo Huang, Daogao Liu, Zexuan Zhong, Weijia Shi, and Yin~Tat Lee. 2023.",$ k $ nn-adapter: Efficient domain adaptation for black-box language models.,$ k $ nn-adapter: Efficient domain adaptation for black-box language models.,,"[{Huang et~al.(2023)Huang, Liu, Zhong, Shi, and Lee}]{huang2023k} Yangsibo Huang, Daogao Liu, Zexuan Zhong, Weijia Shi, and Yin~Tat Lee. 2023. 
 $ k $ nn-adapter: Efficient domain adaptation for black-box language models. 
 \emph{arXiv preprint arXiv:2302.10879}."
2405.03,jeong2024improving,"[{Jeong et~al.(2024)Jeong, Sohn, Sung, and Kang}]{jeong2024improving} Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024.",Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models.,Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models.,,"[{Jeong et~al.(2024)Jeong, Sohn, Sung, and Kang}]{jeong2024improving} Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024. 
 Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models. 
 \emph{arXiv preprint arXiv:2401.15269}."
2405.03,jin2019pubmedqa,"[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu. 2019.",Pubmedqa: A dataset for biomedical research question answering.,Pubmedqa: A dataset for biomedical research question answering.,,"[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu. 2019. 
 Pubmedqa: A dataset for biomedical research question answering. 
 \emph{arXiv preprint arXiv:1909.06146}."
2405.03,karmanov2024efficient,"[{Karmanov et~al.(2024)Karmanov, Guan, Lu, Saddik, and Xing}]{karmanov2024efficient} Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb~El Saddik, and Eric Xing. 2024.",Efficient test-time adaptation of vision-language models.,Efficient test-time adaptation of vision-language models.,,"[{Karmanov et~al.(2024)Karmanov, Guan, Lu, Saddik, and Xing}]{karmanov2024efficient} Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb~El Saddik, and Eric Xing. 2024. 
 Efficient test-time adaptation of vision-language models. 
 \emph{arXiv preprint arXiv:2403.18293}."
2405.03,labrak2024biomistral,"[{Labrak et~al.(2024)Labrak, Bazoge, Morin, Gourraud, Rouvier, and Dufour}]{labrak2024biomistral} Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024.",Biomistral: A collection of open-source pretrained large language models for medical domains.,Biomistral: A collection of open-source pretrained large language models for medical domains.,,"[{Labrak et~al.(2024)Labrak, Bazoge, Morin, Gourraud, Rouvier, and Dufour}]{labrak2024biomistral} Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. 
 Biomistral: A collection of open-source pretrained large language models for medical domains. 
 \emph{arXiv preprint arXiv:2402.10373}."
2405.03,li2022clinical,"[{Li et~al.(2022)Li, Wehbe, Ahmad, Wang, and Luo}]{li2022clinical} Yikuan Li, Ramsey~M Wehbe, Faraz~S Ahmad, Hanyin Wang, and Yuan Luo. 2022.",Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences.,Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences.,,"[{Li et~al.(2022)Li, Wehbe, Ahmad, Wang, and Luo}]{li2022clinical} Yikuan Li, Ramsey~M Wehbe, Faraz~S Ahmad, Hanyin Wang, and Yuan Luo. 2022. 
 Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences. 
 \emph{arXiv preprint arXiv:2201.11838}."
2405.03,li2023textbooks,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2405.03,liang2023comprehensive,"[{Liang et~al.(2023)Liang, He, and Tan}]{liang2023comprehensive} Jian Liang, Ran He, and Tieniu Tan. 2023.",A comprehensive survey on test-time adaptation under distribution shifts.,A comprehensive survey on test-time adaptation under distribution shifts.,,"[{Liang et~al.(2023)Liang, He, and Tan}]{liang2023comprehensive} Jian Liang, Ran He, and Tieniu Tan. 2023. 
 A comprehensive survey on test-time adaptation under distribution shifts. 
 \emph{arXiv preprint arXiv:2303.15361}."
2405.03,liu2024tuning,"[{Liu et~al.(2024)Liu, Han, Wang, Tsvetkov, Choi, and Smith}]{liu2024tuning} Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah~A Smith. 2024.",Tuning language models by proxy.,Tuning language models by proxy.,,"[{Liu et~al.(2024)Liu, Han, Wang, Tsvetkov, Choi, and Smith}]{liu2024tuning} Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah~A Smith. 2024. 
 Tuning language models by proxy. 
 \emph{arXiv preprint arXiv:2401.08565}."
2405.03,luo2023biomedgpt,"[{Luo et~al.(2023)Luo, Zhang, Fan, Yang, Wu, Qiao, and Nie}]{luo2023biomedgpt} Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu~Qiao, and Zaiqing Nie. 2023.",Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine.,Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine.,,"[{Luo et~al.(2023)Luo, Zhang, Fan, Yang, Wu, Qiao, and Nie}]{luo2023biomedgpt} Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu~Qiao, and Zaiqing Nie. 2023. 
 Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. 
 \emph{arXiv preprint arXiv:2308.09442}."
2405.03,maharjan2024openmedlm,"[{Maharjan et~al.(2024)Maharjan, Garikipati, Singh, Cyrus, Sharma, Ciobanu, Barnes, Thapa, Mao, and Das}]{maharjan2024openmedlm} Jenish Maharjan, Anurag Garikipati, Navan~Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, and Ritankar Das. 2024.",Openmedlm: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models.,Openmedlm: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models.,,"[{Maharjan et~al.(2024)Maharjan, Garikipati, Singh, Cyrus, Sharma, Ciobanu, Barnes, Thapa, Mao, and Das}]{maharjan2024openmedlm} Jenish Maharjan, Anurag Garikipati, Navan~Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, and Ritankar Das. 2024. 
 Openmedlm: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models. 
 \emph{arXiv preprint arXiv:2402.19371}."
2405.03,nori2023capabilities,"[{Nori et~al.(2023{\natexlab{a}})Nori, King, McKinney, Carignan, and Horvitz}]{nori2023capabilities} Harsha Nori, Nicholas King, Scott~Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023{\natexlab{a}}.",Capabilities of gpt-4 on medical challenge problems.,Capabilities of gpt-4 on medical challenge problems.,,"[{Nori et~al.(2023{\natexlab{a}})Nori, King, McKinney, Carignan, and Horvitz}]{nori2023capabilities} Harsha Nori, Nicholas King, Scott~Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023{\natexlab{a}}. 
 Capabilities of gpt-4 on medical challenge problems. 
 \emph{arXiv preprint arXiv:2303.13375}."
2405.03,nori2023can,"[{Nori et~al.(2023{\natexlab{b}})Nori, Lee, Zhang, Carignan, Edgar, Fusi, King, Larson, Li, Liu et~al.}]{nori2023can} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023{\natexlab{b}}.",Can generalist foundation models outcompete special-purpose tuning? case study in medicine.,Can generalist foundation models outcompete special-purpose tuning? case study in medicine.,,"[{Nori et~al.(2023{\natexlab{b}})Nori, Lee, Zhang, Carignan, Edgar, Fusi, King, Larson, Li, Liu et~al.}]{nori2023can} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023{\natexlab{b}}. 
 Can generalist foundation models outcompete special-purpose tuning? case study in medicine. 
 \emph{arXiv preprint arXiv:2311.16452}."
2405.03,Saab2024CapabilitiesOG,"[{Saab et~al.(2024)Saab, Tu, Weng, and et~al.}]{Saab2024CapabilitiesOG} Khaled Saab, Tao Tu, Wei-Hung Weng, and et~al. 2024.",Capabilities of gemini models in medicine.,Capabilities of gemini models in medicine.,,"[{Saab et~al.(2024)Saab, Tu, Weng, and et~al.}]{Saab2024CapabilitiesOG} Khaled Saab, Tao Tu, Wei-Hung Weng, and et~al. 2024. 
 Capabilities of gemini models in medicine. 
 \emph{arXiv preprint arXiv:2404.18416}."
2405.03,shi2024ehragent,"[{Shi et~al.(2024{\natexlab{b}})Shi, Xu, Zhuang, Yu, Zhang, Wu, Zhu, Ho, Yang, and Wang}]{shi2024ehragent} Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May~D Wang. 2024{\natexlab{b}}.",Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records.,Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records.,,"[{Shi et~al.(2024{\natexlab{b}})Shi, Xu, Zhuang, Yu, Zhang, Wu, Zhu, Ho, Yang, and Wang}]{shi2024ehragent} Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May~D Wang. 2024{\natexlab{b}}. 
 Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records. 
 \emph{arXiv preprint arXiv:2401.07128}."
2405.03,sun2024bbox,"[{Sun et~al.(2024)Sun, Zhuang, Wei, Zhang, and Dai}]{sun2024bbox} Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, and Bo~Dai. 2024.",Bbox-adapter: Lightweight adapting for black-box large language models.,Bbox-adapter: Lightweight adapting for black-box large language models.,,"[{Sun et~al.(2024)Sun, Zhuang, Wei, Zhang, and Dai}]{sun2024bbox} Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, and Bo~Dai. 2024. 
 Bbox-adapter: Lightweight adapting for black-box large language models. 
 \emph{arXiv preprint arXiv:2402.08219}."
2405.03,tang2023medagents,"[{Tang et~al.(2023)Tang, Zou, Zhang, Zhao, Zhang, Cohan, and Gerstein}]{tang2023medagents} Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023.",Medagents: Large language models as collaborators for zero-shot medical reasoning.,Medagents: Large language models as collaborators for zero-shot medical reasoning.,,"[{Tang et~al.(2023)Tang, Zou, Zhang, Zhao, Zhang, Cohan, and Gerstein}]{tang2023medagents} Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023. 
 Medagents: Large language models as collaborators for zero-shot medical reasoning. 
 \emph{arXiv preprint arXiv:2311.10537}."
2405.03,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2405.03,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2405.03,xiong2024benchmarking,"[{Xiong et~al.(2024)Xiong, Jin, Lu, and Zhang}]{xiong2024benchmarking} Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024.",Benchmarking retrieval-augmented generation for medicine.,Benchmarking retrieval-augmented generation for medicine.,,"[{Xiong et~al.(2024)Xiong, Jin, Lu, and Zhang}]{xiong2024benchmarking} Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. 
 Benchmarking retrieval-augmented generation for medicine. 
 \emph{arXiv preprint arXiv:2402.13178}."
2405.03,xu2023knowledge,"[{Xu et~al.(2023)Xu, Cui, Yu, Kan, Shi, Zhuang, Jin, Ho, and Yang}]{xu2023knowledge} Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, and Carl Yang. 2023.",Knowledge-infused prompting: Assessing and advancing clinical text data generation with large language models.,Knowledge-infused prompting: Assessing and advancing clinical text data generation with large language models.,,"[{Xu et~al.(2023)Xu, Cui, Yu, Kan, Shi, Zhuang, Jin, Ho, and Yang}]{xu2023knowledge} Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, and Carl Yang. 2023. 
 Knowledge-infused prompting: Assessing and advancing clinical text data generation with large language models. 
 \emph{arXiv preprint arXiv:2311.00287}."
2405.03,xu2024ram,"[{Xu et~al.(2024{\natexlab{a}})Xu, Shi, Yu, Zhuang, Jin, Wang, Ho, and Yang}]{xu2024ram} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May~D Wang, Joyce~C Ho, and Carl Yang. 2024{\natexlab{a}}.",Ram-ehr: Retrieval augmentation meets clinical predictions on electronic health records.,Ram-ehr: Retrieval augmentation meets clinical predictions on electronic health records.,,"[{Xu et~al.(2024{\natexlab{a}})Xu, Shi, Yu, Zhuang, Jin, Wang, Ho, and Yang}]{xu2024ram} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May~D Wang, Joyce~C Ho, and Carl Yang. 2024{\natexlab{a}}. 
 Ram-ehr: Retrieval augmentation meets clinical predictions on electronic health records. 
 \emph{arXiv preprint arXiv:2403.00815}."
2405.03,xu2024bmretriever,"[{Xu et~al.(2024{\natexlab{b}})Xu, Shi, Yu, Zhuang, Zhu, Wang, Ho, Zhang, and Yang}]{xu2024bmretriever} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May~D Wang, Joyce~C Ho, Chao Zhang, and Carl Yang. 2024{\natexlab{b}}.",Bmretriever: Tuning large language models as better biomedical text retrievers.,Bmretriever: Tuning large language models as better biomedical text retrievers.,,"[{Xu et~al.(2024{\natexlab{b}})Xu, Shi, Yu, Zhuang, Zhu, Wang, Ho, Zhang, and Yang}]{xu2024bmretriever} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May~D Wang, Joyce~C Ho, Chao Zhang, and Carl Yang. 2024{\natexlab{b}}. 
 Bmretriever: Tuning large language models as better biomedical text retrievers. 
 \emph{arXiv preprint arXiv:2404.18443}."
2405.03,ye2023drugassist,"[{Ye et~al.(2023)Ye, Cai, Lai, Wang, Huang, Wang, Liu, and Zeng}]{ye2023drugassist} Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. 2023.",Drugassist: A large language model for molecule optimization.,Drugassist: A large language model for molecule optimization.,,"[{Ye et~al.(2023)Ye, Cai, Lai, Wang, Huang, Wang, Liu, and Zeng}]{ye2023drugassist} Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. 2023. 
 Drugassist: A large language model for molecule optimization. 
 \emph{arXiv preprint arXiv:2401.10334}."
2405.03,yu2023explanation,"[{Yu et~al.(2023)Yu, Shen, Liu, Qin, Yan, Liu, Zhang, and Bendersky}]{yu2023explanation} Yue Yu, Jiaming Shen, Tianqi Liu, Zhen Qin, Jing~Nathan Yan, Jialu Liu, Chao Zhang, and Michael Bendersky. 2023.",Explanation-aware soft ensemble empowers large language model in-context learning.,Explanation-aware soft ensemble empowers large language model in-context learning.,,"[{Yu et~al.(2023)Yu, Shen, Liu, Qin, Yan, Liu, Zhang, and Bendersky}]{yu2023explanation} Yue Yu, Jiaming Shen, Tianqi Liu, Zhen Qin, Jing~Nathan Yan, Jialu Liu, Chao Zhang, and Michael Bendersky. 2023. 
 Explanation-aware soft ensemble empowers large language model in-context learning. 
 \emph{arXiv preprint arXiv:2311.07099}."
2405.03,yunxiang2023chatdoctor,"[{Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You}]{yunxiang2023chatdoctor} Li~Yunxiang, Li~Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023.",Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,,"[{Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You}]{yunxiang2023chatdoctor} Li~Yunxiang, Li~Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. 
 Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. 
 \emph{arXiv preprint arXiv:2303.14070}."
2405.03,zhou2023survey,"[{Zhou et~al.(2023)Zhou, Gu, Zou, Li, Chen, Zhou, Liu, Hua, Mao, Wu et~al.}]{zhou2023survey} Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam~S Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, et~al. 2023.","A survey of large language models in medicine: Progress, application, and challenge.","A survey of large language models in medicine: Progress, application, and challenge.",,"[{Zhou et~al.(2023)Zhou, Gu, Zou, Li, Chen, Zhou, Liu, Hua, Mao, Wu et~al.}]{zhou2023survey} Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam~S Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, et~al. 2023. 
 A survey of large language models in medicine: Progress, application, and challenge. 
 \emph{arXiv preprint arXiv:2311.05112}."
2405.04086,ahn2022can,"[{Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman et~al.}]{ahn2022can} Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al. 2022.","Do as i can, not as i say: Grounding language in robotic affordances.","Do as i can, not as i say: Grounding language in robotic affordances.",,"[{Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman et~al.}]{ahn2022can} Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al. 2022. 
 Do as i can, not as i say: Grounding language in robotic affordances. 
 \emph{arXiv preprint arXiv:2204.01691}."
2405.04086,amini2019mathqa,"[{Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi}]{amini2019mathqa} Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.",Mathqa: Towards interpretable math word problem solving with operation-based formalisms.,Mathqa: Towards interpretable math word problem solving with operation-based formalisms.,,"[{Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi}]{amini2019mathqa} Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. 
 Mathqa: Towards interpretable math word problem solving with operation-based formalisms. 
 \emph{arXiv preprint arXiv:1905.13319}."
2405.04086,besta2023graph,"[{Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi, Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk et~al.}]{besta2023graph} Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et~al. 2023.",Graph of thoughts: Solving elaborate problems with large language models.,Graph of thoughts: Solving elaborate problems with large language models.,,"[{Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi, Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk et~al.}]{besta2023graph} Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et~al. 2023. 
 Graph of thoughts: Solving elaborate problems with large language models. 
 \emph{arXiv preprint arXiv:2308.09687}."
2405.04086,burns2023weak,"[{Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike et~al.}]{burns2023weak} Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al. 2023.",Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.,Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.,,"[{Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike et~al.}]{burns2023weak} Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al. 2023. 
 Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. 
 \emph{arXiv preprint arXiv:2312.09390}."
2405.04086,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2405.04086,creswell2022faithful,[{Creswell and Shanahan(2022)}]{creswell2022faithful} Antonia Creswell and Murray Shanahan. 2022.,Faithful reasoning using large language models.,Faithful reasoning using large language models.,,"[{Creswell and Shanahan(2022)}]{creswell2022faithful} Antonia Creswell and Murray Shanahan. 2022. 
 Faithful reasoning using large language models. 
 \emph{arXiv preprint arXiv:2208.14271}."
2405.04086,hao2023reasoning,"[{Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu}]{hao2023reasoning} Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and Zhiting Hu. 2023.",Reasoning with language model is planning with world model.,Reasoning with language model is planning with world model.,,"[{Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu}]{hao2023reasoning} Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and Zhiting Hu. 2023. 
 Reasoning with language model is planning with world model. 
 \emph{arXiv preprint arXiv:2305.14992}."
2405.04086,hendrycks2021measuring,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}."
2405.04086,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2405.04086,joshi2017triviaqa,"[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa} Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017.",Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.,Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.,,"[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa} Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017. 
 Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. 
 \emph{arXiv preprint arXiv:1705.03551}."
2405.04086,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2405.04086,laine2016temporal,[{Laine and Aila(2016)}]{laine2016temporal} Samuli Laine and Timo Aila. 2016.,Temporal ensembling for semi-supervised learning.,Temporal ensembling for semi-supervised learning.,,"[{Laine and Aila(2016)}]{laine2016temporal} Samuli Laine and Timo Aila. 2016. 
 Temporal ensembling for semi-supervised learning. 
 \emph{arXiv preprint arXiv:1610.02242}."
2405.04086,lei2023boosting,"[{Lei et~al.(2023)Lei, Liao, Ding et~al.}]{lei2023boosting} Bin Lei, Chunhua Liao, Caiwen Ding, et~al. 2023.",Boosting logical reasoning in large language models through a new framework: The graph of thought.,Boosting logical reasoning in large language models through a new framework: The graph of thought.,,"[{Lei et~al.(2023)Lei, Liao, Ding et~al.}]{lei2023boosting} Bin Lei, Chunhua Liao, Caiwen Ding, et~al. 2023. 
 Boosting logical reasoning in large language models through a new framework: The graph of thought. 
 \emph{arXiv preprint arXiv:2308.08614}."
2405.04086,lightman2023let,"[{Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe}]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.",Let's verify step by step.,Let's verify step by step.,,"[{Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe}]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. 
 Let's verify step by step. 
 \emph{arXiv preprint arXiv:2305.20050}."
2405.04086,lin2021riddlesense,"[{Lin et~al.(2021)Lin, Wu, Yang, Lee, and Ren}]{lin2021riddlesense} Bill~Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021.",Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge.,Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge.,,"[{Lin et~al.(2021)Lin, Wu, Yang, Lee, and Ren}]{lin2021riddlesense} Bill~Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021. 
 Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. 
 \emph{arXiv preprint arXiv:2101.00376}."
2405.04086,ling2017program,"[{Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom}]{ling2017program} Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.",Program induction by rationale generation: Learning to solve and explain algebraic word problems.,Program induction by rationale generation: Learning to solve and explain algebraic word problems.,,"[{Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom}]{ling2017program} Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. 
 Program induction by rationale generation: Learning to solve and explain algebraic word problems. 
 \emph{arXiv preprint arXiv:1705.04146}."
2405.04086,liu2020logiqa,"[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang}]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020.",Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.,Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.,,"[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang}]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. 
 Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. 
 \emph{arXiv preprint arXiv:2007.08124}."
2405.04086,madaan2023self,"[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang et~al.}]{madaan2023self} Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al. 2023.",Self-refine: Iterative refinement with self-feedback.,Self-refine: Iterative refinement with self-feedback.,,"[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang et~al.}]{madaan2023self} Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al. 2023. 
 Self-refine: Iterative refinement with self-feedback. 
 \emph{arXiv preprint arXiv:2303.17651}."
2405.04086,misra2022news,[{Misra(2022)}]{misra2022news} Rishabh Misra. 2022.,News category dataset.,News category dataset.,,"[{Misra(2022)}]{misra2022news} Rishabh Misra. 2022. 
 News category dataset. 
 \emph{arXiv preprint arXiv:2209.11429}."
2405.04086,onoe2021creak,"[{Onoe et~al.(2021)Onoe, Zhang, Choi, and Durrett}]{onoe2021creak} Yasumasa Onoe, Michael~JQ Zhang, Eunsol Choi, and Greg Durrett. 2021.",Creak: A dataset for commonsense reasoning over entity knowledge.,Creak: A dataset for commonsense reasoning over entity knowledge.,,"[{Onoe et~al.(2021)Onoe, Zhang, Choi, and Durrett}]{onoe2021creak} Yasumasa Onoe, Michael~JQ Zhang, Eunsol Choi, and Greg Durrett. 2021. 
 Creak: A dataset for commonsense reasoning over entity knowledge. 
 \emph{arXiv preprint arXiv:2109.01653}."
2405.04086,pan2023automatically,"[{Pan et~al.(2023)Pan, Saxon, Xu, Nathani, Wang, and Wang}]{pan2023automatically} Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William~Yang Wang. 2023.",Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.,Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.,,"[{Pan et~al.(2023)Pan, Saxon, Xu, Nathani, Wang, and Wang}]{pan2023automatically} Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William~Yang Wang. 2023. 
 Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. 
 \emph{arXiv preprint arXiv:2308.03188}."
2405.04086,sharir2020cost,"[{Sharir et~al.(2020)Sharir, Peleg, and Shoham}]{sharir2020cost} Or~Sharir, Barak Peleg, and Yoav Shoham. 2020.",The cost of training nlp models: A concise overview.,The cost of training nlp models: A concise overview.,,"[{Sharir et~al.(2020)Sharir, Peleg, and Shoham}]{sharir2020cost} Or~Sharir, Barak Peleg, and Yoav Shoham. 2020. 
 The cost of training nlp models: A concise overview. 
 \emph{arXiv preprint arXiv:2004.08900}."
2405.04086,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2405.04086,wang2022self,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}."
2405.04086,yu2020reclor,"[{Yu et~al.(2020)Yu, Jiang, Dong, and Feng}]{yu2020reclor} Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020.",Reclor: A reading comprehension dataset requiring logical reasoning.,Reclor: A reading comprehension dataset requiring logical reasoning.,,"[{Yu et~al.(2020)Yu, Jiang, Dong, and Feng}]{yu2020reclor} Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. 
 Reclor: A reading comprehension dataset requiring logical reasoning. 
 \emph{arXiv preprint arXiv:2002.04326}."
2405.04086,zhang2022automatic,"[{Zhang et~al.(2022{\natexlab{b}})Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022{\natexlab{b}}.",Automatic chain of thought prompting in large language models.,Automatic chain of thought prompting in large language models.,,"[{Zhang et~al.(2022{\natexlab{b}})Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022{\natexlab{b}}. 
 Automatic chain of thought prompting in large language models. 
 \emph{arXiv preprint arXiv:2210.03493}."
2405.04086,zhou2022least,"[{Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le et~al.}]{zhou2022least} Denny Zhou, Nathanael Sch{\""a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et~al. 2022.",Least-to-most prompting enables complex reasoning in large language models.,Least-to-most prompting enables complex reasoning in large language models.,,"[{Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le et~al.}]{zhou2022least} Denny Zhou, Nathanael Sch{\""a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et~al. 2022. 
 Least-to-most prompting enables complex reasoning in large language models. 
 \emph{arXiv preprint arXiv:2205.10625}."
2405.04086,ziegler2019fine,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv:1909.08593}."
2405.04793,atanasova2020diagnostic,"[Atanasova et~al.(2020)Atanasova, Simonsen, Lioma, and Augenstein]{atanasova2020diagnostic} P.~Atanasova, J.~G. Simonsen, C.~Lioma, and I.~Augenstein.",A diagnostic study of explainability techniques for text classification.,A diagnostic study of explainability techniques for text classification.,,"[Atanasova et~al.(2020)Atanasova, Simonsen, Lioma, and Augenstein]{atanasova2020diagnostic} P.~Atanasova, J.~G. Simonsen, C.~Lioma, and I.~Augenstein. 
 A diagnostic study of explainability techniques for text classification. 
 \emph{arXiv preprint arXiv:2009.13295}, 2020."
2405.04793,bansal2023large,[Bansal and Sharma(2023)]{bansal2023large} P.~Bansal and A.~Sharma.,Large language models as annotators: Enhancing generalization of nlp models at minimal cost.,Large language models as annotators: Enhancing generalization of nlp models at minimal cost.,,"[Bansal and Sharma(2023)]{bansal2023large} P.~Bansal and A.~Sharma. 
 Large language models as annotators: Enhancing generalization of nlp models at minimal cost. 
 \emph{arXiv preprint arXiv:2306.15766}, 2023."
2405.04793,bhattacharjee2023llms,"[Bhattacharjee et~al.(2023)Bhattacharjee, Moraffah, Garland, and Liu]{bhattacharjee2023llms} A.~Bhattacharjee, R.~Moraffah, J.~Garland, and H.~Liu.",Llms as counterfactual explanation modules: Can chatgpt explain black-box text classifiers?,Llms as counterfactual explanation modules: Can chatgpt explain black-box text classifiers?,,"[Bhattacharjee et~al.(2023)Bhattacharjee, Moraffah, Garland, and Liu]{bhattacharjee2023llms} A.~Bhattacharjee, R.~Moraffah, J.~Garland, and H.~Liu. 
 Llms as counterfactual explanation modules: Can chatgpt explain black-box text classifiers? 
 \emph{arXiv preprint arXiv:2309.13340}, 2023."
2405.04793,bowman2015large,"[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{bowman2015large} S.~R. Bowman, G.~Angeli, C.~Potts, and C.~D. Manning.",A large annotated corpus for learning natural language inference.,A large annotated corpus for learning natural language inference.,,"[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{bowman2015large} S.~R. Bowman, G.~Angeli, C.~Potts, and C.~D. Manning. 
 A large annotated corpus for learning natural language inference. 
 \emph{arXiv preprint arXiv:1508.05326}, 2015."
2405.04793,bubeck2023sparks,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar, P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, et~al.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar, P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, et~al. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}, 2023."
2405.04793,cer2018universal,"[Cer et~al.(2018)Cer, Yang, Kong, Hua, Limtiaco, John, Constant, Guajardo-Cespedes, Yuan, Tar, et~al.]{cer2018universal} D.~Cer, Y.~Yang, S.-y. Kong, N.~Hua, N.~Limtiaco, R.~S. John, N.~Constant, M.~Guajardo-Cespedes, S.~Yuan, C.~Tar, et~al.",Universal sentence encoder.,Universal sentence encoder.,,"[Cer et~al.(2018)Cer, Yang, Kong, Hua, Limtiaco, John, Constant, Guajardo-Cespedes, Yuan, Tar, et~al.]{cer2018universal} D.~Cer, Y.~Yang, S.-y. Kong, N.~Hua, N.~Limtiaco, R.~S. John, N.~Constant, M.~Guajardo-Cespedes, S.~Yuan, C.~Tar, et~al. 
 Universal sentence encoder. 
 \emph{arXiv preprint arXiv:1803.11175}, 2018."
2405.04793,chung2022scaling,"[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling} H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling} H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}, 2022."
2405.04793,dettmers2023qlora,"[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora} T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer.",Qlora: Efficient finetuning of quantized llms.,Qlora: Efficient finetuning of quantized llms.,,"[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora} T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer. 
 Qlora: Efficient finetuning of quantized llms. 
 \emph{arXiv preprint arXiv:2305.14314}, 2023."
2405.04793,dong2022survey,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui.",A survey for in-context learning.,A survey for in-context learning.,,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}, 2022."
2405.04793,gao2020pile,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile} L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile} L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}, 2020."
2405.04793,gardner2020evaluating,"[Gardner et~al.(2020)Gardner, Artzi, Basmova, Berant, Bogin, Chen, Dasigi, Dua, Elazar, Gottumukkala, et~al.]{gardner2020evaluating} M.~Gardner, Y.~Artzi, V.~Basmova, J.~Berant, B.~Bogin, S.~Chen, P.~Dasigi, D.~Dua, Y.~Elazar, A.~Gottumukkala, et~al.",Evaluating models' local decision boundaries via contrast sets.,Evaluating models' local decision boundaries via contrast sets.,,"[Gardner et~al.(2020)Gardner, Artzi, Basmova, Berant, Bogin, Chen, Dasigi, Dua, Elazar, Gottumukkala, et~al.]{gardner2020evaluating} M.~Gardner, Y.~Artzi, V.~Basmova, J.~Berant, B.~Bogin, S.~Chen, P.~Dasigi, D.~Dua, Y.~Elazar, A.~Gottumukkala, et~al. 
 Evaluating models' local decision boundaries via contrast sets. 
 \emph{arXiv preprint arXiv:2004.02709}, 2020."
2405.04793,garg2020bae,[Garg and Ramakrishnan(2020)]{garg2020bae} S.~Garg and G.~Ramakrishnan.,Bae: Bert-based adversarial examples for text classification.,Bae: Bert-based adversarial examples for text classification.,,"[Garg and Ramakrishnan(2020)]{garg2020bae} S.~Garg and G.~Ramakrishnan. 
 Bae: Bert-based adversarial examples for text classification. 
 \emph{arXiv preprint arXiv:2004.01970}, 2020."
2405.04793,he2023annollm,"[He et~al.(2023)He, Lin, Gong, Jin, Zhang, Lin, Jiao, Yiu, Duan, Chen, et~al.]{he2023annollm} X.~He, Z.~Lin, Y.~Gong, A.~Jin, H.~Zhang, C.~Lin, J.~Jiao, S.~M. Yiu, N.~Duan, W.~Chen, et~al.",Annollm: Making large language models to be better crowdsourced annotators.,Annollm: Making large language models to be better crowdsourced annotators.,,"[He et~al.(2023)He, Lin, Gong, Jin, Zhang, Lin, Jiao, Yiu, Duan, Chen, et~al.]{he2023annollm} X.~He, Z.~Lin, Y.~Gong, A.~Jin, H.~Zhang, C.~Lin, J.~Jiao, S.~M. Yiu, N.~Duan, W.~Chen, et~al. 
 Annollm: Making large language models to be better crowdsourced annotators. 
 \emph{arXiv preprint arXiv:2303.16854}, 2023."
2405.04793,kaushik2019learning,"[Kaushik et~al.(2019)Kaushik, Hovy, and Lipton]{kaushik2019learning} D.~Kaushik, E.~Hovy, and Z.~C. Lipton.",Learning the difference that makes a difference with counterfactually-augmented data.,Learning the difference that makes a difference with counterfactually-augmented data.,,"[Kaushik et~al.(2019)Kaushik, Hovy, and Lipton]{kaushik2019learning} D.~Kaushik, E.~Hovy, and Z.~C. Lipton. 
 Learning the difference that makes a difference with counterfactually-augmented data. 
 \emph{arXiv preprint arXiv:1909.12434}, 2019."
2405.04793,khashabi2020more,"[Khashabi et~al.(2020)Khashabi, Khot, and Sabharwal]{khashabi2020more} D.~Khashabi, T.~Khot, and A.~Sabharwal.",More bang for your buck: Natural perturbation for robust question answering.,More bang for your buck: Natural perturbation for robust question answering.,,"[Khashabi et~al.(2020)Khashabi, Khot, and Sabharwal]{khashabi2020more} D.~Khashabi, T.~Khot, and A.~Sabharwal. 
 More bang for your buck: Natural perturbation for robust question answering. 
 \emph{arXiv preprint arXiv:2004.04849}, 2020."
2405.04793,liu2021towards,"[Liu et~al.(2021)Liu, Sun, He, Wu, Wu, Zhang, Jiang, Cao, Huang, and Qiu]{liu2021towards} X.~Liu, T.~Sun, J.~He, J.~Wu, L.~Wu, X.~Zhang, H.~Jiang, Z.~Cao, X.~Huang, and X.~Qiu.",Towards efficient nlp: A standard evaluation and a strong baseline.,Towards efficient nlp: A standard evaluation and a strong baseline.,,"[Liu et~al.(2021)Liu, Sun, He, Wu, Wu, Zhang, Jiang, Cao, Huang, and Qiu]{liu2021towards} X.~Liu, T.~Sun, J.~He, J.~Wu, L.~Wu, X.~Zhang, H.~Jiang, Z.~Cao, X.~Huang, and X.~Qiu. 
 Towards efficient nlp: A standard evaluation and a strong baseline. 
 \emph{arXiv preprint arXiv:2110.07038}, 2021."
2405.04793,madaan2022plug,"[Madaan et~al.(2022)Madaan, Bedathur, and Saha]{madaan2022plug} N.~Madaan, S.~Bedathur, and D.~Saha.",Plug and play counterfactual text generation for model robustness.,Plug and play counterfactual text generation for model robustness.,,"[Madaan et~al.(2022)Madaan, Bedathur, and Saha]{madaan2022plug} N.~Madaan, S.~Bedathur, and D.~Saha. 
 Plug and play counterfactual text generation for model robustness. 
 \emph{arXiv preprint arXiv:2206.10429}, 2022."
2405.04793,penedo2023refinedweb,"[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb} G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli, B.~Pannier, E.~Almazrouei, and J.~Launay.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.",,"[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb} G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli, B.~Pannier, E.~Almazrouei, and J.~Launay. 
 The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 
 \emph{arXiv preprint arXiv:2306.01116}, 2023."
2405.04793,qin2019counterfactual,"[Qin et~al.(2019)Qin, Bosselut, Holtzman, Bhagavatula, Clark, and Choi]{qin2019counterfactual} L.~Qin, A.~Bosselut, A.~Holtzman, C.~Bhagavatula, E.~Clark, and Y.~Choi.",Counterfactual story reasoning and generation.,Counterfactual story reasoning and generation.,,"[Qin et~al.(2019)Qin, Bosselut, Holtzman, Bhagavatula, Clark, and Choi]{qin2019counterfactual} L.~Qin, A.~Bosselut, A.~Holtzman, C.~Bhagavatula, E.~Clark, and Y.~Choi. 
 Counterfactual story reasoning and generation. 
 \emph{arXiv preprint arXiv:1909.04076}, 2019."
2405.04793,rae2021scaling,"[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling} J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song, J.~Aslanides, S.~Henderson, R.~Ring, S.~Young, et~al.","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling} J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song, J.~Aslanides, S.~Henderson, R.~Ring, S.~Young, et~al. 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 \emph{arXiv preprint arXiv:2112.11446}, 2021."
2405.04793,ribeiro2020beyond,"[Ribeiro et~al.(2020)Ribeiro, Wu, Guestrin, and Singh]{ribeiro2020beyond} M.~T. Ribeiro, T.~Wu, C.~Guestrin, and S.~Singh.",Beyond accuracy: Behavioral testing of nlp models with checklist.,Beyond accuracy: Behavioral testing of nlp models with checklist.,,"[Ribeiro et~al.(2020)Ribeiro, Wu, Guestrin, and Singh]{ribeiro2020beyond} M.~T. Ribeiro, T.~Wu, C.~Guestrin, and S.~Singh. 
 Beyond accuracy: Behavioral testing of nlp models with checklist. 
 \emph{arXiv preprint arXiv:2005.04118}, 2020."
2405.04793,sanh2019distilbert,"[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert} V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf.","Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.","Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",,"[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert} V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf. 
 Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. 
 \emph{arXiv preprint arXiv:1910.01108}, 2019."
2405.04793,sun2023text,"[Sun et~al.(2023)Sun, Li, Li, Wu, Guo, Zhang, and Wang]{sun2023text} X.~Sun, X.~Li, J.~Li, F.~Wu, S.~Guo, T.~Zhang, and G.~Wang.",Text classification via large language models.,Text classification via large language models.,,"[Sun et~al.(2023)Sun, Li, Li, Wu, Guo, Zhang, and Wang]{sun2023text} X.~Sun, X.~Li, J.~Li, F.~Wu, S.~Guo, T.~Zhang, and G.~Wang. 
 Text classification via large language models. 
 \emph{arXiv preprint arXiv:2305.08377}, 2023."
2405.04793,tan2024large,"[Tan et~al.(2024)Tan, Beigi, Wang, Guo, Bhattacharjee, Jiang, Karami, Li, Cheng, and Liu]{tan2024large} Z.~Tan, A.~Beigi, S.~Wang, R.~Guo, A.~Bhattacharjee, B.~Jiang, M.~Karami, J.~Li, L.~Cheng, and H.~Liu.",Large language models for data annotation: A survey.,Large language models for data annotation: A survey.,,"[Tan et~al.(2024)Tan, Beigi, Wang, Guo, Bhattacharjee, Jiang, Karami, Li, Cheng, and Liu]{tan2024large} Z.~Tan, A.~Beigi, S.~Wang, R.~Guo, A.~Bhattacharjee, B.~Jiang, M.~Karami, J.~Li, L.~Cheng, and H.~Liu. 
 Large language models for data annotation: A survey. 
 \emph{arXiv preprint arXiv:2402.13446}, 2024."
2405.04793,touvron2023llama,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.04793,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.04793,valmeekam2022large,"[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and Kambhampati]{valmeekam2022large} K.~Valmeekam, A.~Olmo, S.~Sreedharan, and S.~Kambhampati.",Large language models still can't plan (a benchmark for llms on planning and reasoning about change).,Large language models still can't plan (a benchmark for llms on planning and reasoning about change).,,"[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and Kambhampati]{valmeekam2022large} K.~Valmeekam, A.~Olmo, S.~Sreedharan, and S.~Kambhampati. 
 Large language models still can't plan (a benchmark for llms on planning and reasoning about change). 
 \emph{arXiv preprint arXiv:2206.10498}, 2022."
2405.04793,wang2018glue,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}, 2018."
2405.04793,wu2021polyjuice,"[Wu et~al.(2021)Wu, Ribeiro, Heer, and Weld]{wu2021polyjuice} T.~Wu, M.~T. Ribeiro, J.~Heer, and D.~S. Weld.","Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models.","Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models.",,"[Wu et~al.(2021)Wu, Ribeiro, Heer, and Weld]{wu2021polyjuice} T.~Wu, M.~T. Ribeiro, J.~Heer, and D.~S. Weld. 
 Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. 
 \emph{arXiv preprint arXiv:2101.00288}, 2021."
2405.05615,caption,"[Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Doll{\'{a}}r, and Zitnick]{caption} Chen, X., Fang, H., Lin, T., Vedantam, R., Gupta, S., Doll{\'{a}}r, P., and Zitnick, C.~L.",Microsoft {COCO} captions: Data collection and evaluation server.,Microsoft {COCO} captions: Data collection and evaluation server.,,"[Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Doll{\'{a}}r, and Zitnick]{caption} Chen, X., Fang, H., Lin, T., Vedantam, R., Gupta, S., Doll{\'{a}}r, P., and Zitnick, C.~L. 
 Microsoft {COCO} captions: Data collection and evaluation server. 
 \emph{arXiv preprint}, arXiv:1504.00325, 2015."
2405.05615,mobilevlm,"[Chu et~al.(2023)Chu, Qiao, Lin, Xu, Yang, Hu, Wei, Zhang, Zhang, Wei, and Shen]{mobilevlm} Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., and Shen, C.","Mobilevlm : {A} fast, strong and open vision language assistant for mobile devices.","Mobilevlm : {A} fast, strong and open vision language assistant for mobile devices.",,"[Chu et~al.(2023)Chu, Qiao, Lin, Xu, Yang, Hu, Wei, Zhang, Zhang, Wei, and Shen]{mobilevlm} Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., and Shen, C. 
 Mobilevlm : {A} fast, strong and open vision language assistant for mobile devices. 
 \emph{arXiv preprint}, arXiv:2312.16886, 2023."
2405.05615,llava1.5,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{llava1.5} Liu, H., Li, C., Li, Y., and Lee, Y.~J.",Improved baselines with visual instruction tuning.,Improved baselines with visual instruction tuning.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{llava1.5} Liu, H., Li, C., Li, Y., and Lee, Y.~J. 
 Improved baselines with visual instruction tuning. 
 \emph{arXiv preprint}, arXiv:2310.03744, 2023{\natexlab{a}}."
2405.05615,ptuningv2,"[Liu et~al.(2021)Liu, Ji, Fu, Du, Yang, and Tang]{ptuningv2} Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J.",P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.,P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.,,"[Liu et~al.(2021)Liu, Ji, Fu, Du, Yang, and Tang]{ptuningv2} Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J. 
 P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. 
 \emph{arXiv preprint}, arXiv:2110.07602, 2021."
2405.05615,uniadapter,"[Lu et~al.(2023)Lu, Ding, Huo, Yang, Lu, Tomizuka, and Zhan]{uniadapter} Lu, H., Ding, M., Huo, Y., Yang, G., Lu, Z., Tomizuka, M., and Zhan, W.",Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling.,Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling.,,"[Lu et~al.(2023)Lu, Ding, Huo, Yang, Lu, Tomizuka, and Zhan]{uniadapter} Lu, H., Ding, M., Huo, Y., Yang, G., Lu, Z., Tomizuka, M., and Zhan, W. 
 Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling. 
 \emph{arXiv preprint}, arXiv:2302.06605, 2023."
2405.05615,ffn2,"[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{ffn2} Meng, K., Bau, D., Andonian, A., and Belinkov, Y.",Locating and editing factual knowledge in {GPT}.,Locating and editing factual knowledge in {GPT}.,,"[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{ffn2} Meng, K., Bau, D., Andonian, A., and Belinkov, Y. 
 Locating and editing factual knowledge in {GPT}. 
 \emph{arXiv preprint}, arXiv:2202.05262, 2022."
2405.05615,gpt4,[OpenAI(2023)]{gpt4} OpenAI.,{GPT-4} technical report.,{GPT-4} technical report.,,"[OpenAI(2023)]{gpt4} OpenAI. 
 {GPT-4} technical report. 
 \emph{arXiv preprint}, arXiv:2303.08774, 2023."
2405.05615,tang2024rethinking,"[Tang et~al.(2024)Tang, Liu, Ni, Tian, Bai, Hu, Liu, Jui, Han, and Wang]{tang2024rethinking} Tang, Y., Liu, F., Ni, Y., Tian, Y., Bai, Z., Hu, Y.-Q., Liu, S., Jui, S., Han, K., and Wang, Y.",Rethinking optimization and architecture for tiny language models.,Rethinking optimization and architecture for tiny language models.,,"[Tang et~al.(2024)Tang, Liu, Ni, Tian, Bai, Hu, Liu, Jui, Han, and Wang]{tang2024rethinking} Tang, Y., Liu, F., Ni, Y., Tian, Y., Bai, Z., Hu, Y.-Q., Liu, S., Jui, S., Han, K., and Wang, Y. 
 Rethinking optimization and architecture for tiny language models. 
 \emph{arXiv preprint}, arXiv:2402.02791, 2024."
2405.05615,llama,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi{\`{e}}re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi{\`{e}}re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint}, arXiv:2302.13971, 2023."
2405.05615,llama-adapter,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao]{llama-adapter} Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y.",Llama-adapter: Efficient fine-tuning of language models with zero-init attention.,Llama-adapter: Efficient fine-tuning of language models with zero-init attention.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao]{llama-adapter} Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. 
 Llama-adapter: Efficient fine-tuning of language models with zero-init attention. 
 \emph{arXiv preprint}, arXiv:2303.16199, 2023{\natexlab{a}}."
2405.05615,noah,"[Zhang et~al.(2022)Zhang, Zhou, and Liu]{noah} Zhang, Y., Zhou, K., and Liu, Z.",Neural prompt search.,Neural prompt search.,,"[Zhang et~al.(2022)Zhang, Zhou, and Liu]{noah} Zhang, Y., Zhou, K., and Liu, Z. 
 Neural prompt search. 
 \emph{arXiv preprint}, arXiv:2206.04673, 2022."
2405.05615,mm-cot,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Li, Zhao, Karypis, and Smola]{mm-cot} Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A.",Multimodal chain-of-thought reasoning in language models.,Multimodal chain-of-thought reasoning in language models.,,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Li, Zhao, Karypis, and Smola]{mm-cot} Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. 
 Multimodal chain-of-thought reasoning in language models. 
 \emph{arXiv preprint}, arXiv:2302.00923, 2023{\natexlab{c}}."
2405.07551,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2405.07551,gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2405.07551,fu2023specializing,"[{Fu et~al.(2023{\natexlab{b}})Fu, Peng, Ou, Sabharwal, and Khot}]{fu2023specializing} Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023{\natexlab{b}}.",Specializing smaller language models towards multi-step reasoning.,Specializing smaller language models towards multi-step reasoning.,,"[{Fu et~al.(2023{\natexlab{b}})Fu, Peng, Ou, Sabharwal, and Khot}]{fu2023specializing} Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023{\natexlab{b}}. 
 Specializing smaller language models towards multi-step reasoning. 
 \emph{arXiv preprint arXiv:2301.12726}."
2405.07551,hendrycks2021measuring,"[{Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021{\natexlab{b}}.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[{Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021{\natexlab{b}}. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}."
2405.07551,xwin_math,"[{Li et~al.(2024)Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng}]{xwin_math} Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. 2024.",Common 7b language models already possess strong math capabilities.,Common 7b language models already possess strong math capabilities.,,"[{Li et~al.(2024)Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng}]{xwin_math} Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. 2024. 
 Common 7b language models already possess strong math capabilities. 
 \emph{arXiv preprint arXiv:2403.04706}."
2405.07551,longpre2023flan,"[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei et~al.}]{longpre2023flan} Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023.",The flan collection: Designing data and methods for effective instruction tuning.,The flan collection: Designing data and methods for effective instruction tuning.,,"[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei et~al.}]{longpre2023flan} Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023. 
 The flan collection: Designing data and methods for effective instruction tuning. 
 \emph{arXiv preprint arXiv:2301.13688}."
2405.07551,wizardcoder,"[{Luo et~al.(2023{\natexlab{b}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023{\natexlab{b}}.",Wizardcoder: Empowering code large language models with evol-instruct.,Wizardcoder: Empowering code large language models with evol-instruct.,,"[{Luo et~al.(2023{\natexlab{b}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023{\natexlab{b}}. 
 Wizardcoder: Empowering code large language models with evol-instruct. 
 \emph{arXiv preprint arXiv:2306.08568}."
2405.07551,glue,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}."
2405.07609,rolnick2017deep,"[{Rolnick et~al.(2017)Rolnick, Veit, Belongie, and   Shavit}]{rolnick2017deep} David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. 2017.",Deep learning is robust to massive label noise.,Deep learning is robust to massive label noise.,,"[{Rolnick et~al.(2017)Rolnick, Veit, Belongie, and   Shavit}]{rolnick2017deep} David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. 2017. 
 Deep learning is robust to massive label noise. 
 \emph{arXiv preprint arXiv:1705.10694}."
2405.07609,wang2022promix,"[{Wang et~al.(2022)Wang, Xiao, Dong, Feng, and Zhao}]{wang2022promix} Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. 2022.",{ProMix}: combating label noise via maximizing clean sample utility.,{ProMix}: combating label noise via maximizing clean sample utility.,,"[{Wang et~al.(2022)Wang, Xiao, Dong, Feng, and Zhao}]{wang2022promix} Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. 2022. 
 {ProMix}: combating label noise via maximizing clean sample utility. 
 \emph{arXiv preprint arXiv:2207.10276}."
2405.07609,zhou2021learning,[{Zhou and Chen(2021)}]{zhou2021learning} Wenxuan Zhou and Muhao Chen. 2021.,Learning from noisy labels for entity-centric information extraction.,Learning from noisy labels for entity-centric information extraction.,,"[{Zhou and Chen(2021)}]{zhou2021learning} Wenxuan Zhou and Muhao Chen. 2021. 
 Learning from noisy labels for entity-centric information extraction. 
 \emph{arXiv preprint arXiv:2104.08656}."
2405.09605,team2024gemma,"[{Gemma et~al.(2024)Gemma, Mesnard, Hardin, Dadashi, Bhupatiraju,   Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,   Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,   Juliette Love, et~al. 2024.",Gemma: Open models based on {G}emini research and technology.,Gemma: Open models based on {G}emini research and technology.,,"[{Gemma et~al.(2024)Gemma, Mesnard, Hardin, Dadashi, Bhupatiraju,   Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,   Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,   Juliette Love, et~al. 2024. 
 Gemma: Open models based on {G}emini research and technology. 
 \emph{arXiv preprint arXiv:2403.08295}."
2405.09605,gunasekar2023textbooks,"[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,   Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi   et~al.}]{gunasekar2023textbooks} Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie   Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo   de~Rosa, Olli Saarikivi, et~al. 2023.",Textbooks are all you need.,Textbooks are all you need.,,"[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,   Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi   et~al.}]{gunasekar2023textbooks} Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie   Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo   de~Rosa, Olli Saarikivi, et~al. 2023. 
 Textbooks are all you need. 
 \emph{arXiv preprint arXiv:2306.11644}."
2405.09605,ha2018world,"[{Ha and Schmidhuber(2018)}]{ha2018world} David Ha and J{\""u}rgen Schmidhuber. 2018.",World models.,World models.,,"[{Ha and Schmidhuber(2018)}]{ha2018world} David Ha and J{\""u}rgen Schmidhuber. 2018. 
 World models. 
 \emph{arXiv preprint arXiv:1803.10122}."
2405.09605,hu2024auxiliary,[{Hu and Frank(2024)}]{hu2024auxiliary} Jennifer Hu and Michael~C Frank. 2024.,Auxiliary task demands mask the capabilities of smaller language   models.,Auxiliary task demands mask the capabilities of smaller language   models.,,"[{Hu and Frank(2024)}]{hu2024auxiliary} Jennifer Hu and Michael~C Frank. 2024. 
 Auxiliary task demands mask the capabilities of smaller language   models. 
 \emph{arXiv preprint arXiv:2404.02418}."
2405.09605,hu2024language,"[{Hu et~al.(2024)Hu, Mahowald, Lupyan, Ivanova, and   Levy}]{hu2024language} Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, and Roger Levy. 2024.",Language models align with human judgments on key grammatical   constructions.,Language models align with human judgments on key grammatical   constructions.,,"[{Hu et~al.(2024)Hu, Mahowald, Lupyan, Ivanova, and   Levy}]{hu2024language} Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, and Roger Levy. 2024. 
 Language models align with human judgments on key grammatical   constructions. 
 \emph{arXiv preprint arXiv:2402.01676}."
2405.09605,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7{B}.,Mistral 7{B}.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7{B}. 
 \emph{arXiv preprint arXiv:2310.06825}."
2405.09605,jiang2024mixtral,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,   Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al. 2024.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,   Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al. 2024. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2405.09605,kauf2024comparing,"[{Kauf et~al.(2024)Kauf, Chersoni, Lenci, Fedorenko, and   Ivanova}]{kauf2024comparing} Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, and   Anna~A Ivanova. 2024.",Comparing plausibility estimates in base and instruction-tuned large   language models.,Comparing plausibility estimates in base and instruction-tuned large   language models.,,"[{Kauf et~al.(2024)Kauf, Chersoni, Lenci, Fedorenko, and   Ivanova}]{kauf2024comparing} Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, and   Anna~A Ivanova. 2024. 
 Comparing plausibility estimates in base and instruction-tuned large   language models. 
 \emph{arXiv preprint arXiv:2403.14859}."
2405.09605,kosinski2023theory,[{Kosinski(2023)}]{kosinski2023theory} Michal Kosinski. 2023.,Theory of mind may have spontaneously emerged in large language   models.,Theory of mind may have spontaneously emerged in large language   models.,,"[{Kosinski(2023)}]{kosinski2023theory} Michal Kosinski. 2023. 
 Theory of mind may have spontaneously emerged in large language   models. 
 \emph{arXiv preprint arXiv:2302.02083}, 4:169."
2405.09605,lampinen2022can,[{Lampinen(2022)}]{lampinen2022can} Andrew~Kyle Lampinen. 2022.,Can language models handle recursively nested grammatical structures?   a case study on comparing models and humans.,Can language models handle recursively nested grammatical structures?   a case study on comparing models and humans.,,"[{Lampinen(2022)}]{lampinen2022can} Andrew~Kyle Lampinen. 2022. 
 Can language models handle recursively nested grammatical structures?   a case study on comparing models and humans. 
 \emph{arXiv preprint arXiv:2210.15303}."
2405.09605,textbooks2,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and   Lee}]{textbooks2} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee. 2023.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and   Lee}]{textbooks2} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee. 2023. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2405.09605,panickssery2024llm,"[{Panickssery et~al.(2024)Panickssery, Bowman, and   Feng}]{panickssery2024llm} Arjun Panickssery, Samuel~R Bowman, and Shi Feng. 2024.",{LLM} evaluators recognize and favor their own generations.,{LLM} evaluators recognize and favor their own generations.,,"[{Panickssery et~al.(2024)Panickssery, Bowman, and   Feng}]{panickssery2024llm} Arjun Panickssery, Samuel~R Bowman, and Shi Feng. 2024. 
 {LLM} evaluators recognize and favor their own generations. 
 \emph{arXiv preprint arXiv:2404.13076}."
2405.09605,ullman2023large,[{Ullman(2023)}]{ullman2023large} Tomer Ullman. 2023.,Large language models fail on trivial alterations to theory-of-mind   tasks.,Large language models fail on trivial alterations to theory-of-mind   tasks.,,"[{Ullman(2023)}]{ullman2023large} Tomer Ullman. 2023. 
 Large language models fail on trivial alterations to theory-of-mind   tasks. 
 \emph{arXiv preprint arXiv:2302.08399}."
2405.09605,wolf2019huggingface,"[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,   Rault, Louf, Funtowicz et~al.}]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,   Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,   et~al. 2019.",Huggingface's transformers: State-of-the-art natural language   processing.,Huggingface's transformers: State-of-the-art natural language   processing.,,"[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,   Rault, Louf, Funtowicz et~al.}]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,   Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,   et~al. 2019. 
 Huggingface's transformers: State-of-the-art natural language   processing. 
 \emph{arXiv preprint arXiv:1910.03771}."
2405.09605,wong2023word,"[{Wong et~al.(2023)Wong, Grand, Lew, Goodman, Mansinghka, Andreas, and   Tenenbaum}]{wong2023word} Lionel Wong, Gabriel Grand, Alexander~K Lew, Noah~D Goodman, Vikash~K   Mansinghka, Jacob Andreas, and Joshua~B Tenenbaum. 2023.",From word models to world models: Translating from natural language   to the probabilistic language of thought.,From word models to world models: Translating from natural language   to the probabilistic language of thought.,,"[{Wong et~al.(2023)Wong, Grand, Lew, Goodman, Mansinghka, Andreas, and   Tenenbaum}]{wong2023word} Lionel Wong, Gabriel Grand, Alexander~K Lew, Noah~D Goodman, Vikash~K   Mansinghka, Jacob Andreas, and Joshua~B Tenenbaum. 2023. 
 From word models to world models: Translating from natural language   to the probabilistic language of thought. 
 \emph{arXiv preprint arXiv:2306.12672}."
2405.10311,reimagen,"[{Chen et~al.(2022)Chen, Hu, Saharia, and Cohen}]{reimagen} Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William~W Cohen. 2022.",Re-imagen: Retrieval-augmented text-to-image generator.,Re-imagen: Retrieval-augmented text-to-image generator.,,"[{Chen et~al.(2022)Chen, Hu, Saharia, and Cohen}]{reimagen} Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William~W Cohen. 2022. 
 Re-imagen: Retrieval-augmented text-to-image generator. 
 \emph{arXiv preprint arXiv:2209.14491}."
2405.10311,emu,"[{Dai et~al.(2023)Dai, Hou, Ma, Tsai, Wang, Wang, Zhang, Vandenhende, Wang, Dubey et~al.}]{emu} Xiaoliang Dai, Ji~Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et~al. 2023.",Emu: Enhancing image generation models using photogenic needles in a haystack.,Emu: Enhancing image generation models using photogenic needles in a haystack.,,"[{Dai et~al.(2023)Dai, Hou, Ma, Tsai, Wang, Wang, Zhang, Vandenhende, Wang, Dubey et~al.}]{emu} Xiaoliang Dai, Ji~Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et~al. 2023. 
 Emu: Enhancing image generation models using photogenic needles in a haystack. 
 \emph{arXiv preprint arXiv:2309.15807}."
2405.10311,clipscore,"[{Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi}]{clipscore} Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi. 2021.",Clipscore: A reference-free evaluation metric for image captioning.,Clipscore: A reference-free evaluation metric for image captioning.,,"[{Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi}]{clipscore} Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi. 2021. 
 Clipscore: A reference-free evaluation metric for image captioning. 
 \emph{arXiv preprint arXiv:2104.08718}."
2405.10311,lavit,"[{Jin et~al.(2023)Jin, Xu, Chen, Liao, Tan, Chen, Lei, Liu, Song, Lei et~al.}]{lavit} Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An~Liu, Chengru Song, Xiaoqiang Lei, et~al. 2023.",Unified language-vision pretraining with dynamic discrete visual tokenization.,Unified language-vision pretraining with dynamic discrete visual tokenization.,,"[{Jin et~al.(2023)Jin, Xu, Chen, Liao, Tan, Chen, Lei, Liu, Song, Lei et~al.}]{lavit} Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An~Liu, Chengru Song, Xiaoqiang Lei, et~al. 2023. 
 Unified language-vision pretraining with dynamic discrete visual tokenization. 
 \emph{arXiv preprint arXiv:2309.04669}."
2405.10311,emu2,"[{Sun et~al.(2023)Sun, Cui, Zhang, Zhang, Yu, Luo, Wang, Rao, Liu, Huang et~al.}]{emu2} Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et~al. 2023.",Generative multimodal models are in-context learners.,Generative multimodal models are in-context learners.,,"[{Sun et~al.(2023)Sun, Cui, Zhang, Zhang, Yu, Luo, Wang, Rao, Liu, Huang et~al.}]{emu2} Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et~al. 2023. 
 Generative multimodal models are in-context learners. 
 \emph{arXiv preprint arXiv:2312.13286}."
2405.10311,llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2405.10311,uniIR,"[{Wei et~al.(2023)Wei, Chen, Chen, Hu, Zhang, Fu, Ritter, and Chen}]{uniIR} Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge~Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2023.",Uniir: Training and benchmarking universal multimodal information retrievers.,Uniir: Training and benchmarking universal multimodal information retrievers.,,"[{Wei et~al.(2023)Wei, Chen, Chen, Hu, Zhang, Fu, Ritter, and Chen}]{uniIR} Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge~Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2023. 
 Uniir: Training and benchmarking universal multimodal information retrievers. 
 \emph{arXiv preprint arXiv:2311.17136}."
2405.10311,ra-cm3,"[{Yasunaga et~al.(2022)Yasunaga, Aghajanyan, Shi, James, Leskovec, Liang, Lewis, Zettlemoyer, and Yih}]{ra-cm3} Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022.",Retrieval-augmented multimodal language modeling.,Retrieval-augmented multimodal language modeling.,,"[{Yasunaga et~al.(2022)Yasunaga, Aghajanyan, Shi, James, Leskovec, Liang, Lewis, Zettlemoyer, and Yih}]{ra-cm3} Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022. 
 Retrieval-augmented multimodal language modeling. 
 \emph{arXiv preprint arXiv:2211.12561}."
2405.10311,CM3Leon,"[{Yu et~al.(2023)Yu, Shi, Pasunuru, Muller, Golovneva, Wang, Babu, Tang, Karrer, Sheynin et~al.}]{CM3Leon} Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et~al. 2023.",Scaling autoregressive multi-modal models: Pretraining and instruction tuning.,Scaling autoregressive multi-modal models: Pretraining and instruction tuning.,,"[{Yu et~al.(2023)Yu, Shi, Pasunuru, Muller, Golovneva, Wang, Babu, Tang, Karrer, Sheynin et~al.}]{CM3Leon} Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et~al. 2023. 
 Scaling autoregressive multi-modal models: Pretraining and instruction tuning. 
 \emph{arXiv preprint arXiv:2309.02591}."
2405.10311,mmllmSurvay,"[{Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu}]{mmllmSurvay} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024.",Mm-llms: Recent advances in multimodal large language models.,Mm-llms: Recent advances in multimodal large language models.,,"[{Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu}]{mmllmSurvay} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024. 
 Mm-llms: Recent advances in multimodal large language models. 
 \emph{arXiv preprint arXiv:2401.13601}."
2405.10311,mmRAGSurvay,"[{Zhao et~al.(2023)Zhao, Chen, Wang, Jiao, Do, Qin, Ding, Guo, Li, Li et~al.}]{mmRAGSurvay} Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan~Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et~al. 2023.",Retrieving multimodal information for augmented generation: A survey.,Retrieving multimodal information for augmented generation: A survey.,,"[{Zhao et~al.(2023)Zhao, Chen, Wang, Jiao, Do, Qin, Ding, Guo, Li, Li et~al.}]{mmRAGSurvay} Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan~Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et~al. 2023. 
 Retrieving multimodal information for augmented generation: A survey. 
 \emph{arXiv preprint arXiv:2303.10868}."
2405.10431,gallegos2023bias,"[{Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed}]{gallegos2023bias} Isabel~O Gallegos, Ryan~A Rossi, Joe Barrow, Md~Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen~K Ahmed. 2023.",Bias and fairness in large language models: A survey.,Bias and fairness in large language models: A survey.,,"[{Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed}]{gallegos2023bias} Isabel~O Gallegos, Ryan~A Rossi, Joe Barrow, Md~Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen~K Ahmed. 2023. 
 Bias and fairness in large language models: A survey. 
 \emph{arXiv preprint arXiv:2309.00770}."
2405.10431,gehman2020realtoxicityprompts,"[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith}]{gehman2020realtoxicityprompts} Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith. 2020.",Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,,"[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith}]{gehman2020realtoxicityprompts} Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith. 2020. 
 Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 
 \emph{arXiv preprint arXiv:2009.11462}."
2405.10431,li2023survey,"[{Li et~al.(2023{\natexlab{a}})Li, Du, Song, Wang, and Wang}]{li2023survey} Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. 2023{\natexlab{a}}.",A survey on fairness in large language models.,A survey on fairness in large language models.,,"[{Li et~al.(2023{\natexlab{a}})Li, Du, Song, Wang, and Wang}]{li2023survey} Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. 2023{\natexlab{a}}. 
 A survey on fairness in large language models. 
 \emph{arXiv preprint arXiv:2308.10149}."
2405.10431,liang2020towards,"[{Liang et~al.(2020)Liang, Li, Zheng, Lim, Salakhutdinov, and Morency}]{liang2020towards} Paul~Pu Liang, Irene~Mengze Li, Emily Zheng, Yao~Chong Lim, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020.",Towards debiasing sentence representations.,Towards debiasing sentence representations.,,"[{Liang et~al.(2020)Liang, Li, Zheng, Lim, Salakhutdinov, and Morency}]{liang2020towards} Paul~Pu Liang, Irene~Mengze Li, Emily Zheng, Yao~Chong Lim, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. 
 Towards debiasing sentence representations. 
 \emph{arXiv preprint arXiv:2007.08100}."
2405.10431,ma2023fairness,"[{Ma et~al.(2023)Ma, Zhang, Bian, Liu, Zhang, Zhao, Zhang, Fu, Hu, and Wu}]{ma2023fairness} Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2023.",Fairness-guided few-shot prompting for large language models.,Fairness-guided few-shot prompting for large language models.,,"[{Ma et~al.(2023)Ma, Zhang, Bian, Liu, Zhang, Zhao, Zhang, Fu, Hu, and Wu}]{ma2023fairness} Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2023. 
 Fairness-guided few-shot prompting for large language models. 
 \emph{arXiv preprint arXiv:2303.13217}."
2405.10431,nadeem2020stereoset,"[{Nadeem et~al.(2020)Nadeem, Bethke, and Reddy}]{nadeem2020stereoset} Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.",Stereoset: Measuring stereotypical bias in pretrained language models.,Stereoset: Measuring stereotypical bias in pretrained language models.,,"[{Nadeem et~al.(2020)Nadeem, Bethke, and Reddy}]{nadeem2020stereoset} Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. 
 Stereoset: Measuring stereotypical bias in pretrained language models. 
 \emph{arXiv preprint arXiv:2004.09456}."
2405.10431,thakur2023unveiling,[{Thakur(2023)}]{thakur2023unveiling} Vishesh Thakur. 2023.,Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications.,Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications.,,"[{Thakur(2023)}]{thakur2023unveiling} Vishesh Thakur. 2023. 
 Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications. 
 \emph{arXiv preprint arXiv:2307.09162}."
2405.10431,webster2020measuring,"[{Webster et~al.(2020)Webster, Wang, Tenney, Beutel, Pitler, Pavlick, Chen, Chi, and Petrov}]{webster2020measuring} Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed~Chi, and Slav Petrov. 2020.",Measuring and reducing gendered correlations in pre-trained models.,Measuring and reducing gendered correlations in pre-trained models.,,"[{Webster et~al.(2020)Webster, Wang, Tenney, Beutel, Pitler, Pavlick, Chen, Chi, and Petrov}]{webster2020measuring} Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed~Chi, and Slav Petrov. 2020. 
 Measuring and reducing gendered correlations in pre-trained models. 
 \emph{arXiv preprint arXiv:2010.06032}."
2405.10431,zmigrod2019counterfactual,"[{Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell}]{zmigrod2019counterfactual} Ran Zmigrod, Sabrina~J Mielke, Hanna Wallach, and Ryan Cotterell. 2019.",Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,,"[{Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell}]{zmigrod2019counterfactual} Ran Zmigrod, Sabrina~J Mielke, Hanna Wallach, and Ryan Cotterell. 2019. 
 Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. 
 \emph{arXiv preprint arXiv:1906.04571}."
2405.10548,automatic-circuit,"[{Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and Garriga-Alonso}]{automatic-circuit} Arthur Conmy, Augustine~N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri{\`a} Garriga-Alonso. 2023.",Towards automated circuit discovery for mechanistic interpretability.,Towards automated circuit discovery for mechanistic interpretability.,,"[{Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and Garriga-Alonso}]{automatic-circuit} Arthur Conmy, Augustine~N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri{\`a} Garriga-Alonso. 2023. 
 Towards automated circuit discovery for mechanistic interpretability. 
 \emph{arXiv preprint arXiv:2304.14997}."
2405.10548,marton2021efficient,"[{M{\'a}rton et~al.(2021)M{\'a}rton, Gagnon, Lajoie, and Rajan}]{marton2021efficient} Christian~David M{\'a}rton, L{\'e}o Gagnon, Guillaume Lajoie, and Kanaka Rajan. 2021.",Efficient and robust multi-task learning in the brain with modular latent primitives.,Efficient and robust multi-task learning in the brain with modular latent primitives.,,"[{M{\'a}rton et~al.(2021)M{\'a}rton, Gagnon, Lajoie, and Rajan}]{marton2021efficient} Christian~David M{\'a}rton, L{\'e}o Gagnon, Guillaume Lajoie, and Kanaka Rajan. 2021. 
 Efficient and robust multi-task learning in the brain with modular latent primitives. 
 \emph{arXiv preprint arXiv:2105.14108}."
2405.10548,pahune2023several,[{Pahune and Chandrasekharan(2023)}]{pahune2023several} Saurabh Pahune and Manoj Chandrasekharan. 2023.,Several categories of large language models (llms): A short survey.,Several categories of large language models (llms): A short survey.,,"[{Pahune and Chandrasekharan(2023)}]{pahune2023several} Saurabh Pahune and Manoj Chandrasekharan. 2023. 
 Several categories of large language models (llms): A short survey. 
 \emph{arXiv preprint arXiv:2307.10188}."
2405.10548,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2405.10548,interpretability-in-the-wild,"[{Wang et~al.(2022{\natexlab{a}})Wang, Variengien, Conmy, Shlegeris, and Steinhardt}]{interpretability-in-the-wild} Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022{\natexlab{a}}.",Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.,Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.,,"[{Wang et~al.(2022{\natexlab{a}})Wang, Variengien, Conmy, Shlegeris, and Steinhardt}]{interpretability-in-the-wild} Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022{\natexlab{a}}. 
 Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. 
 \emph{arXiv preprint arXiv:2211.00593}."
2405.10938,abnar2021exploring,"[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and   Sedghi]{abnar2021exploring} Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.",Exploring the limits of large scale pre-training.,Exploring the limits of large scale pre-training.,,"[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and   Sedghi]{abnar2021exploring} Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. 
 Exploring the limits of large scale pre-training. 
 \emph{arXiv preprint arXiv:2110.02095}, 2021."
2405.10938,almazrouei2023falcon,"[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,   Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic,   et~al.]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,   Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow,   Julien Launay, Quentin Malartic, et~al.",The falcon series of open language models.,The falcon series of open language models.,,"[Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,   Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic,   et~al.]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,   Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow,   Julien Launay, Quentin Malartic, et~al. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv:2311.16867}, 2023."
2405.10938,anwar2024foundational,"[Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana,   Jenner, Casper, Sourbut, et~al.]{anwar2024foundational} Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter   Hase, Ekdeep~Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut,   et~al.",Foundational challenges in assuring alignment and safety of large   language models.,Foundational challenges in assuring alignment and safety of large   language models.,,"[Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana,   Jenner, Casper, Sourbut, et~al.]{anwar2024foundational} Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter   Hase, Ekdeep~Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut,   et~al. 
 Foundational challenges in assuring alignment and safety of large   language models. 
 \emph{arXiv preprint arXiv:2404.09932}, 2024."
2405.10938,arora2023theory,[Arora and Goyal(2023)]{arora2023theory} Sanjeev Arora and Anirudh Goyal.,A theory for emergence of complex skills in language models.,A theory for emergence of complex skills in language models.,,"[Arora and Goyal(2023)]{arora2023theory} Sanjeev Arora and Anirudh Goyal. 
 A theory for emergence of complex skills in language models. 
 \emph{arXiv preprint arXiv:2307.15936}, 2023."
2405.10938,bahri2021explaining,"[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and   Sharma]{bahri2021explaining} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.",Explaining neural scaling laws.,Explaining neural scaling laws.,,"[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and   Sharma]{bahri2021explaining} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 
 Explaining neural scaling laws. 
 \emph{arXiv preprint arXiv:2102.06701}, 2021."
2405.10938,bai2023qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,   et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,   et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2405.10938,bi2024deepseek,"[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu,   et~al.]{bi2024deepseek} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,   Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al.",Deepseek llm: Scaling open-source language models with longtermism.,Deepseek llm: Scaling open-source language models with longtermism.,,"[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu,   et~al.]{bi2024deepseek} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,   Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al. 
 Deepseek llm: Scaling open-source language models with longtermism. 
 \emph{arXiv preprint arXiv:2401.02954}, 2024."
2405.10938,black2022gptneo,"[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,   He, Leahy, McDonell, Phang, et~al.]{black2022gptneo} Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence   Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.",Gpt-neox-20b: An open-source autoregressive language model.,Gpt-neox-20b: An open-source autoregressive language model.,,"[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,   He, Leahy, McDonell, Phang, et~al.]{black2022gptneo} Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence   Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al. 
 Gpt-neox-20b: An open-source autoregressive language model. 
 \emph{arXiv preprint arXiv:2204.06745}, 2022."
2405.10938,burnell2023revealing,"[Burnell et~al.(2023)Burnell, Hao, Conway, and   Orallo]{burnell2023revealing} Ryan Burnell, Han Hao, Andrew~RA Conway, and Jose~Hernandez Orallo.",Revealing the structure of language model capabilities.,Revealing the structure of language model capabilities.,,"[Burnell et~al.(2023)Burnell, Hao, Conway, and   Orallo]{burnell2023revealing} Ryan Burnell, Han Hao, Andrew~RA Conway, and Jose~Hernandez Orallo. 
 Revealing the structure of language model capabilities. 
 \emph{arXiv preprint arXiv:2306.10062}, 2023."
2405.10938,caballero2022broken,"[Caballero et~al.(2022)Caballero, Gupta, Rish, and   Krueger]{caballero2022broken} Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.",Broken neural scaling laws.,Broken neural scaling laws.,,"[Caballero et~al.(2022)Caballero, Gupta, Rish, and   Krueger]{caballero2022broken} Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. 
 Broken neural scaling laws. 
 \emph{arXiv preprint arXiv:2210.14891}, 2022."
2405.10938,chen2021humaneval,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen2021humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira   Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg   Brockman, et~al.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen2021humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira   Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg   Brockman, et~al. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}, 2021."
2405.10938,clark2018arc,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2405.10938,cobbe2021gsm8k,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,   Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz   Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,   et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,   Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz   Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,   et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2405.10938,du2024emergcaploss,"[Du et~al.(2024)Du, Zeng, Dong, and Tang]{du2024emergcaploss} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang.",Understanding emergent abilities of language models from the loss   perspective.,Understanding emergent abilities of language models from the loss   perspective.,,"[Du et~al.(2024)Du, Zeng, Dong, and Tang]{du2024emergcaploss} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. 
 Understanding emergent abilities of language models from the loss   perspective. 
 \emph{arXiv preprint arXiv:2403.15796}, 2024."
2405.10938,gadre2024language,"[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao,   Mercat, Fang, Li, Keh, et~al.]{gadre2024language} Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan,   Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick   Keh, et~al.",Language models scale reliably with over-training and on downstream   tasks.,Language models scale reliably with over-training and on downstream   tasks.,,"[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao,   Mercat, Fang, Li, Keh, et~al.]{gadre2024language} Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan,   Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick   Keh, et~al. 
 Language models scale reliably with over-training and on downstream   tasks. 
 \emph{arXiv preprint arXiv:2403.08540}, 2024."
2405.10938,ghorbani2021scalingnmt,"[Ghorbani et~al.(2021)Ghorbani, Firat, Freitag, Bapna, Krikun, Garcia,   Chelba, and Cherry]{ghorbani2021scalingnmt} Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun,   Xavier Garcia, Ciprian Chelba, and Colin Cherry.",Scaling laws for neural machine translation.,Scaling laws for neural machine translation.,,"[Ghorbani et~al.(2021)Ghorbani, Firat, Freitag, Bapna, Krikun, Garcia,   Chelba, and Cherry]{ghorbani2021scalingnmt} Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun,   Xavier Garcia, Ciprian Chelba, and Colin Cherry. 
 Scaling laws for neural machine translation. 
 \emph{arXiv preprint arXiv:2109.07740}, 2021."
2405.10938,guo2024deepseekcoder,"[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li,   et~al.]{guo2024deepseekcoder} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting   Chen, Xiao Bi, Y~Wu, YK~Li, et~al.",Deepseek-coder: When the large language model meets programming--the   rise of code intelligence.,Deepseek-coder: When the large language model meets programming--the   rise of code intelligence.,,"[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li,   et~al.]{guo2024deepseekcoder} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting   Chen, Xiao Bi, Y~Wu, YK~Li, et~al. 
 Deepseek-coder: When the large language model meets programming--the   rise of code intelligence. 
 \emph{arXiv preprint arXiv:2401.14196}, 2024."
2405.10938,hendrycks2020mmlu,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,   and Steinhardt]{hendrycks2020mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn   Song, and Jacob Steinhardt.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,   and Steinhardt]{hendrycks2020mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn   Song, and Jacob Steinhardt. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}, 2020."
2405.10938,henighan2020scalingauto,"[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,   Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scalingauto} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob   Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,   Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scalingauto} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob   Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}, 2020."
2405.10938,hernandez2021scalingtransfer,"[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and   McCandlish]{hernandez2021scalingtransfer} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.",Scaling laws for transfer.,Scaling laws for transfer.,,"[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and   McCandlish]{hernandez2021scalingtransfer} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 
 Scaling laws for transfer. 
 \emph{arXiv preprint arXiv:2102.01293}, 2021."
2405.10938,hestness2017deep,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,   Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,   Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,   Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,   Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}, 2017."
2405.10938,hoffmann2022chinchila,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022chinchila} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022chinchila} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2405.10938,huang2024compression,"[Huang et~al.(2024)Huang, Zhang, Shan, and He]{huang2024compression} Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He.",Compression represents intelligence linearly.,Compression represents intelligence linearly.,,"[Huang et~al.(2024)Huang, Zhang, Shan, and He]{huang2024compression} Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. 
 Compression represents intelligence linearly. 
 \emph{arXiv preprint arXiv:2404.09937}, 2024."
2405.10938,ilic2023unveiling,[Ili{\'c}(2023)]{ilic2023unveiling} David Ili{\'c}.,Unveiling the general intelligence factor in language models: A   psychometric approach.,Unveiling the general intelligence factor in language models: A   psychometric approach.,,"[Ili{\'c}(2023)]{ilic2023unveiling} David Ili{\'c}. 
 Unveiling the general intelligence factor in language models: A   psychometric approach. 
 \emph{arXiv preprint arXiv:2310.11616}, 2023."
2405.10938,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2405.10938,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,   Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford,   Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2405.10938,kaplan2020scalinglaw,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei]{kaplan2020scalinglaw} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei]{kaplan2020scalinglaw} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2405.10938,li2023starcoder,"[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov,   Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,   Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.",Starcoder: may the source be with you!,Starcoder: may the source be with you!,,"[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov,   Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,   Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 
 Starcoder: may the source be with you! 
 \emph{arXiv preprint arXiv:2305.06161}, 2023{\natexlab{a}}."
2405.10938,li2023phi,"[Li et~al.(2023{\natexlab{c}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar,   and Lee]{li2023phi} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[Li et~al.(2023{\natexlab{c}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar,   and Lee]{li2023phi} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}, 2023{\natexlab{c}}."
2405.10938,liang2022helm,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,   Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022helm} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,   Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,   et~al.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,   Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022helm} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,   Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,   et~al. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}, 2022."
2405.10938,lin2021truthfulqa,"[Lin et~al.(2021{\natexlab{a}})Lin, Hilton, and   Evans]{lin2021truthfulqa} Stephanie Lin, Jacob Hilton, and Owain Evans.",Truthfulqa: Measuring how models mimic human falsehoods.,Truthfulqa: Measuring how models mimic human falsehoods.,,"[Lin et~al.(2021{\natexlab{a}})Lin, Hilton, and   Evans]{lin2021truthfulqa} Stephanie Lin, Jacob Hilton, and Owain Evans. 
 Truthfulqa: Measuring how models mimic human falsehoods. 
 \emph{arXiv preprint arXiv:2109.07958}, 2021{\natexlab{a}}."
2405.10938,lin2021xglm,"[Lin et~al.(2021{\natexlab{b}})Lin, Mihaylov, Artetxe, Wang, Chen,   Simig, Ott, Goyal, Bhosale, Du, et~al.]{lin2021xglm} Xi~Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,   Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et~al.",Few-shot learning with multilingual language models.,Few-shot learning with multilingual language models.,,"[Lin et~al.(2021{\natexlab{b}})Lin, Mihaylov, Artetxe, Wang, Chen,   Simig, Ott, Goyal, Bhosale, Du, et~al.]{lin2021xglm} Xi~Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,   Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et~al. 
 Few-shot learning with multilingual language models. 
 \emph{arXiv preprint arXiv:2112.10668}, 2021{\natexlab{b}}."
2405.10938,liu2021question,"[Liu et~al.(2021)Liu, Lee, Jia, and Liang]{liu2021question} Nelson~F Liu, Tony Lee, Robin Jia, and Percy Liang.",Do question answering modeling improvements hold across benchmarks?,Do question answering modeling improvements hold across benchmarks?,,"[Liu et~al.(2021)Liu, Lee, Jia, and Liang]{liu2021question} Nelson~F Liu, Tony Lee, Robin Jia, and Percy Liang. 
 Do question answering modeling improvements hold across benchmarks? 
 \emph{arXiv preprint arXiv:2102.01065}, 2021."
2405.10938,lozhkov2024starcoder2,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi,   Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder2} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel   Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang   Wei, et~al.",Starcoder 2 and the stack v2: The next generation.,Starcoder 2 and the stack v2: The next generation.,,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi,   Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder2} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel   Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang   Wei, et~al. 
 Starcoder 2 and the stack v2: The next generation. 
 \emph{arXiv preprint arXiv:2402.19173}, 2024."
2405.10938,lu2023emergincontex,"[Lu et~al.(2023)Lu, Bigoulaeva, Sachdeva, Madabushi, and   Gurevych]{lu2023emergincontex} Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish~Tayyar Madabushi, and   Iryna Gurevych.",Are emergent abilities in large language models just in-context   learning?,Are emergent abilities in large language models just in-context   learning?,,"[Lu et~al.(2023)Lu, Bigoulaeva, Sachdeva, Madabushi, and   Gurevych]{lu2023emergincontex} Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish~Tayyar Madabushi, and   Iryna Gurevych. 
 Are emergent abilities in large language models just in-context   learning? 
 \emph{arXiv preprint arXiv:2309.01809}, 2023."
2405.10938,ma2024agentboard,"[Ma et~al.(2024)Ma, Zhang, Zhu, Yang, Yang, Jin, Lan, Kong, and   He]{ma2024agentboard} Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin,   Zhenzhong Lan, Lingpeng Kong, and Junxian He.",Agentboard: An analytical evaluation board of multi-turn llm agents.,Agentboard: An analytical evaluation board of multi-turn llm agents.,,"[Ma et~al.(2024)Ma, Zhang, Zhu, Yang, Yang, Jin, Lan, Kong, and   He]{ma2024agentboard} Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin,   Zhenzhong Lan, Lingpeng Kong, and Junxian He. 
 Agentboard: An analytical evaluation board of multi-turn llm agents. 
 \emph{arXiv preprint arXiv:2401.13178}, 2024."
2405.10938,mialon2023gaia,"[Mialon et~al.(2023)Mialon, Fourrier, Swift, Wolf, LeCun, and   Scialom]{mialon2023gaia} Gr{\'e}goire Mialon, Cl{\'e}mentine Fourrier, Craig Swift, Thomas Wolf, Yann   LeCun, and Thomas Scialom.",Gaia: a benchmark for general ai assistants.,Gaia: a benchmark for general ai assistants.,,"[Mialon et~al.(2023)Mialon, Fourrier, Swift, Wolf, LeCun, and   Scialom]{mialon2023gaia} Gr{\'e}goire Mialon, Cl{\'e}mentine Fourrier, Craig Swift, Thomas Wolf, Yann   LeCun, and Thomas Scialom. 
 Gaia: a benchmark for general ai assistants. 
 \emph{arXiv preprint arXiv:2311.12983}, 2023."
2405.10938,muennighoff2022xwinograd,"[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts,   Biderman, Scao, Bari, Shen, Yong, Schoelkopf,   et~al.]{muennighoff2022xwinograd} Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella   Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey   Schoelkopf, et~al.",Crosslingual generalization through multitask finetuning.,Crosslingual generalization through multitask finetuning.,,"[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts,   Biderman, Scao, Bari, Shen, Yong, Schoelkopf,   et~al.]{muennighoff2022xwinograd} Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella   Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey   Schoelkopf, et~al. 
 Crosslingual generalization through multitask finetuning. 
 \emph{arXiv preprint arXiv:2211.01786}, 2022."
2405.10938,owen2024predictable,[Owen(2024)]{owen2024predictable} David Owen.,How predictable is language model benchmark performance?,How predictable is language model benchmark performance?,,"[Owen(2024)]{owen2024predictable} David Owen. 
 How predictable is language model benchmark performance? 
 \emph{arXiv preprint arXiv:2401.04757}, 2024."
2405.10938,perlitz2023efficient,"[Perlitz et~al.(2023)Perlitz, Bandel, Gera, Arviv, Ein-Dor, Shnarch,   Slonim, Shmueli-Scheuer, and Choshen]{perlitz2023efficient} Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal   Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.",Efficient benchmarking (of language models).,Efficient benchmarking (of language models).,,"[Perlitz et~al.(2023)Perlitz, Bandel, Gera, Arviv, Ein-Dor, Shnarch,   Slonim, Shmueli-Scheuer, and Choshen]{perlitz2023efficient} Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal   Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. 
 Efficient benchmarking (of language models). 
 \emph{arXiv preprint arXiv:2308.11696}, 2023."
2405.10938,polo2024tinybenchmarks,"[Polo et~al.(2024)Polo, Weber, Choshen, Sun, Xu, and   Yurochkin]{polo2024tinybenchmarks} Felipe~Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and   Mikhail Yurochkin.",tinybenchmarks: evaluating llms with fewer examples.,tinybenchmarks: evaluating llms with fewer examples.,,"[Polo et~al.(2024)Polo, Weber, Choshen, Sun, Xu, and   Yurochkin]{polo2024tinybenchmarks} Felipe~Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and   Mikhail Yurochkin. 
 tinybenchmarks: evaluating llms with fewer examples. 
 \emph{arXiv preprint arXiv:2402.14992}, 2024."
2405.10938,recht2018cifar,"[Recht et~al.(2018)Recht, Roelofs, Schmidt, and   Shankar]{recht2018cifar} Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.",Do cifar-10 classifiers generalize to cifar-10?,Do cifar-10 classifiers generalize to cifar-10?,,"[Recht et~al.(2018)Recht, Roelofs, Schmidt, and   Shankar]{recht2018cifar} Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 
 Do cifar-10 classifiers generalize to cifar-10? 
 \emph{arXiv preprint arXiv:1806.00451}, 2018."
2405.10938,roziere2023codellama,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi,   Liu, Remez, Rapin, et~al.]{roziere2023codellama} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,   Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin,   et~al.",Code llama: Open foundation models for code.,Code llama: Open foundation models for code.,,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi,   Liu, Remez, Rapin, et~al.]{roziere2023codellama} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,   Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin,   et~al. 
 Code llama: Open foundation models for code. 
 \emph{arXiv preprint arXiv:2308.12950}, 2023."
2405.10938,srivastava2022bigbench,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,   Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022bigbench} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar   Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}   Garriga-Alonso, et~al.",Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models.,,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,   Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022bigbench} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar   Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}   Garriga-Alonso, et~al. 
 Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}, 2022."
2405.10938,suzgun2022bbh,"[Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung,   Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022bbh} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay,   Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.",Challenging big-bench tasks and whether chain-of-thought can solve   them.,Challenging big-bench tasks and whether chain-of-thought can solve   them.,,"[Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung,   Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022bbh} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay,   Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 
 Challenging big-bench tasks and whether chain-of-thought can solve   them. 
 \emph{arXiv preprint arXiv:2210.09261}, 2022."
2405.10938,gemmateam2024gemma,"[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,   Sifre, Rivi{\`e}re, Kale, Love, et~al.]{gemmateam2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,   Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,   Juliette Love, et~al.",Gemma: Open models based on gemini research and technology.,Gemma: Open models based on gemini research and technology.,,"[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,   Sifre, Rivi{\`e}re, Kale, Love, et~al.]{gemmateam2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,   Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,   Juliette Love, et~al. 
 Gemma: Open models based on gemini research and technology. 
 \emph{arXiv preprint arXiv:2403.08295}, 2024."
2405.10938,touvron2023llama,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.10938,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.10938,le2023bloom,"[Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c},   Hesslow, Castagn{\'e}, Luccioni, Yvon, et~al.]{le2023bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie   Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha   Luccioni, Fran{\c{c}}ois Yvon, et~al.",Bloom: A 176b-parameter open-access multilingual language model.,Bloom: A 176b-parameter open-access multilingual language model.,,"[Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c},   Hesslow, Castagn{\'e}, Luccioni, Yvon, et~al.]{le2023bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie   Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha   Luccioni, Fran{\c{c}}ois Yvon, et~al. 
 Bloom: A 176b-parameter open-access multilingual language model. 
 \emph{arXiv preprint arXiv:2211.05100}, 2022."
2405.10938,xia2022training,"[Xia et~al.(2022)Xia, Artetxe, Zhou, Lin, Pasunuru, Chen, Zettlemoyer,   and Stoyanov]{xia2022training} Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi~Victoria Lin, Ramakanth   Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.",Training trajectories of language models across scales.,Training trajectories of language models across scales.,,"[Xia et~al.(2022)Xia, Artetxe, Zhou, Lin, Pasunuru, Chen, Zettlemoyer,   and Stoyanov]{xia2022training} Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi~Victoria Lin, Ramakanth   Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. 
 Training trajectories of language models across scales. 
 \emph{arXiv preprint arXiv:2212.09803}, 2022."
2405.10938,xu2023wizardlm,"[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and   Jiang]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang   Tao, and Daxin Jiang.",Wizardlm: Empowering large language models to follow complex   instructions.,Wizardlm: Empowering large language models to follow complex   instructions.,,"[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and   Jiang]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang   Tao, and Daxin Jiang. 
 Wizardlm: Empowering large language models to follow complex   instructions. 
 \emph{arXiv preprint arXiv:2304.12244}, 2023."
2405.10938,young2024yi,"[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen,   Chang, et~al.]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li,   Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen,   Chang, et~al.]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li,   Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}, 2024."
2405.10938,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2405.10938,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2405.11162,guo2024deepseek,"[{Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li et~al.}]{guo2024deepseek} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y~Wu, YK~Li, et~al. 2024.",Deepseek-coder: When the large language model meets programming--the rise of code intelligence.,Deepseek-coder: When the large language model meets programming--the rise of code intelligence.,,"[{Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li et~al.}]{guo2024deepseek} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y~Wu, YK~Li, et~al. 2024. 
 Deepseek-coder: When the large language model meets programming--the rise of code intelligence. 
 \emph{arXiv preprint arXiv:2401.14196}."
2405.11162,hwang2019comprehensive,"[{Hwang et~al.(2019{\natexlab{a}})Hwang, Yim, Park, and Seo}]{hwang2019comprehensive} Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. 2019{\natexlab{a}}.",A comprehensive exploration on wikisql with table-aware word contextualization.,A comprehensive exploration on wikisql with table-aware word contextualization.,,"[{Hwang et~al.(2019{\natexlab{a}})Hwang, Yim, Park, and Seo}]{hwang2019comprehensive} Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. 2019{\natexlab{a}}. 
 A comprehensive exploration on wikisql with table-aware word contextualization. 
 \emph{arXiv preprint arXiv:1902.01069}."
2405.11162,lee2024learning,"[{Lee et~al.(2024{\natexlab{b}})Lee, Kim, Yu, Rossi, and Chen}]{lee2024learning} Younghun Lee, Sungchul Kim, Tong Yu, Ryan~A Rossi, and Xiang Chen. 2024{\natexlab{b}}.",Learning to reduce: Optimal representations of structured data in prompting large language models.,Learning to reduce: Optimal representations of structured data in prompting large language models.,,"[{Lee et~al.(2024{\natexlab{b}})Lee, Kim, Yu, Rossi, and Chen}]{lee2024learning} Younghun Lee, Sungchul Kim, Tong Yu, Ryan~A Rossi, and Xiang Chen. 2024{\natexlab{b}}. 
 Learning to reduce: Optimal representations of structured data in prompting large language models. 
 \emph{arXiv preprint arXiv:2402.14195}."
2405.11162,li2023starcoder,"[{Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim et~al.}]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 2023.",Starcoder: may the source be with you!,Starcoder: may the source be with you!,,"[{Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim et~al.}]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 2023. 
 Starcoder: may the source be with you! 
 \emph{arXiv preprint arXiv:2305.06161}."
2405.11162,roziere2023code,"[{Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin et~al.}]{roziere2023code} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al. 2023.",Code llama: Open foundation models for code.,Code llama: Open foundation models for code.,,"[{Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin et~al.}]{roziere2023code} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al. 2023. 
 Code llama: Open foundation models for code. 
 \emph{arXiv preprint arXiv:2308.12950}."
2405.11465,bai2023transformers,"[{Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei}]{bai2023transformers} Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. 2023.",Transformers as statisticians: Provable in-context learning with in-context algorithm selection.,Transformers as statisticians: Provable in-context learning with in-context algorithm selection.,,"[{Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei}]{bai2023transformers} Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. 2023. 
 Transformers as statisticians: Provable in-context learning with in-context algorithm selection. 
 \emph{arXiv preprint arXiv:2306.04637}."
2405.11465,dai2022can,"[{Dai et~al.(2022)Dai, Sun, Dong, Hao, Sui, and Wei}]{dai2022can} Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022.",Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers.,Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers.,,"[{Dai et~al.(2022)Dai, Sun, Dong, Hao, Sui, and Wei}]{dai2022can} Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. 
 Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. 
 \emph{arXiv preprint arXiv:2212.10559}."
2405.11465,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022.",A survey for in-context learning.,A survey for in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2405.11465,gupta2023coverage,"[{Gupta et~al.(2023)Gupta, Singh, and Gardner}]{gupta2023coverage} Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2023.",Coverage-based example selection for in-context learning.,Coverage-based example selection for in-context learning.,,"[{Gupta et~al.(2023)Gupta, Singh, and Gardner}]{gupta2023coverage} Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2023. 
 Coverage-based example selection for in-context learning. 
 \emph{arXiv preprint arXiv:2305.14907}."
2405.11465,kim2022self,"[{Kim et~al.(2022)Kim, Cho, Kim, Kim, Yoo, and Lee}]{kim2022self} Hyuhng~Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang~Min Yoo, and Sang-goo Lee. 2022.",Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.,Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.,,"[{Kim et~al.(2022)Kim, Cho, Kim, Kim, Yoo, and Lee}]{kim2022self} Hyuhng~Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang~Min Yoo, and Sang-goo Lee. 2022. 
 Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. 
 \emph{arXiv preprint arXiv:2206.08082}."
2405.11465,nie2022improving,"[{Nie et~al.(2022)Nie, Chen, Zhang, and Cheng}]{nie2022improving} Feng Nie, Meixi Chen, Zhirui Zhang, and Xu~Cheng. 2022.",Improving few-shot performance of language models via nearest neighbor calibration.,Improving few-shot performance of language models via nearest neighbor calibration.,,"[{Nie et~al.(2022)Nie, Chen, Zhang, and Cheng}]{nie2022improving} Feng Nie, Meixi Chen, Zhirui Zhang, and Xu~Cheng. 2022. 
 Improving few-shot performance of language models via nearest neighbor calibration. 
 \emph{arXiv preprint arXiv:2212.02216}."
2405.11465,sun2023short,[{Sun(2023)}]{sun2023short} Zhongxiang Sun. 2023.,A short survey of viewing large language models in legal aspect.,A short survey of viewing large language models in legal aspect.,,"[{Sun(2023)}]{sun2023short} Zhongxiang Sun. 2023. 
 A short survey of viewing large language models in legal aspect. 
 \emph{arXiv preprint arXiv:2303.09136}."
2405.11465,wang2023learning,"[{Wang et~al.(2023)Wang, Yang, and Wei}]{wang2023learning} Liang Wang, Nan Yang, and Furu Wei. 2023.",Learning to retrieve in-context examples for large language models.,Learning to retrieve in-context examples for large language models.,,"[{Wang et~al.(2023)Wang, Yang, and Wei}]{wang2023learning} Liang Wang, Nan Yang, and Furu Wei. 2023. 
 Learning to retrieve in-context examples for large language models. 
 \emph{arXiv preprint arXiv:2307.07164}."
2405.11465,yang2022dataset,"[{Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li}]{yang2022dataset} Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2022.",Dataset pruning: Reducing training data by examining generalization influence.,Dataset pruning: Reducing training data by examining generalization influence.,,"[{Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li}]{yang2022dataset} Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2022. 
 Dataset pruning: Reducing training data by examining generalization influence. 
 \emph{arXiv preprint arXiv:2205.09329}."
2405.11465,zhang2023trained,"[{Zhang et~al.(2023)Zhang, Frei, and Bartlett}]{zhang2023trained} Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett. 2023.",Trained transformers learn linear models in-context.,Trained transformers learn linear models in-context.,,"[{Zhang et~al.(2023)Zhang, Frei, and Bartlett}]{zhang2023trained} Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett. 2023. 
 Trained transformers learn linear models in-context. 
 \emph{arXiv preprint arXiv:2306.09927}."
2405.12833,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2405.12833,Radgraphjain2021radgraph,"[Jain et~al.(2021)Jain, Agrawal, Saporta, Truong, Duong, Bui, Chambon, Zhang, Lungren, Ng, et~al.]{Radgraphjain2021radgraph} Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven~QH Truong, Du~Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew~P Lungren, Andrew~Y Ng, et~al.",Radgraph: Extracting clinical entities and relations from radiology reports.,Radgraph: Extracting clinical entities and relations from radiology reports.,,"[Jain et~al.(2021)Jain, Agrawal, Saporta, Truong, Duong, Bui, Chambon, Zhang, Lungren, Ng, et~al.]{Radgraphjain2021radgraph} Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven~QH Truong, Du~Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew~P Lungren, Andrew~Y Ng, et~al. 
 Radgraph: Extracting clinical entities and relations from radiology reports. 
 \emph{arXiv preprint arXiv:2106.14463}, 2021."
2405.12833,MIMIC-CXR-JPGjohnson2019mimic-2,"[Johnson et~al.(2019{\natexlab{b}})Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{MIMIC-CXR-JPGjohnson2019mimic-2} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.",,"[Johnson et~al.(2019{\natexlab{b}})Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{MIMIC-CXR-JPGjohnson2019mimic-2} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng. 
 Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. 
 \emph{arXiv preprint arXiv:1901.07042}, 2019{\natexlab{b}}."
2405.12833,kingma2013auto,[Kingma and Welling(2013)]{kingma2013auto} Diederik~P Kingma and Max Welling.,Auto-encoding variational bayes.,Auto-encoding variational bayes.,,"[Kingma and Welling(2013)]{kingma2013auto} Diederik~P Kingma and Max Welling. 
 Auto-encoding variational bayes. 
 \emph{arXiv preprint arXiv:1312.6114}, 2013."
2405.12833,COV-CTRli2020auxiliary,"[Li et~al.(2020)Li, Wang, Chang, and Liang]{COV-CTRli2020auxiliary} Mingjie Li, Fuyu Wang, Xiaojun Chang, and Xiaodan Liang.",Auxiliary signal-guided knowledge encoder-decoder for medical report generation.,Auxiliary signal-guided knowledge encoder-decoder for medical report generation.,,"[Li et~al.(2020)Li, Wang, Chang, and Liang]{COV-CTRli2020auxiliary} Mingjie Li, Fuyu Wang, Xiaojun Chang, and Xiaodan Liang. 
 Auxiliary signal-guided knowledge encoder-decoder for medical report generation. 
 \emph{arXiv preprint arXiv:2006.03744}, 2020."
2405.12833,liu2023systematic,"[Liu et~al.(2023{\natexlab{a}})Liu, Tian, and Song]{liu2023systematic} Chang Liu, Yuanhe Tian, and Yan Song.",A systematic review of deep learning-based research on radiology report generation.,A systematic review of deep learning-based research on radiology report generation.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Tian, and Song]{liu2023systematic} Chang Liu, Yuanhe Tian, and Yan Song. 
 A systematic review of deep learning-based research on radiology report generation. 
 \emph{arXiv preprint arXiv:2311.14199}, 2023{\natexlab{a}}."
2405.12833,saab2024capabilities,"[Saab et~al.(2024)Saab, Tu, Weng, Tanno, Stutz, Wulczyn, Zhang, Strother, Park, Vedadi, et~al.]{saab2024capabilities} Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et~al.",Capabilities of gemini models in medicine.,Capabilities of gemini models in medicine.,,"[Saab et~al.(2024)Saab, Tu, Weng, Tanno, Stutz, Wulczyn, Zhang, Strother, Park, Vedadi, et~al.]{saab2024capabilities} Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et~al. 
 Capabilities of gemini models in medicine. 
 \emph{arXiv preprint arXiv:2404.18416}, 2024."
2405.12833,VGGsimonyan2014very,[Simonyan and Zisserman(2014)]{VGGsimonyan2014very} Karen Simonyan and Andrew Zisserman.,Very deep convolutional networks for large-scale image recognition.,Very deep convolutional networks for large-scale image recognition.,,"[Simonyan and Zisserman(2014)]{VGGsimonyan2014very} Karen Simonyan and Andrew Zisserman. 
 Very deep convolutional networks for large-scale image recognition. 
 \emph{arXiv preprint arXiv:1409.1556}, 2014."
2405.12833,singhal2023towards,"[Singhal et~al.(2023)Singhal, Tu, Gottweis, Sayres, Wulczyn, Hou, Clark, Pfohl, Cole-Lewis, Neal, et~al.]{singhal2023towards} Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le~Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et~al.",Towards expert-level medical question answering with large language models.,Towards expert-level medical question answering with large language models.,,"[Singhal et~al.(2023)Singhal, Tu, Gottweis, Sayres, Wulczyn, Hou, Clark, Pfohl, Cole-Lewis, Neal, et~al.]{singhal2023towards} Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le~Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et~al. 
 Towards expert-level medical question answering with large language models. 
 \emph{arXiv preprint arXiv:2305.09617}, 2023."
2405.12833,Rotatesun2019rotate,"[Sun et~al.(2019{\natexlab{b}})Sun, Deng, Nie, and Tang]{Rotatesun2019rotate} Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang.",Rotate: Knowledge graph embedding by relational rotation in complex space.,Rotate: Knowledge graph embedding by relational rotation in complex space.,,"[Sun et~al.(2019{\natexlab{b}})Sun, Deng, Nie, and Tang]{Rotatesun2019rotate} Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 
 Rotate: Knowledge graph embedding by relational rotation in complex space. 
 \emph{arXiv preprint arXiv:1902.10197}, 2019{\natexlab{b}}."
2405.12833,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2405.12833,54wang2023chatcad,"[Wang et~al.(2023{\natexlab{a}})Wang, Zhao, Ouyang, Wang, and Shen]{54wang2023chatcad} Sheng Wang, Zihao Zhao, Xi~Ouyang, Qian Wang, and Dinggang Shen.",Chatcad: Interactive computer-aided diagnosis on medical image using large language models.,Chatcad: Interactive computer-aided diagnosis on medical image using large language models.,,"[Wang et~al.(2023{\natexlab{a}})Wang, Zhao, Ouyang, Wang, and Shen]{54wang2023chatcad} Sheng Wang, Zihao Zhao, Xi~Ouyang, Qian Wang, and Dinggang Shen. 
 Chatcad: Interactive computer-aided diagnosis on medical image using large language models. 
 \emph{arXiv preprint arXiv:2302.07257}, 2023{\natexlab{a}}."
2405.12833,multimoda2yan2023multimodal,"[Yan et~al.(2023)Yan, Zhang, Zhou, He, Li, and Sun]{multimoda2yan2023multimodal} Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and Lichao Sun.",Multimodal chatgpt for medical applications: an experimental study of gpt-4v.,Multimodal chatgpt for medical applications: an experimental study of gpt-4v.,,"[Yan et~al.(2023)Yan, Zhang, Zhou, He, Li, and Sun]{multimoda2yan2023multimodal} Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and Lichao Sun. 
 Multimodal chatgpt for medical applications: an experimental study of gpt-4v. 
 \emph{arXiv preprint arXiv:2310.19061}, 2023."
2405.13017,goldberg2019assessing,[{Goldberg(2019)}]{goldberg2019assessing} Yoav Goldberg. 2019.,Assessing bert's syntactic abilities.,Assessing bert's syntactic abilities.,,"[{Goldberg(2019)}]{goldberg2019assessing} Yoav Goldberg. 2019. 
 Assessing bert's syntactic abilities. 
 \emph{arXiv preprint arXiv:1901.05287}."
2405.13017,liu2019roberta,"[{Liu et~al.(2019)Liu, Ott, Goyal et~al.}]{liu2019roberta} Yinhan Liu, Myle Ott, Naman Goyal, et~al. 2019.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[{Liu et~al.(2019)Liu, Ott, Goyal et~al.}]{liu2019roberta} Yinhan Liu, Myle Ott, Naman Goyal, et~al. 2019. 
 Roberta: A robustly optimized bert pretraining approach. 
 \emph{arXiv preprint arXiv:1907.11692}."
2405.13017,mishra2022tweetnerd,"[{Mishra et~al.(2022)Mishra, Saini, Makki et~al.}]{mishra2022tweetnerd} Shubhanshu Mishra, Aman Saini, Raheleh Makki, et~al. 2022.",Tweetnerd--end to end entity linking benchmark for tweets.,Tweetnerd--end to end entity linking benchmark for tweets.,,"[{Mishra et~al.(2022)Mishra, Saini, Makki et~al.}]{mishra2022tweetnerd} Shubhanshu Mishra, Aman Saini, Raheleh Makki, et~al. 2022. 
 Tweetnerd--end to end entity linking benchmark for tweets. 
 \emph{arXiv preprint arXiv:2210.08129}."
2405.13017,AudioPaLM,"[{Paul~K. et~al.(2023)Paul~K., Chulayuth, Duc et~al.}]{AudioPaLM} Rubenstein Paul~K., Asawaroengchai Chulayuth, Dung~Nguyen Duc, et~al. 2023.",Audiopalm: A large language model that can speak and listen.,Audiopalm: A large language model that can speak and listen.,,"[{Paul~K. et~al.(2023)Paul~K., Chulayuth, Duc et~al.}]{AudioPaLM} Rubenstein Paul~K., Asawaroengchai Chulayuth, Dung~Nguyen Duc, et~al. 2023. 
 Audiopalm: A large language model that can speak and listen. 
 \emph{arXiv preprint arXiv:2306.12925}."
2405.13017,PaLM2,"[{Rohan et~al.(2023)Rohan, Andrew~M., Orhan et~al.}]{PaLM2} Anil Rohan, Dai Andrew~M., Firat Orhan, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Rohan et~al.(2023)Rohan, Andrew~M., Orhan et~al.}]{PaLM2} Anil Rohan, Dai Andrew~M., Firat Orhan, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2405.13075,sssd,[\protect\citeauthoryear{Alcaraz and Strodthoff}{2022}]{sssd} Juan Miguel~Lopez Alcaraz and Nils Strodthoff.,Diffusion-based time series imputation and forecasting with structured state space models.,Diffusion-based time series imputation and forecasting with structured state space models.,,"[\protect\citeauthoryear{Alcaraz and Strodthoff}{2022}]{sssd} Juan Miguel~Lopez Alcaraz and Nils Strodthoff. 
 Diffusion-based time series imputation and forecasting with structured state space models. 
 {\em arXiv preprint arXiv:2208.09399}, 2022."
2405.13075,tcn,"[\protect\citeauthoryear{Bai \bgroup \em et al.\egroup }{2018}]{tcn} Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.",An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.,An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.,,"[\protect\citeauthoryear{Bai \bgroup \em et al.\egroup }{2018}]{tcn} Shaojie Bai, J~Zico Kolter, and Vladlen Koltun. 
 An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. 
 {\em arXiv preprint arXiv:1803.01271}, 2018."
2405.13075,rwkv,"[\protect\citeauthoryear{Peng \bgroup \em et al.\egroup }{2023}]{rwkv} Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.",Rwkv: Reinventing rnns for the transformer era.,Rwkv: Reinventing rnns for the transformer era.,,"[\protect\citeauthoryear{Peng \bgroup \em et al.\egroup }{2023}]{rwkv} Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al. 
 Rwkv: Reinventing rnns for the transformer era. 
 {\em arXiv preprint arXiv:2305.13048}, 2023."
2405.13075,NARDIff,[\protect\citeauthoryear{Shen and Kwok}{2023}]{NARDIff} Lifeng Shen and James Kwok.,Non-autoregressive conditional diffusion models for time series prediction.,Non-autoregressive conditional diffusion models for time series prediction.,,"[\protect\citeauthoryear{Shen and Kwok}{2023}]{NARDIff} Lifeng Shen and James Kwok. 
 Non-autoregressive conditional diffusion models for time series prediction. 
 {\em arXiv preprint arXiv:2306.05043}, 2023."
2405.13075,song2020score,"[\protect\citeauthoryear{Song \bgroup \em et al.\egroup }{2020}]{song2020score} Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.",Score-based generative modeling through stochastic differential equations.,Score-based generative modeling through stochastic differential equations.,,"[\protect\citeauthoryear{Song \bgroup \em et al.\egroup }{2020}]{song2020score} Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 
 Score-based generative modeling through stochastic differential equations. 
 {\em arXiv preprint arXiv:2011.13456}, 2020."
2405.13075,aft,"[\protect\citeauthoryear{Zhai \bgroup \em et al.\egroup }{2021}]{aft} Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.",An attention free transformer.,An attention free transformer.,,"[\protect\citeauthoryear{Zhai \bgroup \em et al.\egroup }{2021}]{aft} Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 
 An attention free transformer. 
 {\em arXiv preprint arXiv:2105.14103}, 2021."
2405.1403,chuang2023debiasing,"[Chuang et~al.(2023)Chuang, Jampani, Li, Torralba, and Jegelka]{chuang2023debiasing} Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka.",Debiasing vision-language models via biased prompts.,Debiasing vision-language models via biased prompts.,,"[Chuang et~al.(2023)Chuang, Jampani, Li, Torralba, and Jegelka]{chuang2023debiasing} Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. 
 Debiasing vision-language models via biased prompts. 
 \emph{arXiv preprint arXiv:2302.00070}, 2023."
2405.1403,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2405.1403,dosovitskiy2020image,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.",An image is worth 16x16 words: Transformers for image recognition at scale.,An image is worth 16x16 words: Transformers for image recognition at scale.,,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al. 
 An image is worth 16x16 words: Transformers for image recognition at scale. 
 \emph{arXiv preprint arXiv:2010.11929}, 2020."
2405.1403,kingma2014adam,[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2405.1403,kirichenko2022last,"[Kirichenko et~al.(2022)Kirichenko, Izmailov, and Wilson]{kirichenko2022last} Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.",Last layer re-training is sufficient for robustness to spurious correlations.,Last layer re-training is sufficient for robustness to spurious correlations.,,"[Kirichenko et~al.(2022)Kirichenko, Izmailov, and Wilson]{kirichenko2022last} Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson. 
 Last layer re-training is sufficient for robustness to spurious correlations. 
 \emph{arXiv preprint arXiv:2204.02937}, 2022."
2405.1403,niven2019probing,[Niven and Kao(2019)]{niven2019probing} Timothy Niven and Hung-Yu Kao.,Probing neural network comprehension of natural language arguments.,Probing neural network comprehension of natural language arguments.,,"[Niven and Kao(2019)]{niven2019probing} Timothy Niven and Hung-Yu Kao. 
 Probing neural network comprehension of natural language arguments. 
 \emph{arXiv preprint arXiv:1907.07355}, 2019."
2405.1403,sagawa2019distributionally,"[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and Liang]{sagawa2019distributionally} Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.",Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.,Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.,,"[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and Liang]{sagawa2019distributionally} Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang. 
 Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. 
 \emph{arXiv preprint arXiv:1911.08731}, 2019."
2405.1403,schuhmann2021laion,"[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion} Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.",Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.,Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.,,"[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion} Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 
 Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 
 \emph{arXiv preprint arXiv:2111.02114}, 2021."
2405.1403,zhang2023diagnosing,"[Zhang et~al.(2023)Zhang, HaoChen, Huang, Wang, Zou, and Yeung]{zhang2023diagnosing} Yuhui Zhang, Jeff~Z HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, and Serena Yeung.",Diagnosing and rectifying vision models using language.,Diagnosing and rectifying vision models using language.,,"[Zhang et~al.(2023)Zhang, HaoChen, Huang, Wang, Zou, and Yeung]{zhang2023diagnosing} Yuhui Zhang, Jeff~Z HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, and Serena Yeung. 
 Diagnosing and rectifying vision models using language. 
 \emph{arXiv preprint arXiv:2302.04269}, 2023."
2405.1403,zhou2016places,"[Zhou et~al.(2016)Zhou, Khosla, Lapedriza, Torralba, and Oliva]{zhou2016places} Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, and Aude Oliva.",Places: An image database for deep scene understanding.,Places: An image database for deep scene understanding.,,"[Zhou et~al.(2016)Zhou, Khosla, Lapedriza, Torralba, and Oliva]{zhou2016places} Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, and Aude Oliva. 
 Places: An image database for deep scene understanding. 
 \emph{arXiv preprint arXiv:1610.02055}, 2016."
2405.16057,anil2023palm,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil2023palm} Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,   Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al.",Palm 2 technical report.,Palm 2 technical report.,,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil2023palm} Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,   Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}, 2023."
2405.16057,bengio2013estimating,"[Bengio et~al.(2013)Bengio, L{\'e}onard, and   Courville]{bengio2013estimating} Bengio, Y., L{\'e}onard, N., and Courville, A.",Estimating or propagating gradients through stochastic neurons for   conditional computation.,Estimating or propagating gradients through stochastic neurons for   conditional computation.,,"[Bengio et~al.(2013)Bengio, L{\'e}onard, and   Courville]{bengio2013estimating} Bengio, Y., L{\'e}onard, N., and Courville, A. 
 Estimating or propagating gradients through stochastic neurons for   conditional computation. 
 \emph{arXiv preprint arXiv:1308.3432}, 2013."
2405.16057,dettmers2023qlora,"[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and   Zettlemoyer]{dettmers2023qlora} Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.",Qlora: Efficient finetuning of quantized llms.,Qlora: Efficient finetuning of quantized llms.,,"[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and   Zettlemoyer]{dettmers2023qlora} Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. 
 Qlora: Efficient finetuning of quantized llms. 
 \emph{arXiv preprint arXiv:2305.14314}, 2023."
2405.16057,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.",Bert: Pre-training of deep bidirectional transformers for language   understanding.,Bert: Pre-training of deep bidirectional transformers for language   understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 
 Bert: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2405.16057,hu2021lora,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and   Chen]{hu2021lora} Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and   Chen, W.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and   Chen]{hu2021lora} Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and   Chen, W. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}, 2021."
2405.16057,jaiswal2023compressing,"[Jaiswal et~al.(2023)Jaiswal, Gan, Du, Zhang, Wang, and   Yang]{jaiswal2023compressing} Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and Yang, Y.",Compressing llms: The truth is rarely pure and never simple.,Compressing llms: The truth is rarely pure and never simple.,,"[Jaiswal et~al.(2023)Jaiswal, Gan, Du, Zhang, Wang, and   Yang]{jaiswal2023compressing} Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and Yang, Y. 
 Compressing llms: The truth is rarely pure and never simple. 
 \emph{arXiv preprint arXiv:2310.01382}, 2023."
2405.16057,li2023sparse,"[Li et~al.(2023)Li, Niu, Zhang, Liu, Zhu, and Kang]{li2023sparse} Li, Y., Niu, L., Zhang, X., Liu, K., Zhu, J., and Kang, Z.",E-sparse: Boosting the large language model inference through   entropy-based n: M sparsity.,E-sparse: Boosting the large language model inference through   entropy-based n: M sparsity.,,"[Li et~al.(2023)Li, Niu, Zhang, Liu, Zhu, and Kang]{li2023sparse} Li, Y., Niu, L., Zhang, X., Liu, K., Zhu, J., and Kang, Z. 
 E-sparse: Boosting the large language model inference through   entropy-based n: M sparsity. 
 \emph{arXiv preprint arXiv:2310.15929}, 2023."
2405.16057,lin2020dynamic,"[Lin et~al.(2020)Lin, Stich, Barba, Dmitriev, and   Jaggi]{lin2020dynamic} Lin, T., Stich, S.~U., Barba, L., Dmitriev, D., and Jaggi, M.",Dynamic model pruning with feedback.,Dynamic model pruning with feedback.,,"[Lin et~al.(2020)Lin, Stich, Barba, Dmitriev, and   Jaggi]{lin2020dynamic} Lin, T., Stich, S.~U., Barba, L., Dmitriev, D., and Jaggi, M. 
 Dynamic model pruning with feedback. 
 \emph{arXiv preprint arXiv:2006.07253}, 2020."
2405.16057,liu2018rethinking,"[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking} Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.",Rethinking the value of network pruning.,Rethinking the value of network pruning.,,"[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking} Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. 
 Rethinking the value of network pruning. 
 \emph{arXiv preprint arXiv:1810.05270}, 2018."
2405.16057,merity2016pointer,"[Merity et~al.(2016)Merity, Xiong, Bradbury, and   Socher]{merity2016pointer} Merity, S., Xiong, C., Bradbury, J., and Socher, R.",Pointer sentinel mixture models.,Pointer sentinel mixture models.,,"[Merity et~al.(2016)Merity, Xiong, Bradbury, and   Socher]{merity2016pointer} Merity, S., Xiong, C., Bradbury, J., and Socher, R. 
 Pointer sentinel mixture models. 
 \emph{arXiv preprint arXiv:1609.07843}, 2016."
2405.16057,mishra2021accelerating,"[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh,   Yu, and Micikevicius]{mishra2021accelerating} Mishra, A., Latorre, J.~A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G.,   Yu, C., and Micikevicius, P.",Accelerating sparse deep neural networks.,Accelerating sparse deep neural networks.,,"[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh,   Yu, and Micikevicius]{mishra2021accelerating} Mishra, A., Latorre, J.~A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G.,   Yu, C., and Micikevicius, P. 
 Accelerating sparse deep neural networks. 
 \emph{arXiv preprint arXiv:2104.08378}, 2021."
2405.16057,shoeybi2019megatron,"[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and   Catanzaro]{shoeybi2019megatron} Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,   B.",Megatron-lm: Training multi-billion parameter language models using   model parallelism.,Megatron-lm: Training multi-billion parameter language models using   model parallelism.,,"[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and   Catanzaro]{shoeybi2019megatron} Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,   B. 
 Megatron-lm: Training multi-billion parameter language models using   model parallelism. 
 \emph{arXiv preprint arXiv:1909.08053}, 2019."
2405.16057,sun2023simple,"[Sun et~al.(2023)Sun, Liu, Bair, and Kolter]{sun2023simple} Sun, M., Liu, Z., Bair, A., and Kolter, J.~Z.",A simple and effective pruning approach for large language models.,A simple and effective pruning approach for large language models.,,"[Sun et~al.(2023)Sun, Liu, Bair, and Kolter]{sun2023simple} Sun, M., Liu, Z., Bair, A., and Kolter, J.~Z. 
 A simple and effective pruning approach for large language models. 
 \emph{arXiv preprint arXiv:2306.11695}, 2023."
2405.16057,touvron2023llama1,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama1} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,   T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama1} Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,   T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.16057,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,   Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,   Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.16057,xia2023sheared,"[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared} Xia, M., Gao, T., Zeng, Z., and Chen, D.",Sheared llama: Accelerating language model pre-training via   structured pruning.,Sheared llama: Accelerating language model pre-training via   structured pruning.,,"[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared} Xia, M., Gao, T., Zeng, Z., and Chen, D. 
 Sheared llama: Accelerating language model pre-training via   structured pruning. 
 \emph{arXiv preprint arXiv:2310.06694}, 2023."
2405.16057,xu2023qa,"[Xu et~al.(2023)Xu, Xie, Gu, Chen, Chang, Zhang, Chen, Zhang, and   Tian]{xu2023qa} Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X.,   and Tian, Q.",Qa-lora: Quantization-aware low-rank adaptation of large language   models.,Qa-lora: Quantization-aware low-rank adaptation of large language   models.,,"[Xu et~al.(2023)Xu, Xie, Gu, Chen, Chang, Zhang, Chen, Zhang, and   Tian]{xu2023qa} Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X.,   and Tian, Q. 
 Qa-lora: Quantization-aware low-rank adaptation of large language   models. 
 \emph{arXiv preprint arXiv:2309.14717}, 2023."
2405.16057,zhang2023dynamic,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhao, Lin, Sun, Yao, Han,   Tanner, Liu, and Ji]{zhang2023dynamic} Zhang, Y., Zhao, L., Lin, M., Sun, Y., Yao, Y., Han, X., Tanner, J., Liu, S.,   and Ji, R.",Dynamic sparse no training: Training-free fine-tuning for sparse   llms.,Dynamic sparse no training: Training-free fine-tuning for sparse   llms.,,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhao, Lin, Sun, Yao, Han,   Tanner, Liu, and Ji]{zhang2023dynamic} Zhang, Y., Zhao, L., Lin, M., Sun, Y., Yao, Y., Han, X., Tanner, J., Liu, S.,   and Ji, R. 
 Dynamic sparse no training: Training-free fine-tuning for sparse   llms. 
 \emph{arXiv preprint arXiv:2310.08915}, 2023{\natexlab{b}}."
2405.16057,zhou2021learning,"[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and   Li]{zhou2021learning} Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.",Learning n: m fine-grained structured sparse neural networks from   scratch.,Learning n: m fine-grained structured sparse neural networks from   scratch.,,"[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and   Li]{zhou2021learning} Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. 
 Learning n: m fine-grained structured sparse neural networks from   scratch. 
 \emph{arXiv preprint arXiv:2102.04010}, 2021."
2405.16747,jiang2019smart,"[Jiang et~al.(2019)Jiang, He, Chen, Liu, Gao, and Zhao]{jiang2019smart} Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao.",Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization.,Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization.,,"[Jiang et~al.(2019)Jiang, He, Chen, Liu, Gao, and Zhao]{jiang2019smart} Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. 
 Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. 
 \emph{arXiv preprint arXiv:1911.03437}, 2019."
2405.16747,kumar2022fine,"[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and Liang]{kumar2022fine} Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.",Fine-tuning can distort pretrained features and underperform out-of-distribution.,Fine-tuning can distort pretrained features and underperform out-of-distribution.,,"[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and Liang]{kumar2022fine} Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. 
 Fine-tuning can distort pretrained features and underperform out-of-distribution. 
 \emph{arXiv preprint arXiv:2202.10054}, 2022."
2405.16747,wang2018glue,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}, 2018."
2405.16747,yuan2023revisiting,"[Yuan et~al.(2023)Yuan, Chen, Cui, Gao, Zou, Cheng, Ji, Liu, and Sun]{yuan2023revisiting} Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun.","Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations.","Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations.",,"[Yuan et~al.(2023)Yuan, Chen, Cui, Gao, Zou, Cheng, Ji, Liu, and Sun]{yuan2023revisiting} Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. 
 Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. 
 \emph{arXiv preprint arXiv:2306.04618}, 2023."
2405.16829,kingma2014adam,[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2405.16829,yan2023multiscale3dgs,"[Yan et~al.(2023)Yan, Low, Chen, and Lee]{yan2023multiscale3dgs} Zhiwen Yan, Weng~Fei Low, Yu~Chen, and Gim~Hee Lee.",Multi-scale 3d gaussian splatting for anti-aliased rendering.,Multi-scale 3d gaussian splatting for anti-aliased rendering.,,"[Yan et~al.(2023)Yan, Low, Chen, and Lee]{yan2023multiscale3dgs} Zhiwen Yan, Weng~Fei Low, Yu~Chen, and Gim~Hee Lee. 
 Multi-scale 3d gaussian splatting for anti-aliased rendering. 
 \emph{arXiv preprint arXiv:2311.17089}, 2023."
2405.16829,yu2023mipsplatting,"[Yu et~al.(2023)Yu, Chen, Huang, Sattler, and Geiger]{yu2023mipsplatting} Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger.",Mip-splatting: Alias-free 3d gaussian splatting.,Mip-splatting: Alias-free 3d gaussian splatting.,,"[Yu et~al.(2023)Yu, Chen, Huang, Sattler, and Geiger]{yu2023mipsplatting} Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. 
 Mip-splatting: Alias-free 3d gaussian splatting. 
 \emph{arXiv preprint arXiv:2311.16493}, 2023."
2405.16829,zhang2020nerf++,"[Zhang et~al.(2020)Zhang, Riegler, Snavely, and Koltun]{zhang2020nerf++} Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun.",Nerf++: Analyzing and improving neural radiance fields.,Nerf++: Analyzing and improving neural radiance fields.,,"[Zhang et~al.(2020)Zhang, Riegler, Snavely, and Koltun]{zhang2020nerf++} Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 
 Nerf++: Analyzing and improving neural radiance fields. 
 \emph{arXiv preprint arXiv:2010.07492}, 2020."
2405.1798,bohnet2022attributed,"[{Bohnet et~al.(2022)Bohnet, Tran, Verga, Aharoni, Andor, Soares, Eisenstein, Ganchev, Herzig, Hui et~al.}]{bohnet2022attributed} Bernd Bohnet, Vinh~Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio~Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et~al. 2022.",Attributed question answering: Evaluation and modeling for attributed large language models.,Attributed question answering: Evaluation and modeling for attributed large language models.,,"[{Bohnet et~al.(2022)Bohnet, Tran, Verga, Aharoni, Andor, Soares, Eisenstein, Ganchev, Herzig, Hui et~al.}]{bohnet2022attributed} Bernd Bohnet, Vinh~Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio~Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et~al. 2022. 
 Attributed question answering: Evaluation and modeling for attributed large language models. 
 \emph{arXiv preprint arXiv:2212.08037}."
2405.1798,chen2017reading,"[{Chen et~al.(2017)Chen, Fisch, Weston, and Bordes}]{chen2017reading} Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.",Reading wikipedia to answer open-domain questions.,Reading wikipedia to answer open-domain questions.,,"[{Chen et~al.(2017)Chen, Fisch, Weston, and Bordes}]{chen2017reading} Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. 
 Reading wikipedia to answer open-domain questions. 
 \emph{arXiv preprint arXiv:1704.00051}."
2405.1798,gao2023enabling,"[{Gao et~al.(2023{\natexlab{b}})Gao, Yen, Yu, and Chen}]{gao2023enabling} Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023{\natexlab{b}}.",Enabling large language models to generate text with citations.,Enabling large language models to generate text with citations.,,"[{Gao et~al.(2023{\natexlab{b}})Gao, Yen, Yu, and Chen}]{gao2023enabling} Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023{\natexlab{b}}. 
 Enabling large language models to generate text with citations. 
 \emph{arXiv preprint arXiv:2305.14627}."
2405.1798,huang2023citation,[{Huang and Chang(2023)}]{huang2023citation} Jie Huang and Kevin Chen-Chuan Chang. 2023.,Citation: A key to building responsible and accountable large language models.,Citation: A key to building responsible and accountable large language models.,,"[{Huang and Chang(2023)}]{huang2023citation} Jie Huang and Kevin Chen-Chuan Chang. 2023. 
 Citation: A key to building responsible and accountable large language models. 
 \emph{arXiv preprint arXiv:2307.02185}."
2405.1798,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2405.1798,kamalloo2023hagrid,"[{Kamalloo et~al.(2023)Kamalloo, Jafari, Zhang, Thakur, and Lin}]{kamalloo2023hagrid} Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023.",Hagrid: A human-llm collaborative dataset for generative information-seeking with attribution.,Hagrid: A human-llm collaborative dataset for generative information-seeking with attribution.,,"[{Kamalloo et~al.(2023)Kamalloo, Jafari, Zhang, Thakur, and Lin}]{kamalloo2023hagrid} Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. 
 Hagrid: A human-llm collaborative dataset for generative information-seeking with attribution. 
 \emph{arXiv preprint arXiv:2307.16883}."
2405.1798,lee2019latent,"[{Lee et~al.(2019)Lee, Chang, and Toutanova}]{lee2019latent} Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019.",Latent retrieval for weakly supervised open domain question answering.,Latent retrieval for weakly supervised open domain question answering.,,"[{Lee et~al.(2019)Lee, Chang, and Toutanova}]{lee2019latent} Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. 
 Latent retrieval for weakly supervised open domain question answering. 
 \emph{arXiv preprint arXiv:1906.00300}."
2405.1798,liu2023lost,"[{Liu et~al.(2023{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}]{liu2023lost} Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023{\natexlab{a}}.",Lost in the middle: How language models use long contexts.,Lost in the middle: How language models use long contexts.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}]{liu2023lost} Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023{\natexlab{a}}. 
 Lost in the middle: How language models use long contexts. 
 \emph{arXiv preprint arXiv:2307.03172}."
2405.1798,liu2023evaluating,"[{Liu et~al.(2023{\natexlab{b}})Liu, Zhang, and Liang}]{liu2023evaluating} Nelson~F Liu, Tianyi Zhang, and Percy Liang. 2023{\natexlab{b}}.",Evaluating verifiability in generative search engines.,Evaluating verifiability in generative search engines.,,"[{Liu et~al.(2023{\natexlab{b}})Liu, Zhang, and Liang}]{liu2023evaluating} Nelson~F Liu, Tianyi Zhang, and Percy Liang. 2023{\natexlab{b}}. 
 Evaluating verifiability in generative search engines. 
 \emph{arXiv preprint arXiv:2304.09848}."
2405.1798,mikolov2013efficient,"[{Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean}]{mikolov2013efficient} Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.",Efficient estimation of word representations in vector space.,Efficient estimation of word representations in vector space.,,"[{Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean}]{mikolov2013efficient} Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. 
 Efficient estimation of word representations in vector space. 
 \emph{arXiv preprint arXiv:1301.3781}."
2405.1798,ni2021large,"[{Ni et~al.(2021)Ni, Qu, Lu, Dai, {\'A}brego, Ma, Zhao, Luan, Hall, Chang et~al.}]{ni2021large} Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo~Hern{\'a}ndez {\'A}brego, Ji~Ma, Vincent~Y Zhao, Yi~Luan, Keith~B Hall, Ming-Wei Chang, et~al. 2021.",Large dual encoders are generalizable retrievers.,Large dual encoders are generalizable retrievers.,,"[{Ni et~al.(2021)Ni, Qu, Lu, Dai, {\'A}brego, Ma, Zhao, Luan, Hall, Chang et~al.}]{ni2021large} Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo~Hern{\'a}ndez {\'A}brego, Ji~Ma, Vincent~Y Zhao, Yi~Luan, Keith~B Hall, Ming-Wei Chang, et~al. 2021. 
 Large dual encoders are generalizable retrievers. 
 \emph{arXiv preprint arXiv:2112.07899}."
2405.1798,nogueira2020document,"[{Nogueira et~al.(2020)Nogueira, Jiang, and Lin}]{nogueira2020document} Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.",Document ranking with a pretrained sequence-to-sequence model.,Document ranking with a pretrained sequence-to-sequence model.,,"[{Nogueira et~al.(2020)Nogueira, Jiang, and Lin}]{nogueira2020document} Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. 
 Document ranking with a pretrained sequence-to-sequence model. 
 \emph{arXiv preprint arXiv:2003.06713}."
2405.1798,schuster2023semqa,"[{Schuster et~al.(2023)Schuster, Lelkes, Sun, Gupta, Berant, Cohen, and Metzler}]{schuster2023semqa} Tal Schuster, Adam~D Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William~W Cohen, and Donald Metzler. 2023.",Semqa: Semi-extractive multi-source question answering.,Semqa: Semi-extractive multi-source question answering.,,"[{Schuster et~al.(2023)Schuster, Lelkes, Sun, Gupta, Berant, Cohen, and Metzler}]{schuster2023semqa} Tal Schuster, Adam~D Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William~W Cohen, and Donald Metzler. 2023. 
 Semqa: Semi-extractive multi-source question answering. 
 \emph{arXiv preprint arXiv:2311.04886}."
2405.1798,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2405.1798,weller2023according,"[{Weller et~al.(2023)Weller, Marone, Weir, Lawrie, Khashabi, and Van~Durme}]{weller2023according} Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van~Durme. 2023.",""" according to..."" prompting language models improves quoting from pre-training data.",""" according to..."" prompting language models improves quoting from pre-training data.",,"[{Weller et~al.(2023)Weller, Marone, Weir, Lawrie, Khashabi, and Van~Durme}]{weller2023according} Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van~Durme. 2023. 
 "" according to..."" prompting language models improves quoting from pre-training data. 
 \emph{arXiv preprint arXiv:2305.13252}."
2405.1798,young2024yi,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}."
2405.1798,zhang2022opt,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2405.1798,zou2023representation,"[{Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski et~al.}]{zou2023representation} Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et~al. 2023.",Representation engineering: A top-down approach to ai transparency.,Representation engineering: A top-down approach to ai transparency.,,"[{Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski et~al.}]{zou2023representation} Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et~al. 2023. 
 Representation engineering: A top-down approach to ai transparency. 
 \emph{arXiv preprint arXiv:2310.01405}."
2405.18218,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2405.18218,ainslie2023gqa,"[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai]{ainslie2023gqa} Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'o}n, and Sumit Sanghai.",Gqa: Training generalized multi-query transformer models from multi-head checkpoints.,Gqa: Training generalized multi-query transformer models from multi-head checkpoints.,,"[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai]{ainslie2023gqa} Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'o}n, and Sumit Sanghai. 
 Gqa: Training generalized multi-query transformer models from multi-head checkpoints. 
 \emph{arXiv preprint arXiv:2305.13245}, 2023."
2405.18218,anil2023palm,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.",Palm 2 technical report.,Palm 2 technical report.,,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}, 2023."
2405.18218,allenai:arc,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv:1803.05457v1}, 2018."
2405.18218,hinton2015distilling,"[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.",Distilling the knowledge in a neural network.,Distilling the knowledge in a neural network.,,"[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 
 Distilling the knowledge in a neural network. 
 \emph{arXiv preprint arXiv:1503.02531}, 2015."
2405.18218,hoffmann2022training,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2405.18218,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2405.18218,jiao2019tinybert,"[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu]{jiao2019tinybert} Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.",Tinybert: Distilling bert for natural language understanding.,Tinybert: Distilling bert for natural language understanding.,,"[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and Liu]{jiao2019tinybert} Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 
 Tinybert: Distilling bert for natural language understanding. 
 \emph{arXiv preprint arXiv:1909.10351}, 2019."
2405.18218,men2024shortgpt,"[Men et~al.(2024)Men, Xu, Zhang, Wang, Lin, Lu, Han, and Chen]{men2024shortgpt} Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen.",Shortgpt: Layers in large language models are more redundant than you expect.,Shortgpt: Layers in large language models are more redundant than you expect.,,"[Men et~al.(2024)Men, Xu, Zhang, Wang, Lin, Lu, Han, and Chen]{men2024shortgpt} Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 
 Shortgpt: Layers in large language models are more redundant than you expect. 
 \emph{arXiv preprint arXiv:2403.03853}, 2024."
2405.18218,srivastava2022beyond,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}, 2022."
2405.18218,touvron2023llama1,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.18218,touvron2023llama,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.18218,yang2024laco,"[Yang et~al.(2024)Yang, Cao, and Zhao]{yang2024laco} Yifei Yang, Zouying Cao, and Hai Zhao.",Laco: Large language model pruning via layer collapse.,Laco: Large language model pruning via layer collapse.,,"[Yang et~al.(2024)Yang, Cao, and Zhao]{yang2024laco} Yifei Yang, Zouying Cao, and Hai Zhao. 
 Laco: Large language model pruning via layer collapse. 
 \emph{arXiv preprint arXiv:2402.11187}, 2024."
2405.18351,betancourt2017conceptual,[Betancourt(2017)]{betancourt2017conceptual} Michael Betancourt.,A conceptual introduction to hamiltonian monte carlo.,A conceptual introduction to hamiltonian monte carlo.,,"[Betancourt(2017)]{betancourt2017conceptual} Michael Betancourt. 
 A conceptual introduction to hamiltonian monte carlo. 
 \emph{arXiv preprint arXiv:1701.02434}, 2017."
2405.18351,vireview,"[{Blei} et~al.(2016){Blei}, {Kucukelbir}, and {McAuliffe}]{vireview} David~M. {Blei}, Alp {Kucukelbir}, and Jon~D. {McAuliffe}.",{Variational Inference: A Review for Statisticians}.,{Variational Inference: A Review for Statisticians}.,,"[{Blei} et~al.(2016){Blei}, {Kucukelbir}, and {McAuliffe}]{vireview} David~M. {Blei}, Alp {Kucukelbir}, and Jon~D. {McAuliffe}. 
 {Variational Inference: A Review for Statisticians}. 
 \emph{arXiv e-prints}, art. arXiv:1601.00670, January 2016."
2405.18351,blundell,"[{Blundell} et~al.(2015){Blundell}, {Cornebise}, {Kavukcuoglu}, and {Wierstra}]{blundell} Charles {Blundell}, Julien {Cornebise}, Koray {Kavukcuoglu}, and Daan {Wierstra}.",{Weight Uncertainty in Neural Networks}.,{Weight Uncertainty in Neural Networks}.,,"[{Blundell} et~al.(2015){Blundell}, {Cornebise}, {Kavukcuoglu}, and {Wierstra}]{blundell} Charles {Blundell}, Julien {Cornebise}, Koray {Kavukcuoglu}, and Daan {Wierstra}. 
 {Weight Uncertainty in Neural Networks}. 
 \emph{arXiv e-prints}, art. arXiv:1505.05424, May 2015."
2405.18351,gal2015bayesian,[Gal and Ghahramani(2015)]{gal2015bayesian} Yarin Gal and Zoubin Ghahramani.,Bayesian convolutional neural networks with bernoulli approximate variational inference.,Bayesian convolutional neural networks with bernoulli approximate variational inference.,,"[Gal and Ghahramani(2015)]{gal2015bayesian} Yarin Gal and Zoubin Ghahramani. 
 Bayesian convolutional neural networks with bernoulli approximate variational inference. 
 \emph{arXiv preprint arXiv:1506.02158}, 2015."
2405.18351,khan2021bayesian,[Khan and Rue(2021)]{khan2021bayesian} Mohammad~Emtiyaz Khan and H{\aa}vard Rue.,The bayesian learning rule.,The bayesian learning rule.,,"[Khan and Rue(2021)]{khan2021bayesian} Mohammad~Emtiyaz Khan and H{\aa}vard Rue. 
 The bayesian learning rule. 
 \emph{arXiv preprint arXiv:2107.04562}, 2021."
2405.18351,laves2019well,"[Laves et~al.(2019)Laves, Ihler, Kortmann, and Ortmaier]{laves2019well} Max-Heinrich Laves, Sontje Ihler, Karl-Philipp Kortmann, and Tobias Ortmaier.",Well-calibrated model uncertainty with temperature scaling for dropout variational inference.,Well-calibrated model uncertainty with temperature scaling for dropout variational inference.,,"[Laves et~al.(2019)Laves, Ihler, Kortmann, and Ortmaier]{laves2019well} Max-Heinrich Laves, Sontje Ihler, Karl-Philipp Kortmann, and Tobias Ortmaier. 
 Well-calibrated model uncertainty with temperature scaling for dropout variational inference. 
 \emph{arXiv preprint arXiv:1909.13550}, 2019."
2405.18351,masegosa2019learning,[Masegosa(2019)]{masegosa2019learning} Andr{\'e}s~R Masegosa.,Learning under model misspecification: Applications to variational and ensemble methods.,Learning under model misspecification: Applications to variational and ensemble methods.,,"[Masegosa(2019)]{masegosa2019learning} Andr{\'e}s~R Masegosa. 
 Learning under model misspecification: Applications to variational and ensemble methods. 
 \emph{arXiv preprint arXiv:1912.08335}, 2019."
2405.18351,seligmann2023beyond,"[Seligmann et~al.(2023)Seligmann, Becker, Volpp, and Neumann]{seligmann2023beyond} Florian Seligmann, Philipp Becker, Michael Volpp, and Gerhard Neumann.",Beyond deep ensembles--a large-scale evaluation of bayesian deep learning under distribution shift.,Beyond deep ensembles--a large-scale evaluation of bayesian deep learning under distribution shift.,,"[Seligmann et~al.(2023)Seligmann, Becker, Volpp, and Neumann]{seligmann2023beyond} Florian Seligmann, Philipp Becker, Michael Volpp, and Gerhard Neumann. 
 Beyond deep ensembles--a large-scale evaluation of bayesian deep learning under distribution shift. 
 \emph{arXiv preprint arXiv:2306.12306}, 2023."
2405.18351,shen2024variational,"[Shen et~al.(2024)Shen, Daheim, Cong, Nickl, Marconi, Bazan, Yokota, Gurevych, Cremers, Khan, et~al.]{shen2024variational} Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian~Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad~Emtiyaz Khan, et~al.",Variational learning is effective for large deep networks.,Variational learning is effective for large deep networks.,,"[Shen et~al.(2024)Shen, Daheim, Cong, Nickl, Marconi, Bazan, Yokota, Gurevych, Cremers, Khan, et~al.]{shen2024variational} Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian~Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad~Emtiyaz Khan, et~al. 
 Variational learning is effective for large deep networks. 
 \emph{arXiv preprint arXiv:2402.17641}, 2024."
2405.18711,bai2022training,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}, 2022."
2405.18711,turpin2023language,"[Turpin et~al.(2023)Turpin, Michael, Perez, and Bowman]{turpin2023language} Miles Turpin, Julian Michael, Ethan Perez, and Samuel~R Bowman.",Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.,Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.,,"[Turpin et~al.(2023)Turpin, Michael, Perez, and Bowman]{turpin2023language} Miles Turpin, Julian Michael, Ethan Perez, and Samuel~R Bowman. 
 Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. 
 \emph{arXiv preprint arXiv:2305.04388}, 2023."
2405.18711,lyu2023faithful,"[Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch.",Faithful chain-of-thought reasoning.,Faithful chain-of-thought reasoning.,,"[Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 
 Faithful chain-of-thought reasoning. 
 \emph{arXiv preprint arXiv:2301.13379}, 2023."
2405.18711,lanham2023measuring,"[Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion, et~al.]{lanham2023measuring} Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et~al.",Measuring faithfulness in chain-of-thought reasoning.,Measuring faithfulness in chain-of-thought reasoning.,,"[Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion, et~al.]{lanham2023measuring} Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et~al. 
 Measuring faithfulness in chain-of-thought reasoning. 
 \emph{arXiv preprint arXiv:2307.13702}, 2023."
2405.18711,belrose2023eliciting,"[Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky, McKinney, Biderman, and Steinhardt]{belrose2023eliciting} Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt.",Eliciting latent predictions from transformers with the tuned lens.,Eliciting latent predictions from transformers with the tuned lens.,,"[Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky, McKinney, Biderman, and Steinhardt]{belrose2023eliciting} Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 
 Eliciting latent predictions from transformers with the tuned lens. 
 \emph{arXiv preprint arXiv:2303.08112}, 2023."
2405.18711,ba2016layer,"[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer} Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.",Layer normalization.,Layer normalization.,,"[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer} Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. 
 Layer normalization. 
 \emph{arXiv preprint arXiv:1607.06450}, 2016."
2405.18711,geva2022transformer,"[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer} Mor Geva, Avi Caciularu, Kevin~Ro Wang, and Yoav Goldberg.",Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.,Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.,,"[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer} Mor Geva, Avi Caciularu, Kevin~Ro Wang, and Yoav Goldberg. 
 Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. 
 \emph{arXiv preprint arXiv:2203.14680}, 2022."
2405.18711,ferrando2024primer,"[Ferrando et~al.(2024)Ferrando, Sarti, Bisazza, and Costa-juss{\`a}]{ferrando2024primer} Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta~R Costa-juss{\`a}.",A primer on the inner workings of transformer-based language models.,A primer on the inner workings of transformer-based language models.,,"[Ferrando et~al.(2024)Ferrando, Sarti, Bisazza, and Costa-juss{\`a}]{ferrando2024primer} Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta~R Costa-juss{\`a}. 
 A primer on the inner workings of transformer-based language models. 
 \emph{arXiv preprint arXiv:2405.00208}, 2024."
2405.18711,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2405.18711,alain2016understanding,[Alain and Bengio(2016)]{alain2016understanding} Guillaume Alain and Yoshua Bengio.,Understanding intermediate layers using linear classifier probes.,Understanding intermediate layers using linear classifier probes.,,"[Alain and Bengio(2016)]{alain2016understanding} Guillaume Alain and Yoshua Bengio. 
 Understanding intermediate layers using linear classifier probes. 
 \emph{arXiv preprint arXiv:1610.01644}, 2016."
2405.18711,geng2023survey,"[Geng et~al.(2023)Geng, Cai, Wang, Koeppl, Nakov, and Gurevych]{geng2023survey} Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych.",A survey of language model confidence estimation and calibration.,A survey of language model confidence estimation and calibration.,,"[Geng et~al.(2023)Geng, Cai, Wang, Koeppl, Nakov, and Gurevych]{geng2023survey} Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 
 A survey of language model confidence estimation and calibration. 
 \emph{arXiv preprint arXiv:2311.08298}, 2023."
2405.18711,wang2022self,"[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}, 2022."
2405.18711,jastrzkebski2017residual,"[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Arpit, Ballas, Verma, Che, and Bengio]{jastrzkebski2017residual} Stanis{\l}aw Jastrz{\k{e}}bski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio.",Residual connections encourage iterative inference.,Residual connections encourage iterative inference.,,"[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Arpit, Ballas, Verma, Che, and Bengio]{jastrzkebski2017residual} Stanis{\l}aw Jastrz{\k{e}}bski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. 
 Residual connections encourage iterative inference. 
 \emph{arXiv preprint arXiv:1710.04773}, 2017."
2405.18711,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2405.18711,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2405.18711,clark2019boolq,"[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}, 2019."
2405.18711,tafjord2020proofwriter,"[Tafjord et~al.(2020)Tafjord, Mishra, and Clark]{tafjord2020proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.",,"[Tafjord et~al.(2020)Tafjord, Mishra, and Clark]{tafjord2020proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark. 
 Proofwriter: Generating implications, proofs, and abductive statements over natural language. 
 \emph{arXiv preprint arXiv:2012.13048}, 2020."
2405.18711,wang2024chain,[Wang and Zhou(2024)]{wang2024chain} Xuezhi Wang and Denny Zhou.,Chain-of-thought reasoning without prompting.,Chain-of-thought reasoning without prompting.,,"[Wang and Zhou(2024)]{wang2024chain} Xuezhi Wang and Denny Zhou. 
 Chain-of-thought reasoning without prompting. 
 \emph{arXiv preprint arXiv:2402.10200}, 2024."
2405.18711,lee2024mechanistic,"[Lee et~al.(2024)Lee, Bai, Pres, Wattenberg, Kummerfeld, and Mihalcea]{lee2024mechanistic} Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan~K Kummerfeld, and Rada Mihalcea.",A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.,A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.,,"[Lee et~al.(2024)Lee, Bai, Pres, Wattenberg, Kummerfeld, and Mihalcea]{lee2024mechanistic} Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan~K Kummerfeld, and Rada Mihalcea. 
 A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity. 
 \emph{arXiv preprint arXiv:2401.01967}, 2024."
2405.18711,zou2023representation,"[Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski, et~al.]{zou2023representation} Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et~al.",Representation engineering: A top-down approach to ai transparency.,Representation engineering: A top-down approach to ai transparency.,,"[Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski, et~al.]{zou2023representation} Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et~al. 
 Representation engineering: A top-down approach to ai transparency. 
 \emph{arXiv preprint arXiv:2310.01405}, 2023."
2405.18711,raposo2024mixture,"[Raposo et~al.(2024)Raposo, Ritter, Richards, Lillicrap, Humphreys, and Santoro]{raposo2024mixture} David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter~Conway Humphreys, and Adam Santoro.",Mixture-of-depths: Dynamically allocating compute in transformer-based language models.,Mixture-of-depths: Dynamically allocating compute in transformer-based language models.,,"[Raposo et~al.(2024)Raposo, Ritter, Richards, Lillicrap, Humphreys, and Santoro]{raposo2024mixture} David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter~Conway Humphreys, and Adam Santoro. 
 Mixture-of-depths: Dynamically allocating compute in transformer-based language models. 
 \emph{arXiv preprint arXiv:2404.02258}, 2024."
2405.18711,deng2023implicit,"[Deng et~al.(2023)Deng, Prasad, Fernandez, Smolensky, Chaudhary, and Shieber]{deng2023implicit} Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber.",Implicit chain of thought reasoning via knowledge distillation.,Implicit chain of thought reasoning via knowledge distillation.,,"[Deng et~al.(2023)Deng, Prasad, Fernandez, Smolensky, Chaudhary, and Shieber]{deng2023implicit} Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 
 Implicit chain of thought reasoning via knowledge distillation. 
 \emph{arXiv preprint arXiv:2311.01460}, 2023."
2405.18711,shen2024thermometer,"[Shen et~al.(2024)Shen, Das, Greenewald, Sattigeri, Wornell, and Ghosh]{shen2024thermometer} Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, and Soumya Ghosh.",Thermometer: Towards universal calibration for large language models.,Thermometer: Towards universal calibration for large language models.,,"[Shen et~al.(2024)Shen, Das, Greenewald, Sattigeri, Wornell, and Ghosh]{shen2024thermometer} Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, and Soumya Ghosh. 
 Thermometer: Towards universal calibration for large language models. 
 \emph{arXiv preprint arXiv:2403.08819}, 2024."
2405.18711,yao2022react,"[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react} Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.",React: Synergizing reasoning and acting in language models.,React: Synergizing reasoning and acting in language models.,,"[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react} Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 
 React: Synergizing reasoning and acting in language models. 
 \emph{arXiv preprint arXiv:2210.03629}, 2022."
2405.18711,halawi2023overthinking,"[Halawi et~al.(2023)Halawi, Denain, and Steinhardt]{halawi2023overthinking} Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt.",Overthinking the truth: Understanding how language models process false demonstrations.,Overthinking the truth: Understanding how language models process false demonstrations.,,"[Halawi et~al.(2023)Halawi, Denain, and Steinhardt]{halawi2023overthinking} Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. 
 Overthinking the truth: Understanding how language models process false demonstrations. 
 \emph{arXiv preprint arXiv:2307.09476}, 2023."
2405.18711,langedijk2023decoderlens,"[Langedijk et~al.(2023)Langedijk, Mohebbi, Sarti, Zuidema, and Jumelet]{langedijk2023decoderlens} Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap Jumelet.",Decoderlens: Layerwise interpretation of encoder-decoder transformers.,Decoderlens: Layerwise interpretation of encoder-decoder transformers.,,"[Langedijk et~al.(2023)Langedijk, Mohebbi, Sarti, Zuidema, and Jumelet]{langedijk2023decoderlens} Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap Jumelet. 
 Decoderlens: Layerwise interpretation of encoder-decoder transformers. 
 \emph{arXiv preprint arXiv:2310.03686}, 2023."
2405.18711,wolf2019huggingface,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al.",Huggingface's transformers: State-of-the-art natural language processing.,Huggingface's transformers: State-of-the-art natural language processing.,,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al. 
 Huggingface's transformers: State-of-the-art natural language processing. 
 \emph{arXiv preprint arXiv:1910.03771}, 2019."
2405.18711,holtzman2019curious,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.",The curious case of neural text degeneration.,The curious case of neural text degeneration.,,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 
 The curious case of neural text degeneration. 
 \emph{arXiv preprint arXiv:1904.09751}, 2019."
2405.19654,alsentzer2019publicly,"[Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott]{alsentzer2019publicly} Alsentzer, E., Murphy, J.~R., Boag, W., Weng, W.-H., Jin, D., Naumann, T., and McDermott, M.",Publicly available clinical bert embeddings.,Publicly available clinical bert embeddings.,,"[Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott]{alsentzer2019publicly} Alsentzer, E., Murphy, J.~R., Boag, W., Weng, W.-H., Jin, D., Naumann, T., and McDermott, M. 
 Publicly available clinical bert embeddings. 
 \emph{arXiv preprint arXiv:1904.03323}, 2019."
2405.19654,chambon2022roentgen,"[Chambon et~al.(2022{\natexlab{a}})Chambon, Bluethgen, Delbrouck, Van~der Sluijs, Po{\l}acin, Chaves, Abraham, Purohit, Langlotz, and Chaudhari]{chambon2022roentgen} Chambon, P., Bluethgen, C., Delbrouck, J.-B., Van~der Sluijs, R., Po{\l}acin, M., Chaves, J. M.~Z., Abraham, T.~M., Purohit, S., Langlotz, C.~P., and Chaudhari, A.",Roentgen: Vision-language foundation model for chest x-ray generation.,Roentgen: Vision-language foundation model for chest x-ray generation.,,"[Chambon et~al.(2022{\natexlab{a}})Chambon, Bluethgen, Delbrouck, Van~der Sluijs, Po{\l}acin, Chaves, Abraham, Purohit, Langlotz, and Chaudhari]{chambon2022roentgen} Chambon, P., Bluethgen, C., Delbrouck, J.-B., Van~der Sluijs, R., Po{\l}acin, M., Chaves, J. M.~Z., Abraham, T.~M., Purohit, S., Langlotz, C.~P., and Chaudhari, A. 
 Roentgen: Vision-language foundation model for chest x-ray generation. 
 \emph{arXiv preprint arXiv:2211.12737}, 2022{\natexlab{a}}."
2405.19654,chambon2022adapting,"[Chambon et~al.(2022{\natexlab{b}})Chambon, Bluethgen, Langlotz, and Chaudhari]{chambon2022adapting} Chambon, P., Bluethgen, C., Langlotz, C.~P., and Chaudhari, A.",Adapting pretrained vision-language foundational models to medical imaging domains.,Adapting pretrained vision-language foundational models to medical imaging domains.,,"[Chambon et~al.(2022{\natexlab{b}})Chambon, Bluethgen, Langlotz, and Chaudhari]{chambon2022adapting} Chambon, P., Bluethgen, C., Langlotz, C.~P., and Chaudhari, A. 
 Adapting pretrained vision-language foundational models to medical imaging domains. 
 \emph{arXiv preprint arXiv:2210.04133}, 2022{\natexlab{b}}."
2405.19654,chen2022pali,"[Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali} Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et~al.",Pali: A jointly-scaled multilingual language-image model.,Pali: A jointly-scaled multilingual language-image model.,,"[Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali} Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et~al. 
 Pali: A jointly-scaled multilingual language-image model. 
 \emph{arXiv preprint arXiv:2209.06794}, 2022."
2405.19654,dawidowicz2023limitr,"[Dawidowicz et~al.(2023)Dawidowicz, Hirsch, and Tal]{dawidowicz2023limitr} Dawidowicz, G., Hirsch, E., and Tal, A.",Limitr: Leveraging local information for medical image-text representation.,Limitr: Leveraging local information for medical image-text representation.,,"[Dawidowicz et~al.(2023)Dawidowicz, Hirsch, and Tal]{dawidowicz2023limitr} Dawidowicz, G., Hirsch, E., and Tal, A. 
 Limitr: Leveraging local information for medical image-text representation. 
 \emph{arXiv preprint arXiv:2303.11755}, 2023."
2405.19654,dosovitskiy2020image,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.",An image is worth 16x16 words: Transformers for image recognition at scale.,An image is worth 16x16 words: Transformers for image recognition at scale.,,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al. 
 An image is worth 16x16 words: Transformers for image recognition at scale. 
 \emph{arXiv preprint arXiv:2010.11929}, 2020."
2405.19654,johnson2019mimic,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Johnson, A.~E., Pollard, T.~J., Greenbaum, N.~R., Lungren, M.~P., Deng, C.-y., Peng, Y., Lu, Z., Mark, R.~G., Berkowitz, S.~J., and Horng, S.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.",,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Johnson, A.~E., Pollard, T.~J., Greenbaum, N.~R., Lungren, M.~P., Deng, C.-y., Peng, Y., Lu, Z., Mark, R.~G., Berkowitz, S.~J., and Horng, S. 
 Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. 
 \emph{arXiv preprint arXiv:1901.07042}, 2019."
2405.19654,li2023blip,"[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip} Li, J., Li, D., Savarese, S., and Hoi, S.",Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,,"[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip} Li, J., Li, D., Savarese, S., and Hoi, S. 
 Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. 
 \emph{arXiv preprint arXiv:2301.12597}, 2023."
2405.19654,loshchilov2016sgdr,"[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr} Loshchilov, I. and Hutter, F.",Sgdr: Stochastic gradient descent with warm restarts.,Sgdr: Stochastic gradient descent with warm restarts.,,"[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr} Loshchilov, I. and Hutter, F. 
 Sgdr: Stochastic gradient descent with warm restarts. 
 \emph{arXiv preprint arXiv:1608.03983}, 2016."
2405.19654,loshchilov2017decoupled,"[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled} Loshchilov, I. and Hutter, F.",Decoupled weight decay regularization.,Decoupled weight decay regularization.,,"[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled} Loshchilov, I. and Hutter, F. 
 Decoupled weight decay regularization. 
 \emph{arXiv preprint arXiv:1711.05101}, 2017."
2405.19654,oord2018representation,"[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation} Oord, A. v.~d., Li, Y., and Vinyals, O.",Representation learning with contrastive predictive coding.,Representation learning with contrastive predictive coding.,,"[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation} Oord, A. v.~d., Li, Y., and Vinyals, O. 
 Representation learning with contrastive predictive coding. 
 \emph{arXiv preprint arXiv:1807.03748}, 2018."
2405.19654,qin2022medical,"[Qin et~al.(2022)Qin, Yi, Lao, and Li]{qin2022medical} Qin, Z., Yi, H., Lao, Q., and Li, K.",Medical image understanding with pretrained vision language models: A comprehensive study.,Medical image understanding with pretrained vision language models: A comprehensive study.,,"[Qin et~al.(2022)Qin, Yi, Lao, and Li]{qin2022medical} Qin, Z., Yi, H., Lao, Q., and Li, K. 
 Medical image understanding with pretrained vision language models: A comprehensive study. 
 \emph{arXiv preprint arXiv:2209.15517}, 2022."
2405.19654,shaib2023summarizing,"[Shaib et~al.(2023)Shaib, Li, Joseph, Marshall, Li, and Wallace]{shaib2023summarizing} Shaib, C., Li, M.~L., Joseph, S., Marshall, I.~J., Li, J.~J., and Wallace, B.~C.","Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success).","Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success).",,"[Shaib et~al.(2023)Shaib, Li, Joseph, Marshall, Li, and Wallace]{shaib2023summarizing} Shaib, C., Li, M.~L., Joseph, S., Marshall, I.~J., Li, J.~J., and Wallace, B.~C. 
 Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success). 
 \emph{arXiv preprint arXiv:2305.06299}, 2023."
2405.19654,singhal2022large,"[Singhal et~al.(2022)Singhal, Azizi, Tu, Mahdavi, Wei, Chung, Scales, Tanwani, Cole-Lewis, Pfohl, et~al.]{singhal2022large} Singhal, K., Azizi, S., Tu, T., Mahdavi, S.~S., Wei, J., Chung, H.~W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et~al.",Large language models encode clinical knowledge.,Large language models encode clinical knowledge.,,"[Singhal et~al.(2022)Singhal, Azizi, Tu, Mahdavi, Wei, Chung, Scales, Tanwani, Cole-Lewis, Pfohl, et~al.]{singhal2022large} Singhal, K., Azizi, S., Tu, T., Mahdavi, S.~S., Wei, J., Chung, H.~W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et~al. 
 Large language models encode clinical knowledge. 
 \emph{arXiv preprint arXiv:2212.13138}, 2022."
2405.19654,wan2023med,"[Wan et~al.(2023)Wan, Liu, Zhang, Fu, Wang, Cheng, Ma, Quilodr{\'a}n-Casas, and Arcucci]{wan2023med} Wan, Z., Liu, C., Zhang, M., Fu, J., Wang, B., Cheng, S., Ma, L., Quilodr{\'a}n-Casas, C., and Arcucci, R.",Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias.,Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias.,,"[Wan et~al.(2023)Wan, Liu, Zhang, Fu, Wang, Cheng, Ma, Quilodr{\'a}n-Casas, and Arcucci]{wan2023med} Wan, Z., Liu, C., Zhang, M., Fu, J., Wang, B., Cheng, S., Ma, L., Quilodr{\'a}n-Casas, C., and Arcucci, R. 
 Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias. 
 \emph{arXiv preprint arXiv:2305.19894}, 2023."
2405.19654,wang2022multi,"[Wang et~al.(2022{\natexlab{a}})Wang, Zhou, Wang, Vardhanabhuti, and Yu]{wang2022multi} Wang, F., Zhou, Y., Wang, S., Vardhanabhuti, V., and Yu, L.",Multi-granularity cross-modal alignment for generalized medical visual representation learning.,Multi-granularity cross-modal alignment for generalized medical visual representation learning.,,"[Wang et~al.(2022{\natexlab{a}})Wang, Zhou, Wang, Vardhanabhuti, and Yu]{wang2022multi} Wang, F., Zhou, Y., Wang, S., Vardhanabhuti, V., and Yu, L. 
 Multi-granularity cross-modal alignment for generalized medical visual representation learning. 
 \emph{arXiv preprint arXiv:2210.06044}, 2022{\natexlab{a}}."
2405.19654,wang2022image,"[Wang et~al.(2022{\natexlab{b}})Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal, Mohammed, Singhal, Som, et~al.]{wang2022image} Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.~K., Singhal, S., Som, S., et~al.",Image as a foreign language: Beit pretraining for all vision and vision-language tasks.,Image as a foreign language: Beit pretraining for all vision and vision-language tasks.,,"[Wang et~al.(2022{\natexlab{b}})Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal, Mohammed, Singhal, Som, et~al.]{wang2022image} Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.~K., Singhal, S., Som, S., et~al. 
 Image as a foreign language: Beit pretraining for all vision and vision-language tasks. 
 \emph{arXiv preprint arXiv:2208.10442}, 2022{\natexlab{b}}."
2405.19654,wang2023large,"[Wang et~al.(2023)Wang, Zhao, and Petzold]{wang2023large} Wang, Y., Zhao, Y., and Petzold, L.",Are large language models ready for healthcare? a comparative study on clinical language understanding.,Are large language models ready for healthcare? a comparative study on clinical language understanding.,,"[Wang et~al.(2023)Wang, Zhao, and Petzold]{wang2023large} Wang, Y., Zhao, Y., and Petzold, L. 
 Are large language models ready for healthcare? a comparative study on clinical language understanding. 
 \emph{arXiv preprint arXiv:2304.05368}, 2023."
2405.19654,wang2022medclip,"[Wang et~al.(2022{\natexlab{c}})Wang, Wu, Agarwal, and Sun]{wang2022medclip} Wang, Z., Wu, Z., Agarwal, D., and Sun, J.",Medclip: Contrastive learning from unpaired medical images and text.,Medclip: Contrastive learning from unpaired medical images and text.,,"[Wang et~al.(2022{\natexlab{c}})Wang, Wu, Agarwal, and Sun]{wang2022medclip} Wang, Z., Wu, Z., Agarwal, D., and Sun, J. 
 Medclip: Contrastive learning from unpaired medical images and text. 
 \emph{arXiv preprint arXiv:2210.10163}, 2022{\natexlab{c}}."
2405.19654,yunxiang2023chatdoctor,"[Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You]{yunxiang2023chatdoctor} Yunxiang, L., Zihan, L., Kai, Z., Ruilong, D., and You, Z.",Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,,"[Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You]{yunxiang2023chatdoctor} Yunxiang, L., Zihan, L., Kai, Z., Ruilong, D., and You, Z. 
 Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. 
 \emph{arXiv preprint arXiv:2303.14070}, 2023."
2405.19654,zhou2023advancing,"[Zhou et~al.(2023)Zhou, Lian, Wang, and Yu]{zhou2023advancing} Zhou, H.-Y., Lian, C., Wang, L., and Yu, Y.",Advancing radiograph representation learning with masked record modeling.,Advancing radiograph representation learning with masked record modeling.,,"[Zhou et~al.(2023)Zhou, Lian, Wang, and Yu]{zhou2023advancing} Zhou, H.-Y., Lian, C., Wang, L., and Yu, Y. 
 Advancing radiograph representation learning with masked record modeling. 
 \emph{arXiv preprint arXiv:2301.13155}, 2023."
2405.19716,bai2023qwen,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou and Zhou}]{bai2023qwen} \textsc{Bai, J.}, \textsc{Bai, S.}, \textsc{Yang, S.}, \textsc{Wang, S.}, \textsc{Tan, S.}, \textsc{Wang, P.}, \textsc{Lin, J.}, \textsc{Zhou, C.} and \textsc{Zhou, J.} (2023).","Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.","Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou and Zhou}]{bai2023qwen} \textsc{Bai, J.}, \textsc{Bai, S.}, \textsc{Yang, S.}, \textsc{Wang, S.}, \textsc{Tan, S.}, \textsc{Wang, P.}, \textsc{Lin, J.}, \textsc{Zhou, C.} and \textsc{Zhou, J.} (2023). 
 Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 
 \textit{arXiv preprint arXiv:2308.12966} ."
2405.19716,bai2022training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} \textsc{Bai, Y.}, \textsc{Jones, A.}, \textsc{Ndousse, K.}, \textsc{Askell, A.}, \textsc{Chen, A.}, \textsc{DasSarma, N.}, \textsc{Drain, D.}, \textsc{Fort, S.}, \textsc{Ganguli, D.}, \textsc{Henighan, T.} \textsc{et~al.} (2022).",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} \textsc{Bai, Y.}, \textsc{Jones, A.}, \textsc{Ndousse, K.}, \textsc{Askell, A.}, \textsc{Chen, A.}, \textsc{DasSarma, N.}, \textsc{Drain, D.}, \textsc{Fort, S.}, \textsc{Ganguli, D.}, \textsc{Henighan, T.} \textsc{et~al.} (2022). 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \textit{arXiv preprint arXiv:2204.05862} ."
2405.19716,casper2023open,"[{Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire et~al.}]{casper2023open} \textsc{Casper, S.}, \textsc{Davies, X.}, \textsc{Shi, C.}, \textsc{Gilbert, T.~K.}, \textsc{Scheurer, J.}, \textsc{Rando, J.}, \textsc{Freedman, R.}, \textsc{Korbak, T.}, \textsc{Lindner, D.}, \textsc{Freire, P.} \textsc{et~al.} (2023).",Open problems and fundamental limitations of reinforcement learning from human feedback.,Open problems and fundamental limitations of reinforcement learning from human feedback.,,"[{Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire et~al.}]{casper2023open} \textsc{Casper, S.}, \textsc{Davies, X.}, \textsc{Shi, C.}, \textsc{Gilbert, T.~K.}, \textsc{Scheurer, J.}, \textsc{Rando, J.}, \textsc{Freedman, R.}, \textsc{Korbak, T.}, \textsc{Lindner, D.}, \textsc{Freire, P.} \textsc{et~al.} (2023). 
 Open problems and fundamental limitations of reinforcement learning from human feedback. 
 \textit{arXiv preprint arXiv:2307.15217} ."
2405.19716,pali,"[{Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer et~al.}]{pali} \textsc{Chen, X.}, \textsc{Wang, X.}, \textsc{Changpinyo, S.}, \textsc{Piergiovanni, A.}, \textsc{Padlewski, P.}, \textsc{Salz, D.}, \textsc{Goodman, S.}, \textsc{Grycner, A.}, \textsc{Mustafa, B.}, \textsc{Beyer, L.} \textsc{et~al.} (2022).",Pali: A jointly-scaled multilingual language-image model.,Pali: A jointly-scaled multilingual language-image model.,,"[{Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer et~al.}]{pali} \textsc{Chen, X.}, \textsc{Wang, X.}, \textsc{Changpinyo, S.}, \textsc{Piergiovanni, A.}, \textsc{Padlewski, P.}, \textsc{Salz, D.}, \textsc{Goodman, S.}, \textsc{Grycner, A.}, \textsc{Mustafa, B.}, \textsc{Beyer, L.} \textsc{et~al.} (2022). 
 Pali: A jointly-scaled multilingual language-image model. 
 \textit{arXiv preprint arXiv:2209.06794} ."
2405.19716,chen2023understanding,"[{Chen et~al.(2023{\natexlab{a}})Chen, Deng, Li and Gu}]{chen2023understanding} \textsc{Chen, Z.}, \textsc{Deng, Y.}, \textsc{Li, Y.} and \textsc{Gu, Q.} (2023{\natexlab{a}}).",Understanding transferable representation learning and zero-shot transfer in clip.,Understanding transferable representation learning and zero-shot transfer in clip.,,"[{Chen et~al.(2023{\natexlab{a}})Chen, Deng, Li and Gu}]{chen2023understanding} \textsc{Chen, Z.}, \textsc{Deng, Y.}, \textsc{Li, Y.} and \textsc{Gu, Q.} (2023{\natexlab{a}}). 
 Understanding transferable representation learning and zero-shot transfer in clip. 
 \textit{arXiv preprint arXiv:2310.00927} ."
2405.19716,chen2024self,"[{Chen et~al.(2024)Chen, Deng, Yuan, Ji and Gu}]{chen2024self} \textsc{Chen, Z.}, \textsc{Deng, Y.}, \textsc{Yuan, H.}, \textsc{Ji, K.} and \textsc{Gu, Q.} (2024).",Self-play fine-tuning converts weak language models to strong language models.,Self-play fine-tuning converts weak language models to strong language models.,,"[{Chen et~al.(2024)Chen, Deng, Yuan, Ji and Gu}]{chen2024self} \textsc{Chen, Z.}, \textsc{Deng, Y.}, \textsc{Yuan, H.}, \textsc{Ji, K.} and \textsc{Gu, Q.} (2024). 
 Self-play fine-tuning converts weak language models to strong language models. 
 \textit{arXiv preprint arXiv:2401.01335} ."
2405.19716,chen2023internvl,"[{Chen et~al.(2023{\natexlab{b}})Chen, Wu, Wang, Su, Chen, Xing, Muyan, Zhang, Zhu, Lu et~al.}]{chen2023internvl} \textsc{Chen, Z.}, \textsc{Wu, J.}, \textsc{Wang, W.}, \textsc{Su, W.}, \textsc{Chen, G.}, \textsc{Xing, S.}, \textsc{Muyan, Z.}, \textsc{Zhang, Q.}, \textsc{Zhu, X.}, \textsc{Lu, L.} \textsc{et~al.} (2023{\natexlab{b}}).",Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.,Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.,,"[{Chen et~al.(2023{\natexlab{b}})Chen, Wu, Wang, Su, Chen, Xing, Muyan, Zhang, Zhu, Lu et~al.}]{chen2023internvl} \textsc{Chen, Z.}, \textsc{Wu, J.}, \textsc{Wang, W.}, \textsc{Su, W.}, \textsc{Chen, G.}, \textsc{Xing, S.}, \textsc{Muyan, Z.}, \textsc{Zhang, Q.}, \textsc{Zhu, X.}, \textsc{Lu, L.} \textsc{et~al.} (2023{\natexlab{b}}). 
 Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. 
 \textit{arXiv preprint arXiv:2312.14238} ."
2405.19716,deng2023rephrase,"[{Deng et~al.(2023)Deng, Zhang, Chen and Gu}]{deng2023rephrase} \textsc{Deng, Y.}, \textsc{Zhang, W.}, \textsc{Chen, Z.} and \textsc{Gu, Q.} (2023).",Rephrase and respond: Let large language models ask better questions for themselves.,Rephrase and respond: Let large language models ask better questions for themselves.,,"[{Deng et~al.(2023)Deng, Zhang, Chen and Gu}]{deng2023rephrase} \textsc{Deng, Y.}, \textsc{Zhang, W.}, \textsc{Chen, Z.} and \textsc{Gu, Q.} (2023). 
 Rephrase and respond: Let large language models ask better questions for themselves. 
 \textit{arXiv preprint arXiv:2311.04205} ."
2405.19716,ethayarajh2024kto,"[{Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky and Kiela}]{ethayarajh2024kto} \textsc{Ethayarajh, K.}, \textsc{Xu, W.}, \textsc{Muennighoff, N.}, \textsc{Jurafsky, D.} and \textsc{Kiela, D.} (2024).",Kto: Model alignment as prospect theoretic optimization.,Kto: Model alignment as prospect theoretic optimization.,,"[{Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky and Kiela}]{ethayarajh2024kto} \textsc{Ethayarajh, K.}, \textsc{Xu, W.}, \textsc{Muennighoff, N.}, \textsc{Jurafsky, D.} and \textsc{Kiela, D.} (2024). 
 Kto: Model alignment as prospect theoretic optimization. 
 \textit{arXiv preprint arXiv:2402.01306} ."
2405.19716,franken2024self,"[{Fr{\""a}nken et~al.(2024)Fr{\""a}nken, Zelikman, Rafailov, Gandhi, Gerstenberg and Goodman}]{franken2024self} \textsc{Fr{\""a}nken, J.-P.}, \textsc{Zelikman, E.}, \textsc{Rafailov, R.}, \textsc{Gandhi, K.}, \textsc{Gerstenberg, T.} and \textsc{Goodman, N.~D.} (2024).",Self-supervised alignment with mutual information: Learning to follow principles without preference labels.,Self-supervised alignment with mutual information: Learning to follow principles without preference labels.,,"[{Fr{\""a}nken et~al.(2024)Fr{\""a}nken, Zelikman, Rafailov, Gandhi, Gerstenberg and Goodman}]{franken2024self} \textsc{Fr{\""a}nken, J.-P.}, \textsc{Zelikman, E.}, \textsc{Rafailov, R.}, \textsc{Gandhi, K.}, \textsc{Gerstenberg, T.} and \textsc{Goodman, N.~D.} (2024). 
 Self-supervised alignment with mutual information: Learning to follow principles without preference labels. 
 \textit{arXiv preprint arXiv:2404.14313} ."
2405.19716,gao2023llamaadapterv2,"[{Gao et~al.(2023{\natexlab{b}})Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, Li and Qiao}]{gao2023llamaadapterv2} \textsc{Gao, P.}, \textsc{Han, J.}, \textsc{Zhang, R.}, \textsc{Lin, Z.}, \textsc{Geng, S.}, \textsc{Zhou, A.}, \textsc{Zhang, W.}, \textsc{Lu, P.}, \textsc{He, C.}, \textsc{Yue, X.}, \textsc{Li, H.} and \textsc{Qiao, Y.} (2023{\natexlab{b}}).",Llama-adapter v2: Parameter-efficient visual instruction model.,Llama-adapter v2: Parameter-efficient visual instruction model.,,"[{Gao et~al.(2023{\natexlab{b}})Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, Li and Qiao}]{gao2023llamaadapterv2} \textsc{Gao, P.}, \textsc{Han, J.}, \textsc{Zhang, R.}, \textsc{Lin, Z.}, \textsc{Geng, S.}, \textsc{Zhou, A.}, \textsc{Zhang, W.}, \textsc{Lu, P.}, \textsc{He, C.}, \textsc{Yue, X.}, \textsc{Li, H.} and \textsc{Qiao, Y.} (2023{\natexlab{b}}). 
 Llama-adapter v2: Parameter-efficient visual instruction model. 
 \textit{arXiv preprint arXiv:2304.15010} ."
2405.19716,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} \textsc{Jiang, A.~Q.}, \textsc{Sablayrolles, A.}, \textsc{Mensch, A.}, \textsc{Bamford, C.}, \textsc{Chaplot, D.~S.}, \textsc{Casas, D. d.~l.}, \textsc{Bressand, F.}, \textsc{Lengyel, G.}, \textsc{Lample, G.}, \textsc{Saulnier, L.} \textsc{et~al.} (2023).",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} \textsc{Jiang, A.~Q.}, \textsc{Sablayrolles, A.}, \textsc{Mensch, A.}, \textsc{Bamford, C.}, \textsc{Chaplot, D.~S.}, \textsc{Casas, D. d.~l.}, \textsc{Bressand, F.}, \textsc{Lengyel, G.}, \textsc{Lample, G.}, \textsc{Saulnier, L.} \textsc{et~al.} (2023). 
 Mistral 7b. 
 \textit{arXiv preprint arXiv:2310.06825} ."
2405.19716,josifoski2023exploiting,"[{Josifoski et~al.(2023)Josifoski, Sakota, Peyrard and West}]{josifoski2023exploiting} \textsc{Josifoski, M.}, \textsc{Sakota, M.}, \textsc{Peyrard, M.} and \textsc{West, R.} (2023).",Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction.,Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction.,,"[{Josifoski et~al.(2023)Josifoski, Sakota, Peyrard and West}]{josifoski2023exploiting} \textsc{Josifoski, M.}, \textsc{Sakota, M.}, \textsc{Peyrard, M.} and \textsc{West, R.} (2023). 
 Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. 
 \textit{arXiv preprint arXiv:2303.04132} ."
2405.19716,kim2023solar,"[{Kim et~al.(2023)Kim, Park, Kim, Lee, Song, Kim, Kim, Kim, Lee, Kim et~al.}]{kim2023solar} \textsc{Kim, D.}, \textsc{Park, C.}, \textsc{Kim, S.}, \textsc{Lee, W.}, \textsc{Song, W.}, \textsc{Kim, Y.}, \textsc{Kim, H.}, \textsc{Kim, Y.}, \textsc{Lee, H.}, \textsc{Kim, J.} \textsc{et~al.} (2023).",Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.,Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.,,"[{Kim et~al.(2023)Kim, Park, Kim, Lee, Song, Kim, Kim, Kim, Lee, Kim et~al.}]{kim2023solar} \textsc{Kim, D.}, \textsc{Park, C.}, \textsc{Kim, S.}, \textsc{Lee, W.}, \textsc{Song, W.}, \textsc{Kim, Y.}, \textsc{Kim, H.}, \textsc{Kim, Y.}, \textsc{Lee, H.}, \textsc{Kim, J.} \textsc{et~al.} (2023). 
 Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. 
 \textit{arXiv preprint arXiv:2312.15166} ."
2405.19716,blip,"[{Li et~al.(2023{\natexlab{b}})Li, Li, Savarese and Hoi}]{blip} \textsc{Li, J.}, \textsc{Li, D.}, \textsc{Savarese, S.} and \textsc{Hoi, S.} (2023{\natexlab{b}}).",Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,,"[{Li et~al.(2023{\natexlab{b}})Li, Li, Savarese and Hoi}]{blip} \textsc{Li, J.}, \textsc{Li, D.}, \textsc{Savarese, S.} and \textsc{Hoi, S.} (2023{\natexlab{b}}). 
 Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. 
 \textit{arXiv preprint arXiv:2301.12597} ."
2405.19716,liu2023improved,"[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Li and Lee}]{liu2023improved} \textsc{Liu, H.}, \textsc{Li, C.}, \textsc{Li, Y.} and \textsc{Lee, Y.~J.} (2023{\natexlab{a}}).",Improved baselines with visual instruction tuning.,Improved baselines with visual instruction tuning.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Li and Lee}]{liu2023improved} \textsc{Liu, H.}, \textsc{Li, C.}, \textsc{Li, Y.} and \textsc{Lee, Y.~J.} (2023{\natexlab{a}}). 
 Improved baselines with visual instruction tuning. 
 \textit{arXiv preprint arXiv:2310.03744} ."
2405.19716,liu2023mmbench,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} \textsc{Liu, Y.}, \textsc{Duan, H.}, \textsc{Zhang, Y.}, \textsc{Li, B.}, \textsc{Zhang, S.}, \textsc{Zhao, W.}, \textsc{Yuan, Y.}, \textsc{Wang, J.}, \textsc{He, C.}, \textsc{Liu, Z.} \textsc{et~al.} (2023{\natexlab{c}}).",Mmbench: Is your multi-modal model an all-around player?,Mmbench: Is your multi-modal model an all-around player?,,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} \textsc{Liu, Y.}, \textsc{Duan, H.}, \textsc{Zhang, Y.}, \textsc{Li, B.}, \textsc{Zhang, S.}, \textsc{Zhao, W.}, \textsc{Yuan, Y.}, \textsc{Wang, J.}, \textsc{He, C.}, \textsc{Liu, Z.} \textsc{et~al.} (2023{\natexlab{c}}). 
 Mmbench: Is your multi-modal model an all-around player? 
 \textit{arXiv preprint arXiv:2307.06281} ."
2405.19716,mckinzie2024mm1,"[{McKinzie et~al.(2024)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Weers et~al.}]{mckinzie2024mm1} \textsc{McKinzie, B.}, \textsc{Gan, Z.}, \textsc{Fauconnier, J.-P.}, \textsc{Dodge, S.}, \textsc{Zhang, B.}, \textsc{Dufter, P.}, \textsc{Shah, D.}, \textsc{Du, X.}, \textsc{Peng, F.}, \textsc{Weers, F.} \textsc{et~al.} (2024).","Mm1: Methods, analysis \& insights from multimodal llm pre-training.","Mm1: Methods, analysis \& insights from multimodal llm pre-training.",,"[{McKinzie et~al.(2024)McKinzie, Gan, Fauconnier, Dodge, Zhang, Dufter, Shah, Du, Peng, Weers et~al.}]{mckinzie2024mm1} \textsc{McKinzie, B.}, \textsc{Gan, Z.}, \textsc{Fauconnier, J.-P.}, \textsc{Dodge, S.}, \textsc{Zhang, B.}, \textsc{Dufter, P.}, \textsc{Shah, D.}, \textsc{Du, X.}, \textsc{Peng, F.}, \textsc{Weers, F.} \textsc{et~al.} (2024). 
 Mm1: Methods, analysis \& insights from multimodal llm pre-training. 
 \textit{arXiv preprint arXiv:2403.09611} ."
2405.19716,pang2024iterative,"[{Pang et~al.(2024)Pang, Yuan, Cho, He, Sukhbaatar and Weston}]{pang2024iterative} \textsc{Pang, R.~Y.}, \textsc{Yuan, W.}, \textsc{Cho, K.}, \textsc{He, H.}, \textsc{Sukhbaatar, S.} and \textsc{Weston, J.} (2024).",Iterative reasoning preference optimization.,Iterative reasoning preference optimization.,,"[{Pang et~al.(2024)Pang, Yuan, Cho, He, Sukhbaatar and Weston}]{pang2024iterative} \textsc{Pang, R.~Y.}, \textsc{Yuan, W.}, \textsc{Cho, K.}, \textsc{He, H.}, \textsc{Sukhbaatar, S.} and \textsc{Weston, J.} (2024). 
 Iterative reasoning preference optimization. 
 \textit{arXiv preprint arXiv:2404.19733} ."
2405.19716,prasad2023rephrase,"[{Prasad et~al.(2023)Prasad, Stengel-Eskin and Bansal}]{prasad2023rephrase} \textsc{Prasad, A.}, \textsc{Stengel-Eskin, E.} and \textsc{Bansal, M.} (2023).","Rephrase, augment, reason: Visual grounding of questions for vision-language models.","Rephrase, augment, reason: Visual grounding of questions for vision-language models.",,"[{Prasad et~al.(2023)Prasad, Stengel-Eskin and Bansal}]{prasad2023rephrase} \textsc{Prasad, A.}, \textsc{Stengel-Eskin, E.} and \textsc{Bansal, M.} (2023). 
 Rephrase, augment, reason: Visual grounding of questions for vision-language models. 
 \textit{arXiv preprint arXiv:2310.05861} ."
2405.19716,rafailov2023direct,"[{Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning and Finn}]{rafailov2023direct} \textsc{Rafailov, R.}, \textsc{Sharma, A.}, \textsc{Mitchell, E.}, \textsc{Ermon, S.}, \textsc{Manning, C.~D.} and \textsc{Finn, C.} (2023).",Direct preference optimization: Your language model is secretly a reward model.,Direct preference optimization: Your language model is secretly a reward model.,,"[{Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning and Finn}]{rafailov2023direct} \textsc{Rafailov, R.}, \textsc{Sharma, A.}, \textsc{Mitchell, E.}, \textsc{Ermon, S.}, \textsc{Manning, C.~D.} and \textsc{Finn, C.} (2023). 
 Direct preference optimization: Your language model is secretly a reward model. 
 \textit{arXiv preprint arXiv:2305.18290} ."
2405.19716,rosset2024direct,"[{Rosset et~al.(2024)Rosset, Cheng, Mitra, Santacroce, Awadallah and Xie}]{rosset2024direct} \textsc{Rosset, C.}, \textsc{Cheng, C.-A.}, \textsc{Mitra, A.}, \textsc{Santacroce, M.}, \textsc{Awadallah, A.} and \textsc{Xie, T.} (2024).",Direct nash optimization: Teaching language models to self-improve with general preferences.,Direct nash optimization: Teaching language models to self-improve with general preferences.,,"[{Rosset et~al.(2024)Rosset, Cheng, Mitra, Santacroce, Awadallah and Xie}]{rosset2024direct} \textsc{Rosset, C.}, \textsc{Cheng, C.-A.}, \textsc{Mitra, A.}, \textsc{Santacroce, M.}, \textsc{Awadallah, A.} and \textsc{Xie, T.} (2024). 
 Direct nash optimization: Teaching language models to self-improve with general preferences. 
 \textit{arXiv preprint arXiv:2404.03715} ."
2405.19716,schulman2017proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford and Klimov}]{schulman2017proximal} \textsc{Schulman, J.}, \textsc{Wolski, F.}, \textsc{Dhariwal, P.}, \textsc{Radford, A.} and \textsc{Klimov, O.} (2017).",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford and Klimov}]{schulman2017proximal} \textsc{Schulman, J.}, \textsc{Wolski, F.}, \textsc{Dhariwal, P.}, \textsc{Radford, A.} and \textsc{Klimov, O.} (2017). 
 Proximal policy optimization algorithms. 
 \textit{arXiv preprint arXiv:1707.06347} ."
2405.19716,sun2023aligning,"[{Sun et~al.(2023)Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang et~al.}]{sun2023aligning} \textsc{Sun, Z.}, \textsc{Shen, S.}, \textsc{Cao, S.}, \textsc{Liu, H.}, \textsc{Li, C.}, \textsc{Shen, Y.}, \textsc{Gan, C.}, \textsc{Gui, L.-Y.}, \textsc{Wang, Y.-X.}, \textsc{Yang, Y.} \textsc{et~al.} (2023).",Aligning large multimodal models with factually augmented rlhf.,Aligning large multimodal models with factually augmented rlhf.,,"[{Sun et~al.(2023)Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang et~al.}]{sun2023aligning} \textsc{Sun, Z.}, \textsc{Shen, S.}, \textsc{Cao, S.}, \textsc{Liu, H.}, \textsc{Li, C.}, \textsc{Shen, Y.}, \textsc{Gan, C.}, \textsc{Gui, L.-Y.}, \textsc{Wang, Y.-X.}, \textsc{Yang, Y.} \textsc{et~al.} (2023). 
 Aligning large multimodal models with factually augmented rlhf. 
 \textit{arXiv preprint arXiv:2309.14525} ."
2405.19716,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} \textsc{Team, G.}, \textsc{Anil, R.}, \textsc{Borgeaud, S.}, \textsc{Wu, Y.}, \textsc{Alayrac, J.-B.}, \textsc{Yu, J.}, \textsc{Soricut, R.}, \textsc{Schalkwyk, J.}, \textsc{Dai, A.~M.}, \textsc{Hauth, A.} \textsc{et~al.} (2023).",Gemini: A family of highly capable multimodal models.,Gemini: A family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} \textsc{Team, G.}, \textsc{Anil, R.}, \textsc{Borgeaud, S.}, \textsc{Wu, Y.}, \textsc{Alayrac, J.-B.}, \textsc{Yu, J.}, \textsc{Soricut, R.}, \textsc{Schalkwyk, J.}, \textsc{Dai, A.~M.}, \textsc{Hauth, A.} \textsc{et~al.} (2023). 
 Gemini: A family of highly capable multimodal models. 
 \textit{arXiv preprint arXiv:2312.11805} ."
2405.19716,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} \textsc{Touvron, H.}, \textsc{Martin, L.}, \textsc{Stone, K.}, \textsc{Albert, P.}, \textsc{Almahairi, A.}, \textsc{Babaei, Y.}, \textsc{Bashlykov, N.}, \textsc{Batra, S.}, \textsc{Bhargava, P.}, \textsc{Bhosale, S.} \textsc{et~al.} (2023{\natexlab{a}}).",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} \textsc{Touvron, H.}, \textsc{Martin, L.}, \textsc{Stone, K.}, \textsc{Albert, P.}, \textsc{Almahairi, A.}, \textsc{Babaei, Y.}, \textsc{Bashlykov, N.}, \textsc{Batra, S.}, \textsc{Bhargava, P.}, \textsc{Bhosale, S.} \textsc{et~al.} (2023{\natexlab{a}}). 
 Llama 2: Open foundation and fine-tuned chat models. 
 \textit{arXiv preprint arXiv:2307.09288} ."
2405.19716,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} \textsc{Touvron, H.}, \textsc{Martin, L.}, \textsc{Stone, K.}, \textsc{Albert, P.}, \textsc{Almahairi, A.}, \textsc{Babaei, Y.}, \textsc{Bashlykov, N.}, \textsc{Batra, S.}, \textsc{Bhargava, P.}, \textsc{Bhosale, S.} \textsc{et~al.} (2023{\natexlab{b}}).",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} \textsc{Touvron, H.}, \textsc{Martin, L.}, \textsc{Stone, K.}, \textsc{Albert, P.}, \textsc{Almahairi, A.}, \textsc{Babaei, Y.}, \textsc{Bashlykov, N.}, \textsc{Batra, S.}, \textsc{Bhargava, P.}, \textsc{Bhosale, S.} \textsc{et~al.} (2023{\natexlab{b}}). 
 Llama 2: Open foundation and fine-tuned chat models. 
 \textit{arXiv preprint arXiv:2307.09288} ."
2405.19716,wu2024gpt,"[{Wu et~al.(2024)Wu, Yang, Li, Zhang, Liu, Guibas, Lin and Wetzstein}]{wu2024gpt} \textsc{Wu, T.}, \textsc{Yang, G.}, \textsc{Li, Z.}, \textsc{Zhang, K.}, \textsc{Liu, Z.}, \textsc{Guibas, L.}, \textsc{Lin, D.} and \textsc{Wetzstein, G.} (2024).",Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation.,Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation.,,"[{Wu et~al.(2024)Wu, Yang, Li, Zhang, Liu, Guibas, Lin and Wetzstein}]{wu2024gpt} \textsc{Wu, T.}, \textsc{Yang, G.}, \textsc{Li, Z.}, \textsc{Zhang, K.}, \textsc{Liu, Z.}, \textsc{Guibas, L.}, \textsc{Lin, D.} and \textsc{Wetzstein, G.} (2024). 
 Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation. 
 \textit{arXiv preprint arXiv:2401.04092} ."
2405.19716,xiong2023gibbs,"[{Xiong et~al.(2023)Xiong, Dong, Ye, Zhong, Jiang and Zhang}]{xiong2023gibbs} \textsc{Xiong, W.}, \textsc{Dong, H.}, \textsc{Ye, C.}, \textsc{Zhong, H.}, \textsc{Jiang, N.} and \textsc{Zhang, T.} (2023).",Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf.,Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf.,,"[{Xiong et~al.(2023)Xiong, Dong, Ye, Zhong, Jiang and Zhang}]{xiong2023gibbs} \textsc{Xiong, W.}, \textsc{Dong, H.}, \textsc{Ye, C.}, \textsc{Zhong, H.}, \textsc{Jiang, N.} and \textsc{Zhang, T.} (2023). 
 Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. 
 \textit{arXiv preprint arXiv:2312.11456} ."
2405.19716,xu2023some,"[{Xu et~al.(2023)Xu, Lee, Sukhbaatar and Weston}]{xu2023some} \textsc{Xu, J.}, \textsc{Lee, A.}, \textsc{Sukhbaatar, S.} and \textsc{Weston, J.} (2023).",Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,,"[{Xu et~al.(2023)Xu, Lee, Sukhbaatar and Weston}]{xu2023some} \textsc{Xu, J.}, \textsc{Lee, A.}, \textsc{Sukhbaatar, S.} and \textsc{Weston, J.} (2023). 
 Some things are more cringe than others: Preference optimization with the pairwise cringe loss. 
 \textit{arXiv preprint arXiv:2312.16682} ."
2405.19716,yu2023mm,"[{Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang and Wang}]{yu2023mm} \textsc{Yu, W.}, \textsc{Yang, Z.}, \textsc{Li, L.}, \textsc{Wang, J.}, \textsc{Lin, K.}, \textsc{Liu, Z.}, \textsc{Wang, X.} and \textsc{Wang, L.} (2023).",Mm-vet: Evaluating large multimodal models for integrated capabilities.,Mm-vet: Evaluating large multimodal models for integrated capabilities.,,"[{Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang and Wang}]{yu2023mm} \textsc{Yu, W.}, \textsc{Yang, Z.}, \textsc{Li, L.}, \textsc{Wang, J.}, \textsc{Lin, K.}, \textsc{Liu, Z.}, \textsc{Wang, X.} and \textsc{Wang, L.} (2023). 
 Mm-vet: Evaluating large multimodal models for integrated capabilities. 
 \textit{arXiv preprint arXiv:2308.02490} ."
2405.19716,yuan2024self,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu and Weston}]{yuan2024self} \textsc{Yuan, W.}, \textsc{Pang, R.~Y.}, \textsc{Cho, K.}, \textsc{Sukhbaatar, S.}, \textsc{Xu, J.} and \textsc{Weston, J.} (2024).",Self-rewarding language models.,Self-rewarding language models.,,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu and Weston}]{yuan2024self} \textsc{Yuan, W.}, \textsc{Pang, R.~Y.}, \textsc{Cho, K.}, \textsc{Sukhbaatar, S.}, \textsc{Xu, J.} and \textsc{Weston, J.} (2024). 
 Self-rewarding language models. 
 \textit{arXiv preprint arXiv:2401.10020} ."
2405.19716,zhao2023slic,"[{Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh and Liu}]{zhao2023slic} \textsc{Zhao, Y.}, \textsc{Joshi, R.}, \textsc{Liu, T.}, \textsc{Khalman, M.}, \textsc{Saleh, M.} and \textsc{Liu, P.~J.} (2023).",Slic-hf: Sequence likelihood calibration with human feedback.,Slic-hf: Sequence likelihood calibration with human feedback.,,"[{Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh and Liu}]{zhao2023slic} \textsc{Zhao, Y.}, \textsc{Joshi, R.}, \textsc{Liu, T.}, \textsc{Khalman, M.}, \textsc{Saleh, M.} and \textsc{Liu, P.~J.} (2023). 
 Slic-hf: Sequence likelihood calibration with human feedback. 
 \textit{arXiv preprint arXiv:2305.10425} ."
2405.19716,zheng2024weak,"[{Zheng et~al.(2024)Zheng, Wang, Ji, Huang and Peng}]{zheng2024weak} \textsc{Zheng, C.}, \textsc{Wang, Z.}, \textsc{Ji, H.}, \textsc{Huang, M.} and \textsc{Peng, N.} (2024).",Weak-to-strong extrapolation expedites alignment.,Weak-to-strong extrapolation expedites alignment.,,"[{Zheng et~al.(2024)Zheng, Wang, Ji, Huang and Peng}]{zheng2024weak} \textsc{Zheng, C.}, \textsc{Wang, Z.}, \textsc{Ji, H.}, \textsc{Huang, M.} and \textsc{Peng, N.} (2024). 
 Weak-to-strong extrapolation expedites alignment. 
 \textit{arXiv preprint arXiv:2404.16792} ."
2405.19716,zhou2024aligning,"[{Zhou et~al.(2024)Zhou, Cui, Rafailov, Finn and Yao}]{zhou2024aligning} \textsc{Zhou, Y.}, \textsc{Cui, C.}, \textsc{Rafailov, R.}, \textsc{Finn, C.} and \textsc{Yao, H.} (2024).",Aligning modalities in vision large language models via preference fine-tuning.,Aligning modalities in vision large language models via preference fine-tuning.,,"[{Zhou et~al.(2024)Zhou, Cui, Rafailov, Finn and Yao}]{zhou2024aligning} \textsc{Zhou, Y.}, \textsc{Cui, C.}, \textsc{Rafailov, R.}, \textsc{Finn, C.} and \textsc{Yao, H.} (2024). 
 Aligning modalities in vision large language models via preference fine-tuning. 
 \textit{arXiv preprint arXiv:2402.11411} ."
2405.19744,asai2023buffet,"[{Asai et~al.(2023)Asai, Kudugunta, Yu, Blevins, Gonen, Reid, Tsvetkov, Ruder, and Hajishirzi}]{asai2023buffet} Akari Asai, Sneha Kudugunta, Xinyan~Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023.",Buffet: Benchmarking large language models for few-shot cross-lingual transfer.,Buffet: Benchmarking large language models for few-shot cross-lingual transfer.,,"[{Asai et~al.(2023)Asai, Kudugunta, Yu, Blevins, Gonen, Reid, Tsvetkov, Ruder, and Hajishirzi}]{asai2023buffet} Akari Asai, Sneha Kudugunta, Xinyan~Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023. 
 Buffet: Benchmarking large language models for few-shot cross-lingual transfer. 
 \emph{arXiv preprint arXiv:2305.14857}."
2405.19744,bai2022constitutional,"[{Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 2022.",Constitutional ai: Harmlessness from ai feedback.,Constitutional ai: Harmlessness from ai feedback.,,"[{Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 2022. 
 Constitutional ai: Harmlessness from ai feedback. 
 \emph{arXiv preprint arXiv:2212.08073}."
2405.19744,chen2023alpagasus,"[{Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang et~al.}]{chen2023alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 2023.",Alpagasus: Training a better alpaca with fewer data.,Alpagasus: Training a better alpaca with fewer data.,,"[{Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang et~al.}]{chen2023alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 2023. 
 Alpagasus: Training a better alpaca with fewer data. 
 \emph{arXiv preprint arXiv:2307.08701}."
2405.19744,chung2022scaling,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2405.19744,kopf2023openassistant,"[{K{\""o}pf et~al.(2023)K{\""o}pf, Kilcher, von R{\""u}tte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi et~al.}]{kopf2023openassistant} Andreas K{\""o}pf, Yannic Kilcher, Dimitri von R{\""u}tte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver Stanley, Rich{\'a}rd Nagyfi, et~al. 2023.",Openassistant conversations--democratizing large language model alignment.,Openassistant conversations--democratizing large language model alignment.,,"[{K{\""o}pf et~al.(2023)K{\""o}pf, Kilcher, von R{\""u}tte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi et~al.}]{kopf2023openassistant} Andreas K{\""o}pf, Yannic Kilcher, Dimitri von R{\""u}tte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver Stanley, Rich{\'a}rd Nagyfi, et~al. 2023. 
 Openassistant conversations--democratizing large language model alignment. 
 \emph{arXiv preprint arXiv:2304.07327}."
2405.19744,li2023align,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Zhang, and Zong}]{li2023align} Chong Li, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2023{\natexlab{a}}.",Align after pre-train: Improving multilingual generative models with cross-lingual alignment.,Align after pre-train: Improving multilingual generative models with cross-lingual alignment.,,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Zhang, and Zong}]{li2023align} Chong Li, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2023{\natexlab{a}}. 
 Align after pre-train: Improving multilingual generative models with cross-lingual alignment. 
 \emph{arXiv preprint arXiv:2311.08089}."
2405.19744,li2023bactrian,"[{Li et~al.(2023{\natexlab{b}})Li, Koto, Wu, Aji, and Baldwin}]{li2023bactrian} Haonan Li, Fajri Koto, Minghao Wu, Alham~Fikri Aji, and Timothy Baldwin. 2023{\natexlab{b}}.",Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation.,Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation.,,"[{Li et~al.(2023{\natexlab{b}})Li, Koto, Wu, Aji, and Baldwin}]{li2023bactrian} Haonan Li, Fajri Koto, Minghao Wu, Alham~Fikri Aji, and Timothy Baldwin. 2023{\natexlab{b}}. 
 Bactrian-x: A multilingual replicable instruction-following model with low-rank adaptation. 
 \emph{arXiv preprint arXiv:2305.15011}."
2405.19744,li2023self,"[{Li et~al.(2023{\natexlab{c}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis}]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023{\natexlab{c}}.",Self-alignment with instruction backtranslation.,Self-alignment with instruction backtranslation.,,"[{Li et~al.(2023{\natexlab{c}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis}]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023{\natexlab{c}}. 
 Self-alignment with instruction backtranslation. 
 \emph{arXiv preprint arXiv:2308.06259}."
2405.19744,liu2023multilingual,"[{Liu et~al.(2023{\natexlab{a}})Liu, Koto, Baldwin, and Gurevych}]{liu2023multilingual} Chen~Cecilia Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. 2023{\natexlab{a}}.",Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings.,Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Koto, Baldwin, and Gurevych}]{liu2023multilingual} Chen~Cecilia Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. 2023{\natexlab{a}}. 
 Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings. 
 \emph{arXiv preprint arXiv:2309.08591}."
2405.19744,nguyen2023culturax,"[{Nguyen et~al.(2023)Nguyen, Van~Nguyen, Lai, Man, Ngo, Dernoncourt, Rossi, and Nguyen}]{nguyen2023culturax} Thuat Nguyen, Chien Van~Nguyen, Viet~Dac Lai, Hieu Man, Nghia~Trung Ngo, Franck Dernoncourt, Ryan~A Rossi, and Thien~Huu Nguyen. 2023.","Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.","Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.",,"[{Nguyen et~al.(2023)Nguyen, Van~Nguyen, Lai, Man, Ngo, Dernoncourt, Rossi, and Nguyen}]{nguyen2023culturax} Thuat Nguyen, Chien Van~Nguyen, Viet~Dac Lai, Hieu Man, Nghia~Trung Ngo, Franck Dernoncourt, Ryan~A Rossi, and Thien~Huu Nguyen. 2023. 
 Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages. 
 \emph{arXiv preprint arXiv:2309.09400}."
2405.19744,sun2023principle,"[{Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan}]{sun2023principle} Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023.",Principle-driven self-alignment of language models from scratch with minimal human supervision.,Principle-driven self-alignment of language models from scratch with minimal human supervision.,,"[{Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan}]{sun2023principle} Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. 
 Principle-driven self-alignment of language models from scratch with minimal human supervision. 
 \emph{arXiv preprint arXiv:2305.03047}."
2405.19744,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2405.19744,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2405.19744,xu2023wizardlm,"[{Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang}]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.",Wizardlm: Empowering large language models to follow complex instructions.,Wizardlm: Empowering large language models to follow complex instructions.,,"[{Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang}]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. 
 Wizardlm: Empowering large language models to follow complex instructions. 
 \emph{arXiv preprint arXiv:2304.12244}."
2405.19744,yong2023low,"[{Yong et~al.(2023)Yong, Menghini, and Bach}]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach. 2023.",Low-resource languages jailbreak gpt-4.,Low-resource languages jailbreak gpt-4.,,"[{Yong et~al.(2023)Yong, Menghini, and Bach}]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach. 2023. 
 Low-resource languages jailbreak gpt-4. 
 \emph{arXiv preprint arXiv:2310.02446}."
2405.19744,zheng2023judging,"[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et~al.}]{zheng2023judging} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023.",Judging llm-as-a-judge with mt-bench and chatbot arena.,Judging llm-as-a-judge with mt-bench and chatbot arena.,,"[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et~al.}]{zheng2023judging} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023. 
 Judging llm-as-a-judge with mt-bench and chatbot arena. 
 \emph{arXiv preprint arXiv:2306.05685}."
2405.19744,zhou2023lima,"[{Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu et~al.}]{zhou2023lima} Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2023.",Lima: Less is more for alignment.,Lima: Less is more for alignment.,,"[{Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu et~al.}]{zhou2023lima} Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2023. 
 Lima: Less is more for alignment. 
 \emph{arXiv preprint arXiv:2305.11206}."
2405.20192,alkhamissi2022review,"[AlKhamissi et~al.(2022)AlKhamissi, Li, Celikyilmaz, Diab, and Ghazvininejad]{alkhamissi2022review} Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad.",A review on language models as knowledge bases.,A review on language models as knowledge bases.,,"[AlKhamissi et~al.(2022)AlKhamissi, Li, Celikyilmaz, Diab, and Ghazvininejad]{alkhamissi2022review} Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 
 A review on language models as knowledge bases. 
 \emph{arXiv preprint arXiv:2204.06031}, 2022."
2405.20192,anil2023palm,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.",Palm 2 technical report.,Palm 2 technical report.,,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}, 2023."
2405.20192,qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2405.20192,cao2023instruction,"[Cao et~al.(2023)Cao, Kang, and Sun]{cao2023instruction} Yihan Cao, Yanbin Kang, and Lichao Sun.",Instruction mining: High-quality instruction data selection for large language models.,Instruction mining: High-quality instruction data selection for large language models.,,"[Cao et~al.(2023)Cao, Kang, and Sun]{cao2023instruction} Yihan Cao, Yanbin Kang, and Lichao Sun. 
 Instruction mining: High-quality instruction data selection for large language models. 
 \emph{arXiv preprint arXiv:2307.06290}, 2023."
2405.20192,chen2023huatuogpt,"[Chen et~al.(2023)Chen, Wang, Gao, Jiang, Chen, Zhang, Song, Xie, Kong, Li, et~al.]{chen2023huatuogpt} Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, et~al.","Huatuogpt-ii, one-stage training for medical adaption of llms.","Huatuogpt-ii, one-stage training for medical adaption of llms.",,"[Chen et~al.(2023)Chen, Wang, Gao, Jiang, Chen, Zhang, Song, Xie, Kong, Li, et~al.]{chen2023huatuogpt} Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, et~al. 
 Huatuogpt-ii, one-stage training for medical adaption of llms. 
 \emph{arXiv preprint arXiv:2311.09774}, 2023."
2405.20192,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2405.20192,dou2023loramoe,"[Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan, et~al.]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al.",Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,,"[Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan, et~al.]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al. 
 Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. 
 \emph{arXiv preprint arXiv:2312.09979}, 2023."
2405.20192,ganguli2022red,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red} Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red} Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al. 
 Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. 
 \emph{arXiv preprint arXiv:2209.07858}, 2022."
2405.20192,gekhman2024does,"[Gekhman et~al.(2024)Gekhman, Yona, Aharoni, Eyal, Feder, Reichart, and Herzig]{gekhman2024does} Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig.",Does fine-tuning llms on new knowledge encourage hallucinations?,Does fine-tuning llms on new knowledge encourage hallucinations?,,"[Gekhman et~al.(2024)Gekhman, Yona, Aharoni, Eyal, Feder, Reichart, and Herzig]{gekhman2024does} Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 
 Does fine-tuning llms on new knowledge encourage hallucinations? 
 \emph{arXiv preprint arXiv:2405.05904}, 2024."
2405.20192,geva2020transformer,"[Geva et~al.(2020)Geva, Schuster, Berant, and Levy]{geva2020transformer} Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.",Transformer feed-forward layers are key-value memories.,Transformer feed-forward layers are key-value memories.,,"[Geva et~al.(2020)Geva, Schuster, Berant, and Levy]{geva2020transformer} Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 
 Transformer feed-forward layers are key-value memories. 
 \emph{arXiv preprint arXiv:2012.14913}, 2020."
2405.20192,kumar2024increased,"[Kumar et~al.(2024)Kumar, Kumar, Agarwal, and Harshangi]{kumar2024increased} Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi.",Increased llm vulnerabilities from fine-tuning and quantization.,Increased llm vulnerabilities from fine-tuning and quantization.,,"[Kumar et~al.(2024)Kumar, Kumar, Agarwal, and Harshangi]{kumar2024increased} Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 
 Increased llm vulnerabilities from fine-tuning and quantization. 
 \emph{arXiv preprint arXiv:2404.04392}, 2024."
2405.20192,li2023starcoder,"[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.",Starcoder: may the source be with you!,Starcoder: may the source be with you!,,"[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 
 Starcoder: may the source be with you! 
 \emph{arXiv preprint arXiv:2305.06161}, 2023{\natexlab{a}}."
2405.20192,liao2024ming,"[Liao et~al.(2024)Liao, Jiang, Wang, and Wang]{liao2024ming} Yusheng Liao, Shuyang Jiang, Yu~Wang, and Yanfeng Wang.",Ming-moe: Enhancing medical multi-task learning in large language models with sparse mixture of low-rank adapter experts.,Ming-moe: Enhancing medical multi-task learning in large language models with sparse mixture of low-rank adapter experts.,,"[Liao et~al.(2024)Liao, Jiang, Wang, and Wang]{liao2024ming} Yusheng Liao, Shuyang Jiang, Yu~Wang, and Yanfeng Wang. 
 Ming-moe: Enhancing medical multi-task learning in large language models with sparse mixture of low-rank adapter experts. 
 \emph{arXiv preprint arXiv:2404.09027}, 2024."
2405.20192,liu2020logiqa,"[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.",Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.,Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.,,"[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 
 Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. 
 \emph{arXiv preprint arXiv:2007.08124}, 2020."
2405.20192,liu2023benchmarking,"[Liu et~al.(2023{\natexlab{a}})Liu, Zhou, Hua, Chong, Tian, Liu, Wang, You, Guo, Zhu, et~al.]{liu2023benchmarking} Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et~al.",Benchmarking large language models on cmexam--a comprehensive chinese medical exam dataset.,Benchmarking large language models on cmexam--a comprehensive chinese medical exam dataset.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Zhou, Hua, Chong, Tian, Liu, Wang, You, Guo, Zhu, et~al.]{liu2023benchmarking} Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et~al. 
 Benchmarking large language models on cmexam--a comprehensive chinese medical exam dataset. 
 \emph{arXiv preprint arXiv:2306.03030}, 2023{\natexlab{a}}."
2405.20192,liu2024dora,"[Liu et~al.(2024)Liu, Wang, Yin, Molchanov, Wang, Cheng, and Chen]{liu2024dora} Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang~Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen.",Dora: Weight-decomposed low-rank adaptation.,Dora: Weight-decomposed low-rank adaptation.,,"[Liu et~al.(2024)Liu, Wang, Yin, Molchanov, Wang, Cheng, and Chen]{liu2024dora} Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang~Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 
 Dora: Weight-decomposed low-rank adaptation. 
 \emph{arXiv preprint arXiv:2402.09353}, 2024."
2405.20192,liu2023agentbench,"[Liu et~al.(2023{\natexlab{b}})Liu, Yu, Zhang, Xu, Lei, Lai, Gu, Ding, Men, Yang, et~al.]{liu2023agentbench} Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et~al.",Agentbench: Evaluating llms as agents.,Agentbench: Evaluating llms as agents.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Yu, Zhang, Xu, Lei, Lai, Gu, Ding, Men, Yang, et~al.]{liu2023agentbench} Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et~al. 
 Agentbench: Evaluating llms as agents. 
 \emph{arXiv preprint arXiv:2308.03688}, 2023{\natexlab{b}}."
2405.20192,lozhkov2024starcoder,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.",Starcoder 2 and the stack v2: The next generation.,Starcoder 2 and the stack v2: The next generation.,,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{lozhkov2024starcoder} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al. 
 Starcoder 2 and the stack v2: The next generation. 
 \emph{arXiv preprint arXiv:2402.19173}, 2024."
2405.20192,luo2023wizardmath,"[Luo et~al.(2023{\natexlab{a}})Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath} Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.",Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.,Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.,,"[Luo et~al.(2023{\natexlab{a}})Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath} Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 
 Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 
 \emph{arXiv preprint arXiv:2308.09583}, 2023{\natexlab{a}}."
2405.20192,luo2023empirical,"[Luo et~al.(2023{\natexlab{b}})Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.",An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,,"[Luo et~al.(2023{\natexlab{b}})Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 
 An empirical study of catastrophic forgetting in large language models during continual fine-tuning. 
 \emph{arXiv preprint arXiv:2308.08747}, 2023{\natexlab{b}}."
2405.20192,luo2023wizardcoder,"[Luo et~al.(2023{\natexlab{c}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.",Wizardcoder: Empowering code large language models with evol-instruct.,Wizardcoder: Empowering code large language models with evol-instruct.,,"[Luo et~al.(2023{\natexlab{c}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 
 Wizardcoder: Empowering code large language models with evol-instruct. 
 \emph{arXiv preprint arXiv:2306.08568}, 2023{\natexlab{c}}."
2405.20192,olsson2022context,"[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.",In-context learning and induction heads.,In-context learning and induction heads.,,"[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 
 In-context learning and induction heads. 
 \emph{arXiv preprint arXiv:2209.11895}, 2022."
2405.20192,peng2023instruction,"[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction} Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.",Instruction tuning with gpt-4.,Instruction tuning with gpt-4.,,"[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction} Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 
 Instruction tuning with gpt-4. 
 \emph{arXiv preprint arXiv:2304.03277}, 2023."
2405.20192,qi2023fine,"[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine} Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.","Fine-tuning aligned language models compromises safety, even when users do not intend to!","Fine-tuning aligned language models compromises safety, even when users do not intend to!",,"[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine} Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 
 Fine-tuning aligned language models compromises safety, even when users do not intend to! 
 \emph{arXiv preprint arXiv:2310.03693}, 2023."
2405.20192,qiu2024towards,"[Qiu et~al.(2024)Qiu, Wu, Zhang, Lin, Wang, Zhang, Wang, and Xie]{qiu2024towards} Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya~Zhang, Yanfeng Wang, and Weidi Xie.",Towards building multilingual language model for medicine.,Towards building multilingual language model for medicine.,,"[Qiu et~al.(2024)Qiu, Wu, Zhang, Lin, Wang, Zhang, Wang, and Xie]{qiu2024towards} Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 
 Towards building multilingual language model for medicine. 
 \emph{arXiv preprint arXiv:2402.13963}, 2024."
2405.20192,shazeer2020glu,[Shazeer(2020)]{shazeer2020glu} Noam Shazeer.,Glu variants improve transformer.,Glu variants improve transformer.,,"[Shazeer(2020)]{shazeer2020glu} Noam Shazeer. 
 Glu variants improve transformer. 
 \emph{arXiv preprint arXiv:2002.05202}, 2020."
2405.20192,shi2023replug,"[Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.",Replug: Retrieval-augmented black-box language models.,Replug: Retrieval-augmented black-box language models.,,"[Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 
 Replug: Retrieval-augmented black-box language models. 
 \emph{arXiv preprint arXiv:2301.12652}, 2023."
2405.20192,suzgun2022challenging,"[Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.",Challenging big-bench tasks and whether chain-of-thought can solve them.,Challenging big-bench tasks and whether chain-of-thought can solve them.,,"[Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 
 Challenging big-bench tasks and whether chain-of-thought can solve them. 
 \emph{arXiv preprint arXiv:2210.09261}, 2022."
2405.20192,toshniwal2024openmath,"[Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman]{toshniwal2024openmath} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.",Openmathinstruct-1: A 1.8 million math instruction tuning dataset.,Openmathinstruct-1: A 1.8 million math instruction tuning dataset.,,"[Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman]{toshniwal2024openmath} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. 
 Openmathinstruct-1: A 1.8 million math instruction tuning dataset. 
 \emph{arXiv preprint arXiv: Arxiv-2402.10176}, 2024."
2405.20192,touvron2023llama,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.20192,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.20192,wei2021finetuned,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}, 2021."
2405.20192,wu2023language,"[Wu et~al.(2023{\natexlab{a}})Wu, Yao, Chen, Pan, Wang, Liu, and Yu]{wu2023language} Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.",From language modeling to instruction following: Understanding the behavior shift in llms after instruction tuning.,From language modeling to instruction following: Understanding the behavior shift in llms after instruction tuning.,,"[Wu et~al.(2023{\natexlab{a}})Wu, Yao, Chen, Pan, Wang, Liu, and Yu]{wu2023language} Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu. 
 From language modeling to instruction following: Understanding the behavior shift in llms after instruction tuning. 
 \emph{arXiv preprint arXiv:2310.00492}, 2023{\natexlab{a}}."
2405.20192,yang2023fingpt,"[Yang et~al.(2023)Yang, Liu, and Wang]{yang2023fingpt} Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang.",Fingpt: Open-source financial large language models.,Fingpt: Open-source financial large language models.,,"[Yang et~al.(2023)Yang, Liu, and Wang]{yang2023fingpt} Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang. 
 Fingpt: Open-source financial large language models. 
 \emph{arXiv preprint arXiv:2306.06031}, 2023."
2405.20192,yang2024self,"[Yang et~al.(2024)Yang, Liu, Pang, Wang, Feng, Zhu, and Chen]{yang2024self} Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, and Wei Chen.",Self-distillation bridges distribution gap in language model fine-tuning.,Self-distillation bridges distribution gap in language model fine-tuning.,,"[Yang et~al.(2024)Yang, Liu, Pang, Wang, Feng, Zhu, and Chen]{yang2024self} Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, and Wei Chen. 
 Self-distillation bridges distribution gap in language model fine-tuning. 
 \emph{arXiv preprint arXiv:2402.13669}, 2024."
2405.20192,ye2023partial,"[Ye et~al.(2023)Ye, Huang, Tu, Li, Chen, He, and Ouyang]{ye2023partial} Peng Ye, Yongqi Huang, Chongjun Tu, Minglei Li, Tao Chen, Tong He, and Wanli Ouyang.",Partial fine-tuning: A successor to full fine-tuning for vision transformers.,Partial fine-tuning: A successor to full fine-tuning for vision transformers.,,"[Ye et~al.(2023)Ye, Huang, Tu, Li, Chen, He, and Ouyang]{ye2023partial} Peng Ye, Yongqi Huang, Chongjun Tu, Minglei Li, Tao Chen, Tong He, and Wanli Ouyang. 
 Partial fine-tuning: A successor to full fine-tuning for vision transformers. 
 \emph{arXiv preprint arXiv:2312.15681}, 2023."
2405.20192,yu2023language,"[Yu et~al.(2023)Yu, Yu, Yu, Huang, and Li]{yu2023language} Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li.",Language models are super mario: Absorbing abilities from homologous models as a free lunch.,Language models are super mario: Absorbing abilities from homologous models as a free lunch.,,"[Yu et~al.(2023)Yu, Yu, Yu, Huang, and Li]{yu2023language} Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 
 Language models are super mario: Absorbing abilities from homologous models as a free lunch. 
 \emph{arXiv preprint arXiv:2311.03099}, 2023."
2405.20192,yuan2023scaling,"[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou]{yuan2023scaling} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou.",Scaling relationship on learning mathematical reasoning with large language models.,Scaling relationship on learning mathematical reasoning with large language models.,,"[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou]{yuan2023scaling} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 
 Scaling relationship on learning mathematical reasoning with large language models. 
 \emph{arXiv preprint arXiv:2308.01825}, 2023."
2405.20192,yue2023mammoth,"[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2023mammoth} Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.",Mammoth: Building math generalist models through hybrid instruction tuning.,Mammoth: Building math generalist models through hybrid instruction tuning.,,"[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2023mammoth} Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen. 
 Mammoth: Building math generalist models through hybrid instruction tuning. 
 \emph{arXiv preprint arXiv:2309.05653}, 2023."
2405.20192,zhang2023instruction,"[Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al.",Instruction tuning for large language models: A survey.,Instruction tuning for large language models: A survey.,,"[Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al. 
 Instruction tuning for large language models: A survey. 
 \emph{arXiv preprint arXiv:2308.10792}, 2023."
2405.21068,chen2021evaluating,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.",{Evaluating Large Language Models Trained on Code}.,{Evaluating Large Language Models Trained on Code}.,,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 
 {Evaluating Large Language Models Trained on Code}. 
 \emph{arXiv:2107.03374}, 2021."
2405.21068,geminiteam2024gemini,"[{Gemini Team} et~al.(2024){Gemini Team}, Anil, Borgeaud, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, Silver, Johnson, Antonoglou, Schrittwieser, Glaese, Chen, Pitler, Lillicrap, Lazaridou, Firat, Molloy, Isard, Barham, Hennigan, Lee, Viola, Reynolds, Xu, Doherty, Collins, Meyer, Rutherford, Moreira, Ayoub, Goel, Krawczyk, Du, Chi, Cheng, Ni, Shah, Kane, Chan, Faruqui, Severyn, Lin, Li, Cheng, Ittycheriah, Mahdieh, Chen, Sun, Tran, Bagri, Lakshminarayanan, Liu, Orban, Güra, Zhou, Song, Boffy, Ganapathy, Zheng, Choe, Ágoston Weisz, Zhu, Lu, Gopal, Kahn, Kula, Pitman, Shah, Taropa, Merey, Baeuml, Chen, Shafey, Zhang, Sercinoglu, Tucker, Piqueras, Krikun, Barr, Savinov, Danihelka, Roelofs, White, Andreassen, von Glehn, Yagati, Kazemi, Gonzalez, Khalman, Sygnowski, Frechette, Smith, Culp, Proleev, Luan, Chen, Lottes, Schucher, Lebron, Rrustemi, Clay, Crone, Kocisky, Zhao, Perz, Yu, Howard, Bloniarz, Rae, Lu, Sifre, Maggioni, Alcober, Garrette, Barnes, Thakoor, Austin, Barth-Maron, Wong,   Joshi, Chaabouni, Fatiha, Ahuja, Tomar, Senter, Chadwick, Kornakov, Attaluri, Iturrate, Liu, Li, Cogan, Chen, Jia, Gu, Zhang, Grimstad, Hartman, Garcia, Pillai, Devlin, Laskin, de~Las~Casas, Valter, Tao, Blanco, Badia, Reitter, Chen, Brennan, Rivera, Brin, Iqbal, Surita, Labanowski, Rao, Winkler, Parisotto, Gu, Olszewska, Addanki, Miech, Louis, Teplyashin, Brown, Catt, Balaguer, Xiang, Wang, Ashwood, Briukhov, Webson, Ganapathy, Sanghavi, Kannan, Chang, Stjerngren, Djolonga, Sun, Bapna, Aitchison, Pejman, Michalewski, Yu, Wang, Love, Ahn, Bloxwich, Han, Humphreys, Sellam, Bradbury, Godbole, Samangooei, Damoc, Kaskasoli, Arnold, Vasudevan, Agrawal, Riesa, Lepikhin, Tanburn, Srinivasan, Lim, Hodkinson, Shyam, Ferret, Hand, Garg, Paine, Li, Li, Giang, Neitz, Abbas, York, Reid, Cole, Chowdhery, Das, Rogozińska, Nikolaev, Sprechmann, Nado, Zilka, Prost, He, Monteiro, Mishra, Welty, Newlan, Jia, Allamanis, Hu, de~Liedekerke, Gilmer, Saroufim, Rijhwani, Hou, Shrivastava, Baddepudi, Goldin, Ozturel, Cassirer, Xu,   Sohn, Sachan, Amplayo, Swanson, Petrova, Narayan, Guez, Brahma, Landon, Patel, Zhao, Villela, Wang, Jia, Rahtz, Giménez, Yeung, Keeling, Georgiev, Mincu, Wu, Haykal, Saputro, Vodrahalli, Qin, Cankara, Sharma, Fernando, Hawkins, Neyshabur, Kim, Hutter, Agrawal, Castro-Ros, van~den Driessche, Wang, Yang, yiin Chang, Komarek, McIlroy, Lučić, Zhang, Farhan, Sharman, Natsev, Michel, Bansal, Qiao, Cao, Shakeri, Butterfield, Chung, Rubenstein, Agrawal, Mensch, Soparkar, Lenc, Chung, Pope, Maggiore, Kay, Jhakra, Wang, Maynez, Phuong, Tobin, Tacchetti, Trebacz, Robinson, Katariya, Riedel, Bailey, Xiao, Ghelani, Aroyo, Slone, Houlsby, Xiong, Yang, Gribovskaya, Adler, Wirth, Lee, Li, Kagohara, Pavagadhi, Bridgers, Bortsova, Ghemawat, Ahmed, Liu, Powell, Bolina, Iinuma, Zablotskaia, Besley, Chung, Dozat, Comanescu, Si, Greer, Su, Polacek, Kaufman, Tokumine, Hu, Buchatskaya, Miao, Elhawaty, Siddhant, Tomasev, Xing, Greer, Miller, Ashraf, Roy, Zhang, Ma, Filos, Besta, Blevins, Klimenko, Yeh, Changpinyo, Mu, Chang,   Pajarskas, Muir, Cohen, Lan, Haridasan, Marathe, Hansen, Douglas, Samuel, Wang, Austin, Lan, Jiang, Chiu, Lorenzo, Sjösund, Cevey, Gleicher, Avrahami, Boral, Srinivasan, Selo, May, Aisopos, Hussenot, Soares, Baumli, Chang, Recasens, Caine, Pritzel, Pavetic, Pardo, Gergely, Frye, Ramasesh, Horgan, Badola, Kassner, Roy, Dyer, Campos, Tomala, Tang, Badawy, White, Mustafa, Lang, Jindal, Vikram, Gong, Caelles, Hemsley, Thornton, Feng, Stokowiec, Zheng, Thacker, Çağlar Ünlü, Zhang, Saleh, Svensson, Bileschi, Patil, Anand, Ring, Tsihlas, Vezer, Selvi, Shevlane, Rodriguez, Kwiatkowski, Daruki, Rong, Dafoe, FitzGerald, Gu-Lemberg, Khan, Hendricks, Pellat, Feinberg, Cobon-Kerr, Sainath, Rauh, Hashemi, Ives, Hasson, Noland, Cao, Byrd, Hou, Wang, Sottiaux, Paganini, Lespiau, Moufarek, Hassan, Shivakumar, van Amersfoort, Mandhane, Joshi, Goyal, Tung, Brock, Sheahan, Misra, Li, Rakićević, Dehghani, Liu, Mittal, Oh, Noury, Sezener, Huot, Lamm, Cao, Chen, Mudgal, Stella, Brooks, Vasudevan, Liu, Chain, Melinkeri,   Cohen, Wang, Seymore, Zubkov, Goel, Yue, Krishnakumaran, Albert, Hurley, Sano, Mohananey, Joughin, Filonov, Kepa, Eldawy, Lim, Rishi, Badiezadegan, Bos, Chang, Jain, Padmanabhan, Puttagunta, Krishna, Baker, Kalb, Bedapudi, Kurzrok, Lei, Yu, Litvin, Zhou, Wu, Sobell, Siciliano, Papir, Neale, Bragagnolo, Toor, Chen, Anklin, Wang, Feng, Gholami, Ling, Liu, Walter, Moghaddam, Kishore, Adamek, Mercado, Mallinson, Wandekar, Cagle, Ofek, Garrido, Lombriser, Mukha, Sun, Mohammad, Matak, Qian, Peswani, Janus, Yuan, Schelin, David, Garg, He, Duzhyi, Älgmyr, Lottaz, Li, Yadav, Xu, Chinien, Shivanna, Chuklin, Li, Spadine, Wolfe, Mohamed, Das, Dai, He, von Dincklage, Upadhyay, Maurya, Chi, Krause, Salama, Rabinovitch, M, Selvan, Dektiarev, Ghiasi, Guven, Gupta, Liu, Sharma, Shtacher, Paul, Akerlund, Aubet, Huang, Zhu, Zhu, Teixeira, Fritze, Bertolini, Marinescu, Bölle, Paulus, Gupta, Latkar, Chang, Sanders, Wilson, Wu, Tan, Thiet, Doshi, Lall, Mishra, Chen, Luong, Benjamin, Lee, Andrejczuk, Rabiej, Ranjan, Styrc, Yin,   Simon, Harriott, Bansal, Robsky, Bacon, Greene, Mirylenka, Zhou, Sarvana, Goyal, Andermatt, Siegler, Horn, Israel, Pongetti, Chen, Selvatici, Silva, Wang, Tolins, Guu, Yogev, Cai, Agostini, Shah, Nguyen, Donnaile, Pereira, Friso, Stambler, Kurzrok, Kuang, Romanikhin, Geller, Yan, Jang, Lee, Fica, Malmi, Tan, Banica, Balle, Pham, Huang, Avram, Shi, Singh, Hidey, Ahuja, Saxena, Dooley, Potharaju, O'Neill, Gokulchandran, Foley, Zhao, Dusenberry, Liu, Mehta, Kotikalapudi, Safranek-Shrader, Goodman, Kessinger, Globen, Kolhar, Gorgolewski, Ibrahim, Song, Eichenbaum, Brovelli, Potluri, Lahoti, Baetu, Ghorbani, Chen, Crawford, Pal, Sridhar, Gurita, Mujika, Petrovski, Cedoz, Li, Chen, Santo, Goyal, Punjabi, Kappaganthu, Kwak, LV, Velury, Choudhury, Hall, Shah, Figueira, Thomas, Lu, Zhou, Kumar, Jurdi, Chikkerur, Ma, Yu, Kwak, Ähdel, Rajayogam, Choma, Liu, Barua, Ji, Park, Hellendoorn, Bailey, Bilal, Zhou, Khatir, Sutton, Rzadkowski, Macintosh, Shagin, Medina, Liang, Zhou, Shah, Bi, Dankovics, Banga, Lehmann,   Bredesen, Lin, Hoffmann, Lai, Chung, Yang, Balani, Bražinskas, Sozanschi, Hayes, Alcalde, Makarov, Chen, Stella, Snijders, Mandl, Kärrman, Nowak, Wu, Dyck, Vaidyanathan, R, Mallet, Rudominer, Johnston, Mittal, Udathu, Christensen, Verma, Irving, Santucci, Elsayed, Davoodi, Georgiev, Tenney, Hua, Cideron, Leurent, Alnahlawi, Georgescu, Wei, Zheng, Scandinaro, Jiang, Snoek, Sundararajan, Wang, Ontiveros, Karo, Cole, Rajashekhar, Tumeh, Ben-David, Jain, Uesato, Datta, Bunyan, Wu, Zhang, Stanczyk, Zhang, Steiner, Naskar, Azzam, Johnson, Paszke, Chiu, Elias, Mohiuddin, Muhammad, Miao, Lee, Vieillard, Park, Zhang, Stanway, Garmon, Karmarkar, Dong, Lee, Kumar, Zhou, Evens, Isaac, Irving, Loper, Fink, Arkatkar, Chen, Shafran, Petrychenko, Chen, Jia, Levskaya, Zhu, Grabowski, Mao, Magni, Yao, Snaider, Casagrande, Palmer, Suganthan, Castaño, Giannoumis, Kim, Rybiński, Sreevatsa, Prendki, Soergel, Goedeckemeyer, Gierke, Jafari, Gaba, Wiesner, Wright, Wei, Vashisht, Kulizhskaya, Hoover, Le, Li, Iwuanyanwu, Liu,   Ramirez, Khorlin, Cui, LIN, Wu, Aguilar, Pallo, Chakladar, Perng, Abellan, Zhang, Dasgupta, Kushman, Penchev, Repina, Wu, van~der Weide, Ponnapalli, Kaplan, Simsa, Li, Dousse, Yang, Piper, Ie, Pasumarthi, Lintz, Vijayakumar, Andor, Valenzuela, Lui, Paduraru, Peng, Lee, Zhang, Greene, Nguyen, Kurylowicz, Hardin, Dixon, Janzer, Choo, Feng, Zhang, Singhal, Du, McKinnon, Antropova, Bolukbasi, Keller, Reid, Finchelstein, Raad, Crocker, Hawkins, Dadashi, Gaffney, Franko, Bulanova, Leblond, Chung, Askham, Cobo, Xu, Fischer, Xu, Sorokin, Alberti, Lin, Evans, Dimitriev, Forbes, Banarse, Tung, Omernick, Bishop, Sterneck, Jain, Xia, Amid, Piccinno, Wang, Banzal, Mankowitz, Polozov, Krakovna, Brown, Bateni, Duan, Firoiu, Thotakuri, Natan, Geist, tan Girgin, Li, Ye, Roval, Tojo, Kwong, Lee-Thorp, Yew, Sinopalnikov, Ramos, Mellor, Sharma, Wu, Miller, Sonnerat, Vnukov, Greig, Beattie, Caveness, Bai, Eisenschlos, Korchemniy, Tsai, Jasarevic, Kong, Dao, Zheng, Liu, Yang, Zhu, Teh, Sanmiya, Gladchenko, Trdin, Toyama, Rosen,   Tavakkol, Xue, Elkind, Woodman, Carpenter, Papamakarios, Kemp, Kafle, Grunina, Sinha, Talbert, Wu, Owusu-Afriyie, Du, Thornton, Pont-Tuset, Narayana, Li, Fatehi, Wieting, Ajmeri, Uria, Ko, Knight, Héliou, Niu, Gu, Pang, Li, Levine, Stolovich, Santamaria-Fernandez, Goenka, Yustalim, Strudel, Elqursh, Deck, Lee, Li, Levin, Hoffmann, Holtmann-Rice, Bachem, Arora, Koh, Yeganeh, Põder, Tariq, Sun, Ionita, Seyedhosseini, Tafti, Liu, Gulati, Liu, Ye, Chrzaszcz, Wang, Sethi, Li, Brown, Singh, Fan, Parisi, Stanton, Koverkathu, Choquette-Choo, Li, Lu, Ittycheriah, Shroff, Varadarajan, Bahargam, Willoughby, Gaddy, Desjardins, Cornero, Robenek, Mittal, Albrecht, Shenoy, Moiseev, Jacobsson, Ghaffarkhah, Rivière, Walton, Crepy, Parrish, Zhou, Farabet, Radebaugh, Srinivasan, van~der Salm, Fidjeland, Scellato, Latorre-Chimoto, Klimczak-Plucińska, Bridson, de~Cesare, Hudson, Mendolicchio, Walker, Morris, Mauger, Guseynov, Reid, Odoom, Loher, Cotruta, Yenugula, Grewe, Petrushkina, Duerig, Sanchez, Yadlowsky, Shen,   Globerson, Webb, Dua, Li, Bhupatiraju, Hurt, Qureshi, Agarwal, Shani, Eyal, Khare, Belle, Wang, Tekur, Kale, Wei, Sang, Saeta, Liechty, Sun, Zhao, Lee, Nayak, Fritz, Vuyyuru, Aslanides, Vyas, Wicke, Ma, Eltyshev, Martin, Cate, Manyika, Amiri, Kim, Xiong, Kang, Luisier, Tripuraneni, Madras, Guo, Waters, Wang, Ainslie, Baldridge, Zhang, Pruthi, Bauer, Yang, Mansour, Gelman, Xu, Polovets, Liu, Cai, Chen, Sheng, Xue, Ozair, Angermueller, Li, Sinha, Wang, Wiesinger, Koukoumidis, Tian, Iyer, Gurumurthy, Goldenson, Shah, Blake, Yu, Urbanowicz, Palomaki, Fernando, Durden, Mehta, Momchev, Rahimtoroghi, Georgaki, Raul, Ruder, Redshaw, Lee, Zhou, Jalan, Li, Hechtman, Schuh, Nasr, Milan, Mikulik, Franco, Green, Nguyen, Kelley, Mahendru, Hu, Howland, Vargas, Hui, Bansal, Rao, Ghiya, Wang, Ye, Sarr, Preston, Elish, Li, Kaku, Gupta, Pasupat, Juan, Someswar, M., Chen, Amini, Fabrikant, Chu, Dong, Muthal, Buthpitiya, Jauhari, Hua, Khandelwal, Hitron, Ren, Rinaldi, Drath, Dabush, Jiang, Godhia, Sachs, Chen, Fan, Taitelbaum,   Noga, Dai, Wang, Liang, Hamer, Ferng, Elkind, Atias, Lee, Listík, Carlen, van~de Kerkhof, Pikus, Zaher, Müller, Zykova, Stefanec, Gatsko, Hirnschall, Sethi, Xu, Ahuja, Tsai, Stefanoiu, Feng, Dhandhania, Katyal, Gupta, Parulekar, Pitta, Zhao, Bhatia, Bhavnani, Alhadlaq, Li, Danenberg, Tu, Pine, Filippova, Ghosh, Limonchik, Urala, Lanka, Clive, Sun, Li, Wu, Hongtongsak, Li, Thakkar, Omarov, Majmundar, Alverson, Kucharski, Patel, Jain, Zabelin, Pelagatti, Kohli, Kumar, Kim, Sankar, Shah, Ramachandruni, Zeng, Bariach, Weidinger, Subramanya, Hsiao, Hassabis, Kavukcuoglu, Sadovsky, Le, Strohman, Wu, Petrov, Dean, and Vinyals]{geminiteam2024gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul~R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed~Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff   Pitman, Rushin Shah, Emanuel Taropa, Majd~Al Merey, Martin Baeuml, Zhifeng Chen, Laurent~El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi~Luan, Xi~Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack~W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav~Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,   Jordan Grimstad, Ale~Jakse Hartman, Xavier Garcia, Thanumalayan~Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de~Las~Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià~Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M.~R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry   Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom~Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara~Huiyi Hu, Raoul de~Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald~Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran   Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van~den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul~Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell,   Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël~Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline~Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime~Alonso Lorenzo, Lars~Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio~Baldini Soares, Kate Baumli, Michael~B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic,   Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor~Campos Campos, Alex Tomala, Yunhao Tang, Dalia~El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce~Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa~Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed~Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le~Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van   Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola~De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kepa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri~Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad   Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul~Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi~Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam~G Rabinovitch, Pavan Kumar~Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan~Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora   Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam~Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm~Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei~""Louis"" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah~Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ~Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris   Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya~Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò~Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji~Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou,   Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John~Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor~Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal   Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye~Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume~Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu~Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana~Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le,   Lu~Li, Chimezie Iwuanyanwu, Lu~Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena~Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van~der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc~Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria~Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis~C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin,   Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel~J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian~Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant   Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil~Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher~A. Choquette-Choo, Yunjie Li, TJ~Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona   Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van~der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de~Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas~Rammohan Belle, Lei Wang, Chetan Tekur, Mihir~Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi~Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish~Reddy Vuyyuru, John Aslanides, Nidhi Vyas,   Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi~Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji~Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK~Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua   Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke~Ye, Jean~Michel Sarr, Melanie~Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van~de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu~Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo~Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia,   Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya~Krishna Lanka, Derik Clive, Yi~Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals.",{Gemini: A Family of Highly Capable Multimodal Models}.,{Gemini: A Family of Highly Capable Multimodal Models}.,,"[{Gemini Team} et~al.(2024){Gemini Team}, Anil, Borgeaud, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, Silver, Johnson, Antonoglou, Schrittwieser, Glaese, Chen, Pitler, Lillicrap, Lazaridou, Firat, Molloy, Isard, Barham, Hennigan, Lee, Viola, Reynolds, Xu, Doherty, Collins, Meyer, Rutherford, Moreira, Ayoub, Goel, Krawczyk, Du, Chi, Cheng, Ni, Shah, Kane, Chan, Faruqui, Severyn, Lin, Li, Cheng, Ittycheriah, Mahdieh, Chen, Sun, Tran, Bagri, Lakshminarayanan, Liu, Orban, Güra, Zhou, Song, Boffy, Ganapathy, Zheng, Choe, Ágoston Weisz, Zhu, Lu, Gopal, Kahn, Kula, Pitman, Shah, Taropa, Merey, Baeuml, Chen, Shafey, Zhang, Sercinoglu, Tucker, Piqueras, Krikun, Barr, Savinov, Danihelka, Roelofs, White, Andreassen, von Glehn, Yagati, Kazemi, Gonzalez, Khalman, Sygnowski, Frechette, Smith, Culp, Proleev, Luan, Chen, Lottes, Schucher, Lebron, Rrustemi, Clay, Crone, Kocisky, Zhao, Perz, Yu, Howard, Bloniarz, Rae, Lu, Sifre, Maggioni, Alcober, Garrette, Barnes, Thakoor, Austin, Barth-Maron, Wong,   Joshi, Chaabouni, Fatiha, Ahuja, Tomar, Senter, Chadwick, Kornakov, Attaluri, Iturrate, Liu, Li, Cogan, Chen, Jia, Gu, Zhang, Grimstad, Hartman, Garcia, Pillai, Devlin, Laskin, de~Las~Casas, Valter, Tao, Blanco, Badia, Reitter, Chen, Brennan, Rivera, Brin, Iqbal, Surita, Labanowski, Rao, Winkler, Parisotto, Gu, Olszewska, Addanki, Miech, Louis, Teplyashin, Brown, Catt, Balaguer, Xiang, Wang, Ashwood, Briukhov, Webson, Ganapathy, Sanghavi, Kannan, Chang, Stjerngren, Djolonga, Sun, Bapna, Aitchison, Pejman, Michalewski, Yu, Wang, Love, Ahn, Bloxwich, Han, Humphreys, Sellam, Bradbury, Godbole, Samangooei, Damoc, Kaskasoli, Arnold, Vasudevan, Agrawal, Riesa, Lepikhin, Tanburn, Srinivasan, Lim, Hodkinson, Shyam, Ferret, Hand, Garg, Paine, Li, Li, Giang, Neitz, Abbas, York, Reid, Cole, Chowdhery, Das, Rogozińska, Nikolaev, Sprechmann, Nado, Zilka, Prost, He, Monteiro, Mishra, Welty, Newlan, Jia, Allamanis, Hu, de~Liedekerke, Gilmer, Saroufim, Rijhwani, Hou, Shrivastava, Baddepudi, Goldin, Ozturel, Cassirer, Xu,   Sohn, Sachan, Amplayo, Swanson, Petrova, Narayan, Guez, Brahma, Landon, Patel, Zhao, Villela, Wang, Jia, Rahtz, Giménez, Yeung, Keeling, Georgiev, Mincu, Wu, Haykal, Saputro, Vodrahalli, Qin, Cankara, Sharma, Fernando, Hawkins, Neyshabur, Kim, Hutter, Agrawal, Castro-Ros, van~den Driessche, Wang, Yang, yiin Chang, Komarek, McIlroy, Lučić, Zhang, Farhan, Sharman, Natsev, Michel, Bansal, Qiao, Cao, Shakeri, Butterfield, Chung, Rubenstein, Agrawal, Mensch, Soparkar, Lenc, Chung, Pope, Maggiore, Kay, Jhakra, Wang, Maynez, Phuong, Tobin, Tacchetti, Trebacz, Robinson, Katariya, Riedel, Bailey, Xiao, Ghelani, Aroyo, Slone, Houlsby, Xiong, Yang, Gribovskaya, Adler, Wirth, Lee, Li, Kagohara, Pavagadhi, Bridgers, Bortsova, Ghemawat, Ahmed, Liu, Powell, Bolina, Iinuma, Zablotskaia, Besley, Chung, Dozat, Comanescu, Si, Greer, Su, Polacek, Kaufman, Tokumine, Hu, Buchatskaya, Miao, Elhawaty, Siddhant, Tomasev, Xing, Greer, Miller, Ashraf, Roy, Zhang, Ma, Filos, Besta, Blevins, Klimenko, Yeh, Changpinyo, Mu, Chang,   Pajarskas, Muir, Cohen, Lan, Haridasan, Marathe, Hansen, Douglas, Samuel, Wang, Austin, Lan, Jiang, Chiu, Lorenzo, Sjösund, Cevey, Gleicher, Avrahami, Boral, Srinivasan, Selo, May, Aisopos, Hussenot, Soares, Baumli, Chang, Recasens, Caine, Pritzel, Pavetic, Pardo, Gergely, Frye, Ramasesh, Horgan, Badola, Kassner, Roy, Dyer, Campos, Tomala, Tang, Badawy, White, Mustafa, Lang, Jindal, Vikram, Gong, Caelles, Hemsley, Thornton, Feng, Stokowiec, Zheng, Thacker, Çağlar Ünlü, Zhang, Saleh, Svensson, Bileschi, Patil, Anand, Ring, Tsihlas, Vezer, Selvi, Shevlane, Rodriguez, Kwiatkowski, Daruki, Rong, Dafoe, FitzGerald, Gu-Lemberg, Khan, Hendricks, Pellat, Feinberg, Cobon-Kerr, Sainath, Rauh, Hashemi, Ives, Hasson, Noland, Cao, Byrd, Hou, Wang, Sottiaux, Paganini, Lespiau, Moufarek, Hassan, Shivakumar, van Amersfoort, Mandhane, Joshi, Goyal, Tung, Brock, Sheahan, Misra, Li, Rakićević, Dehghani, Liu, Mittal, Oh, Noury, Sezener, Huot, Lamm, Cao, Chen, Mudgal, Stella, Brooks, Vasudevan, Liu, Chain, Melinkeri,   Cohen, Wang, Seymore, Zubkov, Goel, Yue, Krishnakumaran, Albert, Hurley, Sano, Mohananey, Joughin, Filonov, Kepa, Eldawy, Lim, Rishi, Badiezadegan, Bos, Chang, Jain, Padmanabhan, Puttagunta, Krishna, Baker, Kalb, Bedapudi, Kurzrok, Lei, Yu, Litvin, Zhou, Wu, Sobell, Siciliano, Papir, Neale, Bragagnolo, Toor, Chen, Anklin, Wang, Feng, Gholami, Ling, Liu, Walter, Moghaddam, Kishore, Adamek, Mercado, Mallinson, Wandekar, Cagle, Ofek, Garrido, Lombriser, Mukha, Sun, Mohammad, Matak, Qian, Peswani, Janus, Yuan, Schelin, David, Garg, He, Duzhyi, Älgmyr, Lottaz, Li, Yadav, Xu, Chinien, Shivanna, Chuklin, Li, Spadine, Wolfe, Mohamed, Das, Dai, He, von Dincklage, Upadhyay, Maurya, Chi, Krause, Salama, Rabinovitch, M, Selvan, Dektiarev, Ghiasi, Guven, Gupta, Liu, Sharma, Shtacher, Paul, Akerlund, Aubet, Huang, Zhu, Zhu, Teixeira, Fritze, Bertolini, Marinescu, Bölle, Paulus, Gupta, Latkar, Chang, Sanders, Wilson, Wu, Tan, Thiet, Doshi, Lall, Mishra, Chen, Luong, Benjamin, Lee, Andrejczuk, Rabiej, Ranjan, Styrc, Yin,   Simon, Harriott, Bansal, Robsky, Bacon, Greene, Mirylenka, Zhou, Sarvana, Goyal, Andermatt, Siegler, Horn, Israel, Pongetti, Chen, Selvatici, Silva, Wang, Tolins, Guu, Yogev, Cai, Agostini, Shah, Nguyen, Donnaile, Pereira, Friso, Stambler, Kurzrok, Kuang, Romanikhin, Geller, Yan, Jang, Lee, Fica, Malmi, Tan, Banica, Balle, Pham, Huang, Avram, Shi, Singh, Hidey, Ahuja, Saxena, Dooley, Potharaju, O'Neill, Gokulchandran, Foley, Zhao, Dusenberry, Liu, Mehta, Kotikalapudi, Safranek-Shrader, Goodman, Kessinger, Globen, Kolhar, Gorgolewski, Ibrahim, Song, Eichenbaum, Brovelli, Potluri, Lahoti, Baetu, Ghorbani, Chen, Crawford, Pal, Sridhar, Gurita, Mujika, Petrovski, Cedoz, Li, Chen, Santo, Goyal, Punjabi, Kappaganthu, Kwak, LV, Velury, Choudhury, Hall, Shah, Figueira, Thomas, Lu, Zhou, Kumar, Jurdi, Chikkerur, Ma, Yu, Kwak, Ähdel, Rajayogam, Choma, Liu, Barua, Ji, Park, Hellendoorn, Bailey, Bilal, Zhou, Khatir, Sutton, Rzadkowski, Macintosh, Shagin, Medina, Liang, Zhou, Shah, Bi, Dankovics, Banga, Lehmann,   Bredesen, Lin, Hoffmann, Lai, Chung, Yang, Balani, Bražinskas, Sozanschi, Hayes, Alcalde, Makarov, Chen, Stella, Snijders, Mandl, Kärrman, Nowak, Wu, Dyck, Vaidyanathan, R, Mallet, Rudominer, Johnston, Mittal, Udathu, Christensen, Verma, Irving, Santucci, Elsayed, Davoodi, Georgiev, Tenney, Hua, Cideron, Leurent, Alnahlawi, Georgescu, Wei, Zheng, Scandinaro, Jiang, Snoek, Sundararajan, Wang, Ontiveros, Karo, Cole, Rajashekhar, Tumeh, Ben-David, Jain, Uesato, Datta, Bunyan, Wu, Zhang, Stanczyk, Zhang, Steiner, Naskar, Azzam, Johnson, Paszke, Chiu, Elias, Mohiuddin, Muhammad, Miao, Lee, Vieillard, Park, Zhang, Stanway, Garmon, Karmarkar, Dong, Lee, Kumar, Zhou, Evens, Isaac, Irving, Loper, Fink, Arkatkar, Chen, Shafran, Petrychenko, Chen, Jia, Levskaya, Zhu, Grabowski, Mao, Magni, Yao, Snaider, Casagrande, Palmer, Suganthan, Castaño, Giannoumis, Kim, Rybiński, Sreevatsa, Prendki, Soergel, Goedeckemeyer, Gierke, Jafari, Gaba, Wiesner, Wright, Wei, Vashisht, Kulizhskaya, Hoover, Le, Li, Iwuanyanwu, Liu,   Ramirez, Khorlin, Cui, LIN, Wu, Aguilar, Pallo, Chakladar, Perng, Abellan, Zhang, Dasgupta, Kushman, Penchev, Repina, Wu, van~der Weide, Ponnapalli, Kaplan, Simsa, Li, Dousse, Yang, Piper, Ie, Pasumarthi, Lintz, Vijayakumar, Andor, Valenzuela, Lui, Paduraru, Peng, Lee, Zhang, Greene, Nguyen, Kurylowicz, Hardin, Dixon, Janzer, Choo, Feng, Zhang, Singhal, Du, McKinnon, Antropova, Bolukbasi, Keller, Reid, Finchelstein, Raad, Crocker, Hawkins, Dadashi, Gaffney, Franko, Bulanova, Leblond, Chung, Askham, Cobo, Xu, Fischer, Xu, Sorokin, Alberti, Lin, Evans, Dimitriev, Forbes, Banarse, Tung, Omernick, Bishop, Sterneck, Jain, Xia, Amid, Piccinno, Wang, Banzal, Mankowitz, Polozov, Krakovna, Brown, Bateni, Duan, Firoiu, Thotakuri, Natan, Geist, tan Girgin, Li, Ye, Roval, Tojo, Kwong, Lee-Thorp, Yew, Sinopalnikov, Ramos, Mellor, Sharma, Wu, Miller, Sonnerat, Vnukov, Greig, Beattie, Caveness, Bai, Eisenschlos, Korchemniy, Tsai, Jasarevic, Kong, Dao, Zheng, Liu, Yang, Zhu, Teh, Sanmiya, Gladchenko, Trdin, Toyama, Rosen,   Tavakkol, Xue, Elkind, Woodman, Carpenter, Papamakarios, Kemp, Kafle, Grunina, Sinha, Talbert, Wu, Owusu-Afriyie, Du, Thornton, Pont-Tuset, Narayana, Li, Fatehi, Wieting, Ajmeri, Uria, Ko, Knight, Héliou, Niu, Gu, Pang, Li, Levine, Stolovich, Santamaria-Fernandez, Goenka, Yustalim, Strudel, Elqursh, Deck, Lee, Li, Levin, Hoffmann, Holtmann-Rice, Bachem, Arora, Koh, Yeganeh, Põder, Tariq, Sun, Ionita, Seyedhosseini, Tafti, Liu, Gulati, Liu, Ye, Chrzaszcz, Wang, Sethi, Li, Brown, Singh, Fan, Parisi, Stanton, Koverkathu, Choquette-Choo, Li, Lu, Ittycheriah, Shroff, Varadarajan, Bahargam, Willoughby, Gaddy, Desjardins, Cornero, Robenek, Mittal, Albrecht, Shenoy, Moiseev, Jacobsson, Ghaffarkhah, Rivière, Walton, Crepy, Parrish, Zhou, Farabet, Radebaugh, Srinivasan, van~der Salm, Fidjeland, Scellato, Latorre-Chimoto, Klimczak-Plucińska, Bridson, de~Cesare, Hudson, Mendolicchio, Walker, Morris, Mauger, Guseynov, Reid, Odoom, Loher, Cotruta, Yenugula, Grewe, Petrushkina, Duerig, Sanchez, Yadlowsky, Shen,   Globerson, Webb, Dua, Li, Bhupatiraju, Hurt, Qureshi, Agarwal, Shani, Eyal, Khare, Belle, Wang, Tekur, Kale, Wei, Sang, Saeta, Liechty, Sun, Zhao, Lee, Nayak, Fritz, Vuyyuru, Aslanides, Vyas, Wicke, Ma, Eltyshev, Martin, Cate, Manyika, Amiri, Kim, Xiong, Kang, Luisier, Tripuraneni, Madras, Guo, Waters, Wang, Ainslie, Baldridge, Zhang, Pruthi, Bauer, Yang, Mansour, Gelman, Xu, Polovets, Liu, Cai, Chen, Sheng, Xue, Ozair, Angermueller, Li, Sinha, Wang, Wiesinger, Koukoumidis, Tian, Iyer, Gurumurthy, Goldenson, Shah, Blake, Yu, Urbanowicz, Palomaki, Fernando, Durden, Mehta, Momchev, Rahimtoroghi, Georgaki, Raul, Ruder, Redshaw, Lee, Zhou, Jalan, Li, Hechtman, Schuh, Nasr, Milan, Mikulik, Franco, Green, Nguyen, Kelley, Mahendru, Hu, Howland, Vargas, Hui, Bansal, Rao, Ghiya, Wang, Ye, Sarr, Preston, Elish, Li, Kaku, Gupta, Pasupat, Juan, Someswar, M., Chen, Amini, Fabrikant, Chu, Dong, Muthal, Buthpitiya, Jauhari, Hua, Khandelwal, Hitron, Ren, Rinaldi, Drath, Dabush, Jiang, Godhia, Sachs, Chen, Fan, Taitelbaum,   Noga, Dai, Wang, Liang, Hamer, Ferng, Elkind, Atias, Lee, Listík, Carlen, van~de Kerkhof, Pikus, Zaher, Müller, Zykova, Stefanec, Gatsko, Hirnschall, Sethi, Xu, Ahuja, Tsai, Stefanoiu, Feng, Dhandhania, Katyal, Gupta, Parulekar, Pitta, Zhao, Bhatia, Bhavnani, Alhadlaq, Li, Danenberg, Tu, Pine, Filippova, Ghosh, Limonchik, Urala, Lanka, Clive, Sun, Li, Wu, Hongtongsak, Li, Thakkar, Omarov, Majmundar, Alverson, Kucharski, Patel, Jain, Zabelin, Pelagatti, Kohli, Kumar, Kim, Sankar, Shah, Ramachandruni, Zeng, Bariach, Weidinger, Subramanya, Hsiao, Hassabis, Kavukcuoglu, Sadovsky, Le, Strohman, Wu, Petrov, Dean, and Vinyals]{geminiteam2024gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul~R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed~Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff   Pitman, Rushin Shah, Emanuel Taropa, Majd~Al Merey, Martin Baeuml, Zhifeng Chen, Laurent~El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi~Luan, Xi~Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack~W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav~Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,   Jordan Grimstad, Ale~Jakse Hartman, Xavier Garcia, Thanumalayan~Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de~Las~Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià~Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M.~R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry   Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom~Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara~Huiyi Hu, Raoul de~Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald~Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran   Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van~den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul~Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell,   Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël~Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline~Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime~Alonso Lorenzo, Lars~Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio~Baldini Soares, Kate Baumli, Michael~B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic,   Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor~Campos Campos, Alex Tomala, Yunhao Tang, Dalia~El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce~Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa~Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed~Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le~Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van   Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola~De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kepa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri~Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad   Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul~Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi~Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam~G Rabinovitch, Pavan Kumar~Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan~Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora   Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam~Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm~Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei~""Louis"" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah~Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ~Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris   Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya~Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò~Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji~Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou,   Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John~Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor~Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal   Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye~Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume~Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu~Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana~Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le,   Lu~Li, Chimezie Iwuanyanwu, Lu~Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena~Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van~der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc~Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria~Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis~C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin,   Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel~J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian~Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant   Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil~Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher~A. Choquette-Choo, Yunjie Li, TJ~Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona   Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van~der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de~Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas~Rammohan Belle, Lei Wang, Chetan Tekur, Mihir~Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi~Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish~Reddy Vuyyuru, John Aslanides, Nidhi Vyas,   Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi~Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji~Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK~Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua   Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke~Ye, Jean~Michel Sarr, Melanie~Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van~de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu~Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo~Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia,   Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya~Krishna Lanka, Derik Clive, Yi~Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals. 
 {Gemini: A Family of Highly Capable Multimodal Models}. 
 \emph{arXiv:2312.11805}, 2024."
2405.21068,gemmateam2024gemma,"[{Gemma Team} et~al.(2024){Gemma Team}, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivière, Kale, Love, Tafti, Hussenot, Sessa, Chowdhery, Roberts, Barua, Botev, Castro-Ros, Slone, Héliou, Tacchetti, Bulanova, Paterson, Tsai, Shahriari, Lan, Choquette-Choo, Crepy, Cer, Ippolito, Reid, Buchatskaya, Ni, Noland, Yan, Tucker, Muraru, Rozhdestvenskiy, Michalewski, Tenney, Grishchenko, Austin, Keeling, Labanowski, Lespiau, Stanway, Brennan, Chen, Ferret, Chiu, Mao-Jones, Lee, Yu, Millican, Sjoesund, Lee, Dixon, Reid, Mikuła, Wirth, Sharman, Chinaev, Thain, Bachem, Chang, Wahltinez, Bailey, Michel, Yotov, Chaabouni, Comanescu, Jana, Anil, McIlroy, Liu, Mullins, Smith, Borgeaud, Girgin, Douglas, Pandya, Shakeri, De, Klimenko, Hennigan, Feinberg, Stokowiec, hui Chen, Ahmed, Gong, Warkentin, Peran, Giang, Farabet, Vinyals, Dean, Kavukcuoglu, Hassabis, Ghahramani, Eck, Barral, Pereira, Collins, Joulin, Fiedel, Senter, Andreev, and Kenealy]{gemmateam2024gemma} {Gemma Team}, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier~Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline~Le Lan, Christopher~A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars~Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem,   Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel~L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu~hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.",{Gemma: Open Models Based on Gemini Research and Technology}.,{Gemma: Open Models Based on Gemini Research and Technology}.,,"[{Gemma Team} et~al.(2024){Gemma Team}, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivière, Kale, Love, Tafti, Hussenot, Sessa, Chowdhery, Roberts, Barua, Botev, Castro-Ros, Slone, Héliou, Tacchetti, Bulanova, Paterson, Tsai, Shahriari, Lan, Choquette-Choo, Crepy, Cer, Ippolito, Reid, Buchatskaya, Ni, Noland, Yan, Tucker, Muraru, Rozhdestvenskiy, Michalewski, Tenney, Grishchenko, Austin, Keeling, Labanowski, Lespiau, Stanway, Brennan, Chen, Ferret, Chiu, Mao-Jones, Lee, Yu, Millican, Sjoesund, Lee, Dixon, Reid, Mikuła, Wirth, Sharman, Chinaev, Thain, Bachem, Chang, Wahltinez, Bailey, Michel, Yotov, Chaabouni, Comanescu, Jana, Anil, McIlroy, Liu, Mullins, Smith, Borgeaud, Girgin, Douglas, Pandya, Shakeri, De, Klimenko, Hennigan, Feinberg, Stokowiec, hui Chen, Ahmed, Gong, Warkentin, Peran, Giang, Farabet, Vinyals, Dean, Kavukcuoglu, Hassabis, Ghahramani, Eck, Barral, Pereira, Collins, Joulin, Fiedel, Senter, Andreev, and Kenealy]{gemmateam2024gemma} {Gemma Team}, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier~Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline~Le Lan, Christopher~A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars~Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem,   Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel~L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu~hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 
 {Gemma: Open Models Based on Gemini Research and Technology}. 
 \emph{arXiv:2403.08295}, 2024."
2405.21068,groeneveld2024olmo,"[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{groeneveld2024olmo} Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi.",{{OLMo}: Accelerating the Science of Language Models}.,{{OLMo}: Accelerating the Science of Language Models}.,,"[Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang, Arora, Atkinson, Authur, Chandu, Cohan, Dumas, Elazar, Gu, Hessel, Khot, Merrill, Morrison, Muennighoff, Naik, Nam, Peters, Pyatkin, Ravichander, Schwenk, Shah, Smith, Strubell, Subramani, Wortsman, Dasigi, Lambert, Richardson, Zettlemoyer, Dodge, Lo, Soldaini, Smith, and Hajishirzi]{groeneveld2024olmo} Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi~Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew~E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah~A. Smith, and Hannaneh Hajishirzi. 
 {{OLMo}: Accelerating the Science of Language Models}. 
 \emph{arXiv:2402.00838}, 2024."
2405.21068,guo2024DeepSeekcoder,"[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, Luo, Xiong, and Liang]{guo2024DeepSeekcoder} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.",{DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}.,{DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}.,,"[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, Luo, Xiong, and Liang]{guo2024DeepSeekcoder} Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 
 {DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}. 
 \emph{arXiv:2401.14196}, 2024."
2405.21068,liu2023goat,[Liu \& Low(2023)Liu and Low]{liu2023goat} Tiedong Liu and Bryan Kian~Hsiang Low.,{Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks}.,{Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks}.,,"[Liu \& Low(2023)Liu and Low]{liu2023goat} Tiedong Liu and Bryan Kian~Hsiang Low. 
 {Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks}. 
 \emph{arXiv:2305.14201}, 2023."
2405.21068,shao2024deepseekmath,"[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{shao2024deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.~K. Li, Y.~Wu, and Daya Guo.",{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}.,{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}.,,"[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{shao2024deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.~K. Li, Y.~Wu, and Daya Guo. 
 {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}. 
 \emph{arXiv:2402.03300}, 2024."
2405.21068,toshniwal2024openmathinstruct1,"[Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman]{toshniwal2024openmathinstruct1} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.",{OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}.,{OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}.,,"[Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia, and Gitman]{toshniwal2024openmathinstruct1} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. 
 {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset}. 
 \emph{arXiv:2402.10176}, 2024."
2405.21068,touvron2023llama1,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.",{LLaMA}: Open and efficient foundation language models.,{LLaMA}: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 
 {LLaMA}: Open and efficient foundation language models. 
 \emph{arXiv:2302.13971}, 2023{\natexlab{a}}."
2405.21068,touvron2023llama,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom.",{Llama 2: Open Foundation and Fine-Tuned Chat Models}.,{Llama 2: Open Foundation and Fine-Tuned Chat Models}.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 
 {Llama 2: Open Foundation and Fine-Tuned Chat Models}. 
 \emph{arXiv:2307.09288}, 2023{\natexlab{b}}."
2405.21068,yang2024llm,"[Yang et~al.(2024)Yang, Liu, Wu, Yang, Fung, Li, Huang, Cao, Wang, Wang, Ji, and Zhai]{yang2024llm} Ke~Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi~R. Fung, Sha Li, Zixuan Huang, Xu~Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai.","{If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents}.","{If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents}.",,"[Yang et~al.(2024)Yang, Liu, Wu, Yang, Fung, Li, Huang, Cao, Wang, Wang, Ji, and Zhai]{yang2024llm} Ke~Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi~R. Fung, Sha Li, Zixuan Huang, Xu~Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. 
 {If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents}. 
 \emph{arXiv:2401.00812}, 2024."
2406.00257,bang2023multitask,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.",,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023. 
 A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 
 \emph{arXiv preprint arXiv:2302.04023}."
2406.00257,kantharaj2022opencqa,"[{Kantharaj et~al.(2022{\natexlab{b}})Kantharaj, Do, Leong, Tan, Hoque, and Joty}]{kantharaj2022opencqa} Shankar Kantharaj, Xuan~Long Do, Rixie Tiffany~Ko Leong, Jia~Qing Tan, Enamul Hoque, and Shafiq Joty. 2022{\natexlab{b}}.",Opencqa: Open-ended question answering with charts.,Opencqa: Open-ended question answering with charts.,,"[{Kantharaj et~al.(2022{\natexlab{b}})Kantharaj, Do, Leong, Tan, Hoque, and Joty}]{kantharaj2022opencqa} Shankar Kantharaj, Xuan~Long Do, Rixie Tiffany~Ko Leong, Jia~Qing Tan, Enamul Hoque, and Shafiq Joty. 2022{\natexlab{b}}. 
 Opencqa: Open-ended question answering with charts. 
 \emph{arXiv preprint arXiv:2210.06628}."
2406.00257,lee2022pix2struct,"[{Lee et~al.(2022)Lee, Joshi, Turc, Hu, Liu, Eisenschlos, Khandelwal, Shaw, Chang, and Toutanova}]{lee2022pix2struct} Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2022.",Pix2struct: Screenshot parsing as pretraining for visual language understanding.,Pix2struct: Screenshot parsing as pretraining for visual language understanding.,,"[{Lee et~al.(2022)Lee, Joshi, Turc, Hu, Liu, Eisenschlos, Khandelwal, Shaw, Chang, and Toutanova}]{lee2022pix2struct} Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2022. 
 Pix2struct: Screenshot parsing as pretraining for visual language understanding. 
 \emph{arXiv preprint arXiv:2210.03347}."
2406.00257,liu2022matcha,"[{Liu et~al.(2022)Liu, Piccinno, Krichene, Pang, Lee, Joshi, Altun, Collier, and Eisenschlos}]{liu2022matcha} Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian~Martin Eisenschlos. 2022.",Matcha: Enhancing visual language pretraining with math reasoning and chart derendering.,Matcha: Enhancing visual language pretraining with math reasoning and chart derendering.,,"[{Liu et~al.(2022)Liu, Piccinno, Krichene, Pang, Lee, Joshi, Altun, Collier, and Eisenschlos}]{liu2022matcha} Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian~Martin Eisenschlos. 2022. 
 Matcha: Enhancing visual language pretraining with math reasoning and chart derendering. 
 \emph{arXiv preprint arXiv:2212.09662}."
2406.00257,sellam2020bleurt,"[{Sellam et~al.(2020)Sellam, Das, and Parikh}]{sellam2020bleurt} Thibault Sellam, Dipanjan Das, and Ankur~P Parikh. 2020.",Bleurt: Learning robust metrics for text generation.,Bleurt: Learning robust metrics for text generation.,,"[{Sellam et~al.(2020)Sellam, Das, and Parikh}]{sellam2020bleurt} Thibault Sellam, Dipanjan Das, and Ankur~P Parikh. 2020. 
 Bleurt: Learning robust metrics for text generation. 
 \emph{arXiv preprint arXiv:2004.04696}."
2406.00262,chen2020improved,"[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and   He]{chen2020improved} X.~Chen, H.~Fan, R.~Girshick, and K.~He.",Improved baselines with momentum contrastive learning.,Improved baselines with momentum contrastive learning.,,"[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and   He]{chen2020improved} X.~Chen, H.~Fan, R.~Girshick, and K.~He. 
 Improved baselines with momentum contrastive learning. 
 \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}."
2406.00262,dangovski2021equivariant,"[Dangovski et~al.(2021)Dangovski, Jing, Loh, Han, Srivastava, Cheung,   Agrawal, and Solja{\v{c}}i{\'c}]{dangovski2021equivariant} R.~Dangovski, L.~Jing, C.~Loh, S.~Han, A.~Srivastava, B.~Cheung, P.~Agrawal,   and M.~Solja{\v{c}}i{\'c}.",Equivariant contrastive learning.,Equivariant contrastive learning.,,"[Dangovski et~al.(2021)Dangovski, Jing, Loh, Han, Srivastava, Cheung,   Agrawal, and Solja{\v{c}}i{\'c}]{dangovski2021equivariant} R.~Dangovski, L.~Jing, C.~Loh, S.~Han, A.~Srivastava, B.~Cheung, P.~Agrawal,   and M.~Solja{\v{c}}i{\'c}. 
 Equivariant contrastive learning. 
 \emph{arXiv preprint arXiv:2111.00899}, 2021."
2406.00262,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.",Bert: Pre-training of deep bidirectional transformers for language   understanding.,Bert: Pre-training of deep bidirectional transformers for language   understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2406.00262,Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_et,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,   Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and   Houlsby]{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_et} A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,   T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,   and N.~Houlsby.",An image is worth 16x16 words: Transformers for image recognition at   scale.,An image is worth 16x16 words: Transformers for image recognition at   scale.,,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,   Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and   Houlsby]{Dosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_et} A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,   T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,   and N.~Houlsby. 
 An image is worth 16x16 words: Transformers for image recognition at   scale. 
 \emph{arXiv: Computer Vision and Pattern Recognition,arXiv: Computer   Vision and Pattern Recognition}, Oct 2020."
2406.00262,goyal2017accurate,"[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,   Kyrola, Tulloch, Jia, and He]{goyal2017accurate} P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,   A.~Tulloch, Y.~Jia, and K.~He.","Accurate, large minibatch sgd: Training imagenet in 1 hour.","Accurate, large minibatch sgd: Training imagenet in 1 hour.",,"[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,   Kyrola, Tulloch, Jia, and He]{goyal2017accurate} P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,   A.~Tulloch, Y.~Jia, and K.~He. 
 Accurate, large minibatch sgd: Training imagenet in 1 hour. 
 \emph{arXiv preprint arXiv:1706.02677}, 2017."
2406.00262,gui2023survey,"[Gui et~al.(2023)Gui, Chen, Zhang, Cao, Sun, Luo, and   Tao]{gui2023survey} J.~Gui, T.~Chen, J.~Zhang, Q.~Cao, Z.~Sun, H.~Luo, and D.~Tao.","A survey on self-supervised learning: Algorithms, applications, and   future trends.","A survey on self-supervised learning: Algorithms, applications, and   future trends.",,"[Gui et~al.(2023)Gui, Chen, Zhang, Cao, Sun, Luo, and   Tao]{gui2023survey} J.~Gui, T.~Chen, J.~Zhang, Q.~Cao, Z.~Sun, H.~Luo, and D.~Tao. 
 A survey on self-supervised learning: Algorithms, applications, and   future trends. 
 \emph{arXiv preprint arXiv:2301.05712}, 2023."
2406.00262,liu2024vmamba,"[Liu et~al.(2024)Liu, Tian, Zhao, Yu, Xie, Wang, Ye, and   Liu]{liu2024vmamba} Y.~Liu, Y.~Tian, Y.~Zhao, H.~Yu, L.~Xie, Y.~Wang, Q.~Ye, and Y.~Liu.",Vmamba: Visual state space model.,Vmamba: Visual state space model.,,"[Liu et~al.(2024)Liu, Tian, Zhao, Yu, Xie, Wang, Ye, and   Liu]{liu2024vmamba} Y.~Liu, Y.~Tian, Y.~Zhao, H.~Yu, L.~Xie, Y.~Wang, Q.~Ye, and Y.~Liu. 
 Vmamba: Visual state space model. 
 \emph{arXiv preprint arXiv:2401.10166}, 2024."
2406.00548,basta2019evaluating,"[Basta et~al.(2019)Basta, Costa-Juss{\`a}, and Casas]{basta2019evaluating} Basta, C., Costa-Juss{\`a}, M.~R., and Casas, N.",Evaluating the underlying gender bias in contextualized word embeddings.,Evaluating the underlying gender bias in contextualized word embeddings.,,"[Basta et~al.(2019)Basta, Costa-Juss{\`a}, and Casas]{basta2019evaluating} Basta, C., Costa-Juss{\`a}, M.~R., and Casas, N. 
 Evaluating the underlying gender bias in contextualized word embeddings. 
 \emph{arXiv preprint arXiv:1904.08783}, 2019."
2406.00548,bommasani2021opportunities,"[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities} Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.",On the opportunities and risks of foundation models.,On the opportunities and risks of foundation models.,,"[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities} Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al. 
 On the opportunities and risks of foundation models. 
 \emph{arXiv preprint arXiv:2108.07258}, 2021."
2406.00548,bubeck2023sparks,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks} Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}, 2023."
2406.00548,cohen2023crawling,"[Cohen et~al.(2023)Cohen, Geva, Berant, and Globerson]{cohen2023crawling} Cohen, R., Geva, M., Berant, J., and Globerson, A.",Crawling the internal knowledge-base of language models.,Crawling the internal knowledge-base of language models.,,"[Cohen et~al.(2023)Cohen, Geva, Berant, and Globerson]{cohen2023crawling} Cohen, R., Geva, M., Berant, J., and Globerson, A. 
 Crawling the internal knowledge-base of language models. 
 \emph{arXiv preprint arXiv:2301.12810}, 2023."
2406.00548,dong2022survey,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z.",A survey for in-context learning.,A survey for in-context learning.,,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}, 2022."
2406.00548,fan2018hierarchical,"[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical} Fan, A., Lewis, M., and Dauphin, Y.",Hierarchical neural story generation.,Hierarchical neural story generation.,,"[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical} Fan, A., Lewis, M., and Dauphin, Y. 
 Hierarchical neural story generation. 
 \emph{arXiv preprint arXiv:1805.04833}, 2018."
2406.00548,gallegos2023bias,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} Gallegos, I.~O., Rossi, R.~A., Barrow, J., Tanjim, M.~M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N.~K.",Bias and fairness in large language models: A survey.,Bias and fairness in large language models: A survey.,,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} Gallegos, I.~O., Rossi, R.~A., Barrow, J., Tanjim, M.~M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N.~K. 
 Bias and fairness in large language models: A survey. 
 \emph{arXiv preprint arXiv:2309.00770}, 2023."
2406.00548,gehman2020realtoxicityprompts,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A.",Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A. 
 Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 
 \emph{arXiv preprint arXiv:2009.11462}, 2020."
2406.00548,holtzman2019curious,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.",The curious case of neural text degeneration.,The curious case of neural text degeneration.,,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 
 The curious case of neural text degeneration. 
 \emph{arXiv preprint arXiv:1904.09751}, 2019."
2406.00548,huang2019reducing,"[Huang et~al.(2019)Huang, Zhang, Jiang, Stanforth, Welbl, Rae, Maini, Yogatama, and Kohli]{huang2019reducing} Huang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., and Kohli, P.",Reducing sentiment bias in language models via counterfactual evaluation.,Reducing sentiment bias in language models via counterfactual evaluation.,,"[Huang et~al.(2019)Huang, Zhang, Jiang, Stanforth, Welbl, Rae, Maini, Yogatama, and Kohli]{huang2019reducing} Huang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., and Kohli, P. 
 Reducing sentiment bias in language models via counterfactual evaluation. 
 \emph{arXiv preprint arXiv:1911.03064}, 2019."
2406.00548,kingma2014adam,"[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam} Kingma, D.~P. and Ba, J.",Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam} Kingma, D.~P. and Ba, J. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2406.00548,lewis2019bart,"[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer]{lewis2019bart} Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.","Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.","Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",,"[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer]{lewis2019bart} Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
 Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. 
 \emph{arXiv preprint arXiv:1910.13461}, 2019."
2406.00548,li2023survey,"[Li et~al.(2023)Li, Du, Song, Wang, and Wang]{li2023survey} Li, Y., Du, M., Song, R., Wang, X., and Wang, Y.",A survey on fairness in large language models.,A survey on fairness in large language models.,,"[Li et~al.(2023)Li, Du, Song, Wang, and Wang]{li2023survey} Li, Y., Du, M., Song, R., Wang, X., and Wang, Y. 
 A survey on fairness in large language models. 
 \emph{arXiv preprint arXiv:2308.10149}, 2023."
2406.00548,ranaldi2023trip,"[Ranaldi et~al.(2023)Ranaldi, Ruzzetti, Venditti, Onorati, and Zanzotto]{ranaldi2023trip} Ranaldi, L., Ruzzetti, E.~S., Venditti, D., Onorati, D., and Zanzotto, F.~M.",A trip towards fairness: Bias and de-biasing in large language models.,A trip towards fairness: Bias and de-biasing in large language models.,,"[Ranaldi et~al.(2023)Ranaldi, Ruzzetti, Venditti, Onorati, and Zanzotto]{ranaldi2023trip} Ranaldi, L., Ruzzetti, E.~S., Venditti, D., Onorati, D., and Zanzotto, F.~M. 
 A trip towards fairness: Bias and de-biasing in large language models. 
 \emph{arXiv preprint arXiv:2305.13862}, 2023."
2406.00548,salinas2023not,"[Salinas et~al.(2023)Salinas, Penafiel, McCormack, and Morstatter]{salinas2023not} Salinas, A., Penafiel, L., McCormack, R., and Morstatter, F.",""" im not racist but..."": Discovering bias in the internal knowledge of large language models.",""" im not racist but..."": Discovering bias in the internal knowledge of large language models.",,"[Salinas et~al.(2023)Salinas, Penafiel, McCormack, and Morstatter]{salinas2023not} Salinas, A., Penafiel, L., McCormack, R., and Morstatter, F. 
 "" im not racist but..."": Discovering bias in the internal knowledge of large language models. 
 \emph{arXiv preprint arXiv:2310.08780}, 2023."
2406.00548,see2019massively,"[See et~al.(2019)See, Pappu, Saxena, Yerukola, and Manning]{see2019massively} See, A., Pappu, A., Saxena, R., Yerukola, A., and Manning, C.~D.",Do massively pretrained language models make better storytellers?,Do massively pretrained language models make better storytellers?,,"[See et~al.(2019)See, Pappu, Saxena, Yerukola, and Manning]{see2019massively} See, A., Pappu, A., Saxena, R., Yerukola, A., and Manning, C.~D. 
 Do massively pretrained language models make better storytellers? 
 \emph{arXiv preprint arXiv:1909.10705}, 2019."
2406.00548,sheng2019woman,"[Sheng et~al.(2019)Sheng, Chang, Natarajan, and Peng]{sheng2019woman} Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N.",The woman worked as a babysitter: On biases in language generation.,The woman worked as a babysitter: On biases in language generation.,,"[Sheng et~al.(2019)Sheng, Chang, Natarajan, and Peng]{sheng2019woman} Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. 
 The woman worked as a babysitter: On biases in language generation. 
 \emph{arXiv preprint arXiv:1909.01326}, 2019."
2406.00548,wang2023decodingtrust,"[Wang et~al.(2023{\natexlab{a}})Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., et~al.",Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,,"[Wang et~al.(2023{\natexlab{a}})Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., et~al. 
 Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 
 \emph{arXiv preprint arXiv:2306.11698}, 2023{\natexlab{a}}."
2406.00548,wei2023jailbroken,"[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{wei2023jailbroken} Wei, A., Haghtalab, N., and Steinhardt, J.",Jailbroken: How does llm safety training fail?,Jailbroken: How does llm safety training fail?,,"[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{wei2023jailbroken} Wei, A., Haghtalab, N., and Steinhardt, J. 
 Jailbroken: How does llm safety training fail? 
 \emph{arXiv preprint arXiv:2307.02483}, 2023."
2406.00548,zaken2021bitfit,"[Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg]{zaken2021bitfit} Zaken, E.~B., Ravfogel, S., and Goldberg, Y.",Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.,Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.,,"[Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg]{zaken2021bitfit} Zaken, E.~B., Ravfogel, S., and Goldberg, Y. 
 Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. 
 \emph{arXiv preprint arXiv:2106.10199}, 2021."
2406.00548,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.~V., et~al.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.~V., et~al. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2406.00548,zhao2019gender,"[Zhao et~al.(2019)Zhao, Wang, Yatskar, Cotterell, Ordonez, and Chang]{zhao2019gender} Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., and Chang, K.-W.",Gender bias in contextualized word embeddings.,Gender bias in contextualized word embeddings.,,"[Zhao et~al.(2019)Zhao, Wang, Yatskar, Cotterell, Ordonez, and Chang]{zhao2019gender} Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., and Chang, K.-W. 
 Gender bias in contextualized word embeddings. 
 \emph{arXiv preprint arXiv:1904.03310}, 2019."
2406.00548,zhao2023survey,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et~al.",A survey of large language models.,A survey of large language models.,,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et~al. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}, 2023."
2406.00548,zhou2023comprehensive,"[Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He, et~al.]{zhou2023comprehensive} Zhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., et~al.",A comprehensive survey on pretrained foundation models: A history from bert to chatgpt.,A comprehensive survey on pretrained foundation models: A history from bert to chatgpt.,,"[Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He, et~al.]{zhou2023comprehensive} Zhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., et~al. 
 A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. 
 \emph{arXiv preprint arXiv:2302.09419}, 2023."
2406.00755,ahmed2021synfix,"[{Ahmed et~al.(2021)Ahmed, Ledesma, and Devanbu}]{ahmed2021synfix} Toufique Ahmed, Noah~Rose Ledesma, and Premkumar Devanbu. 2021.",Synfix: Automatically fixing syntax errors using compiler diagnostics.,Synfix: Automatically fixing syntax errors using compiler diagnostics.,,"[{Ahmed et~al.(2021)Ahmed, Ledesma, and Devanbu}]{ahmed2021synfix} Toufique Ahmed, Noah~Rose Ledesma, and Premkumar Devanbu. 2021. 
 Synfix: Automatically fixing syntax errors using compiler diagnostics. 
 \emph{arXiv preprint arXiv:2104.14671}."
2406.00755,an2023learning,"[{An et~al.(2023)An, Ma, Lin, Zheng, Lou, and Chen}]{an2023learning} Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023.",Learning from mistakes makes llm better reasoner.,Learning from mistakes makes llm better reasoner.,,"[{An et~al.(2023)An, Ma, Lin, Zheng, Lou, and Chen}]{an2023learning} Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. 
 Learning from mistakes makes llm better reasoner. 
 \emph{arXiv preprint arXiv:2310.20689}."
2406.00755,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2406.00755,azerbayev2023llemma,"[{Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck}]{azerbayev2023llemma} Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023.",Llemma: An open language model for mathematics.,Llemma: An open language model for mathematics.,,"[{Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck}]{azerbayev2023llemma} Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. 
 Llemma: An open language model for mathematics. 
 \emph{arXiv preprint arXiv:2310.10631}."
2406.00755,bouzenia2024repairagent,"[{Bouzenia et~al.(2024)Bouzenia, Devanbu, and Pradel}]{bouzenia2024repairagent} Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024.","Repairagent: An autonomous, llm-based agent for program repair.","Repairagent: An autonomous, llm-based agent for program repair.",,"[{Bouzenia et~al.(2024)Bouzenia, Devanbu, and Pradel}]{bouzenia2024repairagent} Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024. 
 Repairagent: An autonomous, llm-based agent for program repair. 
 \emph{arXiv preprint arXiv:2403.17134}."
2406.00755,bubeck2023sparks,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}."
2406.00755,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2406.00755,collins2023evaluating,"[{Collins et~al.(2023)Collins, Jiang, Frieder, Wong, Zilka, Bhatt, Lukasiewicz, Wu, Tenenbaum, Hart et~al.}]{collins2023evaluating} Katherine~M Collins, Albert~Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua~B Tenenbaum, William Hart, et~al. 2023.",Evaluating language models for mathematics through interactions.,Evaluating language models for mathematics through interactions.,,"[{Collins et~al.(2023)Collins, Jiang, Frieder, Wong, Zilka, Bhatt, Lukasiewicz, Wu, Tenenbaum, Hart et~al.}]{collins2023evaluating} Katherine~M Collins, Albert~Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua~B Tenenbaum, William Hart, et~al. 2023. 
 Evaluating language models for mathematics through interactions. 
 \emph{arXiv preprint arXiv:2306.01694}."
2406.00755,diekmann2018don,[{Diekmann and Tratt(2018)}]{diekmann2018don} Lukas Diekmann and Laurence Tratt. 2018.,"Don't panic! better, fewer, syntax errors for lr parsers.","Don't panic! better, fewer, syntax errors for lr parsers.",,"[{Diekmann and Tratt(2018)}]{diekmann2018don} Lukas Diekmann and Laurence Tratt. 2018. 
 Don't panic! better, fewer, syntax errors for lr parsers. 
 \emph{arXiv preprint arXiv:1804.07133}."
2406.00755,frieder2023mathematical,"[{Frieder et~al.(2023)Frieder, Pinchetti, Chevalier, Griffiths, Salvatori, Lukasiewicz, Petersen, and Berner}]{frieder2023mathematical} Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp~Christian Petersen, and Julius Berner. 2023.",Mathematical capabilities of chatgpt.,Mathematical capabilities of chatgpt.,,"[{Frieder et~al.(2023)Frieder, Pinchetti, Chevalier, Griffiths, Salvatori, Lukasiewicz, Petersen, and Berner}]{frieder2023mathematical} Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp~Christian Petersen, and Julius Berner. 2023. 
 Mathematical capabilities of chatgpt. 
 \emph{arXiv preprint arXiv:2301.13867}."
2406.00755,fu2023chain,"[{Fu et~al.(2023)Fu, Ou, Chen, Wan, Peng, and Khot}]{fu2023chain} Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023.",Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance.,Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance.,,"[{Fu et~al.(2023)Fu, Ou, Chen, Wan, Peng, and Khot}]{fu2023chain} Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. 
 Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. 
 \emph{arXiv preprint arXiv:2305.17306}."
2406.00755,gaur2023reasoning,[{Gaur and Saunshi(2023)}]{gaur2023reasoning} Vedant Gaur and Nikunj Saunshi. 2023.,Reasoning in large language models through symbolic math word problems.,Reasoning in large language models through symbolic math word problems.,,"[{Gaur and Saunshi(2023)}]{gaur2023reasoning} Vedant Gaur and Nikunj Saunshi. 2023. 
 Reasoning in large language models through symbolic math word problems. 
 \emph{arXiv preprint arXiv:2308.01906}."
2406.00755,hong2024stuck,"[{Hong et~al.(2024)Hong, Ghosal, Majumder, Aditya, Mihalcea, and Poria}]{hong2024stuck} Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria. 2024.","Stuck in the quicksand of numeracy, far from agi summit: Evaluating llms' mathematical competency through ontology-guided perturbations.","Stuck in the quicksand of numeracy, far from agi summit: Evaluating llms' mathematical competency through ontology-guided perturbations.",,"[{Hong et~al.(2024)Hong, Ghosal, Majumder, Aditya, Mihalcea, and Poria}]{hong2024stuck} Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria. 2024. 
 Stuck in the quicksand of numeracy, far from agi summit: Evaluating llms' mathematical competency through ontology-guided perturbations. 
 \emph{arXiv preprint arXiv:2401.09395}."
2406.00755,huang2023large,"[{Huang et~al.(2023)Huang, Chen, Mishra, Zheng, Yu, Song, and Zhou}]{huang2023large} Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu~Steven Zheng, Adams~Wei Yu, Xinying Song, and Denny Zhou. 2023.",Large language models cannot self-correct reasoning yet.,Large language models cannot self-correct reasoning yet.,,"[{Huang et~al.(2023)Huang, Chen, Mishra, Zheng, Yu, Song, and Zhou}]{huang2023large} Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu~Steven Zheng, Adams~Wei Yu, Xinying Song, and Denny Zhou. 2023. 
 Large language models cannot self-correct reasoning yet. 
 \emph{arXiv preprint arXiv:2310.01798}."
2406.00755,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.00755,liu2023novice,"[{Liu et~al.(2023)Liu, Sonkar, Wang, Woodhead, and Baraniuk}]{liu2023novice} Naiming Liu, Shashank Sonkar, Zichao Wang, Simon Woodhead, and Richard~G Baraniuk. 2023.",Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions.,Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions.,,"[{Liu et~al.(2023)Liu, Sonkar, Wang, Woodhead, and Baraniuk}]{liu2023novice} Naiming Liu, Shashank Sonkar, Zichao Wang, Simon Woodhead, and Richard~G Baraniuk. 2023. 
 Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions. 
 \emph{arXiv preprint arXiv:2310.02439}."
2406.00755,lyu2023faithful,"[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023.",Faithful chain-of-thought reasoning.,Faithful chain-of-thought reasoning.,,"[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. 
 Faithful chain-of-thought reasoning. 
 \emph{arXiv preprint arXiv:2301.13379}."
2406.00755,mckenzie2023inverse,"[{McKenzie et~al.(2023)McKenzie, Lyzhov, Pieler, Parrish, Mueller, Prabhu, McLean, Kirtland, Ross, Liu et~al.}]{mckenzie2023inverse} Ian~R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et~al. 2023.",Inverse scaling: When bigger isn't better.,Inverse scaling: When bigger isn't better.,,"[{McKenzie et~al.(2023)McKenzie, Lyzhov, Pieler, Parrish, Mueller, Prabhu, McLean, Kirtland, Ross, Liu et~al.}]{mckenzie2023inverse} Ian~R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et~al. 2023. 
 Inverse scaling: When bigger isn't better. 
 \emph{arXiv preprint arXiv:2306.09479}."
2406.00755,paul2023refiner,"[{Paul et~al.(2023)Paul, Ismayilzada, Peyrard, Borges, Bosselut, West, and Faltings}]{paul2023refiner} Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.",Refiner: Reasoning feedback on intermediate representations.,Refiner: Reasoning feedback on intermediate representations.,,"[{Paul et~al.(2023)Paul, Ismayilzada, Peyrard, Borges, Bosselut, West, and Faltings}]{paul2023refiner} Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. 
 Refiner: Reasoning feedback on intermediate representations. 
 \emph{arXiv preprint arXiv:2304.01904}."
2406.00755,ribeiro2023street,"[{Ribeiro et~al.(2023)Ribeiro, Wang, Ma, Zhu, Dong, Kong, Burger, Ramos, Wang, Huang et~al.}]{ribeiro2023street} Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Wang, Zhiheng Huang, et~al. 2023.",Street: A multi-task structured reasoning and explanation benchmark.,Street: A multi-task structured reasoning and explanation benchmark.,,"[{Ribeiro et~al.(2023)Ribeiro, Wang, Ma, Zhu, Dong, Kong, Burger, Ramos, Wang, Huang et~al.}]{ribeiro2023street} Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Wang, Zhiheng Huang, et~al. 2023. 
 Street: A multi-task structured reasoning and explanation benchmark. 
 \emph{arXiv preprint arXiv:2302.06729}."
2406.00755,shakarian2023independent,"[{Shakarian et~al.(2023)Shakarian, Koyyalamudi, Ngu, and Mareedu}]{shakarian2023independent} Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023.",An independent evaluation of chatgpt on mathematical word problems (mwp).,An independent evaluation of chatgpt on mathematical word problems (mwp).,,"[{Shakarian et~al.(2023)Shakarian, Koyyalamudi, Ngu, and Mareedu}]{shakarian2023independent} Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023. 
 An independent evaluation of chatgpt on mathematical word problems (mwp). 
 \emph{arXiv preprint arXiv:2302.13814}."
2406.00755,shi2022language,"[{Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung, Tay, Ruder, Zhou et~al.}]{shi2022language} Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, et~al. 2022.",Language models are multilingual chain-of-thought reasoners.,Language models are multilingual chain-of-thought reasoners.,,"[{Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung, Tay, Ruder, Zhou et~al.}]{shi2022language} Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, et~al. 2022. 
 Language models are multilingual chain-of-thought reasoners. 
 \emph{arXiv preprint arXiv:2210.03057}."
2406.00755,stechly2023gpt,"[{Stechly et~al.(2023)Stechly, Marquez, and Kambhampati}]{stechly2023gpt} Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023.",Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems.,Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems.,,"[{Stechly et~al.(2023)Stechly, Marquez, and Kambhampati}]{stechly2023gpt} Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. 
 Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. 
 \emph{arXiv preprint arXiv:2310.12397}."
2406.00755,suzgun2022challenging,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou et~al.}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 2022.",Challenging big-bench tasks and whether chain-of-thought can solve them.,Challenging big-bench tasks and whether chain-of-thought can solve them.,,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou et~al.}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 2022. 
 Challenging big-bench tasks and whether chain-of-thought can solve them. 
 \emph{arXiv preprint arXiv:2210.09261}."
2406.00755,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.00755,toh2023veritymath,"[{Toh et~al.(2023)Toh, Puduppully, and Chen}]{toh2023veritymath} Vernon Toh, Ratish Puduppully, and Nancy~F Chen. 2023.",Veritymath: Advancing mathematical reasoning by self-verification through unit consistency.,Veritymath: Advancing mathematical reasoning by self-verification through unit consistency.,,"[{Toh et~al.(2023)Toh, Puduppully, and Chen}]{toh2023veritymath} Vernon Toh, Ratish Puduppully, and Nancy~F Chen. 2023. 
 Veritymath: Advancing mathematical reasoning by self-verification through unit consistency. 
 \emph{arXiv preprint arXiv:2311.07172}."
2406.00755,touvron2023LLaMA,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023LLaMA} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023LLaMA} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.00755,valmeekam2023can,"[{Valmeekam et~al.(2023)Valmeekam, Marquez, and Kambhampati}]{valmeekam2023can} Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.",Can large language models really improve by self-critiquing their own plans?,Can large language models really improve by self-critiquing their own plans?,,"[{Valmeekam et~al.(2023)Valmeekam, Marquez, and Kambhampati}]{valmeekam2023can} Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. 
 Can large language models really improve by self-critiquing their own plans? 
 \emph{arXiv preprint arXiv:2310.08118}."
2406.00755,wang2022towards,"[{Wang et~al.(2022)Wang, Min, Deng, Shen, Wu, Zettlemoyer, and Sun}]{wang2022towards} Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022.",Towards understanding chain-of-thought prompting: An empirical study of what matters.,Towards understanding chain-of-thought prompting: An empirical study of what matters.,,"[{Wang et~al.(2022)Wang, Min, Deng, Shen, Wu, Zettlemoyer, and Sun}]{wang2022towards} Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. 
 Towards understanding chain-of-thought prompting: An empirical study of what matters. 
 \emph{arXiv preprint arXiv:2212.10001}."
2406.00755,wang2023plan,"[{Wang et~al.(2023)Wang, Xu, Lan, Hu, Lan, Lee, and Lim}]{wang2023plan} Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023.",Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.,Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.,,"[{Wang et~al.(2023)Wang, Xu, Lan, Hu, Lan, Lee, and Lim}]{wang2023plan} Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. 
 Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. 
 \emph{arXiv preprint arXiv:2305.04091}."
2406.00755,yu2023metamath,"[{Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu}]{yu2023metamath} Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.",Metamath: Bootstrap your own mathematical questions for large language models.,Metamath: Bootstrap your own mathematical questions for large language models.,,"[{Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu}]{yu2023metamath} Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. 
 Metamath: Bootstrap your own mathematical questions for large language models. 
 \emph{arXiv preprint arXiv:2309.12284}."
2406.00755,zhang2023evaluating,"[{Zhang et~al.(2023)Zhang, Zhou, Wei, Zhao, Sha, Wang, and Wen}]{zhang2023evaluating} Beichen Zhang, Kun Zhou, Xilin Wei, Wayne~Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023.",Evaluating and improving tool-augmented computation-intensive math reasoning.,Evaluating and improving tool-augmented computation-intensive math reasoning.,,"[{Zhang et~al.(2023)Zhang, Zhou, Wei, Zhao, Sha, Wang, and Wen}]{zhang2023evaluating} Beichen Zhang, Kun Zhou, Xilin Wei, Wayne~Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023. 
 Evaluating and improving tool-augmented computation-intensive math reasoning. 
 \emph{arXiv preprint arXiv:2306.02408}."
2406.00755,zhou2023solving,"[{Zhou et~al.(2023)Zhou, Wang, Lu, Shi, Luo, Qin, Lu, Jia, Song, Zhan et~al.}]{zhou2023solving} Aojun Zhou, Ke~Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et~al. 2023.",Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification.,Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification.,,"[{Zhou et~al.(2023)Zhou, Wang, Lu, Shi, Luo, Qin, Lu, Jia, Song, Zhan et~al.}]{zhou2023solving} Aojun Zhou, Ke~Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et~al. 2023. 
 Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. 
 \emph{arXiv preprint arXiv:2308.07921}."
2406.00755,zhuang2023efficiently,"[{Zhuang et~al.(2023)Zhuang, Liu, Ning, Huang, Lv, Huang, Zhao, Zhang, Mao, Wang et~al.}]{zhuang2023efficiently} Yan Zhuang, Qi~Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, et~al. 2023.",Efficiently measuring the cognitive ability of llms: An adaptive testing perspective.,Efficiently measuring the cognitive ability of llms: An adaptive testing perspective.,,"[{Zhuang et~al.(2023)Zhuang, Liu, Ning, Huang, Lv, Huang, Zhao, Zhang, Mao, Wang et~al.}]{zhuang2023efficiently} Yan Zhuang, Qi~Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, et~al. 2023. 
 Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. 
 \emph{arXiv preprint arXiv:2306.10512}."
2406.01224,chen2023self,"[{Chen et~al.(2023)Chen, Wu, and Chen}]{chen2023self} Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. 2023.",Self-icl: Zero-shot in-context learning with self-generated demonstrations.,Self-icl: Zero-shot in-context learning with self-generated demonstrations.,,"[{Chen et~al.(2023)Chen, Wu, and Chen}]{chen2023self} Wei-Lin Chen, Cheng-Kuang Wu, and Hsin-Hsi Chen. 2023. 
 Self-icl: Zero-shot in-context learning with self-generated demonstrations. 
 \emph{arXiv preprint arXiv:2305.15035}."
2406.01224,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022.",A survey for in-context learning.,A survey for in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2406.01224,hao2022structured,"[{Hao et~al.(2022)Hao, Sun, Dong, Han, Gu, and Wei}]{hao2022structured} Yaru Hao, Yutao Sun, Li~Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022.","Structured prompting: Scaling in-context learning to 1,000 examples.","Structured prompting: Scaling in-context learning to 1,000 examples.",,"[{Hao et~al.(2022)Hao, Sun, Dong, Han, Gu, and Wei}]{hao2022structured} Yaru Hao, Yutao Sun, Li~Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. 
 Structured prompting: Scaling in-context learning to 1,000 examples. 
 \emph{arXiv preprint arXiv:2212.06713}."
2406.01224,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.01224,kim2022self,"[{Kim et~al.(2022)Kim, Cho, Kim, Kim, Yoo, and Lee}]{kim2022self} Hyuhng~Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang~Min Yoo, and Sang-goo Lee. 2022.",Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.,Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.,,"[{Kim et~al.(2022)Kim, Cho, Kim, Kim, Yoo, and Lee}]{kim2022self} Hyuhng~Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang~Min Yoo, and Sang-goo Lee. 2022. 
 Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. 
 \emph{arXiv preprint arXiv:2206.08082}."
2406.01224,reimers2019sentence,[{Reimers and Gurevych(2019)}]{reimers2019sentence} Nils Reimers and Iryna Gurevych. 2019.,Sentence-bert: Sentence embeddings using siamese bert-networks.,Sentence-bert: Sentence embeddings using siamese bert-networks.,,"[{Reimers and Gurevych(2019)}]{reimers2019sentence} Nils Reimers and Iryna Gurevych. 2019. 
 Sentence-bert: Sentence embeddings using siamese bert-networks. 
 \emph{arXiv preprint arXiv:1908.10084}."
2406.01224,scao2022bloom,"[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{scao2022bloom} Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2022.",Bloom: A 176b-parameter open-access multilingual language model.,Bloom: A 176b-parameter open-access multilingual language model.,,"[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{scao2022bloom} Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2022. 
 Bloom: A 176b-parameter open-access multilingual language model. 
 \emph{arXiv preprint arXiv:2211.05100}."
2406.01224,srivastava2022beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2406.01224,suzgun2022challenging,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou et~al.}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 2022.",Challenging big-bench tasks and whether chain-of-thought can solve them.,Challenging big-bench tasks and whether chain-of-thought can solve them.,,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou et~al.}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al. 2022. 
 Challenging big-bench tasks and whether chain-of-thought can solve them. 
 \emph{arXiv preprint arXiv:2210.09261}."
2406.01224,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.01224,wang2023openchat,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023.",Openchat: Advancing open-source language models with mixed-quality data.,Openchat: Advancing open-source language models with mixed-quality data.,,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. 
 Openchat: Advancing open-source language models with mixed-quality data. 
 \emph{arXiv preprint arXiv:2309.11235}."
2406.01224,wei2021finetuned,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2406.01224,zhang2022opt,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2406.01288,alon2023detecting,[Alon and Kamfonas(2023)]{alon2023detecting} Gabriel Alon and Michael Kamfonas.,Detecting language model attacks with perplexity.,Detecting language model attacks with perplexity.,,"[Alon and Kamfonas(2023)]{alon2023detecting} Gabriel Alon and Michael Kamfonas. 
 Detecting language model attacks with perplexity. 
 \emph{arXiv preprint arXiv:2308.14132}, 2023."
2406.01288,andriushchenko2024jailbreaking,"[Andriushchenko et~al.(2024)Andriushchenko, Croce, and Flammarion]{andriushchenko2024jailbreaking} Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion.",Jailbreaking leading safety-aligned llms with simple adaptive attacks.,Jailbreaking leading safety-aligned llms with simple adaptive attacks.,,"[Andriushchenko et~al.(2024)Andriushchenko, Croce, and Flammarion]{andriushchenko2024jailbreaking} Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. 
 Jailbreaking leading safety-aligned llms with simple adaptive attacks. 
 \emph{arXiv preprint arXiv:2404.02151}, 2024."
2406.01288,bai2023qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2406.01288,bai2022training,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}, 2022."
2406.01288,cao2023defending,"[Cao et~al.(2023)Cao, Cao, Lin, and Chen]{cao2023defending} Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen.",Defending against alignment-breaking attacks via robustly aligned llm.,Defending against alignment-breaking attacks via robustly aligned llm.,,"[Cao et~al.(2023)Cao, Cao, Lin, and Chen]{cao2023defending} Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen. 
 Defending against alignment-breaking attacks via robustly aligned llm. 
 \emph{arXiv preprint arXiv:2309.14348}, 2023."
2406.01288,chao2023jailbreaking,"[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.",Jailbreaking black box large language models in twenty queries.,Jailbreaking black box large language models in twenty queries.,,"[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong. 
 Jailbreaking black box large language models in twenty queries. 
 \emph{arXiv preprint arXiv:2310.08419}, 2023."
2406.01288,chao2024jailbreakbench,"[Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramer, et~al.]{chao2024jailbreakbench} Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J Pappas, Florian Tramer, et~al.",Jailbreakbench: An open robustness benchmark for jailbreaking large language models.,Jailbreakbench: An open robustness benchmark for jailbreaking large language models.,,"[Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramer, et~al.]{chao2024jailbreakbench} Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J Pappas, Florian Tramer, et~al. 
 Jailbreakbench: An open robustness benchmark for jailbreaking large language models. 
 \emph{arXiv preprint arXiv:2404.01318}, 2024."
2406.01288,deng2023jailbreaker,"[Deng et~al.(2023{\natexlab{a}})Deng, Liu, Li, Wang, Zhang, Li, Wang, Zhang, and Liu]{deng2023jailbreaker} Gelei Deng, Yi~Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu.",Jailbreaker: Automated jailbreak across multiple large language model chatbots.,Jailbreaker: Automated jailbreak across multiple large language model chatbots.,,"[Deng et~al.(2023{\natexlab{a}})Deng, Liu, Li, Wang, Zhang, Li, Wang, Zhang, and Liu]{deng2023jailbreaker} Gelei Deng, Yi~Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 
 Jailbreaker: Automated jailbreak across multiple large language model chatbots. 
 \emph{arXiv preprint arXiv:2307.08715}, 2023{\natexlab{a}}."
2406.01288,deng2023multilingual,"[Deng et~al.(2023{\natexlab{b}})Deng, Zhang, Pan, and Bing]{deng2023multilingual} Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing.",Multilingual jailbreak challenges in large language models.,Multilingual jailbreak challenges in large language models.,,"[Deng et~al.(2023{\natexlab{b}})Deng, Zhang, Pan, and Bing]{deng2023multilingual} Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing. 
 Multilingual jailbreak challenges in large language models. 
 \emph{arXiv preprint arXiv:2310.06474}, 2023{\natexlab{b}}."
2406.01288,gade2023badllama,"[Gade et~al.(2023)Gade, Lermen, Rogers-Smith, and Ladish]{gade2023badllama} Pranav Gade, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.",Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b.,Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b.,,"[Gade et~al.(2023)Gade, Lermen, Rogers-Smith, and Ladish]{gade2023badllama} Pranav Gade, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 
 Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b. 
 \emph{arXiv preprint arXiv:2311.00117}, 2023."
2406.01288,ganguli2022red,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red} Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red} Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al. 
 Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. 
 \emph{arXiv preprint arXiv:2209.07858}, 2022."
2406.01288,hayase2024query,"[Hayase et~al.(2024)Hayase, Borevkovic, Carlini, Tram{\`e}r, and Nasr]{hayase2024query} Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tram{\`e}r, and Milad Nasr.",Query-based adversarial prompt generation.,Query-based adversarial prompt generation.,,"[Hayase et~al.(2024)Hayase, Borevkovic, Carlini, Tram{\`e}r, and Nasr]{hayase2024query} Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tram{\`e}r, and Milad Nasr. 
 Query-based adversarial prompt generation. 
 \emph{arXiv preprint arXiv:2402.12329}, 2024."
2406.01288,helbling2023llm,"[Helbling et~al.(2023)Helbling, Phute, Hull, and Chau]{helbling2023llm} Alec Helbling, Mansi Phute, Matthew Hull, and Duen~Horng Chau.","Llm self defense: By self examination, llms know they are being tricked.","Llm self defense: By self examination, llms know they are being tricked.",,"[Helbling et~al.(2023)Helbling, Phute, Hull, and Chau]{helbling2023llm} Alec Helbling, Mansi Phute, Matthew Hull, and Duen~Horng Chau. 
 Llm self defense: By self examination, llms know they are being tricked. 
 \emph{arXiv preprint arXiv:2308.07308}, 2023."
2406.01288,hu2024gradient,"[Hu et~al.(2024)Hu, Chen, and Ho]{hu2024gradient} Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho.",Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes.,Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes.,,"[Hu et~al.(2024)Hu, Chen, and Ho]{hu2024gradient} Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. 
 Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. 
 \emph{arXiv preprint arXiv:2403.00867}, 2024."
2406.01288,hu2023token,"[Hu et~al.(2023)Hu, Wu, Mitra, Zhang, Sun, Huang, and Swaminathan]{hu2023token} Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, and Vishy Swaminathan.",Token-level adversarial prompt detection based on perplexity measures and contextual information.,Token-level adversarial prompt detection based on perplexity measures and contextual information.,,"[Hu et~al.(2023)Hu, Wu, Mitra, Zhang, Sun, Huang, and Swaminathan]{hu2023token} Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, and Vishy Swaminathan. 
 Token-level adversarial prompt detection based on perplexity measures and contextual information. 
 \emph{arXiv preprint arXiv:2311.11509}, 2023."
2406.01288,inan2023llama,"[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, et~al.]{inan2023llama} Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et~al.",Llama guard: Llm-based input-output safeguard for human-ai conversations.,Llama guard: Llm-based input-output safeguard for human-ai conversations.,,"[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, et~al.]{inan2023llama} Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et~al. 
 Llama guard: Llm-based input-output safeguard for human-ai conversations. 
 \emph{arXiv preprint arXiv:2312.06674}, 2023."
2406.01288,jain2023baseline,"[Jain et~al.(2023)Jain, Schwarzschild, Wen, Somepalli, Kirchenbauer, Chiang, Goldblum, Saha, Geiping, and Goldstein]{jain2023baseline} Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.",Baseline defenses for adversarial attacks against aligned language models.,Baseline defenses for adversarial attacks against aligned language models.,,"[Jain et~al.(2023)Jain, Schwarzschild, Wen, Somepalli, Kirchenbauer, Chiang, Goldblum, Saha, Geiping, and Goldstein]{jain2023baseline} Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 
 Baseline defenses for adversarial attacks against aligned language models. 
 \emph{arXiv preprint arXiv:2309.00614}, 2023."
2406.01288,ji2024defending,"[Ji et~al.(2024)Ji, Hou, Robey, Pappas, Hassani, Zhang, Wong, and Chang]{ji2024defending} Jiabao Ji, Bairu Hou, Alexander Robey, George~J Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang.",Defending large language models against jailbreak attacks via semantic smoothing.,Defending large language models against jailbreak attacks via semantic smoothing.,,"[Ji et~al.(2024)Ji, Hou, Robey, Pappas, Hassani, Zhang, Wong, and Chang]{ji2024defending} Jiabao Ji, Bairu Hou, Alexander Robey, George~J Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang. 
 Defending large language models against jailbreak attacks via semantic smoothing. 
 \emph{arXiv preprint arXiv:2402.16192}, 2024."
2406.01288,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2406.01288,kumar2023certifying,"[Kumar et~al.(2023)Kumar, Agarwal, Srinivas, Feizi, and Lakkaraju]{kumar2023certifying} Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju.",Certifying llm safety against adversarial prompting.,Certifying llm safety against adversarial prompting.,,"[Kumar et~al.(2023)Kumar, Agarwal, Srinivas, Feizi, and Lakkaraju]{kumar2023certifying} Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. 
 Certifying llm safety against adversarial prompting. 
 \emph{arXiv preprint arXiv:2309.02705}, 2023."
2406.01288,lapid2023open,"[Lapid et~al.(2023)Lapid, Langberg, and Sipper]{lapid2023open} Raz Lapid, Ron Langberg, and Moshe Sipper.",Open sesame! universal black box jailbreaking of large language models.,Open sesame! universal black box jailbreaking of large language models.,,"[Lapid et~al.(2023)Lapid, Langberg, and Sipper]{lapid2023open} Raz Lapid, Ron Langberg, and Moshe Sipper. 
 Open sesame! universal black box jailbreaking of large language models. 
 \emph{arXiv preprint arXiv:2309.01446}, 2023."
2406.01288,lermen2023lora,"[Lermen et~al.(2023)Lermen, Rogers-Smith, and Ladish]{lermen2023lora} Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.",Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.,Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.,,"[Lermen et~al.(2023)Lermen, Rogers-Smith, and Ladish]{lermen2023lora} Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 
 Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. 
 \emph{arXiv preprint arXiv:2310.20624}, 2023."
2406.01288,li2023deepinception,"[Li et~al.(2023{\natexlab{a}})Li, Zhou, Zhu, Yao, Liu, and Han]{li2023deepinception} Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo~Han.",Deepinception: Hypnotize large language model to be jailbreaker.,Deepinception: Hypnotize large language model to be jailbreaker.,,"[Li et~al.(2023{\natexlab{a}})Li, Zhou, Zhu, Yao, Liu, and Han]{li2023deepinception} Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo~Han. 
 Deepinception: Hypnotize large language model to be jailbreaker. 
 \emph{arXiv preprint arXiv:2311.03191}, 2023{\natexlab{a}}."
2406.01288,li2023rain,"[Li et~al.(2023{\natexlab{b}})Li, Wei, Zhao, Zhang, and Zhang]{li2023rain} Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang.",Rain: Your language models can align themselves without finetuning.,Rain: Your language models can align themselves without finetuning.,,"[Li et~al.(2023{\natexlab{b}})Li, Wei, Zhao, Zhang, and Zhang]{li2023rain} Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 
 Rain: Your language models can align themselves without finetuning. 
 \emph{arXiv preprint arXiv:2309.07124}, 2023{\natexlab{b}}."
2406.01288,liao2024amplegcg,[Liao and Sun(2024)]{liao2024amplegcg} Zeyi Liao and Huan Sun.,Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.,Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.,,"[Liao and Sun(2024)]{liao2024amplegcg} Zeyi Liao and Huan Sun. 
 Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms. 
 \emph{arXiv preprint arXiv:2404.07921}, 2024."
2406.01288,lin2023unlocking,"[Lin et~al.(2023)Lin, Ravichander, Lu, Dziri, Sclar, Chandu, Bhagavatula, and Choi]{lin2023unlocking} Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi.",The unlocking spell on base llms: Rethinking alignment via in-context learning.,The unlocking spell on base llms: Rethinking alignment via in-context learning.,,"[Lin et~al.(2023)Lin, Ravichander, Lu, Dziri, Sclar, Chandu, Bhagavatula, and Choi]{lin2023unlocking} Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 
 The unlocking spell on base llms: Rethinking alignment via in-context learning. 
 \emph{arXiv preprint arXiv:2312.01552}, 2023."
2406.01288,liu2023autodan,"[Liu et~al.(2023{\natexlab{b}})Liu, Xu, Chen, and Xiao]{liu2023autodan} Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.",Autodan: Generating stealthy jailbreak prompts on aligned large language models.,Autodan: Generating stealthy jailbreak prompts on aligned large language models.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Xu, Chen, and Xiao]{liu2023autodan} Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 
 Autodan: Generating stealthy jailbreak prompts on aligned large language models. 
 \emph{arXiv preprint arXiv:2310.04451}, 2023{\natexlab{b}}."
2406.01288,liu2023jailbreaking,"[Liu et~al.(2023{\natexlab{c}})Liu, Deng, Xu, Li, Zheng, Zhang, Zhao, Zhang, and Liu]{liu2023jailbreaking} Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.",Jailbreaking chatgpt via prompt engineering: An empirical study.,Jailbreaking chatgpt via prompt engineering: An empirical study.,,"[Liu et~al.(2023{\natexlab{c}})Liu, Deng, Xu, Li, Zheng, Zhang, Zhao, Zhang, and Liu]{liu2023jailbreaking} Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 
 Jailbreaking chatgpt via prompt engineering: An empirical study. 
 \emph{arXiv preprint arXiv:2305.13860}, 2023{\natexlab{c}}."
2406.01288,mangaokar2024prp,"[Mangaokar et~al.(2024)Mangaokar, Hooda, Choi, Chandrashekaran, Fawaz, Jha, and Prakash]{mangaokar2024prp} Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, and Atul Prakash.",Prp: Propagating universal perturbations to attack large language model guard-rails.,Prp: Propagating universal perturbations to attack large language model guard-rails.,,"[Mangaokar et~al.(2024)Mangaokar, Hooda, Choi, Chandrashekaran, Fawaz, Jha, and Prakash]{mangaokar2024prp} Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, and Atul Prakash. 
 Prp: Propagating universal perturbations to attack large language model guard-rails. 
 \emph{arXiv preprint arXiv:2402.15911}, 2024."
2406.01288,mazeika2024harmbench,"[Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li, et~al.]{mazeika2024harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al.",Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,,"[Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li, et~al.]{mazeika2024harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al. 
 Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. 
 \emph{arXiv preprint arXiv:2402.04249}, 2024."
2406.01288,mehrotra2023tree,"[Mehrotra et~al.(2023)Mehrotra, Zampetakis, Kassianik, Nelson, Anderson, Singer, and Karbasi]{mehrotra2023tree} Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi.",Tree of attacks: Jailbreaking black-box llms automatically.,Tree of attacks: Jailbreaking black-box llms automatically.,,"[Mehrotra et~al.(2023)Mehrotra, Zampetakis, Kassianik, Nelson, Anderson, Singer, and Karbasi]{mehrotra2023tree} Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. 
 Tree of attacks: Jailbreaking black-box llms automatically. 
 \emph{arXiv preprint arXiv:2312.02119}, 2023."
2406.01288,mo2024studious,"[Mo et~al.(2024)Mo, Wang, Wei, and Wang]{mo2024studious} Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang.",Studious bob fight back against jailbreaking via prompt adversarial tuning.,Studious bob fight back against jailbreaking via prompt adversarial tuning.,,"[Mo et~al.(2024)Mo, Wang, Wei, and Wang]{mo2024studious} Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang. 
 Studious bob fight back against jailbreaking via prompt adversarial tuning. 
 \emph{arXiv preprint arXiv:2402.06255}, 2024."
2406.01288,paulus2024advprompter,"[Paulus et~al.(2024)Paulus, Zharmagambetov, Guo, Amos, and Tian]{paulus2024advprompter} Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian.",Advprompter: Fast adaptive adversarial prompting for llms.,Advprompter: Fast adaptive adversarial prompting for llms.,,"[Paulus et~al.(2024)Paulus, Zharmagambetov, Guo, Amos, and Tian]{paulus2024advprompter} Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. 
 Advprompter: Fast adaptive adversarial prompting for llms. 
 \emph{arXiv preprint arXiv:2404.16873}, 2024."
2406.01288,perez2022red,"[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red} Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.",Red teaming language models with language models.,Red teaming language models with language models.,,"[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red} Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 
 Red teaming language models with language models. 
 \emph{arXiv preprint arXiv:2202.03286}, 2022."
2406.01288,qi2023fine,"[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine} Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.","Fine-tuning aligned language models compromises safety, even when users do not intend to!","Fine-tuning aligned language models compromises safety, even when users do not intend to!",,"[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine} Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 
 Fine-tuning aligned language models compromises safety, even when users do not intend to! 
 \emph{arXiv preprint arXiv:2310.03693}, 2023."
2406.01288,rao2023tricking,"[Rao et~al.(2023)Rao, Vashistha, Naik, Aditya, and Choudhury]{rao2023tricking} Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury.","Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks.","Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks.",,"[Rao et~al.(2023)Rao, Vashistha, Naik, Aditya, and Choudhury]{rao2023tricking} Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. 
 Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks. 
 \emph{arXiv preprint arXiv:2305.14965}, 2023."
2406.01288,robey2023smoothllm,"[Robey et~al.(2023)Robey, Wong, Hassani, and Pappas]{robey2023smoothllm} Alexander Robey, Eric Wong, Hamed Hassani, and George~J Pappas.",Smoothllm: Defending large language models against jailbreaking attacks.,Smoothllm: Defending large language models against jailbreaking attacks.,,"[Robey et~al.(2023)Robey, Wong, Hassani, and Pappas]{robey2023smoothllm} Alexander Robey, Eric Wong, Hamed Hassani, and George~J Pappas. 
 Smoothllm: Defending large language models against jailbreaking attacks. 
 \emph{arXiv preprint arXiv:2310.03684}, 2023."
2406.01288,ruan2023identifying,"[Ruan et~al.(2023)Ruan, Dong, Wang, Pitis, Zhou, Ba, Dubois, Maddison, and Hashimoto]{ruan2023identifying} Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris~J Maddison, and Tatsunori Hashimoto.",Identifying the risks of lm agents with an lm-emulated sandbox.,Identifying the risks of lm agents with an lm-emulated sandbox.,,"[Ruan et~al.(2023)Ruan, Dong, Wang, Pitis, Zhou, Ba, Dubois, Maddison, and Hashimoto]{ruan2023identifying} Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris~J Maddison, and Tatsunori Hashimoto. 
 Identifying the risks of lm agents with an lm-emulated sandbox. 
 \emph{arXiv preprint arXiv:2309.15817}, 2023."
2406.01288,sharma2024spml,"[Sharma et~al.(2024)Sharma, Gupta, and Grossman]{sharma2024spml} Reshabh~K Sharma, Vinayak Gupta, and Dan Grossman.",Spml: A dsl for defending language models against prompt attacks.,Spml: A dsl for defending language models against prompt attacks.,,"[Sharma et~al.(2024)Sharma, Gupta, and Grossman]{sharma2024spml} Reshabh~K Sharma, Vinayak Gupta, and Dan Grossman. 
 Spml: A dsl for defending language models against prompt attacks. 
 \emph{arXiv preprint arXiv:2402.11755}, 2024."
2406.01288,shen2023anything,"[Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.","""do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.","""do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",,"[Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 
 ""do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. 
 \emph{arXiv preprint arXiv:2308.03825}, 2023."
2406.01288,sitawarin2024pal,"[Sitawarin et~al.(2024)Sitawarin, Mu, Wagner, and Araujo]{sitawarin2024pal} Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo.",Pal: Proxy-guided black-box attack on large language models.,Pal: Proxy-guided black-box attack on large language models.,,"[Sitawarin et~al.(2024)Sitawarin, Mu, Wagner, and Araujo]{sitawarin2024pal} Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo. 
 Pal: Proxy-guided black-box attack on large language models. 
 \emph{arXiv preprint arXiv:2402.09674}, 2024."
2406.01288,tian2023evil,"[Tian et~al.(2023)Tian, Yang, Zhang, Dong, and Su]{tian2023evil} Yu~Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su.",Evil geniuses: Delving into the safety of llm-based agents.,Evil geniuses: Delving into the safety of llm-based agents.,,"[Tian et~al.(2023)Tian, Yang, Zhang, Dong, and Su]{tian2023evil} Yu~Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. 
 Evil geniuses: Delving into the safety of llm-based agents. 
 \emph{arXiv preprint arXiv:2311.11855}, 2023."
2406.01288,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.01288,toyer2023tensor,"[Toyer et~al.(2023)Toyer, Watkins, Mendes, Svegliato, Bailey, Wang, Ong, Elmaaroufi, Abbeel, Darrell, et~al.]{toyer2023tensor} Sam Toyer, Olivia Watkins, Ethan~Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et~al.",Tensor trust: Interpretable prompt injection attacks from an online game.,Tensor trust: Interpretable prompt injection attacks from an online game.,,"[Toyer et~al.(2023)Toyer, Watkins, Mendes, Svegliato, Bailey, Wang, Ong, Elmaaroufi, Abbeel, Darrell, et~al.]{toyer2023tensor} Sam Toyer, Olivia Watkins, Ethan~Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et~al. 
 Tensor trust: Interpretable prompt injection attacks from an online game. 
 \emph{arXiv preprint arXiv:2311.01011}, 2023."
2406.01288,wang2023openchat,"[Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.",Openchat: Advancing open-source language models with mixed-quality data.,Openchat: Advancing open-source language models with mixed-quality data.,,"[Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 
 Openchat: Advancing open-source language models with mixed-quality data. 
 \emph{arXiv preprint arXiv:2309.11235}, 2023."
2406.01288,wang2024noise,"[Wang et~al.(2024{\natexlab{a}})Wang, Li, Huang, and Sha]{wang2024noise} Hao Wang, Hao Li, Minlie Huang, and Lei Sha.",From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings.,From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings.,,"[Wang et~al.(2024{\natexlab{a}})Wang, Li, Huang, and Sha]{wang2024noise} Hao Wang, Hao Li, Minlie Huang, and Lei Sha. 
 From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings. 
 \emph{arXiv preprint arXiv:2402.16006}, 2024{\natexlab{a}}."
2406.01288,wang2024defending,"[Wang et~al.(2024{\natexlab{b}})Wang, Shi, Bai, and Hsieh]{wang2024defending} Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh.",Defending llms against jailbreaking attacks via backtranslation.,Defending llms against jailbreaking attacks via backtranslation.,,"[Wang et~al.(2024{\natexlab{b}})Wang, Shi, Bai, and Hsieh]{wang2024defending} Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. 
 Defending llms against jailbreaking attacks via backtranslation. 
 \emph{arXiv preprint arXiv:2402.16459}, 2024{\natexlab{b}}."
2406.01288,wei2023jailbreak,"[Wei et~al.(2023{\natexlab{b}})Wei, Wang, and Wang]{wei2023jailbreak} Zeming Wei, Yifei Wang, and Yisen Wang.",Jailbreak and guard aligned language models with only few in-context demonstrations.,Jailbreak and guard aligned language models with only few in-context demonstrations.,,"[Wei et~al.(2023{\natexlab{b}})Wei, Wang, and Wang]{wei2023jailbreak} Zeming Wei, Yifei Wang, and Yisen Wang. 
 Jailbreak and guard aligned language models with only few in-context demonstrations. 
 \emph{arXiv preprint arXiv:2310.06387}, 2023{\natexlab{b}}."
2406.01288,xu2024safedecoding,"[Xu et~al.(2024)Xu, Jiang, Niu, Jia, Lin, and Poovendran]{xu2024safedecoding} Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill~Yuchen Lin, and Radha Poovendran.",Safedecoding: Defending against jailbreak attacks via safety-aware decoding.,Safedecoding: Defending against jailbreak attacks via safety-aware decoding.,,"[Xu et~al.(2024)Xu, Jiang, Niu, Jia, Lin, and Poovendran]{xu2024safedecoding} Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill~Yuchen Lin, and Radha Poovendran. 
 Safedecoding: Defending against jailbreak attacks via safety-aware decoding. 
 \emph{arXiv preprint arXiv:2402.08983}, 2024."
2406.01288,yang2023shadow,"[Yang et~al.(2023)Yang, Wang, Zhang, Petzold, Wang, Zhao, and Lin]{yang2023shadow} Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin.",Shadow alignment: The ease of subverting safely-aligned language models.,Shadow alignment: The ease of subverting safely-aligned language models.,,"[Yang et~al.(2023)Yang, Wang, Zhang, Petzold, Wang, Zhao, and Lin]{yang2023shadow} Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin. 
 Shadow alignment: The ease of subverting safely-aligned language models. 
 \emph{arXiv preprint arXiv:2310.02949}, 2023."
2406.01288,yong2023low,"[Yong et~al.(2023)Yong, Menghini, and Bach]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach.",Low-resource languages jailbreak gpt-4.,Low-resource languages jailbreak gpt-4.,,"[Yong et~al.(2023)Yong, Menghini, and Bach]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach. 
 Low-resource languages jailbreak gpt-4. 
 \emph{arXiv preprint arXiv:2310.02446}, 2023."
2406.01288,yuan2023gpt,"[Yuan et~al.(2023)Yuan, Jiao, Wang, Huang, He, Shi, and Tu]{yuan2023gpt} Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu.",Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.,Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.,,"[Yuan et~al.(2023)Yuan, Jiao, Wang, Huang, He, Shi, and Tu]{yuan2023gpt} Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 
 Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. 
 \emph{arXiv preprint arXiv:2308.06463}, 2023."
2406.01288,yuan2024rigorllm,"[Yuan et~al.(2024)Yuan, Xiong, Zeng, Yu, Jia, Song, and Li]{yuan2024rigorllm} Zhuowen Yuan, Zidi Xiong, Yi~Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo~Li.",Rigorllm: Resilient guardrails for large language models against undesired content.,Rigorllm: Resilient guardrails for large language models against undesired content.,,"[Yuan et~al.(2024)Yuan, Xiong, Zeng, Yu, Jia, Song, and Li]{yuan2024rigorllm} Zhuowen Yuan, Zidi Xiong, Yi~Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo~Li. 
 Rigorllm: Resilient guardrails for large language models against undesired content. 
 \emph{arXiv preprint arXiv:2403.13031}, 2024."
2406.01288,zeng2024johnny,"[Zeng et~al.(2024)Zeng, Lin, Zhang, Yang, Jia, and Shi]{zeng2024johnny} Yi~Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.",How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.,How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.,,"[Zeng et~al.(2024)Zeng, Lin, Zhang, Yang, Jia, and Shi]{zeng2024johnny} Yi~Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. 
 How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. 
 \emph{arXiv preprint arXiv:2401.06373}, 2024."
2406.01288,zhang2023defending,"[Zhang et~al.(2023)Zhang, Yang, Ke, and Huang]{zhang2023defending} Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang.",Defending large language models against jailbreaking attacks through goal prioritization.,Defending large language models against jailbreaking attacks through goal prioritization.,,"[Zhang et~al.(2023)Zhang, Yang, Ke, and Huang]{zhang2023defending} Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 
 Defending large language models against jailbreaking attacks through goal prioritization. 
 \emph{arXiv preprint arXiv:2311.09096}, 2023."
2406.01288,zhou2024robust,"[Zhou et~al.(2024{\natexlab{a}})Zhou, Li, and Wang]{zhou2024robust} Andy Zhou, Bo~Li, and Haohan Wang.",Robust prompt optimization for defending language models against jailbreaking attacks.,Robust prompt optimization for defending language models against jailbreaking attacks.,,"[Zhou et~al.(2024{\natexlab{a}})Zhou, Li, and Wang]{zhou2024robust} Andy Zhou, Bo~Li, and Haohan Wang. 
 Robust prompt optimization for defending language models against jailbreaking attacks. 
 \emph{arXiv preprint arXiv:2401.17263}, 2024{\natexlab{a}}."
2406.01288,zhou2024defending,"[Zhou et~al.(2024{\natexlab{b}})Zhou, Han, Zhuang, Guo, Guo, Liang, Bao, and Zhang]{zhou2024defending} Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang.",Defending jailbreak prompts via in-context adversarial game.,Defending jailbreak prompts via in-context adversarial game.,,"[Zhou et~al.(2024{\natexlab{b}})Zhou, Han, Zhuang, Guo, Guo, Liang, Bao, and Zhang]{zhou2024defending} Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. 
 Defending jailbreak prompts via in-context adversarial game. 
 \emph{arXiv preprint arXiv:2402.13148}, 2024{\natexlab{b}}."
2406.01288,zhu2023autodan,"[Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun]{zhu2023autodan} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun.",Autodan: Automatic and interpretable adversarial attacks on large language models.,Autodan: Automatic and interpretable adversarial attacks on large language models.,,"[Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun]{zhu2023autodan} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 
 Autodan: Automatic and interpretable adversarial attacks on large language models. 
 \emph{arXiv preprint arXiv:2310.15140}, 2023."
2406.01288,zou2023universal,"[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 
 Universal and transferable adversarial attacks on aligned language models. 
 \emph{arXiv preprint arXiv:2307.15043}, 2023."
2406.01333,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.01333,alain2016understanding,[{Alain and Bengio(2016)}]{alain2016understanding} Guillaume Alain and Yoshua Bengio. 2016.,Understanding intermediate layers using linear classifier probes.,Understanding intermediate layers using linear classifier probes.,,"[{Alain and Bengio(2016)}]{alain2016understanding} Guillaume Alain and Yoshua Bengio. 2016. 
 Understanding intermediate layers using linear classifier probes. 
 \emph{arXiv preprint arXiv:1610.01644}."
2406.01333,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2406.01333,qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2406.01333,golchin2023data,[{Golchin and Surdeanu(2023)}]{golchin2023data} Shahriar Golchin and Mihai Surdeanu. 2023.,Data contamination quiz: A tool to detect and estimate contamination in large language models.,Data contamination quiz: A tool to detect and estimate contamination in large language models.,,"[{Golchin and Surdeanu(2023)}]{golchin2023data} Shahriar Golchin and Mihai Surdeanu. 2023. 
 Data contamination quiz: A tool to detect and estimate contamination in large language models. 
 \emph{arXiv preprint arXiv:2311.06233}."
2406.01333,liu2019roberta,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019roberta} Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019roberta} Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. 
 Roberta: A robustly optimized bert pretraining approach. 
 \emph{arXiv preprint arXiv:1907.11692}."
2406.01333,sainz2023nlp,"[{Sainz et~al.(2023)Sainz, Campos, Garc{\'\i}a-Ferrero, Etxaniz, de~Lacalle, and Agirre}]{sainz2023nlp} Oscar Sainz, Jon~Ander Campos, Iker Garc{\'\i}a-Ferrero, Julen Etxaniz, Oier~Lopez de~Lacalle, and Eneko Agirre. 2023.",Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark.,Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark.,,"[{Sainz et~al.(2023)Sainz, Campos, Garc{\'\i}a-Ferrero, Etxaniz, de~Lacalle, and Agirre}]{sainz2023nlp} Oscar Sainz, Jon~Ander Campos, Iker Garc{\'\i}a-Ferrero, Julen Etxaniz, Oier~Lopez de~Lacalle, and Eneko Agirre. 2023. 
 Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. 
 \emph{arXiv preprint arXiv:2310.18018}."
2406.01333,shi2023detecting,"[{Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer}]{shi2023detecting} Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023.",Detecting pretraining data from large language models.,Detecting pretraining data from large language models.,,"[{Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer}]{shi2023detecting} Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. 
 Detecting pretraining data from large language models. 
 \emph{arXiv preprint arXiv:2310.16789}."
2406.01333,touvron2023llama1,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama1} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.01981,abbas2023semdedup,"[Abbas et~al., 2023]{abbas2023semdedup} Abbas, A., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.~S. (2023).",Semdedup: Data-efficient learning at web-scale through semantic deduplication.,Semdedup: Data-efficient learning at web-scale through semantic deduplication.,,"[Abbas et~al., 2023]{abbas2023semdedup} Abbas, A., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.~S. (2023). 
 Semdedup: Data-efficient learning at web-scale through semantic deduplication. 
 {\em arXiv preprint arXiv:2303.09540}."
2406.01981,achiam2023gpt,"[Achiam et~al., 2023]{achiam2023gpt} Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al. (2023).",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al., 2023]{achiam2023gpt} Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al. (2023). 
 Gpt-4 technical report. 
 {\em arXiv preprint arXiv:2303.08774}."
2406.01981,gao2020pile,"[Gao et~al., 2020]{gao2020pile} Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al. (2020).",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[Gao et~al., 2020]{gao2020pile} Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al. (2020). 
 The pile: An 800gb dataset of diverse text for language modeling. 
 {\em arXiv preprint arXiv:2101.00027}."
2406.01981,hestness2017deep,"[Hestness et~al., 2017]{hestness2017deep} Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M.~A., Yang, Y., and Zhou, Y. (2017).","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[Hestness et~al., 2017]{hestness2017deep} Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M.~A., Yang, Y., and Zhou, Y. (2017). 
 Deep learning scaling is predictable, empirically. 
 {\em arXiv preprint arXiv:1712.00409}."
2406.01981,hoffmann2022training,"[Hoffmann et~al., 2022]{hoffmann2022training} Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al. (2022).",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al., 2022]{hoffmann2022training} Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al. (2022). 
 Training compute-optimal large language models. 
 {\em arXiv preprint arXiv:2203.15556}."
2406.01981,ilyas2022datamodels,"[Ilyas et~al., 2022]{ilyas2022datamodels} Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A. (2022).",Datamodels: Predicting predictions from training data.,Datamodels: Predicting predictions from training data.,,"[Ilyas et~al., 2022]{ilyas2022datamodels} Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A. (2022). 
 Datamodels: Predicting predictions from training data. 
 {\em arXiv preprint arXiv:2202.00622}."
2406.01981,jiang2023mistral,"[Jiang et~al., 2023]{jiang2023mistral} Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al. (2023).",Mistral 7b.,Mistral 7b.,,"[Jiang et~al., 2023]{jiang2023mistral} Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al. (2023). 
 Mistral 7b. 
 {\em arXiv preprint arXiv:2310.06825}."
2406.01981,kaplan2020scaling,"[Kaplan et~al., 2020]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020).",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al., 2020]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). 
 Scaling laws for neural language models. 
 {\em arXiv preprint arXiv:2001.08361}."
2406.01981,lee2021deduplicating,"[Lee et~al., 2021]{lee2021deduplicating} Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2021).",Deduplicating training data makes language models better.,Deduplicating training data makes language models better.,,"[Lee et~al., 2021]{lee2021deduplicating} Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2021). 
 Deduplicating training data makes language models better. 
 {\em arXiv preprint arXiv:2107.06499}."
2406.01981,li2023starcoder,"[Li et~al., 2023]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al. (2023).",Starcoder: may the source be with you!,Starcoder: may the source be with you!,,"[Li et~al., 2023]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al. (2023). 
 Starcoder: may the source be with you! 
 {\em arXiv preprint arXiv:2305.06161}."
2406.01981,maini2024rephrasing,"[Maini et~al., 2024]{maini2024rephrasing} Maini, P., Seto, S., Bai, H., Grangier, D., Zhang, Y., and Jaitly, N. (2024).",Rephrasing the web: A recipe for compute and data-efficient language modeling.,Rephrasing the web: A recipe for compute and data-efficient language modeling.,,"[Maini et~al., 2024]{maini2024rephrasing} Maini, P., Seto, S., Bai, H., Grangier, D., Zhang, Y., and Jaitly, N. (2024). 
 Rephrasing the web: A recipe for compute and data-efficient language modeling. 
 {\em arXiv preprint arXiv:2401.16380}."
2406.01981,marion2023less,"[Marion et~al., 2023]{marion2023less} Marion, M., {\""U}st{\""u}n, A., Pozzobon, L., Wang, A., Fadaee, M., and Hooker, S. (2023).",When less is more: Investigating data pruning for pretraining llms at scale.,When less is more: Investigating data pruning for pretraining llms at scale.,,"[Marion et~al., 2023]{marion2023less} Marion, M., {\""U}st{\""u}n, A., Pozzobon, L., Wang, A., Fadaee, M., and Hooker, S. (2023). 
 When less is more: Investigating data pruning for pretraining llms at scale. 
 {\em arXiv preprint arXiv:2309.04564}."
2406.01981,penedo2023refinedweb,"[Penedo et~al., 2023]{penedo2023refinedweb} Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023).","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.",,"[Penedo et~al., 2023]{penedo2023refinedweb} Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023). 
 The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 
 {\em arXiv preprint arXiv:2306.01116}."
2406.01981,rae2021scaling,"[Rae et~al., 2021]{rae2021scaling} Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al. (2021).","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[Rae et~al., 2021]{rae2021scaling} Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al. (2021). 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 {\em arXiv preprint arXiv:2112.11446}."
2406.01981,shoeybi2019megatron,"[Shoeybi et~al., 2019]{shoeybi2019megatron} Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019).",Megatron-lm: Training multi-billion parameter language models using model parallelism.,Megatron-lm: Training multi-billion parameter language models using model parallelism.,,"[Shoeybi et~al., 2019]{shoeybi2019megatron} Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). 
 Megatron-lm: Training multi-billion parameter language models using model parallelism. 
 {\em arXiv preprint arXiv:1909.08053}."
2406.01981,soldaini2024dolma,"[Soldaini et~al., 2024]{soldaini2024dolma} Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et~al. (2024).",Dolma: An open corpus of three trillion tokens for language model pretraining research.,Dolma: An open corpus of three trillion tokens for language model pretraining research.,,"[Soldaini et~al., 2024]{soldaini2024dolma} Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et~al. (2024). 
 Dolma: An open corpus of three trillion tokens for language model pretraining research. 
 {\em arXiv preprint arXiv:2402.00159}."
2406.01981,team2023gemini,"[Team et~al., 2023]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al. (2023).",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al., 2023]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al. (2023). 
 Gemini: a family of highly capable multimodal models. 
 {\em arXiv preprint arXiv:2312.11805}."
2406.01981,team2024gemma,"[Team et~al., 2024]{team2024gemma} Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi{\`e}re, M., Kale, M.~S., Love, J., et~al. (2024).",Gemma: Open models based on gemini research and technology.,Gemma: Open models based on gemini research and technology.,,"[Team et~al., 2024]{team2024gemma} Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi{\`e}re, M., Kale, M.~S., Love, J., et~al. (2024). 
 Gemma: Open models based on gemini research and technology. 
 {\em arXiv preprint arXiv:2403.08295}."
2406.01981,touvron2023llama,"[Touvron et~al., 2023]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023).",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al., 2023]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023). 
 Llama 2: Open foundation and fine-tuned chat models. 
 {\em arXiv preprint arXiv:2307.09288}."
2406.02224,cai2022autofednlp,"[{Cai et~al.(2022)Cai, Wu, Wang, Lin, and Xu}]{cai2022autofednlp} Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix~Xiaozhu Lin, and Mengwei Xu. 2022.",Autofednlp: An efficient fednlp framework.,Autofednlp: An efficient fednlp framework.,,"[{Cai et~al.(2022)Cai, Wu, Wang, Lin, and Xu}]{cai2022autofednlp} Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix~Xiaozhu Lin, and Mengwei Xu. 2022. 
 Autofednlp: An efficient fednlp framework. 
 \emph{arXiv preprint arXiv:2205.10162}."
2406.02224,chen2021dialogsum,"[{Chen et~al.(2021)Chen, Liu, Chen, and Zhang}]{chen2021dialogsum} Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021.",Dialogsum: A real-life scenario dialogue summarization dataset.,Dialogsum: A real-life scenario dialogue summarization dataset.,,"[{Chen et~al.(2021)Chen, Liu, Chen, and Zhang}]{chen2021dialogsum} Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021. 
 Dialogsum: A real-life scenario dialogue summarization dataset. 
 \emph{arXiv preprint arXiv:2105.06762}."
2406.02224,cho2022heterogeneous,"[{Cho et~al.(2022)Cho, Manoel, Joshi, Sim, and Dimitriadis}]{cho2022heterogeneous} Yae~Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. 2022.",Heterogeneous ensemble knowledge transfer for training large models in federated learning.,Heterogeneous ensemble knowledge transfer for training large models in federated learning.,,"[{Cho et~al.(2022)Cho, Manoel, Joshi, Sim, and Dimitriadis}]{cho2022heterogeneous} Yae~Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. 2022. 
 Heterogeneous ensemble knowledge transfer for training large models in federated learning. 
 \emph{arXiv preprint arXiv:2204.12703}."
2406.02224,clark2019boolq,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}."
2406.02224,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.02224,fan2023fate,"[{Fan et~al.(2023)Fan, Kang, Ma, Chen, Wei, Fan, and Yang}]{fan2023fate} Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. 2023.",Fate-llm: A industrial grade federated learning framework for large language models.,Fate-llm: A industrial grade federated learning framework for large language models.,,"[{Fan et~al.(2023)Fan, Kang, Ma, Chen, Wei, Fan, and Yang}]{fan2023fate} Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. 2023. 
 Fate-llm: A industrial grade federated learning framework for large language models. 
 \emph{arXiv preprint arXiv:2310.10049}."
2406.02224,he2021towards,"[{He et~al.(2021)He, Zhou, Ma, Berg-Kirkpatrick, and Neubig}]{he2021towards} Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021.",Towards a unified view of parameter-efficient transfer learning.,Towards a unified view of parameter-efficient transfer learning.,,"[{He et~al.(2021)He, Zhou, Ma, Berg-Kirkpatrick, and Neubig}]{he2021towards} Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. 
 Towards a unified view of parameter-efficient transfer learning. 
 \emph{arXiv preprint arXiv:2110.04366}."
2406.02224,hinton2015distilling,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.",Distilling the knowledge in a neural network.,Distilling the knowledge in a neural network.,,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. 
 Distilling the knowledge in a neural network. 
 \emph{arXiv preprint arXiv:1503.02531}."
2406.02224,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2406.02224,kang2023grounding,"[{Kang et~al.(2023)Kang, Fan, Gu, Fan, and Yang}]{kang2023grounding} Yan Kang, Tao Fan, Hanlin Gu, Lixin Fan, and Qiang Yang. 2023.",Grounding foundation models through federated transfer learning: A general framework.,Grounding foundation models through federated transfer learning: A general framework.,,"[{Kang et~al.(2023)Kang, Fan, Gu, Fan, and Yang}]{kang2023grounding} Yan Kang, Tao Fan, Hanlin Gu, Lixin Fan, and Qiang Yang. 2023. 
 Grounding foundation models through federated transfer learning: A general framework. 
 \emph{arXiv preprint arXiv:2311.17431}."
2406.02224,lester2021power,"[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power} Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",The power of scale for parameter-efficient prompt tuning.,The power of scale for parameter-efficient prompt tuning.,,"[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power} Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. 
 The power of scale for parameter-efficient prompt tuning. 
 \emph{arXiv preprint arXiv:2104.08691}."
2406.02224,li2019fedmd,[{Li and Wang(2019)}]{li2019fedmd} Daliang Li and Junpu Wang. 2019.,Fedmd: Heterogenous federated learning via model distillation.,Fedmd: Heterogenous federated learning via model distillation.,,"[{Li and Wang(2019)}]{li2019fedmd} Daliang Li and Junpu Wang. 2019. 
 Fedmd: Heterogenous federated learning via model distillation. 
 \emph{arXiv preprint arXiv:1910.03581}."
2406.02224,li2021prefix,[{Li and Liang(2021)}]{li2021prefix} Xiang~Lisa Li and Percy Liang. 2021.,Prefix-tuning: Optimizing continuous prompts for generation.,Prefix-tuning: Optimizing continuous prompts for generation.,,"[{Li and Liang(2021)}]{li2021prefix} Xiang~Lisa Li and Percy Liang. 2021. 
 Prefix-tuning: Optimizing continuous prompts for generation. 
 \emph{arXiv preprint arXiv:2101.00190}."
2406.02224,liu2022completely,"[{Liu et~al.(2022)Liu, Yang, Cai, Ding, and Lu}]{liu2022completely} Chang Liu, Yuwen Yang, Xun Cai, Yue Ding, and Hongtao Lu. 2022.",Completely heterogeneous federated learning.,Completely heterogeneous federated learning.,,"[{Liu et~al.(2022)Liu, Yang, Cai, Ding, and Lu}]{liu2022completely} Chang Liu, Yuwen Yang, Xun Cai, Yue Ding, and Hongtao Lu. 2022. 
 Completely heterogeneous federated learning. 
 \emph{arXiv preprint arXiv:2210.15865}."
2406.02224,scao2022bloom,"[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{scao2022bloom} Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2022.",Bloom: A 176b-parameter open-access multilingual language model.,Bloom: A 176b-parameter open-access multilingual language model.,,"[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{scao2022bloom} Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2022. 
 Bloom: A 176b-parameter open-access multilingual language model. 
 \emph{arXiv preprint arXiv:2211.05100}."
2406.02224,talmor2018commonsenseqa,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.",Commonsenseqa: A question answering challenge targeting commonsense knowledge.,Commonsenseqa: A question answering challenge targeting commonsense knowledge.,,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. 
 Commonsenseqa: A question answering challenge targeting commonsense knowledge. 
 \emph{arXiv preprint arXiv:1811.00937}."
2406.02224,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.02224,wan2024knowledge,"[{Wan et~al.(2024)Wan, Huang, Cai, Quan, Bi, and Shi}]{wan2024knowledge} Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. 2024.",Knowledge fusion of large language models.,Knowledge fusion of large language models.,,"[{Wan et~al.(2024)Wan, Huang, Cai, Quan, Bi, and Shi}]{wan2024knowledge} Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. 2024. 
 Knowledge fusion of large language models. 
 \emph{arXiv preprint arXiv:2401.10491}."
2406.02224,wang2022benchmarking,"[{Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap et~al.}]{wang2022benchmarking} Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva Naik, David Stap, et~al. 2022.","Benchmarking generalization via in-context instructions on 1,600+ language tasks.","Benchmarking generalization via in-context instructions on 1,600+ language tasks.",,"[{Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei, Arunkumar, Ashok, Dhanasekaran, Naik, Stap et~al.}]{wang2022benchmarking} Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva Naik, David Stap, et~al. 2022. 
 Benchmarking generalization via in-context instructions on 1,600+ language tasks. 
 \emph{arXiv preprint arXiv:2204.07705}, 2."
2406.02224,xia2023sheared,"[{Xia et~al.(2023)Xia, Gao, Zeng, and Chen}]{xia2023sheared} Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023.",Sheared llama: Accelerating language model pre-training via structured pruning.,Sheared llama: Accelerating language model pre-training via structured pruning.,,"[{Xia et~al.(2023)Xia, Gao, Zeng, and Chen}]{xia2023sheared} Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. 
 Sheared llama: Accelerating language model pre-training via structured pruning. 
 \emph{arXiv preprint arXiv:2310.06694}."
2406.02224,yi2023fedlora,"[{Yi et~al.(2023)Yi, Yu, Wang, and Liu}]{yi2023fedlora} Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023.",Fedlora: Model-heterogeneous personalized federated learning with lora tuning.,Fedlora: Model-heterogeneous personalized federated learning with lora tuning.,,"[{Yi et~al.(2023)Yi, Yu, Wang, and Liu}]{yi2023fedlora} Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023. 
 Fedlora: Model-heterogeneous personalized federated learning with lora tuning. 
 \emph{arXiv preprint arXiv:2310.13283}."
2406.02224,zhang2022opt,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2406.02224,zhang2022federated,"[{Zhang et~al.(2022{\natexlab{b}})Zhang, Yang, Dai, Qu, and Xu}]{zhang2022federated} Zhuo Zhang, Yuanhang Yang, Yong Dai, Lizhen Qu, and Zenglin Xu. 2022{\natexlab{b}}.",When federated learning meets pre-trained language models' parameter-efficient tuning methods.,When federated learning meets pre-trained language models' parameter-efficient tuning methods.,,"[{Zhang et~al.(2022{\natexlab{b}})Zhang, Yang, Dai, Qu, and Xu}]{zhang2022federated} Zhuo Zhang, Yuanhang Yang, Yong Dai, Lizhen Qu, and Zenglin Xu. 2022{\natexlab{b}}. 
 When federated learning meets pre-trained language models' parameter-efficient tuning methods. 
 \emph{arXiv preprint arXiv:2212.10025}."
2406.02224,zhao2022reduce,"[{Zhao et~al.(2022)Zhao, Du, Li, Li, and Liu}]{zhao2022reduce} Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. 2022.",Reduce communication costs and preserve privacy: Prompt tuning method in federated learning.,Reduce communication costs and preserve privacy: Prompt tuning method in federated learning.,,"[{Zhao et~al.(2022)Zhao, Du, Li, Li, and Liu}]{zhao2022reduce} Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. 2022. 
 Reduce communication costs and preserve privacy: Prompt tuning method in federated learning. 
 \emph{arXiv preprint arXiv:2208.12268}."
2406.02876,chen2023off,"[{Chen et~al.(2023)Chen, Ma, Zhang, Wei, and Chang}]{chen2023off} Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang. 2023.",On the off-target problem of zero-shot multilingual neural machine   translation.,On the off-target problem of zero-shot multilingual neural machine   translation.,,"[{Chen et~al.(2023)Chen, Ma, Zhang, Wei, and Chang}]{chen2023off} Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang. 2023. 
 On the off-target problem of zero-shot multilingual neural machine   translation. 
 \emph{arXiv preprint arXiv:2305.10930}."
2406.02876,elnokrashy2022language,"[{ElNokrashy et~al.(2022)ElNokrashy, Hendy, Maher, Afify, and   Awadalla}]{elnokrashy2022language} Muhammad ElNokrashy, Amr Hendy, Mohamed Maher, Mohamed Afify, and Hany~Hassan   Awadalla. 2022.",Language tokens: A frustratingly simple approach improves zero-shot   performance of multilingual translation.,Language tokens: A frustratingly simple approach improves zero-shot   performance of multilingual translation.,,"[{ElNokrashy et~al.(2022)ElNokrashy, Hendy, Maher, Afify, and   Awadalla}]{elnokrashy2022language} Muhammad ElNokrashy, Amr Hendy, Mohamed Maher, Mohamed Afify, and Hany~Hassan   Awadalla. 2022. 
 Language tokens: A frustratingly simple approach improves zero-shot   performance of multilingual translation. 
 \emph{arXiv preprint arXiv:2208.05852}."
2406.02876,gao2023improving,"[{Gao et~al.(2023)Gao, Zhang, He, Wu, and Wang}]{gao2023improving} Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, and Haifeng Wang. 2023.",Improving zero-shot multilingual neural machine translation by   leveraging cross-lingual consistency regularization.,Improving zero-shot multilingual neural machine translation by   leveraging cross-lingual consistency regularization.,,"[{Gao et~al.(2023)Gao, Zhang, He, Wu, and Wang}]{gao2023improving} Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, and Haifeng Wang. 2023. 
 Improving zero-shot multilingual neural machine translation by   leveraging cross-lingual consistency regularization. 
 \emph{arXiv preprint arXiv:2305.07310}."
2406.02876,liang2023unified,"[{Liang et~al.(2023)Liang, Meng, Xu, Wang, Chen, and   Zhou}]{liang2023unified} Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, and Jie Zhou.   2023.",Unified model learning for various neural machine translation.,Unified model learning for various neural machine translation.,,"[{Liang et~al.(2023)Liang, Meng, Xu, Wang, Chen, and   Zhou}]{liang2023unified} Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, and Jie Zhou.   2023. 
 Unified model learning for various neural machine translation. 
 \emph{arXiv preprint arXiv:2305.02777}."
2406.02876,liu2023branchnorm,"[{Liu et~al.(2023)Liu, Zeng, Meng, and Zhou}]{liu2023branchnorm} Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023.",Branchnorm: Robustly scaling extremely deep transformers.,Branchnorm: Robustly scaling extremely deep transformers.,,"[{Liu et~al.(2023)Liu, Zeng, Meng, and Zhou}]{liu2023branchnorm} Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023. 
 Branchnorm: Robustly scaling extremely deep transformers. 
 \emph{arXiv preprint arXiv:2305.02790}."
2406.02876,mao2023exploring,"[{Mao et~al.(2023)Mao, Dabre, Liu, Song, Chu, and   Kurohashi}]{mao2023exploring} Zhuoyuan Mao, Raj Dabre, Qianying Liu, Haiyue Song, Chenhui Chu, and Sadao   Kurohashi. 2023.",Exploring the impact of layer normalization for zero-shot neural   machine translation.,Exploring the impact of layer normalization for zero-shot neural   machine translation.,,"[{Mao et~al.(2023)Mao, Dabre, Liu, Song, Chu, and   Kurohashi}]{mao2023exploring} Zhuoyuan Mao, Raj Dabre, Qianying Liu, Haiyue Song, Chenhui Chu, and Sadao   Kurohashi. 2023. 
 Exploring the impact of layer normalization for zero-shot neural   machine translation. 
 \emph{arXiv preprint arXiv:2305.09312}."
2406.02876,qu2022adapting,[{Qu and Watanabe(2022)}]{qu2022adapting} Zhi Qu and Taro Watanabe. 2022.,Adapting to non-centered languages for zero-shot multilingual   translation.,Adapting to non-centered languages for zero-shot multilingual   translation.,,"[{Qu and Watanabe(2022)}]{qu2022adapting} Zhi Qu and Taro Watanabe. 2022. 
 Adapting to non-centered languages for zero-shot multilingual   translation. 
 \emph{arXiv preprint arXiv:2209.04138}."
2406.02876,wang2022deepnet,"[{Wang et~al.(2022{\natexlab{a}})Wang, Ma, Dong, Huang, Zhang, and   Wei}]{wang2022deepnet} Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.   2022{\natexlab{a}}.","Deepnet: Scaling transformers to 1,000 layers.","Deepnet: Scaling transformers to 1,000 layers.",,"[{Wang et~al.(2022{\natexlab{a}})Wang, Ma, Dong, Huang, Zhang, and   Wei}]{wang2022deepnet} Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.   2022{\natexlab{a}}. 
 Deepnet: Scaling transformers to 1,000 layers. 
 \emph{arXiv preprint arXiv:2203.00555}."
2406.02876,wang2022clidsum,"[{Wang et~al.(2022{\natexlab{b}})Wang, Meng, Lu, Zheng, Li, Qu, and   Zhou}]{wang2022clidsum} Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, and Jie   Zhou. 2022{\natexlab{b}}.",Clidsum: A benchmark dataset for cross-lingual dialogue   summarization.,Clidsum: A benchmark dataset for cross-lingual dialogue   summarization.,,"[{Wang et~al.(2022{\natexlab{b}})Wang, Meng, Lu, Zheng, Li, Qu, and   Zhou}]{wang2022clidsum} Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, and Jie   Zhou. 2022{\natexlab{b}}. 
 Clidsum: A benchmark dataset for cross-lingual dialogue   summarization. 
 \emph{arXiv preprint arXiv:2202.05599}."
2406.02876,wang2022understanding,"[{Wang et~al.(2022{\natexlab{c}})Wang, Jiao, Wang, Tu, and   Lyu}]{wang2022understanding} Wenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng Tu, and Michael~R Lyu.   2022{\natexlab{c}}.",Understanding and mitigating the uncertainty in zero-shot   translation.,Understanding and mitigating the uncertainty in zero-shot   translation.,,"[{Wang et~al.(2022{\natexlab{c}})Wang, Jiao, Wang, Tu, and   Lyu}]{wang2022understanding} Wenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng Tu, and Michael~R Lyu.   2022{\natexlab{c}}. 
 Understanding and mitigating the uncertainty in zero-shot   translation. 
 \emph{arXiv preprint arXiv:2205.10068}."
2406.02876,zan2023unlikelihood,"[{Zan et~al.(2023)Zan, Ding, Shen, Lei, Zhan, Liu, and   Tao}]{zan2023unlikelihood} Changtong Zan, Liang Ding, Li~Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, and   Dacheng Tao. 2023.",Unlikelihood tuning on negative samples amazingly improves zero-shot   translation.,Unlikelihood tuning on negative samples amazingly improves zero-shot   translation.,,"[{Zan et~al.(2023)Zan, Ding, Shen, Lei, Zhan, Liu, and   Tao}]{zan2023unlikelihood} Changtong Zan, Liang Ding, Li~Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, and   Dacheng Tao. 2023. 
 Unlikelihood tuning on negative samples amazingly improves zero-shot   translation. 
 \emph{arXiv preprint arXiv:2309.16599}."
2406.02876,zhang2021competence,"[{Zhang et~al.(2021)Zhang, Meng, Tong, and Zhou}]{zhang2021competence} Mingliang Zhang, Fandong Meng, Yunhai Tong, and Jie Zhou. 2021.",Competence-based curriculum learning for multilingual machine   translation.,Competence-based curriculum learning for multilingual machine   translation.,,"[{Zhang et~al.(2021)Zhang, Meng, Tong, and Zhou}]{zhang2021competence} Mingliang Zhang, Fandong Meng, Yunhai Tong, and Jie Zhou. 2021. 
 Competence-based curriculum learning for multilingual machine   translation. 
 \emph{arXiv preprint arXiv:2109.04002}."
2406.03872,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.03872,ardila2019common,"[{Ardila et~al.(2019)Ardila, Branson, Davis, Henretty, Kohler, Meyer,   Morais, Saunders, Tyers, and Weber}]{ardila2019common} Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,   Josh Meyer, Reuben Morais, Lindsay Saunders, Francis~M Tyers, and Gregor   Weber. 2019.",Common voice: A massively-multilingual speech corpus.,Common voice: A massively-multilingual speech corpus.,,"[{Ardila et~al.(2019)Ardila, Branson, Davis, Henretty, Kohler, Meyer,   Morais, Saunders, Tyers, and Weber}]{ardila2019common} Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,   Josh Meyer, Reuben Morais, Lindsay Saunders, Francis~M Tyers, and Gregor   Weber. 2019. 
 Common voice: A massively-multilingual speech corpus. 
 \emph{arXiv preprint arXiv:1912.06670}."
2406.03872,chen2021gigaspeech,"[{Chen et~al.(2021)Chen, Chai, Wang, Du, Zhang, Weng, Su, Povey, Trmal,   Zhang et~al.}]{chen2021gigaspeech} Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng,   Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et~al. 2021.","Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of   transcribed audio.","Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of   transcribed audio.",,"[{Chen et~al.(2021)Chen, Chai, Wang, Du, Zhang, Weng, Su, Povey, Trmal,   Zhang et~al.}]{chen2021gigaspeech} Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng,   Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et~al. 2021. 
 Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of   transcribed audio. 
 \emph{arXiv preprint arXiv:2106.06909}."
2406.03872,chen2023lauragpt,"[{Chen et~al.(2023)Chen, Chu, Gao, Li, Hu, Zhou, Xu, Ma, Wang, Zheng   et~al.}]{chen2023lauragpt} Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu,   Ziyang Ma, Wen Wang, Siqi Zheng, et~al. 2023.","Lauragpt: Listen, attend, understand, and regenerate audio with gpt.","Lauragpt: Listen, attend, understand, and regenerate audio with gpt.",,"[{Chen et~al.(2023)Chen, Chu, Gao, Li, Hu, Zhou, Xu, Ma, Wang, Zheng   et~al.}]{chen2023lauragpt} Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu,   Ziyang Ma, Wen Wang, Siqi Zheng, et~al. 2023. 
 Lauragpt: Listen, attend, understand, and regenerate audio with gpt. 
 \emph{arXiv preprint arXiv:2310.04673}."
2406.03872,chung2022h,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,   Dehghani, Brahma et~al.}]{chung2022h} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.","H. chi, jeff dean, jacob devlin, adam roberts, denny zhou, quoc v.   le, and jason wei. 2022. scaling instruction-finetuned language models.","H. chi, jeff dean, jacob devlin, adam roberts, denny zhou, quoc v.   le, and jason wei. 2022. scaling instruction-finetuned language models.",,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,   Dehghani, Brahma et~al.}]{chung2022h} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 H. chi, jeff dean, jacob devlin, adam roberts, denny zhou, quoc v.   le, and jason wei. 2022. scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2406.03872,dong2024interlm,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang,   Duan, Cao, Zhang, Li, Yan, Gao, Zhang, Li, Li, Chen, He, Zhang, Qiao, Lin,   and Wang}]{dong2024interlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin   Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang   Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,   Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang. 2024.",Internlm-xcomposer2: Mastering free-form text-image composition and   comprehension in vision-language large model.,Internlm-xcomposer2: Mastering free-form text-image composition and   comprehension in vision-language large model.,,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang,   Duan, Cao, Zhang, Li, Yan, Gao, Zhang, Li, Li, Chen, He, Zhang, Qiao, Lin,   and Wang}]{dong2024interlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin   Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang   Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,   Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang. 2024. 
 Internlm-xcomposer2: Mastering free-form text-image composition and   comprehension in vision-language large model. 
 \emph{arXiv preprint arXiv:2401.16420}."
2406.03872,hu2024wavllm,"[{Hu et~al.(2024)Hu, Zhou, Liu, Chen, Hao, Pan, Liu, Li, Sivasankaran,   Liu et~al.}]{hu2024wavllm} Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying   Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et~al. 2024.",Wavllm: Towards robust and adaptive speech large language model.,Wavllm: Towards robust and adaptive speech large language model.,,"[{Hu et~al.(2024)Hu, Zhou, Liu, Chen, Hao, Pan, Liu, Li, Sivasankaran,   Liu et~al.}]{hu2024wavllm} Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying   Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et~al. 2024. 
 Wavllm: Towards robust and adaptive speech large language model. 
 \emph{arXiv preprint arXiv:2404.00656}."
2406.03872,lian2024merbench,"[{Lian et~al.(2024)Lian, Sun, Ren, Gu, Sun, Chen, Liu, and   Tao}]{lian2024merbench} Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and   Jianhua Tao. 2024.",Merbench: A unified evaluation benchmark for multimodal emotion   recognition.,Merbench: A unified evaluation benchmark for multimodal emotion   recognition.,,"[{Lian et~al.(2024)Lian, Sun, Ren, Gu, Sun, Chen, Liu, and   Tao}]{lian2024merbench} Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and   Jianhua Tao. 2024. 
 Merbench: A unified evaluation benchmark for multimodal emotion   recognition. 
 \emph{arXiv preprint arXiv:2401.03429}."
2406.03872,lin2024advancing,"[{Lin et~al.(2024)Lin, Chiang, and Lee}]{lin2024advancing} Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee. 2024.",Advancing large language models to capture varied speaking styles and   respond properly in spoken conversations.,Advancing large language models to capture varied speaking styles and   respond properly in spoken conversations.,,"[{Lin et~al.(2024)Lin, Chiang, and Lee}]{lin2024advancing} Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee. 2024. 
 Advancing large language models to capture varied speaking styles and   respond properly in spoken conversations. 
 \emph{arXiv preprint arXiv:2402.12786}."
2406.03872,poria2018meld,"[{Poria et~al.(2018)Poria, Hazarika, Majumder, Naik, Cambria, and   Mihalcea}]{poria2018meld} Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik   Cambria, and Rada Mihalcea. 2018.",Meld: A multimodal multi-party dataset for emotion recognition in   conversations.,Meld: A multimodal multi-party dataset for emotion recognition in   conversations.,,"[{Poria et~al.(2018)Poria, Hazarika, Majumder, Naik, Cambria, and   Mihalcea}]{poria2018meld} Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik   Cambria, and Rada Mihalcea. 2018. 
 Meld: A multimodal multi-party dataset for emotion recognition in   conversations. 
 \emph{arXiv preprint arXiv:1810.02508}."
2406.03872,radford2022robust,"[{Radford et~al.(2022)Radford, Kim, Xu, Brockman, McLeavey, and   Sutskever}]{radford2022robust} Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and   Ilya Sutskever. 2022.",Robust speech recognition via large-scale weak supervision. arxiv.,Robust speech recognition via large-scale weak supervision. arxiv.,,"[{Radford et~al.(2022)Radford, Kim, Xu, Brockman, McLeavey, and   Sutskever}]{radford2022robust} Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and   Ilya Sutskever. 2022. 
 Robust speech recognition via large-scale weak supervision. arxiv. 
 \emph{arXiv preprint arXiv:2212.04356}."
2406.03872,rubenstein2023audiopalm,"[{Rubenstein et~al.(2023)Rubenstein, Asawaroengchai, Nguyen, Bapna,   Borsos, Quitry, Chen, Badawy, Han, Kharitonov   et~al.}]{rubenstein2023audiopalm} Paul~K Rubenstein, Chulayuth Asawaroengchai, Duc~Dung Nguyen, Ankur Bapna,   Zal{\'a}n Borsos, F{\'e}lix de~Chaumont Quitry, Peter Chen, Dalia~El Badawy,   Wei Han, Eugene Kharitonov, et~al. 2023.",Audiopalm: A large language model that can speak and listen.,Audiopalm: A large language model that can speak and listen.,,"[{Rubenstein et~al.(2023)Rubenstein, Asawaroengchai, Nguyen, Bapna,   Borsos, Quitry, Chen, Badawy, Han, Kharitonov   et~al.}]{rubenstein2023audiopalm} Paul~K Rubenstein, Chulayuth Asawaroengchai, Duc~Dung Nguyen, Ankur Bapna,   Zal{\'a}n Borsos, F{\'e}lix de~Chaumont Quitry, Peter Chen, Dalia~El Badawy,   Wei Han, Eugene Kharitonov, et~al. 2023. 
 Audiopalm: A large language model that can speak and listen. 
 \emph{arXiv preprint arXiv:2306.12925}."
2406.03872,tang2023salmonn,"[{Tang et~al.(2023)Tang, Yu, Sun, Chen, Tan, Li, Lu, Ma, and   Zhang}]{tang2023salmonn} Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu~Lu,   Zejun Ma, and Chao Zhang. 2023.",Salmonn: Towards generic hearing abilities for large language models.,Salmonn: Towards generic hearing abilities for large language models.,,"[{Tang et~al.(2023)Tang, Yu, Sun, Chen, Tan, Li, Lu, Ma, and   Zhang}]{tang2023salmonn} Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu~Lu,   Zejun Ma, and Chao Zhang. 2023. 
 Salmonn: Towards generic hearing abilities for large language models. 
 \emph{arXiv preprint arXiv:2310.13289}."
2406.03872,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,   Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,   Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.03872,wang2023viola,"[{Wang et~al.(2023{\natexlab{b}})Wang, Zhou, Zhang, Wu, Liu, Gaur,   Chen, Li, and Wei}]{wang2023viola} Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu~Wu, Shujie Liu, Yashesh Gaur, Zhuo   Chen, Jinyu Li, and Furu Wei. 2023{\natexlab{b}}.","Viola: Unified codec language models for speech recognition,   synthesis, and translation.","Viola: Unified codec language models for speech recognition,   synthesis, and translation.",,"[{Wang et~al.(2023{\natexlab{b}})Wang, Zhou, Zhang, Wu, Liu, Gaur,   Chen, Li, and Wei}]{wang2023viola} Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu~Wu, Shujie Liu, Yashesh Gaur, Zhuo   Chen, Jinyu Li, and Furu Wei. 2023{\natexlab{b}}. 
 Viola: Unified codec language models for speech recognition,   synthesis, and translation. 
 \emph{arXiv preprint arXiv:2305.16107}."
2406.03872,xue2023chat,"[{Xue et~al.(2023)Xue, Liang, Mu, Zhang, Chen, and Xie}]{xue2023chat} Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Qian Chen, and Lei Xie.   2023.",E-chat: Emotion-sensitive spoken dialogue system with large language   models.,E-chat: Emotion-sensitive spoken dialogue system with large language   models.,,"[{Xue et~al.(2023)Xue, Liang, Mu, Zhang, Chen, and Xie}]{xue2023chat} Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Qian Chen, and Lei Xie.   2023. 
 E-chat: Emotion-sensitive spoken dialogue system with large language   models. 
 \emph{arXiv preprint arXiv:2401.00475}."
2406.03872,zhang2023speechgpt,"[{Zhang et~al.(2023)Zhang, Li, Zhang, Zhan, Wang, Zhou, and   Qiu}]{zhang2023speechgpt} Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and   Xipeng Qiu. 2023.",Speechgpt: Empowering large language models with intrinsic   cross-modal conversational abilities.,Speechgpt: Empowering large language models with intrinsic   cross-modal conversational abilities.,,"[{Zhang et~al.(2023)Zhang, Li, Zhang, Zhan, Wang, Zhou, and   Qiu}]{zhang2023speechgpt} Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and   Xipeng Qiu. 2023. 
 Speechgpt: Empowering large language models with intrinsic   cross-modal conversational abilities. 
 \emph{arXiv preprint arXiv:2305.11000}."
2406.04165,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, et~al.",{OPT}: Open pre-trained transformer language models.,{OPT}: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, et~al. 
 {OPT}: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2406.04274,bai2022constitutional,"[Bai et~al., 2022]{bai2022constitutional} Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A.,   Goldie, A., Mirhoseini, A., McKinnon, C., et~al. (2022).",Constitutional ai: Harmlessness from ai feedback.,Constitutional ai: Harmlessness from ai feedback.,,"[Bai et~al., 2022]{bai2022constitutional} Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A.,   Goldie, A., Mirhoseini, A., McKinnon, C., et~al. (2022). 
 Constitutional ai: Harmlessness from ai feedback. 
 {\em arXiv preprint arXiv:2212.08073}."
2406.04274,baker2019emergent,"[Baker et~al., 2019]{baker2019emergent} Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and   Mordatch, I. (2019).",Emergent tool use from multi-agent autocurricula.,Emergent tool use from multi-agent autocurricula.,,"[Baker et~al., 2019]{baker2019emergent} Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and   Mordatch, I. (2019). 
 Emergent tool use from multi-agent autocurricula. 
 {\em arXiv preprint arXiv:1909.07528}."
2406.04274,bansal2017emergent,"[Bansal et~al., 2017]{bansal2017emergent} Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2017).",Emergent complexity via multi-agent competition.,Emergent complexity via multi-agent competition.,,"[Bansal et~al., 2017]{bansal2017emergent} Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2017). 
 Emergent complexity via multi-agent competition. 
 {\em arXiv preprint arXiv:1710.03748}."
2406.04274,calandriello2024human,"[Calandriello et~al., 2024]{calandriello2024human} Calandriello, D., Guo, D., Munos, R., Rowland, M., Tang, Y., Pires, B.~A.,   Richemond, P.~H., Lan, C.~L., Valko, M., Liu, T., et~al. (2024).",Human alignment of large language models through online preference   optimisation.,Human alignment of large language models through online preference   optimisation.,,"[Calandriello et~al., 2024]{calandriello2024human} Calandriello, D., Guo, D., Munos, R., Rowland, M., Tang, Y., Pires, B.~A.,   Richemond, P.~H., Lan, C.~L., Valko, M., Liu, T., et~al. (2024). 
 Human alignment of large language models through online preference   optimisation. 
 {\em arXiv preprint arXiv:2403.08635}."
2406.04274,chen2024self,"[Chen et~al., 2024]{chen2024self} Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. (2024).",Self-play fine-tuning converts weak language models to strong   language models.,Self-play fine-tuning converts weak language models to strong   language models.,,"[Chen et~al., 2024]{chen2024self} Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. (2024). 
 Self-play fine-tuning converts weak language models to strong   language models. 
 {\em arXiv preprint arXiv:2401.01335}."
2406.04274,dwaracherla2024efficient,"[Dwaracherla et~al., 2024]{dwaracherla2024efficient} Dwaracherla, V., Asghari, S.~M., Hao, B., and Van~Roy, B. (2024).",Efficient exploration for llms.,Efficient exploration for llms.,,"[Dwaracherla et~al., 2024]{dwaracherla2024efficient} Dwaracherla, V., Asghari, S.~M., Hao, B., and Van~Roy, B. (2024). 
 Efficient exploration for llms. 
 {\em arXiv preprint arXiv:2402.00396}."
2406.04274,gao2024rebel,"[Gao et~al., 2024]{gao2024rebel} Gao, Z., Chang, J.~D., Zhan, W., Oertell, O., Swamy, G., Brantley, K.,   Joachims, T., Bagnell, J.~A., Lee, J.~D., and Sun, W. (2024).",Rebel: Reinforcement learning via regressing relative rewards.,Rebel: Reinforcement learning via regressing relative rewards.,,"[Gao et~al., 2024]{gao2024rebel} Gao, Z., Chang, J.~D., Zhan, W., Oertell, O., Swamy, G., Brantley, K.,   Joachims, T., Bagnell, J.~A., Lee, J.~D., and Sun, W. (2024). 
 Rebel: Reinforcement learning via regressing relative rewards. 
 {\em arXiv preprint arXiv:2404.16767}."
2406.04274,hejna2023contrastive,"[Hejna et~al., 2023]{hejna2023contrastive} Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W.~B., and   Sadigh, D. (2023).",Contrastive prefence learning: Learning from human feedback without   rl.,Contrastive prefence learning: Learning from human feedback without   rl.,,"[Hejna et~al., 2023]{hejna2023contrastive} Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W.~B., and   Sadigh, D. (2023). 
 Contrastive prefence learning: Learning from human feedback without   rl. 
 {\em arXiv preprint arXiv:2310.13639}."
2406.04274,levine2020offline,"[Levine et~al., 2020]{levine2020offline} Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).","Offline reinforcement learning: Tutorial, review, and perspectives on   open problems.","Offline reinforcement learning: Tutorial, review, and perspectives on   open problems.",,"[Levine et~al., 2020]{levine2020offline} Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). 
 Offline reinforcement learning: Tutorial, review, and perspectives on   open problems. 
 {\em arXiv preprint arXiv:2005.01643}."
2406.04274,li2024q,"[Li et~al., 2024b]{li2024q} Li, K., Jelassi, S., Zhang, H., Kakade, S., Wattenberg, M., and Brandfonbrener,   D. (2024b).",Q-probe: A lightweight approach to reward maximization for language   models.,Q-probe: A lightweight approach to reward maximization for language   models.,,"[Li et~al., 2024b]{li2024q} Li, K., Jelassi, S., Zhang, H., Kakade, S., Wattenberg, M., and Brandfonbrener,   D. (2024b). 
 Q-probe: A lightweight approach to reward maximization for language   models. 
 {\em arXiv preprint arXiv:2402.14688}."
2406.04274,li2023textbooks,"[Li et~al., 2023]{li2023textbooks} Li, Y., Bubeck, S., Eldan, R., Del~Giorno, A., Gunasekar, S., and Lee, Y.~T.   (2023).",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[Li et~al., 2023]{li2023textbooks} Li, Y., Bubeck, S., Eldan, R., Del~Giorno, A., Gunasekar, S., and Lee, Y.~T.   (2023). 
 Textbooks are all you need ii: phi-1.5 technical report. 
 {\em arXiv preprint arXiv:2309.05463}."
2406.04274,mehta2023kernelized,"[Mehta et~al., 2023]{mehta2023kernelized} Mehta, V., Neopane, O., Das, V., Lin, S., Schneider, J., and Neiswanger, W.   (2023).",Kernelized offline contextual dueling bandits.,Kernelized offline contextual dueling bandits.,,"[Mehta et~al., 2023]{mehta2023kernelized} Mehta, V., Neopane, O., Das, V., Lin, S., Schneider, J., and Neiswanger, W.   (2023). 
 Kernelized offline contextual dueling bandits. 
 {\em arXiv preprint arXiv:2307.11288}."
2406.04274,miao2024mitigating,"[Miao et~al., 2024]{miao2024mitigating} Miao, Y., Zhang, S., Ding, L., Bao, R., Zhang, L., and Tao, D. (2024).",Mitigating reward hacking via information-theoretic reward modeling.,Mitigating reward hacking via information-theoretic reward modeling.,,"[Miao et~al., 2024]{miao2024mitigating} Miao, Y., Zhang, S., Ding, L., Bao, R., Zhang, L., and Tao, D. (2024). 
 Mitigating reward hacking via information-theoretic reward modeling. 
 {\em arXiv preprint arXiv:2402.09345}."
2406.04274,mishra2021cross,"[Mishra et~al., 2021]{mishra2021cross} Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2021).",Cross-task generalization via natural language crowdsourcing   instructions.,Cross-task generalization via natural language crowdsourcing   instructions.,,"[Mishra et~al., 2021]{mishra2021cross} Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2021). 
 Cross-task generalization via natural language crowdsourcing   instructions. 
 {\em arXiv preprint arXiv:2104.08773}."
2406.04274,nguyen2021sample,"[Nguyen-Tang et~al., 2021]{nguyen2021sample} Nguyen-Tang, T., Gupta, S., Tran-The, H., and Venkatesh, S. (2021).",Sample complexity of offline reinforcement learning with deep relu   networks.,Sample complexity of offline reinforcement learning with deep relu   networks.,,"[Nguyen-Tang et~al., 2021]{nguyen2021sample} Nguyen-Tang, T., Gupta, S., Tran-The, H., and Venkatesh, S. (2021). 
 Sample complexity of offline reinforcement learning with deep relu   networks. 
 {\em arXiv preprint arXiv:2103.06671}."
2406.04274,pacchiano2021dueling,"[Pacchiano et~al., 2021]{pacchiano2021dueling} Pacchiano, A., Saha, A., and Lee, J. (2021).",Dueling rl: reinforcement learning with trajectory preferences.,Dueling rl: reinforcement learning with trajectory preferences.,,"[Pacchiano et~al., 2021]{pacchiano2021dueling} Pacchiano, A., Saha, A., and Lee, J. (2021). 
 Dueling rl: reinforcement learning with trajectory preferences. 
 {\em arXiv preprint arXiv:2111.04850}."
2406.04274,pan2022effects,"[Pan et~al., 2022]{pan2022effects} Pan, A., Bhatia, K., and Steinhardt, J. (2022).",The effects of reward misspecification: Mapping and mitigating   misaligned models.,The effects of reward misspecification: Mapping and mitigating   misaligned models.,,"[Pan et~al., 2022]{pan2022effects} Pan, A., Bhatia, K., and Steinhardt, J. (2022). 
 The effects of reward misspecification: Mapping and mitigating   misaligned models. 
 {\em arXiv preprint arXiv:2201.03544}."
2406.04274,rame2024warm,"[Ram{\'e} et~al., 2024]{rame2024warm} Ram{\'e}, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem,   O., and Ferret, J. (2024).",Warm: On the benefits of weight averaged reward models.,Warm: On the benefits of weight averaged reward models.,,"[Ram{\'e} et~al., 2024]{rame2024warm} Ram{\'e}, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem,   O., and Ferret, J. (2024). 
 Warm: On the benefits of weight averaged reward models. 
 {\em arXiv preprint arXiv:2401.12187}."
2406.04274,rosset2024direct,"[Rosset et~al., 2024]{rosset2024direct} Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T.   (2024).",Direct nash optimization: Teaching language models to self-improve   with general preferences.,Direct nash optimization: Teaching language models to self-improve   with general preferences.,,"[Rosset et~al., 2024]{rosset2024direct} Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T.   (2024). 
 Direct nash optimization: Teaching language models to self-improve   with general preferences. 
 {\em arXiv preprint arXiv:2404.03715}."
2406.04274,schulman2017proximal,"[Schulman et~al., 2017]{schulman2017proximal} Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[Schulman et~al., 2017]{schulman2017proximal} Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). 
 Proximal policy optimization algorithms. 
 {\em arXiv preprint arXiv:1707.06347}."
2406.04274,shen2024principled,"[Shen et~al., 2024]{shen2024principled} Shen, H., Yang, Z., and Chen, T. (2024).",Principled penalty-based methods for bilevel reinforcement learning   and rlhf.,Principled penalty-based methods for bilevel reinforcement learning   and rlhf.,,"[Shen et~al., 2024]{shen2024principled} Shen, H., Yang, Z., and Chen, T. (2024). 
 Principled penalty-based methods for bilevel reinforcement learning   and rlhf. 
 {\em arXiv preprint arXiv:2402.06886}."
2406.04274,swamy2024minimaximalist,"[Swamy et~al., 2024]{swamy2024minimaximalist} Swamy, G., Dann, C., Kidambi, R., Wu, Z.~S., and Agarwal, A. (2024).",A minimaximalist approach to reinforcement learning from human   feedback.,A minimaximalist approach to reinforcement learning from human   feedback.,,"[Swamy et~al., 2024]{swamy2024minimaximalist} Swamy, G., Dann, C., Kidambi, R., Wu, Z.~S., and Agarwal, A. (2024). 
 A minimaximalist approach to reinforcement learning from human   feedback. 
 {\em arXiv preprint arXiv:2401.04056}."
2406.04274,touvron2023llama,"[Touvron et~al., 2023]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,   Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023).",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al., 2023]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,   Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023). 
 Llama 2: Open foundation and fine-tuned chat models. 
 {\em arXiv preprint arXiv:2307.09288}."
2406.04274,wang2020statistical,"[Wang et~al., 2020]{wang2020statistical} Wang, R., Foster, D.~P., and Kakade, S.~M. (2020).",What are the statistical limits of offline rl with linear function   approximation?,What are the statistical limits of offline rl with linear function   approximation?,,"[Wang et~al., 2020]{wang2020statistical} Wang, R., Foster, D.~P., and Kakade, S.~M. (2020). 
 What are the statistical limits of offline rl with linear function   approximation? 
 {\em arXiv preprint arXiv:2010.11895}."
2406.04274,wang2024transforming,"[Wang et~al., 2024]{wang2024transforming} Wang, Z., Nagpal, C., Berant, J., Eisenstein, J., D'Amour, A., Koyejo, S., and   Veitch, V. (2024).",Transforming and combining rewards for aligning large language   models.,Transforming and combining rewards for aligning large language   models.,,"[Wang et~al., 2024]{wang2024transforming} Wang, Z., Nagpal, C., Berant, J., Eisenstein, J., D'Amour, A., Koyejo, S., and   Veitch, V. (2024). 
 Transforming and combining rewards for aligning large language   models. 
 {\em arXiv preprint arXiv:2402.00742}."
2406.04274,wei2021finetuned,"[Wei et~al., 2021]{wei2021finetuned} Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,   A.~M., and Le, Q.~V. (2021).",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[Wei et~al., 2021]{wei2021finetuned} Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,   A.~M., and Le, Q.~V. (2021). 
 Finetuned language models are zero-shot learners. 
 {\em arXiv preprint arXiv:2109.01652}."
2406.04274,xiong2023gibbs,"[Xiong et~al., 2023]{xiong2023gibbs} Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N., and Zhang, T. (2023).",Gibbs sampling from human feedback: A provable kl-constrained   framework for rlhf.,Gibbs sampling from human feedback: A provable kl-constrained   framework for rlhf.,,"[Xiong et~al., 2023]{xiong2023gibbs} Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N., and Zhang, T. (2023). 
 Gibbs sampling from human feedback: A provable kl-constrained   framework for rlhf. 
 {\em arXiv preprint arXiv:2312.11456}."
2406.04274,xu2024contrastive,"[Xu et~al., 2024]{xu2024contrastive} Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van~Durme, B., Murray, K., and   Kim, Y.~J. (2024).",Contrastive preference optimization: Pushing the boundaries of llm   performance in machine translation.,Contrastive preference optimization: Pushing the boundaries of llm   performance in machine translation.,,"[Xu et~al., 2024]{xu2024contrastive} Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van~Durme, B., Murray, K., and   Kim, Y.~J. (2024). 
 Contrastive preference optimization: Pushing the boundaries of llm   performance in machine translation. 
 {\em arXiv preprint arXiv:2401.08417}."
2406.04274,yin2022near,"[Yin et~al., 2022a]{yin2022near} Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022a).",Near-optimal offline reinforcement learning with linear   representation: Leveraging variance information with pessimism.,Near-optimal offline reinforcement learning with linear   representation: Leveraging variance information with pessimism.,,"[Yin et~al., 2022a]{yin2022near} Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022a). 
 Near-optimal offline reinforcement learning with linear   representation: Leveraging variance information with pessimism. 
 {\em arXiv preprint arXiv:2203.05804}."
2406.04274,yin2022offline,"[Yin et~al., 2022b]{yin2022offline} Yin, M., Wang, M., and Wang, Y.-X. (2022b).",Offline reinforcement learning with differentiable function   approximation is provably efficient.,Offline reinforcement learning with differentiable function   approximation is provably efficient.,,"[Yin et~al., 2022b]{yin2022offline} Yin, M., Wang, M., and Wang, Y.-X. (2022b). 
 Offline reinforcement learning with differentiable function   approximation is provably efficient. 
 {\em arXiv preprint arXiv:2210.00750}."
2406.04274,zhu2023principled,"[Zhu et~al., 2023]{zhu2023principled} Zhu, B., Jiao, J., and Jordan, M.~I. (2023).",Principled reinforcement learning with human feedback from pairwise   or $ k $-wise comparisons.,Principled reinforcement learning with human feedback from pairwise   or $ k $-wise comparisons.,,"[Zhu et~al., 2023]{zhu2023principled} Zhu, B., Jiao, J., and Jordan, M.~I. (2023). 
 Principled reinforcement learning with human feedback from pairwise   or $ k $-wise comparisons. 
 {\em arXiv preprint arXiv:2301.11270}."
2406.06007,bai2023qwen,"[Bai et~al.(2023{\natexlab{a}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.",Qwen-vl: A frontier large vision-language model with versatile abilities.,Qwen-vl: A frontier large vision-language model with versatile abilities.,,"[Bai et~al.(2023{\natexlab{a}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 
 Qwen-vl: A frontier large vision-language model with versatile abilities. 
 \emph{arXiv preprint arXiv:2308.12966}, 2023{\natexlab{a}}."
2406.06007,chen2024halc,"[Chen et~al.(2024)Chen, Zhao, Luo, Yao, Li, and Zhou]{chen2024halc} Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo~Li, and Jiawei Zhou.",Halc: Object hallucination reduction via adaptive focal-contrast decoding.,Halc: Object hallucination reduction via adaptive focal-contrast decoding.,,"[Chen et~al.(2024)Chen, Zhao, Luo, Yao, Li, and Zhou]{chen2024halc} Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo~Li, and Jiawei Zhou. 
 Halc: Object hallucination reduction via adaptive focal-contrast decoding. 
 \emph{arXiv preprint arXiv:2403.00425}, 2024."
2406.06007,cui2023holistic,"[Cui et~al.(2023)Cui, Zhou, Yang, Wu, Zhang, Zou, and Yao]{cui2023holistic} Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao.",Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.,Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.,,"[Cui et~al.(2023)Cui, Zhou, Yang, Wu, Zhang, Zou, and Yao]{cui2023holistic} Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 
 Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. 
 \emph{arXiv preprint arXiv:2311.03287}, 2023."
2406.06007,fu2023mme,"[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng, et~al.]{fu2023mme} Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al.",Mme: A comprehensive evaluation benchmark for multimodal large language models.,Mme: A comprehensive evaluation benchmark for multimodal large language models.,,"[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng, et~al.]{fu2023mme} Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al. 
 Mme: A comprehensive evaluation benchmark for multimodal large language models. 
 \emph{arXiv preprint arXiv:2306.13394}, 2023."
2406.06007,gao2023ophglm,"[Gao et~al.(2023)Gao, Deng, Niu, Rong, Chen, Gong, Zhang, Xiao, Li, Cao, et~al.]{gao2023ophglm} Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, et~al.",Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue.,Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue.,,"[Gao et~al.(2023)Gao, Deng, Niu, Rong, Chen, Gong, Zhang, Xiao, Li, Cao, et~al.]{gao2023ophglm} Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, et~al. 
 Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue. 
 \emph{arXiv preprint arXiv:2306.12174}, 2023."
2406.06007,gunjal2023detecting,"[Gunjal et~al.(2023)Gunjal, Yin, and Bas]{gunjal2023detecting} Anisha Gunjal, Jihan Yin, and Erhan Bas.",Detecting and preventing hallucinations in large vision language models.,Detecting and preventing hallucinations in large vision language models.,,"[Gunjal et~al.(2023)Gunjal, Yin, and Bas]{gunjal2023detecting} Anisha Gunjal, Jihan Yin, and Erhan Bas. 
 Detecting and preventing hallucinations in large vision language models. 
 \emph{arXiv preprint arXiv:2308.06394}, 2023."
2406.06007,he2024meddr,"[He et~al.(2024)He, Nie, Chen, Cai, Wang, Yang, and Chen]{he2024meddr} Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen.",Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning.,Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning.,,"[He et~al.(2024)He, Nie, Chen, Cai, Wang, Yang, and Chen]{he2024meddr} Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. 
 Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. 
 \emph{arXiv preprint arXiv:2404.15127}, 2024."
2406.06007,hosseini2017deceiving,"[Hosseini et~al.(2017)Hosseini, Kannan, Zhang, and Poovendran]{hosseini2017deceiving} Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran.",Deceiving google's perspective api built for detecting toxic comments.,Deceiving google's perspective api built for detecting toxic comments.,,"[Hosseini et~al.(2017)Hosseini, Kannan, Zhang, and Poovendran]{hosseini2017deceiving} Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 
 Deceiving google's perspective api built for detecting toxic comments. 
 \emph{arXiv preprint arXiv:1702.08138}, 2017."
2406.06007,hu2024omnimedvqa,"[Hu et~al.(2024)Hu, Li, Lu, Shao, He, Qiao, and Luo]{hu2024omnimedvqa} Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu~Qiao, and Ping Luo.",Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm.,Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm.,,"[Hu et~al.(2024)Hu, Li, Lu, Shao, He, Qiao, and Luo]{hu2024omnimedvqa} Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu~Qiao, and Ping Luo. 
 Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm. 
 \emph{arXiv preprint arXiv:2402.09181}, 2024."
2406.06007,huang2023catastrophic,"[Huang et~al.(2023)Huang, Gupta, Xia, Li, and Chen]{huang2023catastrophic} Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen.",Catastrophic jailbreak of open-source llms via exploiting generation.,Catastrophic jailbreak of open-source llms via exploiting generation.,,"[Huang et~al.(2023)Huang, Gupta, Xia, Li, and Chen]{huang2023catastrophic} Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 
 Catastrophic jailbreak of open-source llms via exploiting generation. 
 \emph{arXiv preprint arXiv:2310.06987}, 2023."
2406.06007,johnson2019mimic,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.",,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng. 
 Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. 
 \emph{arXiv preprint arXiv:1901.07042}, 2019."
2406.06007,lee2022surgical,"[Lee et~al.(2022)Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{lee2022surgical} Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn.",Surgical fine-tuning improves adaptation to distribution shifts.,Surgical fine-tuning improves adaptation to distribution shifts.,,"[Lee et~al.(2022)Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{lee2022surgical} Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. 
 Surgical fine-tuning improves adaptation to distribution shifts. 
 \emph{arXiv preprint arXiv:2210.11466}, 2022."
2406.06007,li2023seed,"[Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.",Seed-bench: Benchmarking multimodal llms with generative comprehension.,Seed-bench: Benchmarking multimodal llms with generative comprehension.,,"[Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 
 Seed-bench: Benchmarking multimodal llms with generative comprehension. 
 \emph{arXiv preprint arXiv:2307.16125}, 2023{\natexlab{a}}."
2406.06007,li2023comprehensive,"[Li et~al.(2023{\natexlab{d}})Li, Liu, Wang, Liang, Liu, Wang, Cui, Tu, Wang, and Zhou]{li2023comprehensive} Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou.",A comprehensive study of gpt-4v's multimodal capabilities in medical imaging.,A comprehensive study of gpt-4v's multimodal capabilities in medical imaging.,,"[Li et~al.(2023{\natexlab{d}})Li, Liu, Wang, Liang, Liu, Wang, Cui, Tu, Wang, and Zhou]{li2023comprehensive} Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. 
 A comprehensive study of gpt-4v's multimodal capabilities in medical imaging. 
 \emph{arXiv preprint arXiv:2310.20381}, 2023{\natexlab{d}}."
2406.06007,liu2023improved,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023improved} Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.",Improved baselines with visual instruction tuning.,Improved baselines with visual instruction tuning.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023improved} Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee. 
 Improved baselines with visual instruction tuning. 
 \emph{arXiv preprint arXiv:2310.03744}, 2023{\natexlab{a}}."
2406.06007,liu2023visual,"[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023visual} Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.",Visual instruction tuning.,Visual instruction tuning.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023visual} Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 
 Visual instruction tuning. 
 \emph{arXiv preprint arXiv:2304.08485}, 2023{\natexlab{b}}."
2406.06007,lu2024gpt,"[Lu et~al.(2024)Lu, Qian, Zheng, Fan, Gao, Zhang, Shao, Deng, Fu, Huang, et~al.]{lu2024gpt} Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, et~al.","From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities.","From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities.",,"[Lu et~al.(2024)Lu, Qian, Zheng, Fan, Gao, Zhang, Shao, Deng, Fu, Huang, et~al.]{lu2024gpt} Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, et~al. 
 From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities. 
 \emph{arXiv preprint arXiv:2401.15071}, 2024."
2406.06007,lu2023foundational,"[Lu et~al.(2023)Lu, Chen, Williamson, Chen, Ikamura, Gerber, Liang, Le, Ding, Parwani, et~al.]{lu2023foundational} Ming~Y Lu, Bowen Chen, Drew~FK Williamson, Richard~J Chen, Kenji Ikamura, Georg Gerber, Ivy Liang, Long~Phi Le, Tong Ding, Anil~V Parwani, et~al.",A foundational multimodal vision language ai assistant for human pathology.,A foundational multimodal vision language ai assistant for human pathology.,,"[Lu et~al.(2023)Lu, Chen, Williamson, Chen, Ikamura, Gerber, Liang, Le, Ding, Parwani, et~al.]{lu2023foundational} Ming~Y Lu, Bowen Chen, Drew~FK Williamson, Richard~J Chen, Kenji Ikamura, Georg Gerber, Ivy Liang, Long~Phi Le, Tong Ding, Anil~V Parwani, et~al. 
 A foundational multimodal vision language ai assistant for human pathology. 
 \emph{arXiv preprint arXiv:2312.07814}, 2023."
2406.06007,luo2024fairclip,"[Luo et~al.(2024)Luo, Shi, Khan, Afzal, Huang, Yuan, Tian, Song, Kouhana, Elze, et~al.]{luo2024fairclip} Yan Luo, Min Shi, Muhammad~Osama Khan, Muhammad~Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu~Tian, Luo Song, Ava Kouhana, Tobias Elze, et~al.",Fairclip: Harnessing fairness in vision-language learning.,Fairclip: Harnessing fairness in vision-language learning.,,"[Luo et~al.(2024)Luo, Shi, Khan, Afzal, Huang, Yuan, Tian, Song, Kouhana, Elze, et~al.]{luo2024fairclip} Yan Luo, Min Shi, Muhammad~Osama Khan, Muhammad~Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu~Tian, Luo Song, Ava Kouhana, Tobias Elze, et~al. 
 Fairclip: Harnessing fairness in vision-language learning. 
 \emph{arXiv preprint arXiv:2403.19949}, 2024."
2406.06007,mao2023last,"[Mao et~al.(2023)Mao, Deng, Yao, Ye, Kawaguchi, and Zou]{mao2023last} Yuzhen Mao, Zhun Deng, Huaxiu Yao, Ting Ye, Kenji Kawaguchi, and James Zou.",Last-layer fairness fine-tuning is simple and effective for neural networks.,Last-layer fairness fine-tuning is simple and effective for neural networks.,,"[Mao et~al.(2023)Mao, Deng, Yao, Ye, Kawaguchi, and Zou]{mao2023last} Yuzhen Mao, Zhun Deng, Huaxiu Yao, Ting Ye, Kenji Kawaguchi, and James Zou. 
 Last-layer fairness fine-tuning is simple and effective for neural networks. 
 \emph{arXiv preprint arXiv:2304.03935}, 2023."
2406.06007,pi2024mllm,"[Pi et~al.(2024)Pi, Han, Xie, Pan, Lian, Dong, Zhang, and Zhang]{pi2024mllm} Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang.",Mllm-protector: Ensuring mllm's safety without hurting performance.,Mllm-protector: Ensuring mllm's safety without hurting performance.,,"[Pi et~al.(2024)Pi, Han, Xie, Pan, Lian, Dong, Zhang, and Zhang]{pi2024mllm} Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. 
 Mllm-protector: Ensuring mllm's safety without hurting performance. 
 \emph{arXiv preprint arXiv:2401.02906}, 2024."
2406.06007,royer2024multimedeval,"[Royer et~al.(2024)Royer, Menze, and Sekuboyina]{royer2024multimedeval} Corentin Royer, Bjoern Menze, and Anjany Sekuboyina.",Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models.,Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models.,,"[Royer et~al.(2024)Royer, Menze, and Sekuboyina]{royer2024multimedeval} Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. 
 Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. 
 \emph{arXiv preprint arXiv:2402.09262}, 2024."
2406.06007,sun2024trustllm,"[Sun et~al.(2024)Sun, Huang, Wang, Wu, Zhang, Gao, Huang, Lyu, Zhang, Li, et~al.]{sun2024trustllm} Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al.",Trustllm: Trustworthiness in large language models.,Trustllm: Trustworthiness in large language models.,,"[Sun et~al.(2024)Sun, Huang, Wang, Wu, Zhang, Gao, Huang, Lyu, Zhang, Li, et~al.]{sun2024trustllm} Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al. 
 Trustllm: Trustworthiness in large language models. 
 \emph{arXiv preprint arXiv:2401.05561}, 2024."
2406.06007,thawkar2023xraygpt,"[Thawkar et~al.(2023)Thawkar, Shaker, Mullappilly, Cholakkal, Anwer, Khan, Laaksonen, and Khan]{thawkar2023xraygpt} Omkar Thawkar, Abdelrahman Shaker, Sahal~Shaji Mullappilly, Hisham Cholakkal, Rao~Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad~Shahbaz Khan.",Xraygpt: Chest radiographs summarization using medical vision-language models.,Xraygpt: Chest radiographs summarization using medical vision-language models.,,"[Thawkar et~al.(2023)Thawkar, Shaker, Mullappilly, Cholakkal, Anwer, Khan, Laaksonen, and Khan]{thawkar2023xraygpt} Omkar Thawkar, Abdelrahman Shaker, Sahal~Shaji Mullappilly, Hisham Cholakkal, Rao~Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad~Shahbaz Khan. 
 Xraygpt: Chest radiographs summarization using medical vision-language models. 
 \emph{arXiv preprint arXiv:2306.07971}, 2023."
2406.06007,tu2023many,"[Tu et~al.(2023{\natexlab{a}})Tu, Cui, Wang, Zhou, Zhao, Han, Zhou, Yao, and Xie]{tu2023many} Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie.",How many unicorns are in this image? a safety evaluation benchmark for vision llms.,How many unicorns are in this image? a safety evaluation benchmark for vision llms.,,"[Tu et~al.(2023{\natexlab{a}})Tu, Cui, Wang, Zhou, Zhao, Han, Zhou, Yao, and Xie]{tu2023many} Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. 
 How many unicorns are in this image? a safety evaluation benchmark for vision llms. 
 \emph{arXiv preprint arXiv:2311.16101}, 2023{\natexlab{a}}."
2406.06007,tu2023towards,"[Tu et~al.(2023{\natexlab{b}})Tu, Azizi, Driess, Schaekermann, Amin, Chang, Carroll, Lau, Tanno, and Ktena]{tu2023towards} Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, and Ira Ktena.",Towards generalist biomedical ai.,Towards generalist biomedical ai.,,"[Tu et~al.(2023{\natexlab{b}})Tu, Azizi, Driess, Schaekermann, Amin, Chang, Carroll, Lau, Tanno, and Ktena]{tu2023towards} Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, and Ira Ktena. 
 Towards generalist biomedical ai. 
 \emph{arXiv preprint arXiv:2307.14334}, 2023{\natexlab{b}}."
2406.06007,tu2024towards,"[Tu et~al.(2024)Tu, Palepu, Schaekermann, Saab, Freyberg, Tanno, Wang, Li, Amin, Tomasev, et~al.]{tu2024towards} Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et~al.",Towards conversational diagnostic ai.,Towards conversational diagnostic ai.,,"[Tu et~al.(2024)Tu, Palepu, Schaekermann, Saab, Freyberg, Tanno, Wang, Li, Amin, Tomasev, et~al.]{tu2024towards} Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et~al. 
 Towards conversational diagnostic ai. 
 \emph{arXiv preprint arXiv:2401.05654}, 2024."
2406.06007,wang2023decodingtrust,"[Wang et~al.(2023)Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et~al.",Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,,"[Wang et~al.(2023)Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et~al. 
 Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 
 \emph{arXiv preprint arXiv:2306.11698}, 2023."
2406.06007,wang2024asclepius,"[Wang et~al.(2024{\natexlab{a}})Wang, Su, Huan, Liu, Chen, Zhang, Li, Chang, Xin, Shen, et~al.]{wang2024asclepius} Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, et~al.",Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models.,Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models.,,"[Wang et~al.(2024{\natexlab{a}})Wang, Su, Huan, Liu, Chen, Zhang, Li, Chang, Xin, Shen, et~al.]{wang2024asclepius} Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, et~al. 
 Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models. 
 \emph{arXiv preprint arXiv:2402.11217}, 2024{\natexlab{a}}."
2406.06007,wang2024enhancing,"[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Wang, Zhou, Zhou, Yao, Zhou, Goldstein, Bhatia, Huang, et~al.]{wang2024enhancing} Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et~al.",Enhancing visual-language modality alignment in large vision language models via self-improvement.,Enhancing visual-language modality alignment in large vision language models via self-improvement.,,"[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Wang, Zhou, Zhou, Yao, Zhou, Goldstein, Bhatia, Huang, et~al.]{wang2024enhancing} Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et~al. 
 Enhancing visual-language modality alignment in large vision language models via self-improvement. 
 \emph{arXiv preprint arXiv:2405.15973}, 2024{\natexlab{b}}."
2406.06007,wang2024mementos,"[Wang et~al.(2024{\natexlab{c}})Wang, Zhou, Liu, Lu, Xu, He, Yoon, Lu, Bertasius, Bansal, et~al.]{wang2024mementos} Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et~al.",Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences.,Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences.,,"[Wang et~al.(2024{\natexlab{c}})Wang, Zhou, Liu, Lu, Xu, He, Yoon, Lu, Bertasius, Bansal, et~al.]{wang2024mementos} Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et~al. 
 Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences. 
 \emph{arXiv preprint arXiv:2401.10529}, 2024{\natexlab{c}}."
2406.06007,wu2023can,"[Wu et~al.(2023{\natexlab{a}})Wu, Lei, Zheng, Zhao, Lin, Zhang, Zhou, Zhao, Zhang, Wang, et~al.]{wu2023can} Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya~Zhang, Yanfeng Wang, et~al.",Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis.,Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis.,,"[Wu et~al.(2023{\natexlab{a}})Wu, Lei, Zheng, Zhao, Lin, Zhang, Zhou, Zhao, Zhang, Wang, et~al.]{wu2023can} Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya~Zhang, Yanfeng Wang, et~al. 
 Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. 
 \emph{arXiv preprint arXiv:2310.09909}, 2023{\natexlab{a}}."
2406.06007,wu2023towards,"[Wu et~al.(2023{\natexlab{b}})Wu, Zhang, Zhang, Wang, and Xie]{wu2023towards} Chaoyi Wu, Xiaoman Zhang, Ya~Zhang, Yanfeng Wang, and Weidi Xie.",Towards generalist foundation model for radiology.,Towards generalist foundation model for radiology.,,"[Wu et~al.(2023{\natexlab{b}})Wu, Zhang, Zhang, Wang, and Xie]{wu2023towards} Chaoyi Wu, Xiaoman Zhang, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 
 Towards generalist foundation model for radiology. 
 \emph{arXiv preprint arXiv:2308.02463}, 2023{\natexlab{b}}."
2406.06007,xu2023lvlm,"[Xu et~al.(2023)Xu, Shao, Zhang, Gao, Liu, Lei, Meng, Huang, Qiao, and Luo]{xu2023lvlm} Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu~Qiao, and Ping Luo.",Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.,Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.,,"[Xu et~al.(2023)Xu, Shao, Zhang, Gao, Liu, Lei, Meng, Huang, Qiao, and Luo]{xu2023lvlm} Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu~Qiao, and Ping Luo. 
 Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. 
 \emph{arXiv preprint arXiv:2306.09265}, 2023."
2406.06007,zhang2023r,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Diao, Lin, Fung, Lian, Wang, Chen, Ji, and Zhang]{zhang2023r} Hanning Zhang, Shizhe Diao, Yong Lin, Yi~R Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang.",R-tuning: Teaching large language models to refuse unknown questions.,R-tuning: Teaching large language models to refuse unknown questions.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Diao, Lin, Fung, Lian, Wang, Chen, Ji, and Zhang]{zhang2023r} Hanning Zhang, Shizhe Diao, Yong Lin, Yi~R Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 
 R-tuning: Teaching large language models to refuse unknown questions. 
 \emph{arXiv preprint arXiv:2311.09677}, 2023{\natexlab{a}}."
2406.06007,zhang2023pmc,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Wu, Zhao, Lin, Zhang, Wang, and Xie]{zhang2023pmc} Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang, and Weidi Xie.",Pmc-vqa: Visual instruction tuning for medical visual question answering.,Pmc-vqa: Visual instruction tuning for medical visual question answering.,,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Wu, Zhao, Lin, Zhang, Wang, and Xie]{zhang2023pmc} Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 
 Pmc-vqa: Visual instruction tuning for medical visual question answering. 
 \emph{arXiv preprint arXiv:2305.10415}, 2023{\natexlab{b}}."
2406.06007,zhou2023analyzing,"[Zhou et~al.(2023)Zhou, Cui, Yoon, Zhang, Deng, Finn, Bansal, and Yao]{zhou2023analyzing} Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.",Analyzing and mitigating object hallucination in large vision-language models.,Analyzing and mitigating object hallucination in large vision-language models.,,"[Zhou et~al.(2023)Zhou, Cui, Yoon, Zhang, Deng, Finn, Bansal, and Yao]{zhou2023analyzing} Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 
 Analyzing and mitigating object hallucination in large vision-language models. 
 \emph{arXiv preprint arXiv:2310.00754}, 2023."
2406.06007,zhou2024aligning,"[Zhou et~al.(2024{\natexlab{a}})Zhou, Cui, Rafailov, Finn, and Yao]{zhou2024aligning} Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.",Aligning modalities in vision large language models via preference fine-tuning.,Aligning modalities in vision large language models via preference fine-tuning.,,"[Zhou et~al.(2024{\natexlab{a}})Zhou, Cui, Rafailov, Finn, and Yao]{zhou2024aligning} Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. 
 Aligning modalities in vision large language models via preference fine-tuning. 
 \emph{arXiv preprint arXiv:2402.11411}, 2024{\natexlab{a}}."
2406.06007,zhou2024calibrated,"[Zhou et~al.(2024{\natexlab{b}})Zhou, Fan, Cheng, Yang, Chen, Cui, Wang, Li, Zhang, and Yao]{zhou2024calibrated} Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao.",Calibrated self-rewarding vision language models.,Calibrated self-rewarding vision language models.,,"[Zhou et~al.(2024{\natexlab{b}})Zhou, Fan, Cheng, Yang, Chen, Cui, Wang, Li, Zhang, and Yao]{zhou2024calibrated} Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 
 Calibrated self-rewarding vision language models. 
 \emph{arXiv preprint arXiv:2405.14622}, 2024{\natexlab{b}}."
2406.06007,zhu2023minigpt,"[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",Minigpt-4: Enhancing vision-language understanding with advanced large language models.,Minigpt-4: Enhancing vision-language understanding with advanced large language models.,,"[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 
 Minigpt-4: Enhancing vision-language understanding with advanced large language models. 
 \emph{arXiv preprint arXiv:2304.10592}, 2023."
2406.06366,custom:devlin2018bert,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{custom:devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{custom:devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}."
2406.06366,custom:he2023simplifying,[{He and Hofmann(2023)}]{custom:he2023simplifying} Bobby He and Thomas Hofmann. 2023.,Simplifying transformer blocks.,Simplifying transformer blocks.,,"[{He and Hofmann(2023)}]{custom:he2023simplifying} Bobby He and Thomas Hofmann. 2023. 
 Simplifying transformer blocks. 
 \emph{arXiv preprint arXiv:2311.01906}."
2406.06366,custom:kingma2014adam,[{Kingma and Ba(2014)}]{custom:kingma2014adam} Diederik~P Kingma and Jimmy Ba. 2014.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[{Kingma and Ba(2014)}]{custom:kingma2014adam} Diederik~P Kingma and Jimmy Ba. 2014. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}."
2406.06366,custom:kitaev2020reformer,"[{Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya}]{custom:kitaev2020reformer} Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya. 2020.",Reformer: The efficient transformer.,Reformer: The efficient transformer.,,"[{Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya}]{custom:kitaev2020reformer} Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya. 2020. 
 Reformer: The efficient transformer. 
 \emph{arXiv preprint arXiv:2001.04451}."
2406.06435,almazrouei2023falcon,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.",The falcon series of open language models.,The falcon series of open language models.,,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv:2311.16867}."
2406.06435,chan2023ic,"[{Chan et~al.(2023)Chan, Myers, Vijayanarasimhan, Ross, and Canny}]{chan2023ic} David~M Chan, Austin Myers, Sudheendra Vijayanarasimhan, David~A Ross, and John Canny. 2023.",$ ic^{3} $: Image captioning by committee consensus.,$ ic^{3} $: Image captioning by committee consensus.,,"[{Chan et~al.(2023)Chan, Myers, Vijayanarasimhan, Ross, and Canny}]{chan2023ic} David~M Chan, Austin Myers, Sudheendra Vijayanarasimhan, David~A Ross, and John Canny. 2023. 
 $ ic^{3} $: Image captioning by committee consensus. 
 \emph{arXiv preprint arXiv:2302.01328}."
2406.06435,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022.",A survey for in-context learning.,A survey for in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2406.06435,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.06435,jiang2021delphi,"[{Jiang et~al.(2021)Jiang, Hwang, Bhagavatula, Bras, Liang, Dodge, Sakaguchi, Forbes, Borchardt, Gabriel et~al.}]{jiang2021delphi} Liwei Jiang, Jena~D Hwang, Chandra Bhagavatula, Ronan~Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et~al. 2021.",Can machines learn morality? the delphi experiment.,Can machines learn morality? the delphi experiment.,,"[{Jiang et~al.(2021)Jiang, Hwang, Bhagavatula, Bras, Liang, Dodge, Sakaguchi, Forbes, Borchardt, Gabriel et~al.}]{jiang2021delphi} Liwei Jiang, Jena~D Hwang, Chandra Bhagavatula, Ronan~Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et~al. 2021. 
 Can machines learn morality? the delphi experiment. 
 \emph{arXiv preprint arXiv:2110.07574}."
2406.06435,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2406.06435,lanham2023measuring,"[{Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion et~al.}]{lanham2023measuring} Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et~al. 2023.",Measuring faithfulness in chain-of-thought reasoning.,Measuring faithfulness in chain-of-thought reasoning.,,"[{Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion et~al.}]{lanham2023measuring} Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et~al. 2023. 
 Measuring faithfulness in chain-of-thought reasoning. 
 \emph{arXiv preprint arXiv:2307.13702}."
2406.06435,nori2023medprompt,"[{Nori et~al.(2023)Nori, Lee, Zhang, Carignan, Edgar, Fusi, King, Larson, Li, Liu et~al.}]{nori2023medprompt} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023.",Can generalist foundation models outcompete special-purpose tuning? case study in medicine.,Can generalist foundation models outcompete special-purpose tuning? case study in medicine.,,"[{Nori et~al.(2023)Nori, Lee, Zhang, Carignan, Edgar, Fusi, King, Larson, Li, Liu et~al.}]{nori2023medprompt} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023. 
 Can generalist foundation models outcompete special-purpose tuning? case study in medicine. 
 \emph{arXiv preprint arXiv:2311.16452}."
2406.06435,oli2023behavior,"[{Oli et~al.(2023)Oli, Banjade, Chapagain, and Rus}]{oli2023behavior} Priti Oli, Rabin Banjade, Jeevan Chapagain, and Vasile Rus. 2023.",The behavior of large language models when prompted to generate code explanations.,The behavior of large language models when prompted to generate code explanations.,,"[{Oli et~al.(2023)Oli, Banjade, Chapagain, and Rus}]{oli2023behavior} Priti Oli, Rabin Banjade, Jeevan Chapagain, and Vasile Rus. 2023. 
 The behavior of large language models when prompted to generate code explanations. 
 \emph{arXiv preprint arXiv:2311.01490}."
2406.06435,openai2023gpt,[{OpenAI(2023)}]{openai2023gpt} OpenAI. 2023.,Gpt-4 technical report.,Gpt-4 technical report.,,"[{OpenAI(2023)}]{openai2023gpt} OpenAI. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.06435,sorensen2023value,"[{Sorensen et~al.(2023)Sorensen, Jiang, Hwang, Levine, Pyatkin, West, Dziri, Lu, Rao, Bhagavatula et~al.}]{sorensen2023value} Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et~al. 2023.","Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties.","Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties.",,"[{Sorensen et~al.(2023)Sorensen, Jiang, Hwang, Levine, Pyatkin, West, Dziri, Lu, Rao, Bhagavatula et~al.}]{sorensen2023value} Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et~al. 2023. 
 Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties. 
 \emph{arXiv preprint arXiv:2309.00779}."
2406.06435,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.06435,wu2023fine,"[{Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi}]{wu2023fine} Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah~A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023.",Fine-grained human feedback gives better rewards for language model training.,Fine-grained human feedback gives better rewards for language model training.,,"[{Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi}]{wu2023fine} Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah~A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. 
 Fine-grained human feedback gives better rewards for language model training. 
 \emph{arXiv preprint arXiv:2306.01693}."
2406.06586,chung2022scaling,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2406.06586,han2022folio,"[{Han et~al.(2022)Han, Schoelkopf, Zhao, Qi, Riddell, Benson, Sun, Zubova, Qiao, Burtell et~al.}]{han2022folio} Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et~al. 2022.",Folio: Natural language reasoning with first-order logic.,Folio: Natural language reasoning with first-order logic.,,"[{Han et~al.(2022)Han, Schoelkopf, Zhao, Qi, Riddell, Benson, Sun, Zubova, Qiao, Burtell et~al.}]{han2022folio} Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et~al. 2022. 
 Folio: Natural language reasoning with first-order logic. 
 \emph{arXiv preprint arXiv:2209.00840}."
2406.06586,liang2022holistic,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}."
2406.06586,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.0745,cui2022democratizing,"[Cui et~al., 2022]{cui2022democratizing} Cui, Y., Zhao, L., Liang, F., Li, Y., and Shao, J. (2022).","Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision.","Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision.",,"[Cui et~al., 2022]{cui2022democratizing} Cui, Y., Zhao, L., Liang, F., Li, Y., and Shao, J. (2022). 
 Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision. 
 {\em arXiv preprint arXiv:2203.05796}."
2406.0745,eslami2021does,"[Eslami et~al., 2021]{eslami2021does} Eslami, S., de~Melo, G., and Meinel, C. (2021).",Does clip benefit visual question answering in the medical domain as much as it does in the general domain?,Does clip benefit visual question answering in the medical domain as much as it does in the general domain?,,"[Eslami et~al., 2021]{eslami2021does} Eslami, S., de~Melo, G., and Meinel, C. (2021). 
 Does clip benefit visual question answering in the medical domain as much as it does in the general domain? 
 {\em arXiv preprint arXiv:2112.13906}."
2406.0745,he2020pathvqa,"[He et~al., 2020]{he2020pathvqa} He, X., Zhang, Y., Mou, L., Xing, E., and Xie, P. (2020).",Pathvqa: 30000+ questions for medical visual question answering.,Pathvqa: 30000+ questions for medical visual question answering.,,"[He et~al., 2020]{he2020pathvqa} He, X., Zhang, Y., Mou, L., Xing, E., and Xie, P. (2020). 
 Pathvqa: 30000+ questions for medical visual question answering. 
 {\em arXiv preprint arXiv:2003.10286}."
2406.0745,kaplan2020scaling,"[Kaplan et~al., 2020]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020).",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al., 2020]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). 
 Scaling laws for neural language models. 
 {\em arXiv preprint arXiv:2001.08361}."
2406.0745,li2020unimo,"[Li et~al., 2020]{li2020unimo} Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H. (2020).",Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning.,Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning.,,"[Li et~al., 2020]{li2020unimo} Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H. (2020). 
 Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. 
 {\em arXiv preprint arXiv:2012.15409}."
2406.0745,li2021supervision,"[Li et~al., 2021b]{li2021supervision} Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., and Yan, J. (2021b).",Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.,Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.,,"[Li et~al., 2021b]{li2021supervision} Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., and Yan, J. (2021b). 
 Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. 
 {\em arXiv preprint arXiv:2110.05208}."
2406.0745,liu2023imitate,"[Liu et~al., 2023]{liu2023imitate} Liu, C., Cheng, S., Shi, M., Shah, A., Bai, W., and Arcucci, R. (2023).",Imitate: Clinical prior guided hierarchical vision-language pre-training.,Imitate: Clinical prior guided hierarchical vision-language pre-training.,,"[Liu et~al., 2023]{liu2023imitate} Liu, C., Cheng, S., Shi, M., Shah, A., Bai, W., and Arcucci, R. (2023). 
 Imitate: Clinical prior guided hierarchical vision-language pre-training. 
 {\em arXiv preprint arXiv:2310.07355}."
2406.0745,loshchilov2017decoupled,"[Loshchilov and Hutter, 2017]{loshchilov2017decoupled} Loshchilov, I. and Hutter, F. (2017).",Decoupled weight decay regularization.,Decoupled weight decay regularization.,,"[Loshchilov and Hutter, 2017]{loshchilov2017decoupled} Loshchilov, I. and Hutter, F. (2017). 
 Decoupled weight decay regularization. 
 {\em arXiv preprint arXiv:1711.05101}."
2406.0745,oord2018representation,"[Oord et~al., 2018]{oord2018representation} Oord, A. v.~d., Li, Y., and Vinyals, O. (2018).",Representation learning with contrastive predictive coding.,Representation learning with contrastive predictive coding.,,"[Oord et~al., 2018]{oord2018representation} Oord, A. v.~d., Li, Y., and Vinyals, O. (2018). 
 Representation learning with contrastive predictive coding. 
 {\em arXiv preprint arXiv:1807.03748}."
2406.0745,wang2023one,"[Wang et~al., 2023]{wang2023one} Wang, P., Wang, S., Lin, J., Bai, S., Zhou, X., Zhou, J., Wang, X., and Zhou, C. (2023).",One-peace: Exploring one general representation model toward unlimited modalities.,One-peace: Exploring one general representation model toward unlimited modalities.,,"[Wang et~al., 2023]{wang2023one} Wang, P., Wang, S., Lin, J., Bai, S., Zhou, X., Zhou, J., Wang, X., and Zhou, C. (2023). 
 One-peace: Exploring one general representation model toward unlimited modalities. 
 {\em arXiv preprint arXiv:2305.11172}."
2406.0745,yao2021filip,"[Yao et~al., 2021]{yao2021filip} Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., and Xu, C. (2021).",Filip: Fine-grained interactive language-image pre-training.,Filip: Fine-grained interactive language-image pre-training.,,"[Yao et~al., 2021]{yao2021filip} Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., and Xu, C. (2021). 
 Filip: Fine-grained interactive language-image pre-training. 
 {\em arXiv preprint arXiv:2111.07783}."
2406.0745,yu2022coca,"[Yu et~al., 2022]{yu2022coca} Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. (2022).",Coca: Contrastive captioners are image-text foundation models.,Coca: Contrastive captioners are image-text foundation models.,,"[Yu et~al., 2022]{yu2022coca} Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. (2022). 
 Coca: Contrastive captioners are image-text foundation models. 
 {\em arXiv preprint arXiv:2205.01917}."
2406.0745,zhang2023biomedclip,"[Zhang et~al., 2023a]{zhang2023biomedclip} Zhang, S., Xu, Y., Usuyama, N., Xu, H., Bagga, J., Tinn, R., Preston, S., Rao, R., Wei, M., Valluri, N., et~al. (2023a).",Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,,"[Zhang et~al., 2023a]{zhang2023biomedclip} Zhang, S., Xu, Y., Usuyama, N., Xu, H., Bagga, J., Tinn, R., Preston, S., Rao, R., Wei, M., Valluri, N., et~al. (2023a). 
 Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. 
 {\em arXiv preprint arXiv:2303.00915}."
2406.0745,zhang2023multimodal,"[Zhang et~al., 2023b]{zhang2023multimodal} Zhang, X., Yoon, J., Bansal, M., and Yao, H. (2023b).",Multimodal representation learning by alternating unimodal adaptation.,Multimodal representation learning by alternating unimodal adaptation.,,"[Zhang et~al., 2023b]{zhang2023multimodal} Zhang, X., Yoon, J., Bansal, M., and Yao, H. (2023b). 
 Multimodal representation learning by alternating unimodal adaptation. 
 {\em arXiv preprint arXiv:2311.10707}."
2406.0745,zhao2023clip,"[Zhao et~al., 2023]{zhao2023clip} Zhao, Z., Liu, Y., Wu, H., Li, Y., Wang, S., Teng, L., Liu, D., Li, X., Cui, Z., Wang, Q., et~al. (2023).",Clip in medical imaging: A comprehensive survey.,Clip in medical imaging: A comprehensive survey.,,"[Zhao et~al., 2023]{zhao2023clip} Zhao, Z., Liu, Y., Wu, H., Li, Y., Wang, S., Teng, L., Liu, D., Li, X., Cui, Z., Wang, Q., et~al. (2023). 
 Clip in medical imaging: A comprehensive survey. 
 {\em arXiv preprint arXiv:2312.07353}."
2406.07778,kandpal2023backdoor,"[{Kandpal et~al.(2023)Kandpal, Jagielski, Tram{\`e}r, and Carlini}]{kandpal2023backdoor} Kandpal, N.; Jagielski, M.; Tram{\`e}r, F.; and Carlini, N. 2023.",Backdoor Attacks for In-Context Learning with Language Models.,Backdoor Attacks for In-Context Learning with Language Models.,,"[{Kandpal et~al.(2023)Kandpal, Jagielski, Tram{\`e}r, and Carlini}]{kandpal2023backdoor} Kandpal, N.; Jagielski, M.; Tram{\`e}r, F.; and Carlini, N. 2023. 
 Backdoor Attacks for In-Context Learning with Language Models. 
 \emph{arXiv preprint arXiv:2307.14692}."
2406.07778,lyu2024task,"[{Lyu et~al.(2024)Lyu, Lin, Zheng, Pang, Ling, Jha, and Chen}]{lyu2024task} Lyu, W.; Lin, X.; Zheng, S.; Pang, L.; Ling, H.; Jha, S.; and Chen, C. 2024.",Task-Agnostic Detector For Insertion-Based Backdoor Attacks.,Task-Agnostic Detector For Insertion-Based Backdoor Attacks.,,"[{Lyu et~al.(2024)Lyu, Lin, Zheng, Pang, Ling, Jha, and Chen}]{lyu2024task} Lyu, W.; Lin, X.; Zheng, S.; Pang, L.; Ling, H.; Jha, S.; and Chen, C. 2024. 
 Task-Agnostic Detector For Insertion-Based Backdoor Attacks. 
 \emph{arXiv preprint arXiv:2403.17155}."
2406.07778,zhao2024universal,"[{Zhao et~al.(2024)Zhao, Jia, Tuan, Pan, and Wen}]{zhao2024universal} Zhao, S.; Jia, M.; Tuan, L.~A.; Pan, F.; and Wen, J. 2024.",{Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-Context Learning}.,{Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-Context Learning}.,,"[{Zhao et~al.(2024)Zhao, Jia, Tuan, Pan, and Wen}]{zhao2024universal} Zhao, S.; Jia, M.; Tuan, L.~A.; Pan, F.; and Wen, J. 2024. 
 {Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-Context Learning}. 
 \emph{arXiv preprint arXiv:2401.05949}."
2406.09454,driess2023palm,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, et~al.",{PaLM-E}: An embodied multimodal language model.,{PaLM-E}: An embodied multimodal language model.,,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, et~al. 
 {PaLM-E}: An embodied multimodal language model. 
 \emph{arXiv preprint arXiv:2303.03378}, 2023."
2406.09454,han2023medalpaca,"[Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem]{han2023medalpaca} T.~Han, L.~C. Adams, J.-M. Papaioannou, P.~Grundmann, T.~Oberhauser, A.~L{\""o}ser, D.~Truhn, and K.~K. Bressem.",Medalpaca--an open-source collection of medical conversational ai models and training data.,Medalpaca--an open-source collection of medical conversational ai models and training data.,,"[Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem]{han2023medalpaca} T.~Han, L.~C. Adams, J.-M. Papaioannou, P.~Grundmann, T.~Oberhauser, A.~L{\""o}ser, D.~Truhn, and K.~K. Bressem. 
 Medalpaca--an open-source collection of medical conversational ai models and training data. 
 \emph{arXiv preprint arXiv:2304.08247}, 2023."
2406.09454,he2020pathvqa,"[He et~al.(2020{\natexlab{a}})He, Zhang, Mou, Xing, and Xie]{he2020pathvqa} X.~He, Y.~Zhang, L.~Mou, E.~Xing, and P.~Xie.",Pathvqa: 30000+ questions for medical visual question answering.,Pathvqa: 30000+ questions for medical visual question answering.,,"[He et~al.(2020{\natexlab{a}})He, Zhang, Mou, Xing, and Xie]{he2020pathvqa} X.~He, Y.~Zhang, L.~Mou, E.~Xing, and P.~Xie. 
 Pathvqa: 30000+ questions for medical visual question answering. 
 \emph{arXiv preprint arXiv:2003.10286}, 2020{\natexlab{a}}."
2406.09454,li2022self,"[Li et~al.(2022)Li, Liu, Tan, Liao, and Zhong]{li2022self} P.~Li, G.~Liu, L.~Tan, J.~Liao, and S.~Zhong.",Self-supervised vision-language pretraining for medical visual question answering.,Self-supervised vision-language pretraining for medical visual question answering.,,"[Li et~al.(2022)Li, Liu, Tan, Liao, and Zhong]{li2022self} P.~Li, G.~Liu, L.~Tan, J.~Liao, and S.~Zhong. 
 Self-supervised vision-language pretraining for medical visual question answering. 
 \emph{arXiv preprint arXiv:2211.13594}, 2022."
2406.09454,liu2023visual,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and Lee]{liu2023visual} H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.",Visual instruction tuning.,Visual instruction tuning.,,"[Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and Lee]{liu2023visual} H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee. 
 Visual instruction tuning. 
 \emph{arXiv preprint arXiv:2304.08485}, 2023{\natexlab{a}}."
2406.09454,liu2023q2atransformer,"[Liu et~al.(2023{\natexlab{b}})Liu, Wang, Xu, and Zhou]{liu2023q2atransformer} Y.~Liu, Z.~Wang, D.~Xu, and L.~Zhou.",Q2atransformer: Improving medical vqa via an answer querying decoder.,Q2atransformer: Improving medical vqa via an answer querying decoder.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Wang, Xu, and Zhou]{liu2023q2atransformer} Y.~Liu, Z.~Wang, D.~Xu, and L.~Zhou. 
 Q2atransformer: Improving medical vqa via an answer querying decoder. 
 \emph{arXiv preprint arXiv:2304.01611}, 2023{\natexlab{b}}."
2406.09454,radford2021learning,"[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning} A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.",Learning transferable visual models from natural language supervision.,Learning transferable visual models from natural language supervision.,,"[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning} A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al. 
 Learning transferable visual models from natural language supervision. 
 \emph{arXiv preprint arXiv:2103.00020}, 2021."
2406.09454,touvron2023llama,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023."
2406.09454,van2023open,"[van Sonsbeek et~al.(2023)van Sonsbeek, Derakhshani, Najdenkoska, Snoek, and Worring]{van2023open} T.~van Sonsbeek, M.~M. Derakhshani, I.~Najdenkoska, C.~G. Snoek, and M.~Worring.",Open-ended medical visual question answering through prefix tuning of language models.,Open-ended medical visual question answering through prefix tuning of language models.,,"[van Sonsbeek et~al.(2023)van Sonsbeek, Derakhshani, Najdenkoska, Snoek, and Worring]{van2023open} T.~van Sonsbeek, M.~M. Derakhshani, I.~Najdenkoska, C.~G. Snoek, and M.~Worring. 
 Open-ended medical visual question answering through prefix tuning of language models. 
 \emph{arXiv preprint arXiv:2303.05977}, 2023."
2406.09454,wu2023pmc,"[Wu et~al.(2023)Wu, Zhang, Zhang, Wang, and Xie]{wu2023pmc} C.~Wu, X.~Zhang, Y.~Zhang, Y.~Wang, and W.~Xie.",Pmc-llama: Further finetuning llama on medical papers.,Pmc-llama: Further finetuning llama on medical papers.,,"[Wu et~al.(2023)Wu, Zhang, Zhang, Wang, and Xie]{wu2023pmc} C.~Wu, X.~Zhang, Y.~Zhang, Y.~Wang, and W.~Xie. 
 Pmc-llama: Further finetuning llama on medical papers. 
 \emph{arXiv preprint arXiv:2304.14454}, 2023."
2406.09454,xiong2023doctorglm,"[Xiong et~al.(2023)Xiong, Wang, Zhu, Zhao, Liu, Wang, and Shen]{xiong2023doctorglm} H.~Xiong, S.~Wang, Y.~Zhu, Z.~Zhao, Y.~Liu, Q.~Wang, and D.~Shen.",Doctorglm: Fine-tuning your chinese doctor is not a herculean task.,Doctorglm: Fine-tuning your chinese doctor is not a herculean task.,,"[Xiong et~al.(2023)Xiong, Wang, Zhu, Zhao, Liu, Wang, and Shen]{xiong2023doctorglm} H.~Xiong, S.~Wang, Y.~Zhu, Z.~Zhao, Y.~Liu, Q.~Wang, and D.~Shen. 
 Doctorglm: Fine-tuning your chinese doctor is not a herculean task. 
 \emph{arXiv preprint arXiv:2304.01097}, 2023."
2406.09454,yunxiang2023chatdoctor,"[Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You]{yunxiang2023chatdoctor} L.~Yunxiang, L.~Zihan, Z.~Kai, D.~Ruilong, and Z.~You.",Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge.,,"[Yunxiang et~al.(2023)Yunxiang, Zihan, Kai, Ruilong, and You]{yunxiang2023chatdoctor} L.~Yunxiang, L.~Zihan, Z.~Kai, D.~Ruilong, and Z.~You. 
 Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. 
 \emph{arXiv preprint arXiv:2303.14070}, 2023."
2406.09454,zhang2023large,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Xu, Usuyama, Bagga, Tinn, Preston, Rao, Wei, Valluri, Wong, et~al.]{zhang2023large} S.~Zhang, Y.~Xu, N.~Usuyama, J.~Bagga, R.~Tinn, S.~Preston, R.~Rao, M.~Wei, N.~Valluri, C.~Wong, et~al.",Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Xu, Usuyama, Bagga, Tinn, Preston, Rao, Wei, Valluri, Wong, et~al.]{zhang2023large} S.~Zhang, Y.~Xu, N.~Usuyama, J.~Bagga, R.~Tinn, S.~Preston, R.~Rao, M.~Wei, N.~Valluri, C.~Wong, et~al. 
 Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. 
 \emph{arXiv preprint arXiv:2303.00915}, 2023{\natexlab{b}}."
2406.0976,dubois2024length,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto.",Length-controlled alpacaeval: A simple way to debias automatic evaluators.,Length-controlled alpacaeval: A simple way to debias automatic evaluators.,,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 
 Length-controlled alpacaeval: A simple way to debias automatic evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}, 2024."
2406.0976,ethayarajh2024kto,"[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto} Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.",Kto: Model alignment as prospect theoretic optimization.,Kto: Model alignment as prospect theoretic optimization.,,"[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto} Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 
 Kto: Model alignment as prospect theoretic optimization. 
 \emph{arXiv preprint arXiv:2402.01306}, 2024."
2406.0976,guo2024direct,"[Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot, et~al.]{guo2024direct} Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et~al.",Direct language model alignment from online ai feedback.,Direct language model alignment from online ai feedback.,,"[Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot, et~al.]{guo2024direct} Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et~al. 
 Direct language model alignment from online ai feedback. 
 \emph{arXiv preprint arXiv:2402.04792}, 2024."
2406.0976,huang2022large,"[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{huang2022large} Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.",Large language models can self-improve.,Large language models can self-improve.,,"[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{huang2022large} Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 
 Large language models can self-improve. 
 \emph{arXiv preprint arXiv:2210.11610}, 2022."
2406.0976,lee2023rlaif,"[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.",Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,,"[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 
 Rlaif: Scaling reinforcement learning from human feedback with ai feedback. 
 \emph{arXiv preprint arXiv:2309.00267}, 2023."
2406.0976,li2023self,"[Li et~al.(2023{\natexlab{a}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis.",Self-alignment with instruction backtranslation.,Self-alignment with instruction backtranslation.,,"[Li et~al.(2023{\natexlab{a}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 
 Self-alignment with instruction backtranslation. 
 \emph{arXiv preprint arXiv:2308.06259}, 2023{\natexlab{a}}."
2406.0976,park2024disentangling,"[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn.",Disentangling length from quality in direct preference optimization.,Disentangling length from quality in direct preference optimization.,,"[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 
 Disentangling length from quality in direct preference optimization. 
 \emph{arXiv preprint arXiv:2403.19159}, 2024."
2406.0976,rafailov2024r,"[Rafailov et~al.(2024{\natexlab{a}})Rafailov, Hejna, Park, and Finn]{rafailov2024r} Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn.",From $ r $ to $ q^* $: Your language model is secretly a q-function.,From $ r $ to $ q^* $: Your language model is secretly a q-function.,,"[Rafailov et~al.(2024{\natexlab{a}})Rafailov, Hejna, Park, and Finn]{rafailov2024r} Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. 
 From $ r $ to $ q^* $: Your language model is secretly a q-function. 
 \emph{arXiv preprint arXiv:2404.12358}, 2024{\natexlab{a}}."
2406.0976,schulman2017proximal,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}, 2017."
2406.0976,sun2023salmon,"[Sun et~al.(2023)Sun, Shen, Zhang, Zhou, Chen, Cox, Yang, and Gan]{sun2023salmon} Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.",Salmon: Self-alignment with principle-following reward models.,Salmon: Self-alignment with principle-following reward models.,,"[Sun et~al.(2023)Sun, Shen, Zhang, Zhou, Chen, Cox, Yang, and Gan]{sun2023salmon} Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 
 Salmon: Self-alignment with principle-following reward models. 
 \emph{arXiv preprint arXiv:2310.05910}, 2023."
2406.0976,tajwar2024preference,"[Tajwar et~al.(2024)Tajwar, Singh, Sharma, Rafailov, Schneider, Xie, Ermon, Finn, and Kumar]{tajwar2024preference} Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar.","Preference fine-tuning of llms should leverage suboptimal, on-policy data.","Preference fine-tuning of llms should leverage suboptimal, on-policy data.",,"[Tajwar et~al.(2024)Tajwar, Singh, Sharma, Rafailov, Schneider, Xie, Ermon, Finn, and Kumar]{tajwar2024preference} Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. 
 Preference fine-tuning of llms should leverage suboptimal, on-policy data. 
 \emph{arXiv preprint arXiv:2404.14367}, 2024."
2406.0976,tang2024understanding,"[Tang et~al.(2024)Tang, Guo, Zheng, Calandriello, Cao, Tarassov, Munos, Ávila Pires, Valko, Cheng, and Dabney]{tang2024understanding} Yunhao Tang, Daniel~Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, and Will Dabney.",Understanding the performance gap between online and offline alignment algorithms.,Understanding the performance gap between online and offline alignment algorithms.,,"[Tang et~al.(2024)Tang, Guo, Zheng, Calandriello, Cao, Tarassov, Munos, Ávila Pires, Valko, Cheng, and Dabney]{tang2024understanding} Yunhao Tang, Daniel~Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos, Bernardo Ávila Pires, Michal Valko, Yong Cheng, and Will Dabney. 
 Understanding the performance gap between online and offline alignment algorithms. 
 \emph{arXiv preprint arXiv: 2405.08448}, 2024."
2406.0976,xie2024monte,"[Xie et~al.(2024)Xie, Goyal, Zheng, Kan, Lillicrap, Kawaguchi, and Shieh]{xie2024monte} Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy~P Lillicrap, Kenji Kawaguchi, and Michael Shieh.",Monte carlo tree search boosts reasoning via iterative preference learning.,Monte carlo tree search boosts reasoning via iterative preference learning.,,"[Xie et~al.(2024)Xie, Goyal, Zheng, Kan, Lillicrap, Kawaguchi, and Shieh]{xie2024monte} Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy~P Lillicrap, Kenji Kawaguchi, and Michael Shieh. 
 Monte carlo tree search boosts reasoning via iterative preference learning. 
 \emph{arXiv preprint arXiv:2405.00451}, 2024."
2406.0976,yuan2024self,"[Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.",Self-rewarding language models.,Self-rewarding language models.,,"[Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 
 Self-rewarding language models. 
 \emph{arXiv preprint arXiv:2401.10020}, 2024."
2406.0976,zhao2023slic,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu.",Slic-hf: Sequence likelihood calibration with human feedback.,Slic-hf: Sequence likelihood calibration with human feedback.,,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu. 
 Slic-hf: Sequence likelihood calibration with human feedback. 
 \emph{arXiv preprint arXiv:2305.10425}, 2023."
2406.0976,zhong2024dpomeetsppo,"[Zhong et~al.(2024)Zhong, Feng, Xiong, Zhao, He, Bian, and Wang]{zhong2024dpomeetsppo} Han Zhong, Guhao Feng, Wei Xiong, Li~Zhao, Di~He, Jiang Bian, and Liwei Wang.",Dpo meets ppo: Reinforced token optimization for rlhf.,Dpo meets ppo: Reinforced token optimization for rlhf.,,"[Zhong et~al.(2024)Zhong, Feng, Xiong, Zhao, He, Bian, and Wang]{zhong2024dpomeetsppo} Han Zhong, Guhao Feng, Wei Xiong, Li~Zhao, Di~He, Jiang Bian, and Liwei Wang. 
 Dpo meets ppo: Reinforced token optimization for rlhf. 
 \emph{arXiv preprint arXiv: 2404.18922}, 2024."
2406.1067,abbas2023semdedup,"[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{abbas2023semdedup} Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos.",Semdedup: Data-efficient learning at web-scale through semantic deduplication.,Semdedup: Data-efficient learning at web-scale through semantic deduplication.,,"[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{abbas2023semdedup} Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos. 
 Semdedup: Data-efficient learning at web-scale through semantic deduplication. 
 \emph{arXiv preprint arXiv:2303.09540}, 2023."
2406.1067,ash2019deep,"[Ash et~al.(2019)Ash, Zhang, Krishnamurthy, Langford, and Agarwal]{ash2019deep} Jordan~T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.","Deep batch active learning by diverse, uncertain gradient lower bounds.","Deep batch active learning by diverse, uncertain gradient lower bounds.",,"[Ash et~al.(2019)Ash, Zhang, Krishnamurthy, Langford, and Agarwal]{ash2019deep} Jordan~T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. 
 Deep batch active learning by diverse, uncertain gradient lower bounds. 
 \emph{arXiv preprint arXiv:1906.03671}, 2019."
2406.1067,clark2019boolq,"[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}, 2019."
2406.1067,clark2018think,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2406.1067,evans2023bad,"[Evans et~al.(2023)Evans, Pathak, Merzic, Schwarz, Tanno, and Henaff]{evans2023bad} Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier~J Henaff.",Bad students make great teachers: Active learning accelerates large-scale visual understanding.,Bad students make great teachers: Active learning accelerates large-scale visual understanding.,,"[Evans et~al.(2023)Evans, Pathak, Merzic, Schwarz, Tanno, and Henaff]{evans2023bad} Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier~J Henaff. 
 Bad students make great teachers: Active learning accelerates large-scale visual understanding. 
 \emph{arXiv preprint arXiv:2312.05328}, 2023."
2406.1067,ilyas2022datamodels,"[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{ilyas2022datamodels} Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.",Datamodels: Predicting predictions from training data.,Datamodels: Predicting predictions from training data.,,"[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{ilyas2022datamodels} Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 
 Datamodels: Predicting predictions from training data. 
 \emph{arXiv preprint arXiv:2202.00622}, 2022."
2406.1067,mihaylov2018can,"[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.",Can a suit of armor conduct electricity? a new dataset for open book question answering.,Can a suit of armor conduct electricity? a new dataset for open book question answering.,,"[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 
 Can a suit of armor conduct electricity? a new dataset for open book question answering. 
 \emph{arXiv preprint arXiv:1809.02789}, 2018."
2406.1067,park2023trak,"[Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry]{park2023trak} Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry.",Trak: Attributing model behavior at scale.,Trak: Attributing model behavior at scale.,,"[Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry]{park2023trak} Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 
 Trak: Attributing model behavior at scale. 
 \emph{arXiv preprint arXiv:2303.14186}, 2023."
2406.1067,penedo2023refinedweb,"[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.",,"[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 
 The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 
 \emph{arXiv preprint arXiv:2306.01116}, 2023."
2406.1067,rae2021scaling,"[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 \emph{arXiv preprint arXiv:2112.11446}, 2021."
2406.1067,soldaini2024dolma,"[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma} Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.",Dolma: An open corpus of three trillion tokens for language model pretraining research.,Dolma: An open corpus of three trillion tokens for language model pretraining research.,,"[Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar, et~al.]{soldaini2024dolma} Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 
 Dolma: An open corpus of three trillion tokens for language model pretraining research. 
 \emph{arXiv preprint arXiv:2402.00159}, 2024."
2406.1067,welbl2017crowdsourcing,"[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing} Johannes Welbl, Nelson~F Liu, and Matt Gardner.",Crowdsourcing multiple choice science questions.,Crowdsourcing multiple choice science questions.,,"[Welbl et~al.(2017)Welbl, Liu, and Gardner]{welbl2017crowdsourcing} Johannes Welbl, Nelson~F Liu, and Matt Gardner. 
 Crowdsourcing multiple choice science questions. 
 \emph{arXiv preprint arXiv:1707.06209}, 2017."
2406.1067,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2406.11288,aneja2021cosmos,"[{Aneja et~al.(2021)Aneja, Bregler, and Nie{\ss}ner}]{aneja2021cosmos} Shivangi Aneja, Chris Bregler, and Matthias Nie{\ss}ner. 2021.",{COSMOS}: Catching {O}ut-of-{C}ontext {M}isinformation with {S}elf-{S}upervised {L}earning.,{COSMOS}: Catching {O}ut-of-{C}ontext {M}isinformation with {S}elf-{S}upervised {L}earning.,,"[{Aneja et~al.(2021)Aneja, Bregler, and Nie{\ss}ner}]{aneja2021cosmos} Shivangi Aneja, Chris Bregler, and Matthias Nie{\ss}ner. 2021. 
 {COSMOS}: Catching {O}ut-of-{C}ontext {M}isinformation with {S}elf-{S}upervised {L}earning. 
 In \emph{ArXiv preprint arXiv:2101.06278}."
2406.11288,bai2023qwen,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.",Qwen-vl: A frontier large vision-language model with versatile abilities.,Qwen-vl: A frontier large vision-language model with versatile abilities.,,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. 
 Qwen-vl: A frontier large vision-language model with versatile abilities. 
 \emph{arXiv preprint arXiv:2308.12966}."
2406.11288,chowdhery2022palm,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022.",Palm: Scaling language modeling with pathways.,Palm: Scaling language modeling with pathways.,,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022. 
 Palm: Scaling language modeling with pathways. 
 \emph{arXiv preprint arXiv:2204.02311}."
2406.11288,driess2023palm,"[{Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu et~al.}]{driess2023palm} Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al. 2023.",Palm-e: An embodied multimodal language model.,Palm-e: An embodied multimodal language model.,,"[{Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu et~al.}]{driess2023palm} Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al. 2023. 
 Palm-e: An embodied multimodal language model. 
 \emph{arXiv preprint arXiv:2303.03378}."
2406.11288,fu2023mme,"[{Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng et~al.}]{fu2023mme} Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al. 2023.",Mme: A comprehensive evaluation benchmark for multimodal large language models.,Mme: A comprehensive evaluation benchmark for multimodal large language models.,,"[{Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng et~al.}]{fu2023mme} Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al. 2023. 
 Mme: A comprehensive evaluation benchmark for multimodal large language models. 
 \emph{arXiv preprint arXiv:2306.13394}."
2406.11288,gong2023multimodal,"[{Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang, Luo, and Chen}]{gong2023multimodal} Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.",Multimodal-gpt: A vision and language model for dialogue with humans.,Multimodal-gpt: A vision and language model for dialogue with humans.,,"[{Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang, Luo, and Chen}]{gong2023multimodal} Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. 
 Multimodal-gpt: A vision and language model for dialogue with humans. 
 \emph{arXiv preprint arXiv:2305.04790}."
2406.11288,hu2024minicpm,"[{Hu et~al.(2024{\natexlab{a}})Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao et~al.}]{hu2024minicpm} Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al. 2024{\natexlab{a}}.",Minicpm: Unveiling the potential of small language models with scalable training strategies.,Minicpm: Unveiling the potential of small language models with scalable training strategies.,,"[{Hu et~al.(2024{\natexlab{a}})Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao et~al.}]{hu2024minicpm} Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al. 2024{\natexlab{a}}. 
 Minicpm: Unveiling the potential of small language models with scalable training strategies. 
 \emph{arXiv preprint arXiv:2404.06395}."
2406.11288,kadavath2022language,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022.",Language models (mostly) know what they know.,Language models (mostly) know what they know.,,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022. 
 Language models (mostly) know what they know. 
 \emph{arXiv preprint arXiv:2207.05221}."
2406.11288,li2023seed,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}.",Seed-bench: Benchmarking multimodal llms with generative comprehension.,Seed-bench: Benchmarking multimodal llms with generative comprehension.,,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}. 
 Seed-bench: Benchmarking multimodal llms with generative comprehension. 
 \emph{arXiv preprint arXiv:2307.16125}."
2406.11288,lin2024goat,"[{Lin et~al.(2024)Lin, Luo, Wang, Yang, and Ma}]{lin2024goat} Hongzhan Lin, Ziyang Luo, Bo~Wang, Ruichao Yang, and Jing Ma. 2024.",Goat-bench: Safety insights to large multimodal models through meme-based social abuse.,Goat-bench: Safety insights to large multimodal models through meme-based social abuse.,,"[{Lin et~al.(2024)Lin, Luo, Wang, Yang, and Ma}]{lin2024goat} Hongzhan Lin, Ziyang Luo, Bo~Wang, Ruichao Yang, and Jing Ma. 2024. 
 Goat-bench: Safety insights to large multimodal models through meme-based social abuse. 
 \emph{arXiv preprint arXiv:2401.01523}."
2406.11288,lin2022teaching,"[{Lin et~al.(2022{\natexlab{b}})Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022{\natexlab{b}}.",Teaching models to express their uncertainty in words.,Teaching models to express their uncertainty in words.,,"[{Lin et~al.(2022{\natexlab{b}})Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022{\natexlab{b}}. 
 Teaching models to express their uncertainty in words. 
 \emph{arXiv preprint arXiv:2205.14334}."
2406.11288,liu2023mmbench,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023{\natexlab{c}}.",Mmbench: Is your multi-modal model an all-around player?,Mmbench: Is your multi-modal model an all-around player?,,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023{\natexlab{c}}. 
 Mmbench: Is your multi-modal model an all-around player? 
 \emph{arXiv preprint arXiv:2307.06281}."
2406.11288,luo2023wizardcoder,"[{Luo et~al.(2023{\natexlab{b}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023{\natexlab{b}}.",Wizardcoder: Empowering code large language models with evol-instruct.,Wizardcoder: Empowering code large language models with evol-instruct.,,"[{Luo et~al.(2023{\natexlab{b}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023{\natexlab{b}}. 
 Wizardcoder: Empowering code large language models with evol-instruct. 
 \emph{arXiv preprint arXiv:2306.08568}."
2406.11288,powers2020evaluation,[{Powers(2020)}]{powers2020evaluation} David~MW Powers. 2020.,"Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation.","Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation.",,"[{Powers(2020)}]{powers2020evaluation} David~MW Powers. 2020. 
 Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation. 
 \emph{arXiv preprint arXiv:2010.16061}."
2406.11288,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: A family of highly capable multimodal models.,Gemini: A family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: A family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.11288,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.11288,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.11288,wang2023cogvlm,"[{Wang et~al.(2023{\natexlab{b}})Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song et~al.}]{wang2023cogvlm} Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al. 2023{\natexlab{b}}.",Cogvlm: Visual expert for pretrained language models.,Cogvlm: Visual expert for pretrained language models.,,"[{Wang et~al.(2023{\natexlab{b}})Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song et~al.}]{wang2023cogvlm} Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al. 2023{\natexlab{b}}. 
 Cogvlm: Visual expert for pretrained language models. 
 \emph{arXiv preprint arXiv:2311.03079}."
2406.11288,wei2023zero,"[{Wei et~al.(2023)Wei, Cui, Cheng, Wang, Zhang, Huang, Xie, Xu, Chen, Zhang et~al.}]{wei2023zero} Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et~al. 2023.",Zero-shot information extraction via chatting with chatgpt.,Zero-shot information extraction via chatting with chatgpt.,,"[{Wei et~al.(2023)Wei, Cui, Cheng, Wang, Zhang, Huang, Xie, Xu, Chen, Zhang et~al.}]{wei2023zero} Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et~al. 2023. 
 Zero-shot information extraction via chatting with chatgpt. 
 \emph{arXiv preprint arXiv:2302.10205}."
2406.11288,yang2023dawn,"[{Yang et~al.(2023)Yang, Li, Lin, Wang, Lin, Liu, and Wang}]{yang2023dawn} Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023.",The dawn of lmms: Preliminary explorations with gpt-4v (ision).,The dawn of lmms: Preliminary explorations with gpt-4v (ision).,,"[{Yang et~al.(2023)Yang, Li, Lin, Wang, Lin, Liu, and Wang}]{yang2023dawn} Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. 
 The dawn of lmms: Preliminary explorations with gpt-4v (ision). 
 \emph{arXiv preprint arXiv:2309.17421}, 9(1)."
2406.11288,ye2023mplug,"[{Ye et~al.(2023{\natexlab{b}})Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi et~al.}]{ye2023mplug} Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al. 2023{\natexlab{b}}.",mplug-owl: Modularization empowers large language models with multimodality.,mplug-owl: Modularization empowers large language models with multimodality.,,"[{Ye et~al.(2023{\natexlab{b}})Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi et~al.}]{ye2023mplug} Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al. 2023{\natexlab{b}}. 
 mplug-owl: Modularization empowers large language models with multimodality. 
 \emph{arXiv preprint arXiv:2304.14178}."
2406.11288,yin2023lamm,"[{Yin et~al.(2023)Yin, Wang, Cao, Shi, Liu, Li, Sheng, Bai, Huang, Wang et~al.}]{yin2023lamm} Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu~Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et~al. 2023.","Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.","Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.",,"[{Yin et~al.(2023)Yin, Wang, Cao, Shi, Liu, Li, Sheng, Bai, Huang, Wang et~al.}]{yin2023lamm} Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu~Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et~al. 2023. 
 Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. 
 \emph{arXiv preprint arXiv:2306.06687}."
2406.11288,yu2023mm,"[{Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang}]{yu2023mm} Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023.",Mm-vet: Evaluating large multimodal models for integrated capabilities.,Mm-vet: Evaluating large multimodal models for integrated capabilities.,,"[{Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang}]{yu2023mm} Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. 
 Mm-vet: Evaluating large multimodal models for integrated capabilities. 
 \emph{arXiv preprint arXiv:2308.02490}."
2406.11288,zhang2024mm,"[{Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu}]{zhang2024mm} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024.",Mm-llms: Recent advances in multimodal large language models.,Mm-llms: Recent advances in multimodal large language models.,,"[{Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu}]{zhang2024mm} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024. 
 Mm-llms: Recent advances in multimodal large language models. 
 \emph{arXiv preprint arXiv:2401.13601}."
2406.11288,zhu2023minigpt,"[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",Minigpt-4: Enhancing vision-language understanding with advanced large language models.,Minigpt-4: Enhancing vision-language understanding with advanced large language models.,,"[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. 
 Minigpt-4: Enhancing vision-language understanding with advanced large language models. 
 \emph{arXiv preprint arXiv:2304.10592}."
2406.11353,bang2023multitask,"[Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung, et~al.]{bang2023multitask} Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et~al.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.",,"[Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung, et~al.]{bang2023multitask} Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et~al. 
 A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 
 \emph{arXiv preprint arXiv:2302.04023}, 2023."
2406.11353,brown2020language,"[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language} Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.",Language models are few-shot learners.,Language models are few-shot learners.,,"[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language} Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. 
 Language models are few-shot learners. 
 \emph{arXiv preprint arXiv:2005.14165}, 2020."
2406.11353,kaplan2020scaling,"[Kaplan et~al.(2020{\natexlab{a}})Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020{\natexlab{a}})Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020{\natexlab{a}}."
2406.11353,lin2021truthfulqa,"[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa} Lin, S., Hilton, J., and Evans, O.",Truthfulqa: Measuring how models mimic human falsehoods.,Truthfulqa: Measuring how models mimic human falsehoods.,,"[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa} Lin, S., Hilton, J., and Evans, O. 
 Truthfulqa: Measuring how models mimic human falsehoods. 
 \emph{arXiv preprint arXiv:2109.07958}, 2021."
2406.11353,peng2023check,"[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.]{peng2023check} Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et~al.",Check your facts and try again: Improving large language models with external knowledge and automated feedback.,Check your facts and try again: Improving large language models with external knowledge and automated feedback.,,"[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.]{peng2023check} Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et~al. 
 Check your facts and try again: Improving large language models with external knowledge and automated feedback. 
 \emph{arXiv preprint arXiv:2302.12813}, 2023."
2406.11353,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.11353,zhang2023siren,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen, et~al.]{zhang2023siren} Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et~al.",Siren's song in the ai ocean: A survey on hallucination in large language models.,Siren's song in the ai ocean: A survey on hallucination in large language models.,,"[Zhang et~al.(2023{\natexlab{c}})Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen, et~al.]{zhang2023siren} Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., et~al. 
 Siren's song in the ai ocean: A survey on hallucination in large language models. 
 \emph{arXiv preprint arXiv:2309.01219}, 2023{\natexlab{c}}."
2406.11514,gpt4,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.11514,balepur2023s,"[{Balepur et~al.(2023)Balepur, Palta, and Rudinger}]{balepur2023s} Nishant Balepur, Shramay Palta, and Rachel Rudinger. 2023.",It's not easy being wrong: Evaluating process of elimination reasoning in large language models.,It's not easy being wrong: Evaluating process of elimination reasoning in large language models.,,"[{Balepur et~al.(2023)Balepur, Palta, and Rudinger}]{balepur2023s} Nishant Balepur, Shramay Palta, and Rachel Rudinger. 2023. 
 It's not easy being wrong: Evaluating process of elimination reasoning in large language models. 
 \emph{arXiv preprint arXiv:2311.07532}."
2406.11514,bang2023multitask,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.",,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023. 
 A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 
 \emph{arXiv preprint arXiv:2302.04023}."
2406.11514,bhattacharjee2024zero,"[{Bhattacharjee et~al.(2024)Bhattacharjee, Moraffah, Garland, and Liu}]{bhattacharjee2024zero} Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. 2024.",Zero-shot llm-guided counterfactual generation for text.,Zero-shot llm-guided counterfactual generation for text.,,"[{Bhattacharjee et~al.(2024)Bhattacharjee, Moraffah, Garland, and Liu}]{bhattacharjee2024zero} Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. 2024. 
 Zero-shot llm-guided counterfactual generation for text. 
 \emph{arXiv preprint arXiv:2405.04793}."
2406.11514,bubeck2023sparks,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}."
2406.11514,clark2019boolq,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}."
2406.11514,MAD_fact,"[{Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch}]{MAD_fact} Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch. 2023.",Improving factuality and reasoning in language models through multiagent debate.,Improving factuality and reasoning in language models through multiagent debate.,,"[{Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch}]{MAD_fact} Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch. 2023. 
 Improving factuality and reasoning in language models through multiagent debate. 
 \emph{arXiv preprint arXiv:2305.14325}."
2406.11514,feng2024don,"[{Feng et~al.(2024)Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov}]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024.","Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration.","Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration.",,"[{Feng et~al.(2024)Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov}]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024. 
 Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. 
 \emph{arXiv preprint arXiv:2402.00367}."
2406.11514,huang2019cosmos,"[{Huang et~al.(2019)Huang, Bras, Bhagavatula, and Choi}]{huang2019cosmos} Lifu Huang, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.",Cosmos qa: Machine reading comprehension with contextual commonsense reasoning.,Cosmos qa: Machine reading comprehension with contextual commonsense reasoning.,,"[{Huang et~al.(2019)Huang, Bras, Bhagavatula, and Choi}]{huang2019cosmos} Lifu Huang, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. 
 Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. 
 \emph{arXiv preprint arXiv:1909.00277}."
2406.11514,kadavath2022language,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022.",Language models (mostly) know what they know.,Language models (mostly) know what they know.,,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022. 
 Language models (mostly) know what they know. 
 \emph{arXiv preprint arXiv:2207.05221}."
2406.11514,li2024think,"[{Li et~al.(2024)Li, Wang, Feng, Zhu, Wang, and Chua}]{li2024think} Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. 2024.",Think twice before assure: Confidence estimation for large language models through reflection on multiple answers.,Think twice before assure: Confidence estimation for large language models through reflection on multiple answers.,,"[{Li et~al.(2024)Li, Wang, Feng, Zhu, Wang, and Chua}]{li2024think} Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. 2024. 
 Think twice before assure: Confidence estimation for large language models through reflection on multiple answers. 
 \emph{arXiv preprint arXiv:2403.09972}."
2406.11514,MAD_diverse,"[{Liang et~al.(2023)Liang, He, Jiao, Wang, Wang, Wang, Yang, Tu, and Shi}]{MAD_diverse} Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023.",Encouraging divergent thinking in large language models through multi-agent debate.,Encouraging divergent thinking in large language models through multi-agent debate.,,"[{Liang et~al.(2023)Liang, He, Jiao, Wang, Wang, Wang, Yang, Tu, and Shi}]{MAD_diverse} Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. 
 Encouraging divergent thinking in large language models through multi-agent debate. 
 \emph{arXiv preprint arXiv:2305.19118}."
2406.11514,lin2022teaching,"[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.",Teaching models to express their uncertainty in words.,Teaching models to express their uncertainty in words.,,"[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. 
 Teaching models to express their uncertainty in words. 
 \emph{arXiv preprint arXiv:2205.14334}."
2406.11514,liu2023score,"[{Liu et~al.(2023)Liu, Lee, Du, Sanyal, and Zhao}]{liu2023score} Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2023.",Score: A framework for self-contradictory reasoning evaluation.,Score: A framework for self-contradictory reasoning evaluation.,,"[{Liu et~al.(2023)Liu, Lee, Du, Sanyal, and Zhao}]{liu2023score} Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2023. 
 Score: A framework for self-contradictory reasoning evaluation. 
 \emph{arXiv preprint arXiv:2311.09603}."
2406.11514,nguyen2024llms,"[{Nguyen et~al.(2024)Nguyen, Youssef, Schl{\""o}tterer, and Seifert}]{nguyen2024llms} Van~Bach Nguyen, Paul Youssef, J{\""o}rg Schl{\""o}tterer, and Christin Seifert. 2024.",Llms for generating and evaluating counterfactuals: A comprehensive study.,Llms for generating and evaluating counterfactuals: A comprehensive study.,,"[{Nguyen et~al.(2024)Nguyen, Youssef, Schl{\""o}tterer, and Seifert}]{nguyen2024llms} Van~Bach Nguyen, Paul Youssef, J{\""o}rg Schl{\""o}tterer, and Christin Seifert. 2024. 
 Llms for generating and evaluating counterfactuals: A comprehensive study. 
 \emph{arXiv preprint arXiv:2405.00722}."
2406.11514,stechly2023gpt,"[{Stechly et~al.(2023)Stechly, Marquez, and Kambhampati}]{stechly2023gpt} Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023.",Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems.,Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems.,,"[{Stechly et~al.(2023)Stechly, Marquez, and Kambhampati}]{stechly2023gpt} Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. 
 Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. 
 \emph{arXiv preprint arXiv:2310.12397}."
2406.11514,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.11514,valmeekam2023can,"[{Valmeekam et~al.(2023)Valmeekam, Marquez, and Kambhampati}]{valmeekam2023can} Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.",Can large language models really improve by self-critiquing their own plans?,Can large language models really improve by self-critiquing their own plans?,,"[{Valmeekam et~al.(2023)Valmeekam, Marquez, and Kambhampati}]{valmeekam2023can} Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. 
 Can large language models really improve by self-critiquing their own plans? 
 \emph{arXiv preprint arXiv:2310.08118}."
2406.11514,wang2023explainable,[{Wang and Shu(2023)}]{wang2023explainable} Haoran Wang and Kai Shu. 2023.,Explainable claim verification via knowledge-grounded reasoning with large language models.,Explainable claim verification via knowledge-grounded reasoning with large language models.,,"[{Wang and Shu(2023)}]{wang2023explainable} Haoran Wang and Kai Shu. 2023. 
 Explainable claim verification via knowledge-grounded reasoning with large language models. 
 \emph{arXiv preprint arXiv:2310.05253}."
2406.11514,wang2024multi,"[{Wang et~al.(2024{\natexlab{a}})Wang, Wang, Diao, He, Dong, and Xu}]{wang2024multi} Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, and Weiran Xu. 2024{\natexlab{a}}.",Multi-perspective consistency enhances confidence estimation in large language models.,Multi-perspective consistency enhances confidence estimation in large language models.,,"[{Wang et~al.(2024{\natexlab{a}})Wang, Wang, Diao, He, Dong, and Xu}]{wang2024multi} Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, and Weiran Xu. 2024{\natexlab{a}}. 
 Multi-perspective consistency enhances confidence estimation in large language models. 
 \emph{arXiv preprint arXiv:2402.11279}."
2406.11514,wang2024rethinking,"[{Wang et~al.(2024{\natexlab{b}})Wang, Wang, Su, Tong, and Song}]{wang2024rethinking} Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024{\natexlab{b}}.",Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?,Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?,,"[{Wang et~al.(2024{\natexlab{b}})Wang, Wang, Su, Tong, and Song}]{wang2024rethinking} Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024{\natexlab{b}}. 
 Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? 
 \emph{arXiv preprint arXiv:2402.18272}."
2406.11514,xiong2023can,"[{Xiong et~al.(2023)Xiong, Hu, Lu, Li, Fu, He, and Hooi}]{xiong2023can} Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023.",Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms.,Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms.,,"[{Xiong et~al.(2023)Xiong, Hu, Lu, Li, Fu, He, and Hooi}]{xiong2023can} Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. 
 Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. 
 \emph{arXiv preprint arXiv:2306.13063}."
2406.11514,yang2024confidence,"[{Yang et~al.(2024)Yang, Rajagopa, Hayati, Hu, and Kang}]{yang2024confidence} Ruixin Yang, Dheeraj Rajagopa, Shirley~Anugrah Hayati, Bin Hu, and Dongyeop Kang. 2024.",Confidence calibration and rationalization for llms via multi-agent deliberation.,Confidence calibration and rationalization for llms via multi-agent deliberation.,,"[{Yang et~al.(2024)Yang, Rajagopa, Hayati, Hu, and Kang}]{yang2024confidence} Ruixin Yang, Dheeraj Rajagopa, Shirley~Anugrah Hayati, Bin Hu, and Dongyeop Kang. 2024. 
 Confidence calibration and rationalization for llms via multi-agent deliberation. 
 \emph{arXiv preprint arXiv:2404.09127}."
2406.11514,yoran2023answering,"[{Yoran et~al.(2023)Yoran, Wolfson, Bogin, Katz, Deutch, and Berant}]{yoran2023answering} Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023.",Answering questions by meta-reasoning over multiple chains of thought.,Answering questions by meta-reasoning over multiple chains of thought.,,"[{Yoran et~al.(2023)Yoran, Wolfson, Bogin, Katz, Deutch, and Berant}]{yoran2023answering} Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. 
 Answering questions by meta-reasoning over multiple chains of thought. 
 \emph{arXiv preprint arXiv:2304.13007}."
2406.11514,zhang2024calibrating,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Huang, Shi, Guo, Peng, Yan, Zhou, and Qiu}]{zhang2024calibrating} Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, and Xipeng Qiu. 2024{\natexlab{a}}.",Calibrating the confidence of large language models by eliciting fidelity.,Calibrating the confidence of large language models by eliciting fidelity.,,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Huang, Shi, Guo, Peng, Yan, Zhou, and Qiu}]{zhang2024calibrating} Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, and Xipeng Qiu. 2024{\natexlab{a}}. 
 Calibrating the confidence of large language models by eliciting fidelity. 
 \emph{arXiv preprint arXiv:2404.02655}."
2406.11514,zhang2023siren,"[{Zhang et~al.(2023)Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen et~al.}]{zhang2023siren} Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al. 2023.",Siren's song in the ai ocean: a survey on hallucination in large language models.,Siren's song in the ai ocean: a survey on hallucination in large language models.,,"[{Zhang et~al.(2023)Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen et~al.}]{zhang2023siren} Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al. 2023. 
 Siren's song in the ai ocean: a survey on hallucination in large language models. 
 \emph{arXiv preprint arXiv:2309.01219}."
2406.11514,zhao2023survey,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023.",A survey of large language models.,A survey of large language models.,,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}."
2406.11514,zheng2023does,"[{Zheng et~al.(2023)Zheng, Huang, and Chang}]{zheng2023does} Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023.",Why does chatgpt fall short in answering questions faithfully?,Why does chatgpt fall short in answering questions faithfully?,,"[{Zheng et~al.(2023)Zheng, Huang, and Chang}]{zheng2023does} Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. 
 Why does chatgpt fall short in answering questions faithfully? 
 \emph{arXiv preprint arXiv:2304.10513}."
2406.11801,meng2022memit,"[{Meng et~al.(2022{\natexlab{b}})Meng, Sen~Sharma, Andonian, Belinkov, and Bau}]{meng2022memit} Kevin Meng, Arnab Sen~Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022{\natexlab{b}}.",Mass editing memory in a transformer.,Mass editing memory in a transformer.,,"[{Meng et~al.(2022{\natexlab{b}})Meng, Sen~Sharma, Andonian, Belinkov, and Bau}]{meng2022memit} Kevin Meng, Arnab Sen~Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022{\natexlab{b}}. 
 Mass editing memory in a transformer. 
 \emph{arXiv preprint arXiv:2210.07229}."
2406.11817,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2406.11817,dubois2024length,"[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto}]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 2024.",Length-controlled alpacaeval: A simple way to debias automatic evaluators.,Length-controlled alpacaeval: A simple way to debias automatic evaluators.,,"[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto}]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 2024. 
 Length-controlled alpacaeval: A simple way to debias automatic evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}."
2406.11817,fisch2024robust,"[{Fisch et~al.(2024)Fisch, Eisenstein, Zayats, Agarwal, Beirami, Nagpal, Shaw, and Berant}]{fisch2024robust} Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan Berant. 2024.",Robust preference optimization through reward model distillation.,Robust preference optimization through reward model distillation.,,"[{Fisch et~al.(2024)Fisch, Eisenstein, Zayats, Agarwal, Beirami, Nagpal, Shaw, and Berant}]{fisch2024robust} Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan Berant. 2024. 
 Robust preference optimization through reward model distillation. 
 \emph{arXiv preprint arXiv:2405.19316}."
2406.11817,guo2024direct,"[{Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot et~al.}]{guo2024direct} Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et~al. 2024.",Direct language model alignment from online ai feedback.,Direct language model alignment from online ai feedback.,,"[{Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot et~al.}]{guo2024direct} Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et~al. 2024. 
 Direct language model alignment from online ai feedback. 
 \emph{arXiv preprint arXiv:2402.04792}."
2406.11817,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.11817,kingma2014adam,[{Kingma and Ba(2014)}]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 2014.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[{Kingma and Ba(2014)}]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 2014. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}."
2406.11817,lambert2024rewardbench,"[{Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi et~al.}]{lambert2024rewardbench} Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et~al. 2024.",Rewardbench: Evaluating reward models for language modeling.,Rewardbench: Evaluating reward models for language modeling.,,"[{Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu, Dziri, Kumar, Zick, Choi et~al.}]{lambert2024rewardbench} Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et~al. 2024. 
 Rewardbench: Evaluating reward models for language modeling. 
 \emph{arXiv preprint arXiv:2403.13787}."
2406.11817,meng2024simpo,"[{Meng et~al.(2024)Meng, Xia, and Chen}]{meng2024simpo} Yu~Meng, Mengzhou Xia, and Danqi Chen. 2024.",Simpo: Simple preference optimization with a reference-free reward.,Simpo: Simple preference optimization with a reference-free reward.,,"[{Meng et~al.(2024)Meng, Xia, and Chen}]{meng2024simpo} Yu~Meng, Mengzhou Xia, and Danqi Chen. 2024. 
 Simpo: Simple preference optimization with a reference-free reward. 
 \emph{arXiv preprint arXiv:2405.14734}."
2406.11817,park2024disentangling,"[{Park et~al.(2024)Park, Rafailov, Ermon, and Finn}]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 2024.",Disentangling length from quality in direct preference optimization.,Disentangling length from quality in direct preference optimization.,,"[{Park et~al.(2024)Park, Rafailov, Ermon, and Finn}]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 2024. 
 Disentangling length from quality in direct preference optimization. 
 \emph{arXiv preprint arXiv:2403.19159}."
2406.11817,wang2023openchat,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023.",Openchat: Advancing open-source language models with mixed-quality data.,Openchat: Advancing open-source language models with mixed-quality data.,,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. 
 Openchat: Advancing open-source language models with mixed-quality data. 
 \emph{arXiv preprint arXiv:2309.11235}."
2406.11817,xu2023some,"[{Xu et~al.(2023)Xu, Lee, Sukhbaatar, and Weston}]{xu2023some} Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. 2023.",Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,,"[{Xu et~al.(2023)Xu, Lee, Sukhbaatar, and Weston}]{xu2023some} Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. 2023. 
 Some things are more cringe than others: Preference optimization with the pairwise cringe loss. 
 \emph{arXiv preprint arXiv:2312.16682}."
2406.11817,xu2024dpo,"[{Xu et~al.(2024)Xu, Fu, Gao, Ye, Liu, Mei, Wang, Yu, and Wu}]{xu2024dpo} Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi~Wu. 2024.",Is dpo superior to ppo for llm alignment? a comprehensive study.,Is dpo superior to ppo for llm alignment? a comprehensive study.,,"[{Xu et~al.(2024)Xu, Fu, Gao, Ye, Liu, Mei, Wang, Yu, and Wu}]{xu2024dpo} Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi~Wu. 2024. 
 Is dpo superior to ppo for llm alignment? a comprehensive study. 
 \emph{arXiv preprint arXiv:2404.10719}."
2406.11817,yuan2024self,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston}]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024.",Self-rewarding language models.,Self-rewarding language models.,,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston}]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. 
 Self-rewarding language models. 
 \emph{arXiv preprint arXiv:2401.10020}."
2406.11817,zhou2023beyond,"[{Zhou et~al.(2023)Zhou, Liu, Yang, Shao, Liu, Yue, Ouyang, and Qiao}]{zhou2023beyond} Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu~Liu, Xiangyu Yue, Wanli Ouyang, and Yu~Qiao. 2023.",Beyond one-preference-for-all: Multi-objective direct preference optimization.,Beyond one-preference-for-all: Multi-objective direct preference optimization.,,"[{Zhou et~al.(2023)Zhou, Liu, Yang, Shao, Liu, Yue, Ouyang, and Qiao}]{zhou2023beyond} Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu~Liu, Xiangyu Yue, Wanli Ouyang, and Yu~Qiao. 2023. 
 Beyond one-preference-for-all: Multi-objective direct preference optimization. 
 \emph{arXiv preprint arXiv:2310.03708}."
2406.11978,abdulhai2023lmrl,"[Abdulhai et~al., 2023]{abdulhai2023lmrl} Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y., Xu, K., and Levine, S. (2023).",Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models.,Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models.,,"[Abdulhai et~al., 2023]{abdulhai2023lmrl} Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y., Xu, K., and Levine, S. (2023). 
 Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. 
 {\em arXiv preprint arXiv:2311.18232}."
2406.11978,andrychowicz2020matters,"[Andrychowicz et~al., 2020]{andrychowicz2020matters} Andrychowicz, M., Raichuk, A., Sta{\'n}czyk, P., Orsini, M., Girgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et~al. (2020).",What matters in on-policy reinforcement learning? a large-scale empirical study.,What matters in on-policy reinforcement learning? a large-scale empirical study.,,"[Andrychowicz et~al., 2020]{andrychowicz2020matters} Andrychowicz, M., Raichuk, A., Sta{\'n}czyk, P., Orsini, M., Girgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et~al. (2020). 
 What matters in on-policy reinforcement learning? a large-scale empirical study. 
 {\em arXiv preprint arXiv:2006.05990}."
2406.11978,andukuri2024star,"[Andukuri et~al., 2024]{andukuri2024star} Andukuri, C., Fr{\""a}nken, J.-P., Gerstenberg, T., and Goodman, N.~D. (2024).",Star-gate: Teaching language models to ask clarifying questions.,Star-gate: Teaching language models to ask clarifying questions.,,"[Andukuri et~al., 2024]{andukuri2024star} Andukuri, C., Fr{\""a}nken, J.-P., Gerstenberg, T., and Goodman, N.~D. (2024). 
 Star-gate: Teaching language models to ask clarifying questions. 
 {\em arXiv preprint arXiv:2403.19154}."
2406.11978,avitan2024changed,"[Avitan et~al., 2024]{avitan2024changed} Avitan, M., Cotterell, R., Goldberg, Y., and Ravfogel, S. (2024).",What changed? converting representational interventions to natural language.,What changed? converting representational interventions to natural language.,,"[Avitan et~al., 2024]{avitan2024changed} Avitan, M., Cotterell, R., Goldberg, Y., and Ravfogel, S. (2024). 
 What changed? converting representational interventions to natural language. 
 {\em arXiv preprint arXiv:2402.11355}."
2406.11978,bianchi2024well,"[Bianchi et~al., 2024]{bianchi2024well} Bianchi, F., Chia, P.~J., Yuksekgonul, M., Tagliabue, J., Jurafsky, D., and Zou, J. (2024).",How well can llms negotiate? negotiationarena platform and analysis.,How well can llms negotiate? negotiationarena platform and analysis.,,"[Bianchi et~al., 2024]{bianchi2024well} Bianchi, F., Chia, P.~J., Yuksekgonul, M., Tagliabue, J., Jurafsky, D., and Zou, J. (2024). 
 How well can llms negotiate? negotiationarena platform and analysis. 
 {\em arXiv preprint arXiv:2402.05863}."
2406.11978,chao2023jailbreaking,"[Chao et~al., 2023]{chao2023jailbreaking} Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.~J., and Wong, E. (2023).",Jailbreaking black box large language models in twenty queries.,Jailbreaking black box large language models in twenty queries.,,"[Chao et~al., 2023]{chao2023jailbreaking} Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.~J., and Wong, E. (2023). 
 Jailbreaking black box large language models in twenty queries. 
 {\em arXiv preprint arXiv:2310.08419}."
2406.11978,clive2021control,"[Clive et~al., 2021]{clive2021control} Clive, J., Cao, K., and Rei, M. (2021).",Control prefixes for parameter-efficient text generation.,Control prefixes for parameter-efficient text generation.,,"[Clive et~al., 2021]{clive2021control} Clive, J., Cao, K., and Rei, M. (2021). 
 Control prefixes for parameter-efficient text generation. 
 {\em arXiv preprint arXiv:2110.08329}."
2406.11978,dathathri2019plug,"[Dathathri et~al., 2019]{dathathri2019plug} Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. (2019).",Plug and play language models: A simple approach to controlled text generation.,Plug and play language models: A simple approach to controlled text generation.,,"[Dathathri et~al., 2019]{dathathri2019plug} Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. (2019). 
 Plug and play language models: A simple approach to controlled text generation. 
 {\em arXiv preprint arXiv:1912.02164}."
2406.11978,ganguli2022red,"[Ganguli et~al., 2022]{ganguli2022red} Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et~al. (2022).","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.","Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",,"[Ganguli et~al., 2022]{ganguli2022red} Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et~al. (2022). 
 Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. 
 {\em arXiv preprint arXiv:2209.07858}."
2406.11978,he2018decoupling,"[He et~al., 2018]{he2018decoupling} He, H., Chen, D., Balakrishnan, A., and Liang, P. (2018).",Decoupling strategy and generation in negotiation dialogues.,Decoupling strategy and generation in negotiation dialogues.,,"[He et~al., 2018]{he2018decoupling} He, H., Chen, D., Balakrishnan, A., and Liang, P. (2018). 
 Decoupling strategy and generation in negotiation dialogues. 
 {\em arXiv preprint arXiv:1808.09637}."
2406.11978,hong2023zero,"[Hong et~al., 2023]{hong2023zero} Hong, J., Levine, S., and Dragan, A. (2023).",Zero-shot goal-directed dialogue via rl on imagined conversations.,Zero-shot goal-directed dialogue via rl on imagined conversations.,,"[Hong et~al., 2023]{hong2023zero} Hong, J., Levine, S., and Dragan, A. (2023). 
 Zero-shot goal-directed dialogue via rl on imagined conversations. 
 {\em arXiv preprint arXiv:2311.05584}."
2406.11978,jain2023baseline,"[Jain et~al., 2023]{jain2023baseline} Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., Chiang, P.-y., Goldblum, M., Saha, A., Geiping, J., and Goldstein, T. (2023).",Baseline defenses for adversarial attacks against aligned language models.,Baseline defenses for adversarial attacks against aligned language models.,,"[Jain et~al., 2023]{jain2023baseline} Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., Chiang, P.-y., Goldblum, M., Saha, A., Geiping, J., and Goldstein, T. (2023). 
 Baseline defenses for adversarial attacks against aligned language models. 
 {\em arXiv preprint arXiv:2309.00614}."
2406.11978,jaques2019way,"[Jaques et~al., 2019]{jaques2019way} Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. (2019).",Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.,Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.,,"[Jaques et~al., 2019]{jaques2019way} Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. (2019). 
 Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. 
 {\em arXiv preprint arXiv:1907.00456}."
2406.11978,kang2019recommendation,"[Kang et~al., 2019]{kang2019recommendation} Kang, D., Balakrishnan, A., Shah, P., Crook, P., Boureau, Y.-L., and Weston, J. (2019).",Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue.,Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue.,,"[Kang et~al., 2019]{kang2019recommendation} Kang, D., Balakrishnan, A., Shah, P., Crook, P., Boureau, Y.-L., and Weston, J. (2019). 
 Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue. 
 {\em arXiv preprint arXiv:1909.03922}."
2406.11978,krause2020gedi,"[Krause et~al., 2020]{krause2020gedi} Krause, B., Gotmare, A.~D., McCann, B., Keskar, N.~S., Joty, S., Socher, R., and Rajani, N.~F. (2020).",Gedi: Generative discriminator guided sequence generation.,Gedi: Generative discriminator guided sequence generation.,,"[Krause et~al., 2020]{krause2020gedi} Krause, B., Gotmare, A.~D., McCann, B., Keskar, N.~S., Joty, S., Socher, R., and Rajani, N.~F. (2020). 
 Gedi: Generative discriminator guided sequence generation. 
 {\em arXiv preprint arXiv:2009.06367}."
2406.11978,kwon2023reward,"[Kwon et~al., 2023]{kwon2023reward} Kwon, M., Xie, S.~M., Bullard, K., and Sadigh, D. (2023).",Reward design with language models.,Reward design with language models.,,"[Kwon et~al., 2023]{kwon2023reward} Kwon, M., Xie, S.~M., Bullard, K., and Sadigh, D. (2023). 
 Reward design with language models. 
 {\em arXiv preprint arXiv:2303.00001}."
2406.11978,lee2022evaluating,"[Lee et~al., 2022]{lee2022evaluating} Lee, M., Srivastava, M., Hardy, A., Thickstun, J., Durmus, E., Paranjape, A., Gerard-Ursin, I., Li, X.~L., Ladhak, F., Rong, F., et~al. (2022).",Evaluating human-language model interaction.,Evaluating human-language model interaction.,,"[Lee et~al., 2022]{lee2022evaluating} Lee, M., Srivastava, M., Hardy, A., Thickstun, J., Durmus, E., Paranjape, A., Gerard-Ursin, I., Li, X.~L., Ladhak, F., Rong, F., et~al. (2022). 
 Evaluating human-language model interaction. 
 {\em arXiv preprint arXiv:2212.09746}."
2406.11978,levine2020offline,"[Levine et~al., 2020]{levine2020offline} Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).","Offline reinforcement learning: Tutorial, review, and perspectives on open problems.","Offline reinforcement learning: Tutorial, review, and perspectives on open problems.",,"[Levine et~al., 2020]{levine2020offline} Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). 
 Offline reinforcement learning: Tutorial, review, and perspectives on open problems. 
 {\em arXiv preprint arXiv:2005.01643}."
2406.11978,lewis2017deal,"[Lewis et~al., 2017]{lewis2017deal} Lewis, M., Yarats, D., Dauphin, Y.~N., Parikh, D., and Batra, D. (2017).",Deal or no deal? end-to-end learning for negotiation dialogues.,Deal or no deal? end-to-end learning for negotiation dialogues.,,"[Lewis et~al., 2017]{lewis2017deal} Lewis, M., Yarats, D., Dauphin, Y.~N., Parikh, D., and Batra, D. (2017). 
 Deal or no deal? end-to-end learning for negotiation dialogues. 
 {\em arXiv preprint arXiv:1706.05125}."
2406.11978,li2016deep,"[Li et~al., 2016]{li2016deep} Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016).",Deep reinforcement learning for dialogue generation.,Deep reinforcement learning for dialogue generation.,,"[Li et~al., 2016]{li2016deep} Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016). 
 Deep reinforcement learning for dialogue generation. 
 {\em arXiv preprint arXiv:1606.01541}."
2406.11978,li2024measuring,"[Li et~al., 2024a]{li2024measuring} Li, K., Liu, T., Bashkansky, N., Bau, D., Vi{\'e}gas, F., Pfister, H., and Wattenberg, M. (2024a).",Measuring and controlling persona drift in language model dialogs.,Measuring and controlling persona drift in language model dialogs.,,"[Li et~al., 2024a]{li2024measuring} Li, K., Liu, T., Bashkansky, N., Bau, D., Vi{\'e}gas, F., Pfister, H., and Wattenberg, M. (2024a). 
 Measuring and controlling persona drift in language model dialogs. 
 {\em arXiv preprint arXiv:2402.10962}."
2406.11978,li2024social,"[Li et~al., 2024c]{li2024social} Li, M., Shi, W., Ziems, C., and Yang, D. (2024c).",Social intelligence data infrastructure: Structuring the present and navigating the future.,Social intelligence data infrastructure: Structuring the present and navigating the future.,,"[Li et~al., 2024c]{li2024social} Li, M., Shi, W., Ziems, C., and Yang, D. (2024c). 
 Social intelligence data infrastructure: Structuring the present and navigating the future. 
 {\em arXiv preprint arXiv:2403.14659}."
2406.11978,li2021prefix,"[Li and Liang, 2021]{li2021prefix} Li, X.~L. and Liang, P. (2021).",Prefix-tuning: Optimizing continuous prompts for generation.,Prefix-tuning: Optimizing continuous prompts for generation.,,"[Li and Liang, 2021]{li2021prefix} Li, X.~L. and Liang, P. (2021). 
 Prefix-tuning: Optimizing continuous prompts for generation. 
 {\em arXiv preprint arXiv:2101.00190}."
2406.11978,lightman2023let,"[Lightman et~al., 2023]{lightman2023let} Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. (2023).",Let's verify step by step.,Let's verify step by step.,,"[Lightman et~al., 2023]{lightman2023let} Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. (2023). 
 Let's verify step by step. 
 {\em arXiv preprint arXiv:2305.20050}."
2406.11978,lin2023decision,"[Lin et~al., 2023]{lin2023decision} Lin, J., Tomlin, N., Andreas, J., and Eisner, J. (2023).",Decision-oriented dialogue for human-ai collaboration.,Decision-oriented dialogue for human-ai collaboration.,,"[Lin et~al., 2023]{lin2023decision} Lin, J., Tomlin, N., Andreas, J., and Eisner, J. (2023). 
 Decision-oriented dialogue for human-ai collaboration. 
 {\em arXiv preprint arXiv:2305.20076}."
2406.11978,mazeika2024harmbench,"[Mazeika et~al., 2024]{mazeika2024harmbench} Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., et~al. (2024).",Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,,"[Mazeika et~al., 2024]{mazeika2024harmbench} Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., et~al. (2024). 
 Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. 
 {\em arXiv preprint arXiv:2402.04249}."
2406.11978,nakano2021webgpt,"[Nakano et~al., 2021]{nakano2021webgpt} Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et~al. (2021).",Webgpt: Browser-assisted question-answering with human feedback.,Webgpt: Browser-assisted question-answering with human feedback.,,"[Nakano et~al., 2021]{nakano2021webgpt} Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et~al. (2021). 
 Webgpt: Browser-assisted question-answering with human feedback. 
 {\em arXiv preprint arXiv:2112.09332}."
2406.11978,russinovich2024great,"[Russinovich et~al., 2024]{russinovich2024great} Russinovich, M., Salem, A., and Eldan, R. (2024).","Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.","Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",,"[Russinovich et~al., 2024]{russinovich2024great} Russinovich, M., Salem, A., and Eldan, R. (2024). 
 Great, now write an article about that: The crescendo multi-turn llm jailbreak attack. 
 {\em arXiv preprint arXiv:2404.01833}."
2406.11978,sharma2024critical,"[Sharma et~al., 2024]{sharma2024critical} Sharma, A., Keh, S., Mitchell, E., Finn, C., Arora, K., and Kollar, T. (2024).",A critical evaluation of ai feedback for aligning large language models.,A critical evaluation of ai feedback for aligning large language models.,,"[Sharma et~al., 2024]{sharma2024critical} Sharma, A., Keh, S., Mitchell, E., Finn, C., Arora, K., and Kollar, T. (2024). 
 A critical evaluation of ai feedback for aligning large language models. 
 {\em arXiv preprint arXiv:2402.12366}."
2406.11978,snell2022offline,"[Snell et~al., 2022]{snell2022offline} Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S. (2022).",Offline rl for natural language generation with implicit language q learning.,Offline rl for natural language generation with implicit language q learning.,,"[Snell et~al., 2022]{snell2022offline} Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S. (2022). 
 Offline rl for natural language generation with implicit language q learning. 
 {\em arXiv preprint arXiv:2206.11871}."
2406.11978,souly2024strongreject,"[Souly et~al., 2024]{souly2024strongreject} Souly, A., Lu, Q., Bowen, D., Trinh, T., Hsieh, E., Pandey, S., Abbeel, P., Svegliato, J., Emmons, S., Watkins, O., et~al. (2024).",A strongreject for empty jailbreaks.,A strongreject for empty jailbreaks.,,"[Souly et~al., 2024]{souly2024strongreject} Souly, A., Lu, Q., Bowen, D., Trinh, T., Hsieh, E., Pandey, S., Abbeel, P., Svegliato, J., Emmons, S., Watkins, O., et~al. (2024). 
 A strongreject for empty jailbreaks. 
 {\em arXiv preprint arXiv:2402.10260}."
2406.11978,subramani2022extracting,"[Subramani et~al., 2022]{subramani2022extracting} Subramani, N., Suresh, N., and Peters, M.~E. (2022).",Extracting latent steering vectors from pretrained language models.,Extracting latent steering vectors from pretrained language models.,,"[Subramani et~al., 2022]{subramani2022extracting} Subramani, N., Suresh, N., and Peters, M.~E. (2022). 
 Extracting latent steering vectors from pretrained language models. 
 {\em arXiv preprint arXiv:2205.05124}."
2406.11978,tajwar2024preference,"[Tajwar et~al., 2024]{tajwar2024preference} Tajwar, F., Singh, A., Sharma, A., Rafailov, R., Schneider, J., Xie, T., Ermon, S., Finn, C., and Kumar, A. (2024).","Preference fine-tuning of llms should leverage suboptimal, on-policy data.","Preference fine-tuning of llms should leverage suboptimal, on-policy data.",,"[Tajwar et~al., 2024]{tajwar2024preference} Tajwar, F., Singh, A., Sharma, A., Rafailov, R., Schneider, J., Xie, T., Ermon, S., Finn, C., and Kumar, A. (2024). 
 Preference fine-tuning of llms should leverage suboptimal, on-policy data. 
 {\em arXiv preprint arXiv:2404.14367}."
2406.11978,tamkin2022task,"[Tamkin et~al., 2022]{tamkin2022task} Tamkin, A., Handa, K., Shrestha, A., and Goodman, N. (2022).",Task ambiguity in humans and language models.,Task ambiguity in humans and language models.,,"[Tamkin et~al., 2022]{tamkin2022task} Tamkin, A., Handa, K., Shrestha, A., and Goodman, N. (2022). 
 Task ambiguity in humans and language models. 
 {\em arXiv preprint arXiv:2212.10711}."
2406.11978,tang2019target,"[Tang et~al., 2019]{tang2019target} Tang, J., Zhao, T., Xiong, C., Liang, X., Xing, E.~P., and Hu, Z. (2019).",Target-guided open-domain conversation.,Target-guided open-domain conversation.,,"[Tang et~al., 2019]{tang2019target} Tang, J., Zhao, T., Xiong, C., Liang, X., Xing, E.~P., and Hu, Z. (2019). 
 Target-guided open-domain conversation. 
 {\em arXiv preprint arXiv:1905.11553}."
2406.11978,timkey2021all,"[Timkey and Van~Schijndel, 2021]{timkey2021all} Timkey, W. and Van~Schijndel, M. (2021).",All bark and no bite: Rogue dimensions in transformer language models obscure representational quality.,All bark and no bite: Rogue dimensions in transformer language models obscure representational quality.,,"[Timkey and Van~Schijndel, 2021]{timkey2021all} Timkey, W. and Van~Schijndel, M. (2021). 
 All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. 
 {\em arXiv preprint arXiv:2109.04404}."
2406.11978,verma2022chai,"[Verma et~al., 2022]{verma2022chai} Verma, S., Fu, J., Yang, M., and Levine, S. (2022).",Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning.,Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning.,,"[Verma et~al., 2022]{verma2022chai} Verma, S., Fu, J., Yang, M., and Levine, S. (2022). 
 Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. 
 {\em arXiv preprint arXiv:2204.08426}."
2406.11978,wang2023rolellm,"[Wang et~al., 2023]{wang2023rolellm} Wang, Z.~M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Zhang, M., et~al. (2023).","Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.","Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.",,"[Wang et~al., 2023]{wang2023rolellm} Wang, Z.~M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Zhang, M., et~al. (2023). 
 Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 
 {\em arXiv preprint arXiv:2310.00746}."
2406.11978,yang2024social,"[Yang et~al., 2024a]{yang2024social} Yang, D., Ziems, C., Held, W., Shaikh, O., Bernstein, M.~S., and Mitchell, J. (2024a).",Social skill training with large language models.,Social skill training with large language models.,,"[Yang et~al., 2024a]{yang2024social} Yang, D., Ziems, C., Held, W., Shaikh, O., Bernstein, M.~S., and Mitchell, J. (2024a). 
 Social skill training with large language models. 
 {\em arXiv preprint arXiv:2404.04204}."
2406.11978,yang2024chain,"[Yang et~al., 2024b]{yang2024chain} Yang, X., Tang, X., Hu, S., and Han, J. (2024b).",Chain of attack: a semantic-driven contextual multi-turn attacker for llm.,Chain of attack: a semantic-driven contextual multi-turn attacker for llm.,,"[Yang et~al., 2024b]{yang2024chain} Yang, X., Tang, X., Hu, S., and Han, J. (2024b). 
 Chain of attack: a semantic-driven contextual multi-turn attacker for llm. 
 {\em arXiv preprint arXiv:2405.05610}."
2406.11978,zeng2024johnny,"[Zeng et~al., 2024]{zeng2024johnny} Zeng, Y., Lin, H., Zhang, J., Yang, D., Jia, R., and Shi, W. (2024).",How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.,How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.,,"[Zeng et~al., 2024]{zeng2024johnny} Zeng, Y., Lin, H., Zhang, J., Yang, D., Jia, R., and Shi, W. (2024). 
 How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. 
 {\em arXiv preprint arXiv:2401.06373}."
2406.11978,zhou2023sotopia,"[Zhou et~al., 2023]{zhou2023sotopia} Zhou, X., Zhu, H., Mathur, L., Zhang, R., Yu, H., Qi, Z., Morency, L.-P., Bisk, Y., Fried, D., Neubig, G., et~al. (2023).",Sotopia: Interactive evaluation for social intelligence in language agents.,Sotopia: Interactive evaluation for social intelligence in language agents.,,"[Zhou et~al., 2023]{zhou2023sotopia} Zhou, X., Zhu, H., Mathur, L., Zhang, R., Yu, H., Qi, Z., Morency, L.-P., Bisk, Y., Fried, D., Neubig, G., et~al. (2023). 
 Sotopia: Interactive evaluation for social intelligence in language agents. 
 {\em arXiv preprint arXiv:2310.11667}."
2406.11978,zhou2024archer,"[Zhou et~al., 2024]{zhou2024archer} Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A. (2024).",Archer: Training language model agents via hierarchical multi-turn rl.,Archer: Training language model agents via hierarchical multi-turn rl.,,"[Zhou et~al., 2024]{zhou2024archer} Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A. (2024). 
 Archer: Training language model agents via hierarchical multi-turn rl. 
 {\em arXiv preprint arXiv:2402.19446}."
2406.11978,ziegler2019fine,"[Ziegler et~al., 2019]{ziegler2019fine} Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019).",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[Ziegler et~al., 2019]{ziegler2019fine} Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019). 
 Fine-tuning language models from human preferences. 
 {\em arXiv preprint arXiv:1909.08593}."
2406.11978,zou2023universal,"[Zou et~al., 2023]{zou2023universal} Zou, A., Wang, Z., Kolter, J.~Z., and Fredrikson, M. (2023).",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[Zou et~al., 2023]{zou2023universal} Zou, A., Wang, Z., Kolter, J.~Z., and Fredrikson, M. (2023). 
 Universal and transferable adversarial attacks on aligned language models. 
 {\em arXiv preprint arXiv:2307.15043}."
2406.12169,adeyemi2023zero,"[{Adeyemi et~al.(2023)Adeyemi, Oladipo, Pradeep, and Lin}]{adeyemi2023zero} Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, and Jimmy Lin. 2023.",Zero-shot cross-lingual reranking with large language models for low-resource languages.,Zero-shot cross-lingual reranking with large language models for low-resource languages.,,"[{Adeyemi et~al.(2023)Adeyemi, Oladipo, Pradeep, and Lin}]{adeyemi2023zero} Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, and Jimmy Lin. 2023. 
 Zero-shot cross-lingual reranking with large language models for low-resource languages. 
 \emph{arXiv preprint arXiv:2312.16159}."
2406.12169,agarwal2023gkd,"[{Agarwal et~al.(2023)Agarwal, Vieillard, Stanczyk, Ramos, Geist, and Bachem}]{agarwal2023gkd} Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023.",Gkd: Generalized knowledge distillation for auto-regressive sequence models.,Gkd: Generalized knowledge distillation for auto-regressive sequence models.,,"[{Agarwal et~al.(2023)Agarwal, Vieillard, Stanczyk, Ramos, Geist, and Bachem}]{agarwal2023gkd} Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. 
 Gkd: Generalized knowledge distillation for auto-regressive sequence models. 
 \emph{arXiv preprint arXiv:2306.13649}."
2406.12169,he2022metric,"[{He et~al.(2022)He, Gong, Jin, Qi, Zhang, Jiao, Zhou, Cheng, Yiu, Duan et~al.}]{he2022metric} Xingwei He, Yeyun Gong, A~Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, Siu~Ming Yiu, Nan Duan, et~al. 2022.",Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense reasoning.,Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense reasoning.,,"[{He et~al.(2022)He, Gong, Jin, Qi, Zhang, Jiao, Zhou, Cheng, Yiu, Duan et~al.}]{he2022metric} Xingwei He, Yeyun Gong, A~Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, Siu~Ming Yiu, Nan Duan, et~al. 2022. 
 Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense reasoning. 
 \emph{arXiv preprint arXiv:2210.11708}."
2406.12169,hinton2015distilling,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.",Distilling the knowledge in a neural network.,Distilling the knowledge in a neural network.,,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. 
 Distilling the knowledge in a neural network. 
 \emph{arXiv preprint arXiv:1503.02531}."
2406.12169,ho2022large,"[{Ho et~al.(2022)Ho, Schmid, and Yun}]{ho2022large} Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.",Large language models are reasoning teachers.,Large language models are reasoning teachers.,,"[{Ho et~al.(2022)Ho, Schmid, and Yun}]{ho2022large} Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. 
 Large language models are reasoning teachers. 
 \emph{arXiv preprint arXiv:2212.10071}."
2406.12169,hsieh2023distilling,"[{Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister}]{hsieh2023distilling} Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.",Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.,Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.,,"[{Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister}]{hsieh2023distilling} Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. 
 Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. 
 \emph{arXiv preprint arXiv:2305.02301}."
2406.12169,khaliq2024ragar,"[{Khaliq et~al.(2024)Khaliq, Chang, Ma, Pflugfelder, and Mileti{\'c}}]{khaliq2024ragar} M~Abdul Khaliq, P~Chang, M~Ma, Bernhard Pflugfelder, and F~Mileti{\'c}. 2024.","Ragar, your falsehood radar: Rag-augmented reasoning for political fact-checking using multimodal large language models.","Ragar, your falsehood radar: Rag-augmented reasoning for political fact-checking using multimodal large language models.",,"[{Khaliq et~al.(2024)Khaliq, Chang, Ma, Pflugfelder, and Mileti{\'c}}]{khaliq2024ragar} M~Abdul Khaliq, P~Chang, M~Ma, Bernhard Pflugfelder, and F~Mileti{\'c}. 2024. 
 Ragar, your falsehood radar: Rag-augmented reasoning for political fact-checking using multimodal large language models. 
 \emph{arXiv preprint arXiv:2404.12065}."
2406.12169,li2022explanations,"[{Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao et~al.}]{li2022explanations} Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi~Mao, et~al. 2022.",Explanations from large language models make small reasoners better.,Explanations from large language models make small reasoners better.,,"[{Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao et~al.}]{li2022explanations} Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi~Mao, et~al. 2022. 
 Explanations from large language models make small reasoners better. 
 \emph{arXiv preprint arXiv:2210.06726}."
2406.12169,ma2023fine,"[{Ma et~al.(2023)Ma, Wang, Yang, Wei, and Lin}]{ma2023fine} Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023.",Fine-tuning llama for multi-stage text retrieval.,Fine-tuning llama for multi-stage text retrieval.,,"[{Ma et~al.(2023)Ma, Wang, Yang, Wei, and Lin}]{ma2023fine} Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. 
 Fine-tuning llama for multi-stage text retrieval. 
 \emph{arXiv preprint arXiv:2310.08319}."
2406.12169,min2022nonparametric,"[{Min et~al.(2022)Min, Shi, Lewis, Chen, Yih, Hajishirzi, and Zettlemoyer}]{min2022nonparametric} Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.",Nonparametric masked language modeling.,Nonparametric masked language modeling.,,"[{Min et~al.(2022)Min, Shi, Lewis, Chen, Yih, Hajishirzi, and Zettlemoyer}]{min2022nonparametric} Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. 
 Nonparametric masked language modeling. 
 \emph{arXiv preprint arXiv:2212.01349}."
2406.12169,nogueira2020document,"[{Nogueira et~al.(2020)Nogueira, Jiang, and Lin}]{nogueira2020document} Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.",Document ranking with a pretrained sequence-to-sequence model.,Document ranking with a pretrained sequence-to-sequence model.,,"[{Nogueira et~al.(2020)Nogueira, Jiang, and Lin}]{nogueira2020document} Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. 
 Document ranking with a pretrained sequence-to-sequence model. 
 \emph{arXiv preprint arXiv:2003.06713}."
2406.12169,rubin2023long,[{Rubin and Berant(2023)}]{rubin2023long} Ohad Rubin and Jonathan Berant. 2023.,Long-range language modeling with self-retrieval.,Long-range language modeling with self-retrieval.,,"[{Rubin and Berant(2023)}]{rubin2023long} Ohad Rubin and Jonathan Berant. 2023. 
 Long-range language modeling with self-retrieval. 
 \emph{arXiv preprint arXiv:2306.13421}."
2406.12169,shi2023replug,"[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.",Replug: Retrieval-augmented black-box language models.,Replug: Retrieval-augmented black-box language models.,,"[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. 
 Replug: Retrieval-augmented black-box language models. 
 \emph{arXiv preprint arXiv:2301.12652}."
2406.12169,shuster2021retrieval,"[{Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston}]{shuster2021retrieval} Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.",Retrieval augmentation reduces hallucination in conversation.,Retrieval augmentation reduces hallucination in conversation.,,"[{Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston}]{shuster2021retrieval} Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. 
 Retrieval augmentation reduces hallucination in conversation. 
 \emph{arXiv preprint arXiv:2104.07567}."
2406.12169,sun2023chatgpt,"[{Sun et~al.(2023)Sun, Yan, Ma, Ren, Yin, and Ren}]{sun2023chatgpt} Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023.",Is chatgpt good at search? investigating large language models as re-ranking agent.,Is chatgpt good at search? investigating large language models as re-ranking agent.,,"[{Sun et~al.(2023)Sun, Yan, Ma, Ren, Yin, and Ren}]{sun2023chatgpt} Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. 
 Is chatgpt good at search? investigating large language models as re-ranking agent. 
 \emph{arXiv preprint arXiv:2304.09542}."
2406.12169,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.12169,wang2024unims,"[{Wang et~al.(2024{\natexlab{a}})Wang, Huang, Deng, Wang, Wang, Wang, Mi, Pan, and Wong}]{wang2024unims} Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff~Z Pan, and Kam-Fai Wong. 2024{\natexlab{a}}.",Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems.,Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems.,,"[{Wang et~al.(2024{\natexlab{a}})Wang, Huang, Deng, Wang, Wang, Wang, Mi, Pan, and Wong}]{wang2024unims} Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff~Z Pan, and Kam-Fai Wong. 2024{\natexlab{a}}. 
 Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems. 
 \emph{arXiv preprint arXiv:2401.13256}."
2406.12169,wu2023bloomberggpt,"[{Wu et~al.(2023)Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann}]{wu2023bloomberggpt} Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.",Bloomberggpt: A large language model for finance.,Bloomberggpt: A large language model for finance.,,"[{Wu et~al.(2023)Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann}]{wu2023bloomberggpt} Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. 
 Bloomberggpt: A large language model for finance. 
 \emph{arXiv preprint arXiv:2303.17564}."
2406.12169,xi2023rise,"[{Xi et~al.(2023)Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou et~al.}]{xi2023rise} Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et~al. 2023.",The rise and potential of large language model based agents: A survey.,The rise and potential of large language model based agents: A survey.,,"[{Xi et~al.(2023)Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou et~al.}]{xi2023rise} Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et~al. 2023. 
 The rise and potential of large language model based agents: A survey. 
 \emph{arXiv preprint arXiv:2309.07864}."
2406.12169,xu2024bmretriever,"[{Xu et~al.(2024)Xu, Shi, Yu, Zhuang, Zhu, Wang, Ho, Zhang, and Yang}]{xu2024bmretriever} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May~D Wang, Joyce~C Ho, Chao Zhang, and Carl Yang. 2024.",Bmretriever: Tuning large language models as better biomedical text retrievers.,Bmretriever: Tuning large language models as better biomedical text retrievers.,,"[{Xu et~al.(2024)Xu, Shi, Yu, Zhuang, Zhu, Wang, Ho, Zhang, and Yang}]{xu2024bmretriever} Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May~D Wang, Joyce~C Ho, Chao Zhang, and Carl Yang. 2024. 
 Bmretriever: Tuning large language models as better biomedical text retrievers. 
 \emph{arXiv preprint arXiv:2404.18443}."
2406.12169,zhang2024raft,"[{Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez}]{zhang2024raft} Tianjun Zhang, Shishir~G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph~E Gonzalez. 2024.",Raft: Adapting language model to domain specific rag.,Raft: Adapting language model to domain specific rag.,,"[{Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez}]{zhang2024raft} Tianjun Zhang, Shishir~G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph~E Gonzalez. 2024. 
 Raft: Adapting language model to domain specific rag. 
 \emph{arXiv preprint arXiv:2403.10131}."
2406.12169,zhong2022training,"[{Zhong et~al.(2022)Zhong, Lei, and Chen}]{zhong2022training} Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.",Training language models with memory augmentation.,Training language models with memory augmentation.,,"[{Zhong et~al.(2022)Zhong, Lei, and Chen}]{zhong2022training} Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. 
 Training language models with memory augmentation. 
 \emph{arXiv preprint arXiv:2205.12674}."
2406.12182,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.12182,bai2022training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2406.12182,dong2023abilities,"[{Dong et~al.(2023{\natexlab{a}})Dong, Yuan, Lu, Li, Xue, Liu, Wang, Yuan, Zhou, and Zhou}]{dong2023abilities} Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023{\natexlab{a}}.",How abilities in large language models are affected by supervised fine-tuning data composition.,How abilities in large language models are affected by supervised fine-tuning data composition.,,"[{Dong et~al.(2023{\natexlab{a}})Dong, Yuan, Lu, Li, Xue, Liu, Wang, Yuan, Zhou, and Zhou}]{dong2023abilities} Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023{\natexlab{a}}. 
 How abilities in large language models are affected by supervised fine-tuning data composition. 
 \emph{arXiv preprint arXiv:2310.05492}."
2406.12182,dong2023raft,"[{Dong et~al.(2023{\natexlab{b}})Dong, Xiong, Goyal, Zhang, Chow, Pan, Diao, Zhang, Shum, and Zhang}]{dong2023raft} Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023{\natexlab{b}}.",Raft: Reward ranked finetuning for generative foundation model alignment.,Raft: Reward ranked finetuning for generative foundation model alignment.,,"[{Dong et~al.(2023{\natexlab{b}})Dong, Xiong, Goyal, Zhang, Chow, Pan, Diao, Zhang, Shum, and Zhang}]{dong2023raft} Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023{\natexlab{b}}. 
 Raft: Reward ranked finetuning for generative foundation model alignment. 
 \emph{arXiv preprint arXiv:2304.06767}."
2406.12182,hendrycks2020measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2406.12182,jin2019pubmedqa,"[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu. 2019.",Pubmedqa: A dataset for biomedical research question answering.,Pubmedqa: A dataset for biomedical research question answering.,,"[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu. 2019. 
 Pubmedqa: A dataset for biomedical research question answering. 
 \emph{arXiv preprint arXiv:1909.06146}."
2406.12182,lu2024online,"[{Lu et~al.(2024)Lu, Yu, Huang, Fan, Lin, and Zhou}]{lu2024online} Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. 2024.",Online merging optimizers for boosting rewards and mitigating tax in alignment.,Online merging optimizers for boosting rewards and mitigating tax in alignment.,,"[{Lu et~al.(2024)Lu, Yu, Huang, Fan, Lin, and Zhou}]{lu2024online} Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. 2024. 
 Online merging optimizers for boosting rewards and mitigating tax in alignment. 
 \emph{arXiv preprint arXiv:2405.17931}."
2406.12182,wang2023cmb,"[{Wang et~al.(2023)Wang, Chen, Song, Zhang, Chen, Xiao, Jiang, Li, Wan, Wang et~al.}]{wang2023cmb} Xidong Wang, Guiming~Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et~al. 2023.",Cmb: A comprehensive medical benchmark in chinese.,Cmb: A comprehensive medical benchmark in chinese.,,"[{Wang et~al.(2023)Wang, Chen, Song, Zhang, Chen, Xiao, Jiang, Li, Wan, Wang et~al.}]{wang2023cmb} Xidong Wang, Guiming~Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et~al. 2023. 
 Cmb: A comprehensive medical benchmark in chinese. 
 \emph{arXiv preprint arXiv:2308.08833}."
2406.12182,yang2024advancing,"[{Yang et~al.(2024)Yang, Xu, Sellergren, Kohlberger, Zhou, Ktena, Kiraly, Ahmed, Hormozdiari, Jaroensri et~al.}]{yang2024advancing} Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, et~al. 2024.",Advancing multimodal medical capabilities of gemini.,Advancing multimodal medical capabilities of gemini.,,"[{Yang et~al.(2024)Yang, Xu, Sellergren, Kohlberger, Zhou, Ktena, Kiraly, Ahmed, Hormozdiari, Jaroensri et~al.}]{yang2024advancing} Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, et~al. 2024. 
 Advancing multimodal medical capabilities of gemini. 
 \emph{arXiv preprint arXiv:2405.03162}."
2406.12182,zeng2024divtod,"[{Zeng et~al.(2024{\natexlab{a}})Zeng, Fu, He, Wang, Xu, and Xu}]{zeng2024divtod} Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, and Weiran Xu. 2024{\natexlab{a}}.",Divtod: Unleashing the power of llms for diversifying task-oriented dialogue representations.,Divtod: Unleashing the power of llms for diversifying task-oriented dialogue representations.,,"[{Zeng et~al.(2024{\natexlab{a}})Zeng, Fu, He, Wang, Xu, and Xu}]{zeng2024divtod} Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, and Weiran Xu. 2024{\natexlab{a}}. 
 Divtod: Unleashing the power of llms for diversifying task-oriented dialogue representations. 
 \emph{arXiv preprint arXiv:2404.00557}."
2406.12182,zeng2023futuretod,"[{Zeng et~al.(2023)Zeng, He, Wang, Zeng, Wang, Xian, and Xu}]{zeng2023futuretod} Weihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jingang Wang, Yunsen Xian, and Weiran Xu. 2023.",Futuretod: Teaching future knowledge to pre-trained language model for task-oriented dialogue.,Futuretod: Teaching future knowledge to pre-trained language model for task-oriented dialogue.,,"[{Zeng et~al.(2023)Zeng, He, Wang, Zeng, Wang, Xian, and Xu}]{zeng2023futuretod} Weihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jingang Wang, Yunsen Xian, and Weiran Xu. 2023. 
 Futuretod: Teaching future knowledge to pre-trained language model for task-oriented dialogue. 
 \emph{arXiv preprint arXiv:2306.10315}."
2406.12182,zeng2024automatic,"[{Zeng et~al.(2024{\natexlab{b}})Zeng, Xu, Zhao, Lou, and Chen}]{zeng2024automatic} Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024{\natexlab{b}}.",Automatic instruction evolving for large language models.,Automatic instruction evolving for large language models.,,"[{Zeng et~al.(2024{\natexlab{b}})Zeng, Xu, Zhao, Lou, and Chen}]{zeng2024automatic} Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024{\natexlab{b}}. 
 Automatic instruction evolving for large language models. 
 \emph{arXiv preprint arXiv:2406.00770}."
2406.12182,zhang2023huatuogpt,"[{Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Jiang, Yu, Chen, Li, Chen, Wu, Zhang, Xiao et~al.}]{zhang2023huatuogpt} Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et~al. 2023{\natexlab{b}}.","Huatuogpt, towards taming language model to be a doctor.","Huatuogpt, towards taming language model to be a doctor.",,"[{Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Jiang, Yu, Chen, Li, Chen, Wu, Zhang, Xiao et~al.}]{zhang2023huatuogpt} Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et~al. 2023{\natexlab{b}}. 
 Huatuogpt, towards taming language model to be a doctor. 
 \emph{arXiv preprint arXiv:2305.15075}."
2406.12182,zhang2023alpacare,"[{Zhang et~al.(2023{\natexlab{c}})Zhang, Tian, Yang, Chen, Li, and Petzold}]{zhang2023alpacare} Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, and Linda~Ruth Petzold. 2023{\natexlab{c}}.",Alpacare: Instruction-tuned large language models for medical application.,Alpacare: Instruction-tuned large language models for medical application.,,"[{Zhang et~al.(2023{\natexlab{c}})Zhang, Tian, Yang, Chen, Li, and Petzold}]{zhang2023alpacare} Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, and Linda~Ruth Petzold. 2023{\natexlab{c}}. 
 Alpacare: Instruction-tuned large language models for medical application. 
 \emph{arXiv preprint arXiv:2310.14558}."
2406.12295,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.12295,austin2021program,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.",Program synthesis with large language models.,Program synthesis with large language models.,,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021. 
 Program synthesis with large language models. 
 \emph{arXiv preprint arXiv:2108.07732}."
2406.12295,qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2406.12295,fan2018hierarchical,"[{Fan et~al.(2018)Fan, Lewis, and Dauphin}]{fan2018hierarchical} Angela Fan, Mike Lewis, and Yann Dauphin. 2018.",Hierarchical neural story generation.,Hierarchical neural story generation.,,"[{Fan et~al.(2018)Fan, Lewis, and Dauphin}]{fan2018hierarchical} Angela Fan, Mike Lewis, and Yann Dauphin. 2018. 
 Hierarchical neural story generation. 
 \emph{arXiv preprint arXiv:1805.04833}."
2406.12295,fu2023chain,"[{Fu et~al.(2023{\natexlab{a}})Fu, Ou, Chen, Wan, Peng, and Khot}]{fu2023chain} Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023{\natexlab{a}}.",Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance.,Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance.,,"[{Fu et~al.(2023{\natexlab{a}})Fu, Ou, Chen, Wan, Peng, and Khot}]{fu2023chain} Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023{\natexlab{a}}. 
 Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. 
 \emph{arXiv preprint arXiv:2305.17306}."
2406.12295,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2406.12295,li2022contrastive,"[{Li et~al.(2022)Li, Holtzman, Fried, Liang, Eisner, Hashimoto, Zettlemoyer, and Lewis}]{li2022contrastive} Xiang~Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022.",Contrastive decoding: Open-ended text generation as optimization.,Contrastive decoding: Open-ended text generation as optimization.,,"[{Li et~al.(2022)Li, Holtzman, Fried, Liang, Eisner, Hashimoto, Zettlemoyer, and Lewis}]{li2022contrastive} Xiang~Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022. 
 Contrastive decoding: Open-ended text generation as optimization. 
 \emph{arXiv preprint arXiv:2210.15097}."
2406.12295,liu2024tuning,"[{Liu et~al.(2024{\natexlab{a}})Liu, Han, Wang, Tsvetkov, Choi, and Smith}]{liu2024tuning} Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah~A Smith. 2024{\natexlab{a}}.",Tuning language models by proxy.,Tuning language models by proxy.,,"[{Liu et~al.(2024{\natexlab{a}})Liu, Han, Wang, Tsvetkov, Choi, and Smith}]{liu2024tuning} Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah~A Smith. 2024{\natexlab{a}}. 
 Tuning language models by proxy. 
 \emph{arXiv preprint arXiv:2401.08565}."
2406.12295,mitchell2023emulator,"[{Mitchell et~al.(2023)Mitchell, Rafailov, Sharma, Finn, and Manning}]{mitchell2023emulator} Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher~D Manning. 2023.",An emulator for fine-tuning large language models using small language models.,An emulator for fine-tuning large language models using small language models.,,"[{Mitchell et~al.(2023)Mitchell, Rafailov, Sharma, Finn, and Manning}]{mitchell2023emulator} Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher~D Manning. 2023. 
 An emulator for fine-tuning large language models using small language models. 
 \emph{arXiv preprint arXiv:2310.12962}."
2406.12295,o2023contrastive,[{O'Brien and Lewis(2023)}]{o2023contrastive} Sean O'Brien and Mike Lewis. 2023.,Contrastive decoding improves reasoning in large language models.,Contrastive decoding improves reasoning in large language models.,,"[{O'Brien and Lewis(2023)}]{o2023contrastive} Sean O'Brien and Mike Lewis. 2023. 
 Contrastive decoding improves reasoning in large language models. 
 \emph{arXiv preprint arXiv:2309.09117}."
2406.12295,qian2023communicative,"[{Qian et~al.(2023)Qian, Cong, Yang, Chen, Su, Xu, Liu, and Sun}]{qian2023communicative} Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023.",Communicative agents for software development.,Communicative agents for software development.,,"[{Qian et~al.(2023)Qian, Cong, Yang, Chen, Su, Xu, Liu, and Sun}]{qian2023communicative} Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. 
 Communicative agents for software development. 
 \emph{arXiv preprint arXiv:2307.07924}."
2406.12295,sennrich2023mitigating,"[{Sennrich et~al.(2023)Sennrich, Vamvas, and Mohammadshahi}]{sennrich2023mitigating} Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023.",Mitigating hallucinations and off-target machine translation with source-contrastive and language-contrastive decoding.,Mitigating hallucinations and off-target machine translation with source-contrastive and language-contrastive decoding.,,"[{Sennrich et~al.(2023)Sennrich, Vamvas, and Mohammadshahi}]{sennrich2023mitigating} Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. 
 Mitigating hallucinations and off-target machine translation with source-contrastive and language-contrastive decoding. 
 \emph{arXiv preprint arXiv:2309.07098}."
2406.12295,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.12295,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.12295,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.12295,vijayakumar2016diverse,"[{Vijayakumar et~al.(2016)Vijayakumar, Cogswell, Selvaraju, Sun, Lee, Crandall, and Batra}]{vijayakumar2016diverse} Ashwin~K Vijayakumar, Michael Cogswell, Ramprasath~R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016.",Diverse beam search: Decoding diverse solutions from neural sequence models.,Diverse beam search: Decoding diverse solutions from neural sequence models.,,"[{Vijayakumar et~al.(2016)Vijayakumar, Cogswell, Selvaraju, Sun, Lee, Crandall, and Batra}]{vijayakumar2016diverse} Ashwin~K Vijayakumar, Michael Cogswell, Ramprasath~R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. 
 Diverse beam search: Decoding diverse solutions from neural sequence models. 
 \emph{arXiv preprint arXiv:1610.02424}."
2406.12295,xia2024unlocking,"[{Xia et~al.(2024)Xia, Yang, Dong, Wang, Li, Ge, Liu, Li, and Sui}]{xia2024unlocking} Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024.",Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding.,Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding.,,"[{Xia et~al.(2024)Xia, Yang, Dong, Wang, Li, Ge, Liu, Li, and Sui}]{xia2024unlocking} Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. 
 Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. 
 \emph{arXiv preprint arXiv:2401.07851}."
2406.12442,liu2024best,"[{Liu et~al.(2024)Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang, Zhou et~al.}]{liu2024best} Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al. 2024.",Best practices and lessons learned on synthetic data for language models.,Best practices and lessons learned on synthetic data for language models.,,"[{Liu et~al.(2024)Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang, Zhou et~al.}]{liu2024best} Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al. 2024. 
 Best practices and lessons learned on synthetic data for language models. 
 \emph{arXiv preprint arXiv:2404.07503}."
2406.12944,dang2022study,"[Dang et~al.(2022)Dang, Kornblith, Nguyen, Chin, and Khademi]{dang2022study} Trung Dang, Simon Kornblith, Huy~Thong Nguyen, Peter Chin, and Maryam Khademi.",A study on self-supervised object detection pretraining.,A study on self-supervised object detection pretraining.,,"[Dang et~al.(2022)Dang, Kornblith, Nguyen, Chin, and Khademi]{dang2022study} Trung Dang, Simon Kornblith, Huy~Thong Nguyen, Peter Chin, and Maryam Khademi. 
 A study on self-supervised object detection pretraining. 
 \emph{arXiv preprint arXiv: Arxiv-2207.04186}, 2022."
2406.12944,huang2021shuffle,"[Huang et~al.(2021)Huang, Ben, Luo, Cheng, Yu, and Fu]{huang2021shuffle} Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu.",Shuffle transformer: Rethinking spatial shuffle for vision transformer.,Shuffle transformer: Rethinking spatial shuffle for vision transformer.,,"[Huang et~al.(2021)Huang, Ben, Luo, Cheng, Yu, and Fu]{huang2021shuffle} Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. 
 Shuffle transformer: Rethinking spatial shuffle for vision transformer. 
 \emph{arXiv preprint arXiv:2106.03650}, 2021."
2406.12944,kipf2016semi,[Kipf and Welling(2016)]{kipf2016semi} Thomas~N Kipf and Max Welling.,Semi-supervised classification with graph convolutional networks.,Semi-supervised classification with graph convolutional networks.,,"[Kipf and Welling(2016)]{kipf2016semi} Thomas~N Kipf and Max Welling. 
 Semi-supervised classification with graph convolutional networks. 
 \emph{arXiv preprint arXiv:1609.02907}, 2016."
2406.12944,loshchilov2017decoupled,[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter.,Decoupled weight decay regularization.,Decoupled weight decay regularization.,,"[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter. 
 Decoupled weight decay regularization. 
 \emph{arXiv preprint arXiv: 1711.05101}, 2017."
2406.12944,xie2021selfsupervised,"[Xie et~al.(2021{\natexlab{a}})Xie, Lin, Yao, Zhang, Dai, Cao, and Hu]{xie2021selfsupervised} Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu.",Self-supervised learning with swin transformers.,Self-supervised learning with swin transformers.,,"[Xie et~al.(2021{\natexlab{a}})Xie, Lin, Yao, Zhang, Dai, Cao, and Hu]{xie2021selfsupervised} Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. 
 Self-supervised learning with swin transformers. 
 \emph{arXiv preprint arXiv: 2105.04553}, 2021{\natexlab{a}}."
2406.12944,xu2018powerful,"[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.",How powerful are graph neural networks?,How powerful are graph neural networks?,,"[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 
 How powerful are graph neural networks? 
 \emph{arXiv preprint arXiv:1810.00826}, 2018."
2406.12944,dang2022study,"[Dang et~al.(2022)Dang, Kornblith, Nguyen, Chin, and Khademi]{dang2022study} Trung Dang, Simon Kornblith, Huy~Thong Nguyen, Peter Chin, and Maryam Khademi.",A study on self-supervised object detection pretraining.,A study on self-supervised object detection pretraining.,,"[Dang et~al.(2022)Dang, Kornblith, Nguyen, Chin, and Khademi]{dang2022study} Trung Dang, Simon Kornblith, Huy~Thong Nguyen, Peter Chin, and Maryam Khademi. 
 A study on self-supervised object detection pretraining. 
 \emph{arXiv preprint arXiv: Arxiv-2207.04186}, 2022."
2406.12944,huang2021shuffle,"[Huang et~al.(2021)Huang, Ben, Luo, Cheng, Yu, and Fu]{huang2021shuffle} Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu.",Shuffle transformer: Rethinking spatial shuffle for vision transformer.,Shuffle transformer: Rethinking spatial shuffle for vision transformer.,,"[Huang et~al.(2021)Huang, Ben, Luo, Cheng, Yu, and Fu]{huang2021shuffle} Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. 
 Shuffle transformer: Rethinking spatial shuffle for vision transformer. 
 \emph{arXiv preprint arXiv:2106.03650}, 2021."
2406.12944,kipf2016semi,[Kipf and Welling(2016)]{kipf2016semi} Thomas~N Kipf and Max Welling.,Semi-supervised classification with graph convolutional networks.,Semi-supervised classification with graph convolutional networks.,,"[Kipf and Welling(2016)]{kipf2016semi} Thomas~N Kipf and Max Welling. 
 Semi-supervised classification with graph convolutional networks. 
 \emph{arXiv preprint arXiv:1609.02907}, 2016."
2406.12944,loshchilov2017decoupled,[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter.,Decoupled weight decay regularization.,Decoupled weight decay regularization.,,"[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter. 
 Decoupled weight decay regularization. 
 \emph{arXiv preprint arXiv: 1711.05101}, 2017."
2406.12944,xie2021selfsupervised,"[Xie et~al.(2021{\natexlab{a}})Xie, Lin, Yao, Zhang, Dai, Cao, and Hu]{xie2021selfsupervised} Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu.",Self-supervised learning with swin transformers.,Self-supervised learning with swin transformers.,,"[Xie et~al.(2021{\natexlab{a}})Xie, Lin, Yao, Zhang, Dai, Cao, and Hu]{xie2021selfsupervised} Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. 
 Self-supervised learning with swin transformers. 
 \emph{arXiv preprint arXiv: 2105.04553}, 2021{\natexlab{a}}."
2406.12944,xu2018powerful,"[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.",How powerful are graph neural networks?,How powerful are graph neural networks?,,"[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 
 How powerful are graph neural networks? 
 \emph{arXiv preprint arXiv:1810.00826}, 2018."
2406.13233,an2022input,"[{An et~al.(2022)An, Li, Lin, Liu, Chen, Fu, Chen, Zheng, and Lou}]{an2022input} Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. 2022.",Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.,Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.,,"[{An et~al.(2022)An, Li, Lin, Liu, Chen, Fu, Chen, Zheng, and Lou}]{an2022input} Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. 2022. 
 Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. 
 \emph{arXiv preprint arXiv:2203.03131}."
2406.13233,allenai:arc,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv:1803.05457v1}."
2406.13233,dai2024deepseekmoe,"[{Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu et~al.}]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al. 2024.",Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.,Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.,,"[{Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu et~al.}]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al. 2024. 
 Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. 
 \emph{arXiv preprint arXiv:2401.06066}."
2406.13233,dou2023loramoe,"[{Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan et~al.}]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al. 2023.",Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment.,,"[{Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan et~al.}]{dou2023loramoe} Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et~al. 2023. 
 Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. 
 \emph{arXiv preprint arXiv:2312.09979}."
2406.13233,gao2024higher,"[{Gao et~al.(2024)Gao, Chen, Rao, Sun, Liu, Peng, Zhang, Guo, Yang, and Subrahmanian}]{gao2024higher} Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, and VS~Subrahmanian. 2024.",Higher layers need more lora experts.,Higher layers need more lora experts.,,"[{Gao et~al.(2024)Gao, Chen, Rao, Sun, Liu, Peng, Zhang, Guo, Yang, and Subrahmanian}]{gao2024higher} Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, and VS~Subrahmanian. 2024. 
 Higher layers need more lora experts. 
 \emph{arXiv preprint arXiv:2402.08562}."
2406.13233,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2406.13233,jiang2024mixtral,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2406.13233,lepikhin2020gshard,"[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen}]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.",Gshard: Scaling giant models with conditional computation and automatic sharding.,Gshard: Scaling giant models with conditional computation and automatic sharding.,,"[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen}]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. 
 Gshard: Scaling giant models with conditional computation and automatic sharding. 
 \emph{arXiv preprint arXiv:2006.16668}."
2406.13233,li2024mixlora,"[{Li et~al.(2024)Li, Ma, Wang, Cheng, Duan, Zuo, Yang, and Tang}]{li2024mixlora} Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo, Cal Yang, and Mingjie Tang. 2024.",Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts.,Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts.,,"[{Li et~al.(2024)Li, Ma, Wang, Cheng, Duan, Zuo, Yang, and Tang}]{li2024mixlora} Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo, Cal Yang, and Mingjie Tang. 2024. 
 Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts. 
 \emph{arXiv preprint arXiv:2404.15159}."
2406.13233,liu2023moelora,"[{Liu et~al.(2023)Liu, Wu, Zhao, Zhu, Xu, Tian, and Zheng}]{liu2023moelora} Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2023.",Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications.,Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications.,,"[{Liu et~al.(2023)Liu, Wu, Zhao, Zhu, Xu, Tian, and Zheng}]{liu2023moelora} Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2023. 
 Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications. 
 \emph{arXiv preprint arXiv:2310.18339}."
2406.13233,mihaylov2018can,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.",Can a suit of armor conduct electricity? a new dataset for open book question answering.,Can a suit of armor conduct electricity? a new dataset for open book question answering.,,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. 
 Can a suit of armor conduct electricity? a new dataset for open book question answering. 
 \emph{arXiv preprint arXiv:1809.02789}."
2406.13233,raposo2024mixture,"[{Raposo et~al.(2024)Raposo, Ritter, Richards, Lillicrap, Humphreys, and Santoro}]{raposo2024mixture} David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter~Conway Humphreys, and Adam Santoro. 2024.",Mixture-of-depths: Dynamically allocating compute in transformer-based language models.,Mixture-of-depths: Dynamically allocating compute in transformer-based language models.,,"[{Raposo et~al.(2024)Raposo, Ritter, Richards, Lillicrap, Humphreys, and Santoro}]{raposo2024mixture} David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter~Conway Humphreys, and Adam Santoro. 2024. 
 Mixture-of-depths: Dynamically allocating compute in transformer-based language models. 
 \emph{arXiv preprint arXiv:2404.02258}."
2406.13233,sap2019socialiqa,"[{Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi}]{sap2019socialiqa} Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019.",Socialiqa: Commonsense reasoning about social interactions.,Socialiqa: Commonsense reasoning about social interactions.,,"[{Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi}]{sap2019socialiqa} Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. 
 Socialiqa: Commonsense reasoning about social interactions. 
 \emph{arXiv preprint arXiv:1904.09728}."
2406.13233,shazeer2017outrageously,"[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.",Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,,"[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. 
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 
 \emph{arXiv preprint arXiv:1701.06538}."
2406.13233,talmor2018commonsenseqa,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.",Commonsenseqa: A question answering challenge targeting commonsense knowledge.,Commonsenseqa: A question answering challenge targeting commonsense knowledge.,,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. 
 Commonsenseqa: A question answering challenge targeting commonsense knowledge. 
 \emph{arXiv preprint arXiv:1811.00937}."
2406.13233,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.13233,wang2018glue,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}."
2406.13233,zadouri2023pushing,"[{Zadouri et~al.(2023)Zadouri, {\""U}st{\""u}n, Ahmadian, Ermi{\c{s}}, Locatelli, and Hooker}]{zadouri2023pushing} Ted Zadouri, Ahmet {\""U}st{\""u}n, Arash Ahmadian, Beyza Ermi{\c{s}}, Acyr Locatelli, and Sara Hooker. 2023.",Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.,Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.,,"[{Zadouri et~al.(2023)Zadouri, {\""U}st{\""u}n, Ahmadian, Ermi{\c{s}}, Locatelli, and Hooker}]{zadouri2023pushing} Ted Zadouri, Ahmet {\""U}st{\""u}n, Arash Ahmadian, Beyza Ermi{\c{s}}, Acyr Locatelli, and Sara Hooker. 2023. 
 Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. 
 \emph{arXiv preprint arXiv:2309.05444}."
2406.13233,zellers2019hellaswag,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}."
2406.13233,zhang2022opt,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2406.13342,behnamghader2024llm2vec,"[{BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy}]{behnamghader2024llm2vec} Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024.",Llm2vec: Large language models are secretly powerful text encoders.,Llm2vec: Large language models are secretly powerful text encoders.,,"[{BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy}]{behnamghader2024llm2vec} Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. 
 Llm2vec: Large language models are secretly powerful text encoders. 
 \emph{arXiv preprint arXiv:2404.05961}."
2406.13342,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} AQ~Jiang, A~Sablayrolles, A~Mensch, C~Bamford, DS~Chaplot, D~de~las Casas, F~Bressand, G~Lengyel, G~Lample, L~Saulnier, et~al. 2023.",Mistral 7b (2023).,Mistral 7b (2023).,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} AQ~Jiang, A~Sablayrolles, A~Mensch, C~Bamford, DS~Chaplot, D~de~las Casas, F~Bressand, G~Lengyel, G~Lample, L~Saulnier, et~al. 2023. 
 Mistral 7b (2023). 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.13342,mizrahi2023state,"[{Mizrahi et~al.(2023)Mizrahi, Kaplan, Malkin, Dror, Shahaf, and Stanovsky}]{mizrahi2023state} Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2023.",State of what art? a call for multi-prompt llm evaluation.,State of what art? a call for multi-prompt llm evaluation.,,"[{Mizrahi et~al.(2023)Mizrahi, Kaplan, Malkin, Dror, Shahaf, and Stanovsky}]{mizrahi2023state} Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2023. 
 State of what art? a call for multi-prompt llm evaluation. 
 \emph{arXiv preprint arXiv:2401.00595}."
2406.13342,petukhova2024text,"[{Petukhova et~al.(2024)Petukhova, Matos-Carvalho, and Fachada}]{petukhova2024text} Alina Petukhova, Joao~P Matos-Carvalho, and Nuno Fachada. 2024.",Text clustering with llm embeddings.,Text clustering with llm embeddings.,,"[{Petukhova et~al.(2024)Petukhova, Matos-Carvalho, and Fachada}]{petukhova2024text} Alina Petukhova, Joao~P Matos-Carvalho, and Nuno Fachada. 2024. 
 Text clustering with llm embeddings. 
 \emph{arXiv preprint arXiv:2403.15112}."
2406.13551,chen2023fast,"[{Chen et~al.(2023)Chen, Yang, Xiong, Bai, Hu, Hao, Feng, Zhou, Wu, and Liu}]{chen2023fast} Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey~Tianyi Zhou, Jian Wu, and Zuozhu Liu. 2023.",Fast model debias with machine unlearning.,Fast model debias with machine unlearning.,,"[{Chen et~al.(2023)Chen, Yang, Xiong, Bai, Hu, Hao, Feng, Zhou, Wu, and Liu}]{chen2023fast} Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey~Tianyi Zhou, Jian Wu, and Zuozhu Liu. 2023. 
 Fast model debias with machine unlearning. 
 \emph{arXiv preprint arXiv:2310.12560}."
2406.13551,dukler2023safe,"[{Dukler et~al.(2023)Dukler, Bowman, Achille, Golatkar, Swaminathan, and Soatto}]{dukler2023safe} Yonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya Golatkar, Ashwin Swaminathan, and Stefano Soatto. 2023.",Safe: Machine unlearning with shard graphs.,Safe: Machine unlearning with shard graphs.,,"[{Dukler et~al.(2023)Dukler, Bowman, Achille, Golatkar, Swaminathan, and Soatto}]{dukler2023safe} Yonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya Golatkar, Ashwin Swaminathan, and Stefano Soatto. 2023. 
 Safe: Machine unlearning with shard graphs. 
 \emph{arXiv preprint arXiv:2304.13169}."
2406.13551,grosse2023studying,"[{Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez et~al.}]{grosse2023studying} Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et~al. 2023.",Studying large language model generalization with influence functions.,Studying large language model generalization with influence functions.,,"[{Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez et~al.}]{grosse2023studying} Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et~al. 2023. 
 Studying large language model generalization with influence functions. 
 \emph{arXiv preprint arXiv:2308.03296}."
2406.13551,ilharco2022editing,"[{Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi}]{ilharco2022editing} Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022.",Editing models with task arithmetic.,Editing models with task arithmetic.,,"[{Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi}]{ilharco2022editing} Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. 
 Editing models with task arithmetic. 
 \emph{arXiv preprint arXiv:2212.04089}."
2406.13551,jang2022knowledge,"[{Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo}]{jang2022knowledge} Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022.",Knowledge unlearning for mitigating privacy risks in language models.,Knowledge unlearning for mitigating privacy risks in language models.,,"[{Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo}]{jang2022knowledge} Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. 
 Knowledge unlearning for mitigating privacy risks in language models. 
 \emph{arXiv preprint arXiv:2210.01504}."
2406.13551,merity2016pointer,"[{Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.",Pointer sentinel mixture models.,Pointer sentinel mixture models.,,"[{Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. 
 Pointer sentinel mixture models. 
 \emph{arXiv preprint arXiv:1609.07843}."
2406.13551,nangia2020crows,"[{Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman}]{nangia2020crows} Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel~R Bowman. 2020.",Crows-pairs: A challenge dataset for measuring social biases in masked language models.,Crows-pairs: A challenge dataset for measuring social biases in masked language models.,,"[{Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman}]{nangia2020crows} Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel~R Bowman. 2020. 
 Crows-pairs: A challenge dataset for measuring social biases in masked language models. 
 \emph{arXiv preprint arXiv:2010.00133}."
2406.13551,nguyen2022survey,"[{Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and Nguyen}]{nguyen2022survey} Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet~Hung Nguyen. 2022.",A survey of machine unlearning.,A survey of machine unlearning.,,"[{Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and Nguyen}]{nguyen2022survey} Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet~Hung Nguyen. 2022. 
 A survey of machine unlearning. 
 \emph{arXiv preprint arXiv:2209.02299}."
2406.13551,parrish2021bbq,"[{Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman}]{parrish2021bbq} Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman. 2021.",Bbq: A hand-built bias benchmark for question answering.,Bbq: A hand-built bias benchmark for question answering.,,"[{Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman}]{parrish2021bbq} Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman. 2021. 
 Bbq: A hand-built bias benchmark for question answering. 
 \emph{arXiv preprint arXiv:2110.08193}."
2406.13551,sakaguchi2019winogrande,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.",Winogrande: An adversarial winograd schema challenge at scale.,Winogrande: An adversarial winograd schema challenge at scale.,,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. 
 Winogrande: An adversarial winograd schema challenge at scale. 
 \emph{arXiv preprint arXiv:1907.10641}."
2406.13551,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.13551,ung2021saferdialogues,"[{Ung et~al.(2021)Ung, Xu, and Boureau}]{ung2021saferdialogues} Megan Ung, Jing Xu, and Y-Lan Boureau. 2021.",Saferdialogues: Taking feedback gracefully after conversational safety failures.,Saferdialogues: Taking feedback gracefully after conversational safety failures.,,"[{Ung et~al.(2021)Ung, Xu, and Boureau}]{ung2021saferdialogues} Megan Ung, Jing Xu, and Y-Lan Boureau. 2021. 
 Saferdialogues: Taking feedback gracefully after conversational safety failures. 
 \emph{arXiv preprint arXiv:2110.07518}."
2406.13551,wang2023kga,"[{Wang et~al.(2023)Wang, Chen, Yuan, Zeng, Wong, and Yin}]{wang2023kga} Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. 2023.",Kga: A general machine unlearning framework based on knowledge gap alignment.,Kga: A general machine unlearning framework based on knowledge gap alignment.,,"[{Wang et~al.(2023)Wang, Chen, Yuan, Zeng, Wong, and Yin}]{wang2023kga} Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. 2023. 
 Kga: A general machine unlearning framework based on knowledge gap alignment. 
 \emph{arXiv preprint arXiv:2305.06535}."
2406.13551,zaman2023fuse,"[{Zaman et~al.(2023)Zaman, Choshen, and Srivastava}]{zaman2023fuse} Kerem Zaman, Leshem Choshen, and Shashank Srivastava. 2023.",Fuse to forget: Bias reduction and selective memorization through model fusion.,Fuse to forget: Bias reduction and selective memorization through model fusion.,,"[{Zaman et~al.(2023)Zaman, Choshen, and Srivastava}]{zaman2023fuse} Kerem Zaman, Leshem Choshen, and Shashank Srivastava. 2023. 
 Fuse to forget: Bias reduction and selective memorization through model fusion. 
 \emph{arXiv preprint arXiv:2311.07682}."
2406.13551,zhu2020modifying,"[{Zhu et~al.(2020)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar}]{zhu2020modifying} Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020.",Modifying memories in transformer models.,Modifying memories in transformer models.,,"[{Zhu et~al.(2020)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar}]{zhu2020modifying} Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. 
 Modifying memories in transformer models. 
 \emph{arXiv preprint arXiv:2012.00363}."
2406.13551,zmigrod2019counterfactual,"[{Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell}]{zmigrod2019counterfactual} Ran Zmigrod, Sabrina~J Mielke, Hanna Wallach, and Ryan Cotterell. 2019.",Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,,"[{Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell}]{zmigrod2019counterfactual} Ran Zmigrod, Sabrina~J Mielke, Hanna Wallach, and Ryan Cotterell. 2019. 
 Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. 
 \emph{arXiv preprint arXiv:1906.04571}."
2406.1394,chen2024m,"[{Chen et~al.(2024)Chen, Qin, Zhang, Chen, Xu, and Che}]{chen2024m} Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. 2024.",M$^3$cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought.,M$^3$cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought.,,"[{Chen et~al.(2024)Chen, Qin, Zhang, Chen, Xu, and Che}]{chen2024m} Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. 2024. 
 M$^3$cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. 
 \emph{arXiv preprint arXiv:2405.16473}."
2406.1394,qin2024large,"[{Qin et~al.(2024{\natexlab{a}})Qin, Chen, Feng, Wu, Zhang, Li, Li, Che, and Yu}]{qin2024large} Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip~S Yu. 2024{\natexlab{a}}.",Large language models meet nlp: A survey.,Large language models meet nlp: A survey.,,"[{Qin et~al.(2024{\natexlab{a}})Qin, Chen, Feng, Wu, Zhang, Li, Li, Che, and Yu}]{qin2024large} Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip~S Yu. 2024{\natexlab{a}}. 
 Large language models meet nlp: A survey. 
 \emph{arXiv preprint arXiv:2405.12819}."
2406.14005,jastrzkebski:2017,"[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer, Bengio, and Storkey]{jastrzkebski:2017} Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey.",Three factors influencing minima in sgd.,Three factors influencing minima in sgd.,,"[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer, Bengio, and Storkey]{jastrzkebski:2017} Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. 
 Three factors influencing minima in sgd. 
 \emph{arXiv preprint arXiv:1711.04623}, 2017."
2406.14005,phang:2018,"[Phang et~al.(2018)Phang, F{\'e}vry, and Bowman]{phang:2018} Jason Phang, Thibault F{\'e}vry, and Samuel~R Bowman.",Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks.,Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks.,,"[Phang et~al.(2018)Phang, F{\'e}vry, and Bowman]{phang:2018} Jason Phang, Thibault F{\'e}vry, and Samuel~R Bowman. 
 Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. 
 \emph{arXiv preprint arXiv:1811.01088}, 2018."
2406.14005,d2g_survey,"[Sharma et~al.(2022)Sharma, Gogineni, and Ramakrishnan]{d2g_survey} Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan.",Innovations in neural data-to-text generation: A survey.,Innovations in neural data-to-text generation: A survey.,,"[Sharma et~al.(2022)Sharma, Gogineni, and Ramakrishnan]{d2g_survey} Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. 
 Innovations in neural data-to-text generation: A survey. 
 \emph{arXiv preprint arXiv:2207.12571}, 2022."
2406.14005,llama2,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.14192,bai2022training,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}, 2022."
2406.14192,chu2023survey,"[Chu et~al.(2023)Chu, Chen, Chen, Yu, He, Wang, Peng, Liu, Qin, and Liu]{chu2023survey} Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu.","A survey of chain of thought reasoning: Advances, frontiers and future.","A survey of chain of thought reasoning: Advances, frontiers and future.",,"[Chu et~al.(2023)Chu, Chen, Chen, Yu, He, Wang, Peng, Liu, Qin, and Liu]{chu2023survey} Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 
 A survey of chain of thought reasoning: Advances, frontiers and future. 
 \emph{arXiv preprint arXiv:2309.15402}, 2023."
2406.14192,fernandes2023devil,"[Fernandes et~al.(2023)Fernandes, Deutsch, Finkelstein, Riley, Martins, Neubig, Garg, Clark, Freitag, and Firat]{fernandes2023devil} Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr{\'e}~FT Martins, Graham Neubig, Ankush Garg, Jonathan~H Clark, Markus Freitag, and Orhan Firat.",The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.,The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation.,,"[Fernandes et~al.(2023)Fernandes, Deutsch, Finkelstein, Riley, Martins, Neubig, Garg, Clark, Freitag, and Firat]{fernandes2023devil} Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr{\'e}~FT Martins, Graham Neubig, Ankush Garg, Jonathan~H Clark, Markus Freitag, and Orhan Firat. 
 The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. 
 \emph{arXiv preprint arXiv:2308.07286}, 2023."
2406.14192,luo2023wizardmath,"[Luo et~al.(2023{\natexlab{a}})Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath} Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.",Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.,Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.,,"[Luo et~al.(2023{\natexlab{a}})Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath} Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 
 Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 
 \emph{arXiv preprint arXiv:2308.09583}, 2023{\natexlab{a}}."
2406.14192,pan2023automatically,"[Pan et~al.(2023)Pan, Saxon, Xu, Nathani, Wang, and Wang]{pan2023automatically} Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William~Yang Wang.",Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.,Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.,,"[Pan et~al.(2023)Pan, Saxon, Xu, Nathani, Wang, and Wang]{pan2023automatically} Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William~Yang Wang. 
 Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. 
 \emph{arXiv preprint arXiv:2308.03188}, 2023."
2406.14192,roziere2023code,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al.",Code llama: Open foundation models for code.,Code llama: Open foundation models for code.,,"[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code} Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al. 
 Code llama: Open foundation models for code. 
 \emph{arXiv preprint arXiv:2308.12950}, 2023."
2406.14192,saha2023branch,"[Saha et~al.(2023)Saha, Levy, Celikyilmaz, Bansal, Weston, and Li]{saha2023branch} Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.",Branch-solve-merge improves large language model evaluation and generation.,Branch-solve-merge improves large language model evaluation and generation.,,"[Saha et~al.(2023)Saha, Levy, Celikyilmaz, Bansal, Weston, and Li]{saha2023branch} Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 
 Branch-solve-merge improves large language model evaluation and generation. 
 \emph{arXiv preprint arXiv:2310.15123}, 2023."
2406.14192,schulman2017proximal,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}, 2017."
2406.14192,su2024living,"[Su et~al.(2024)Su, Li, Zhang, Zhu, Qu, Zhou, Bowen, Cheng, et~al.]{su2024living} Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu~Cheng, et~al.",Living in the moment: Can large language models grasp co-temporal reasoning?,Living in the moment: Can large language models grasp co-temporal reasoning?,,"[Su et~al.(2024)Su, Li, Zhang, Zhu, Qu, Zhou, Bowen, Cheng, et~al.]{su2024living} Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu~Cheng, et~al. 
 Living in the moment: Can large language models grasp co-temporal reasoning? 
 \emph{arXiv preprint arXiv:2406.09072}, 2024."
2406.14192,tan2023towards,"[Tan et~al.(2023{\natexlab{b}})Tan, Ng, and Bing]{tan2023towards} Qingyu Tan, Hwee~Tou Ng, and Lidong Bing.",Towards robust temporal reasoning of large language models via a multi-hop qa dataset and pseudo-instruction tuning.,Towards robust temporal reasoning of large language models via a multi-hop qa dataset and pseudo-instruction tuning.,,"[Tan et~al.(2023{\natexlab{b}})Tan, Ng, and Bing]{tan2023towards} Qingyu Tan, Hwee~Tou Ng, and Lidong Bing. 
 Towards robust temporal reasoning of large language models via a multi-hop qa dataset and pseudo-instruction tuning. 
 \emph{arXiv preprint arXiv:2311.09821}, 2023{\natexlab{b}}."
2406.14192,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.14192,tunstall2023zephyr,"[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr} Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al.",Zephyr: Direct distillation of lm alignment.,Zephyr: Direct distillation of lm alignment.,,"[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr} Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al. 
 Zephyr: Direct distillation of lm alignment. 
 \emph{arXiv preprint arXiv:2310.16944}, 2023."
2406.14192,wang2023tram,[Wang \& Zhao(2023)Wang and Zhao]{wang2023tram} Yuqing Wang and Yun Zhao.,Tram: Benchmarking temporal reasoning for large language models.,Tram: Benchmarking temporal reasoning for large language models.,,"[Wang \& Zhao(2023)Wang and Zhao]{wang2023tram} Yuqing Wang and Yun Zhao. 
 Tram: Benchmarking temporal reasoning for large language models. 
 \emph{arXiv preprint arXiv:2310.00835}, 2023."
2406.14192,xiong2024large,"[Xiong et~al.(2024)Xiong, Payani, Kompella, and Fekri]{xiong2024large} Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri.",Large language models can learn temporal reasoning.,Large language models can learn temporal reasoning.,,"[Xiong et~al.(2024)Xiong, Payani, Kompella, and Fekri]{xiong2024large} Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. 
 Large language models can learn temporal reasoning. 
 \emph{arXiv preprint arXiv:2401.06853}, 2024."
2406.14192,yuan2023back,"[Yuan et~al.(2023{\natexlab{a}})Yuan, Xie, Huang, and Ananiadou]{yuan2023back} Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou.",Back to the future: Towards explainable temporal reasoning with large language models.,Back to the future: Towards explainable temporal reasoning with large language models.,,"[Yuan et~al.(2023{\natexlab{a}})Yuan, Xie, Huang, and Ananiadou]{yuan2023back} Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 
 Back to the future: Towards explainable temporal reasoning with large language models. 
 \emph{arXiv preprint arXiv:2310.01074}, 2023{\natexlab{a}}."
2406.14192,yue2023mammoth,"[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2023mammoth} Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.",Mammoth: Building math generalist models through hybrid instruction tuning.,Mammoth: Building math generalist models through hybrid instruction tuning.,,"[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue2023mammoth} Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen. 
 Mammoth: Building math generalist models through hybrid instruction tuning. 
 \emph{arXiv preprint arXiv:2309.05653}, 2023."
2406.14192,zhao2023survey,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.",A survey of large language models.,A survey of large language models.,,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}, 2023."
2406.14192,zhu2023question,"[Zhu et~al.(2023{\natexlab{b}})Zhu, Yang, Chen, Li, Lou, and Yang]{zhu2023question} Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, and Yujiu Yang.",Question answering as programming for solving time-sensitive questions.,Question answering as programming for solving time-sensitive questions.,,"[Zhu et~al.(2023{\natexlab{b}})Zhu, Yang, Chen, Li, Lou, and Yang]{zhu2023question} Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, and Yujiu Yang. 
 Question answering as programming for solving time-sensitive questions. 
 \emph{arXiv preprint arXiv:2305.14221}, 2023{\natexlab{b}}."
2406.14192,ziegler2019fine,"[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv:1909.08593}, 2019."
2406.14491,data_selection_survey,"[{Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang,   Muennighoff, Hou, Pan, Jeong et~al.}]{data_selection_survey} Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert,   Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong,   et~al. 2024.",A survey on data selection for language models.,A survey on data selection for language models.,,"[{Albalak et~al.(2024)Albalak, Elazar, Xie, Longpre, Lambert, Wang,   Muennighoff, Hou, Pan, Jeong et~al.}]{data_selection_survey} Alon Albalak, Yanai Elazar, Sang~Michael Xie, Shayne Longpre, Nathan Lambert,   Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong,   et~al. 2024. 
 A survey on data selection for language models. 
 \emph{arXiv preprint arXiv:2402.16827}."
2406.14491,falcon,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,   Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,   Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow,   Julien Launay, Quentin Malartic, et~al. 2023.",The falcon series of open language models.,The falcon series of open language models.,,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli,   Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,   Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow,   Julien Launay, Quentin Malartic, et~al. 2023. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv:2311.16867}."
2406.14491,Qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang   et~al.}]{Qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang   et~al.}]{Qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2406.14491,arc,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord}]{arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord}]{arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.14491,Pile,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima et~al.}]{Pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima et~al.}]{Pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2406.14491,mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,   Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,   Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.14491,Mixtral,"[{Jiang et~al.(2024{\natexlab{a}})Jiang, Sablayrolles, Roux, Mensch,   Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{Mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al. 2024{\natexlab{a}}.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024{\natexlab{a}})Jiang, Sablayrolles, Roux, Mensch,   Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{Mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche   Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou   Hanna, Florian Bressand, et~al. 2024{\natexlab{a}}. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2406.14491,adaptllm1.5,"[{Jiang et~al.(2024{\natexlab{b}})Jiang, Huang, Luo, Zhang, Huang, Wei,   Deng, Sun, Zhang, Wang et~al.}]{adaptllm1.5} Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei,   Weiwei Deng, Feng Sun, Qi~Zhang, Deqing Wang, et~al. 2024{\natexlab{b}}.",Improving domain adaptation through extended-text reading   comprehension.,Improving domain adaptation through extended-text reading   comprehension.,,"[{Jiang et~al.(2024{\natexlab{b}})Jiang, Huang, Luo, Zhang, Huang, Wei,   Deng, Sun, Zhang, Wang et~al.}]{adaptllm1.5} Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei,   Weiwei Deng, Feng Sun, Qi~Zhang, Deqing Wang, et~al. 2024{\natexlab{b}}. 
 Improving domain adaptation through extended-text reading   comprehension. 
 \emph{arXiv preprint arXiv:2401.07284}."
2406.14491,pit,"[{Jiang et~al.(2024{\natexlab{c}})Jiang, Sun, Shi, Rodriguez, Zhou,   Neubig, Lin, Yih, and Iyer}]{pit} Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham   Neubig, Xi~Victoria Lin, Wen-tau Yih, and Srinivasan Iyer.   2024{\natexlab{c}}.",Instruction-tuned language models are better knowledge learners.,Instruction-tuned language models are better knowledge learners.,,"[{Jiang et~al.(2024{\natexlab{c}})Jiang, Sun, Shi, Rodriguez, Zhou,   Neubig, Lin, Yih, and Iyer}]{pit} Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham   Neubig, Xi~Victoria Lin, Wen-tau Yih, and Srinivasan Iyer.   2024{\natexlab{c}}. 
 Instruction-tuned language models are better knowledge learners. 
 \emph{arXiv preprint arXiv:2402.12847}."
2406.14491,fasttext,"[{Joulin et~al.(2016)Joulin, Grave, Bojanowski, Douze, J{\'e}gou, and   Mikolov}]{fasttext} Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H{\'e}rve   J{\'e}gou, and Tomas Mikolov. 2016.",Fasttext. zip: Compressing text classification models.,Fasttext. zip: Compressing text classification models.,,"[{Joulin et~al.(2016)Joulin, Grave, Bojanowski, Douze, J{\'e}gou, and   Mikolov}]{fasttext} Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H{\'e}rve   J{\'e}gou, and Tomas Mikolov. 2016. 
 Fasttext. zip: Compressing text classification models. 
 \emph{arXiv preprint arXiv:1612.03651}."
2406.14491,llm2llm,"[{Lee et~al.(2024)Lee, Wattanawong, Kim, Mangalam, Shen, Anumanchipali,   Mahoney, Keutzer, and Gholami}]{llm2llm} Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng   Shen, Gopala Anumanchipali, Michael~W Mahoney, Kurt Keutzer, and Amir   Gholami. 2024.",Llm2llm: Boosting llms with novel iterative data enhancement.,Llm2llm: Boosting llms with novel iterative data enhancement.,,"[{Lee et~al.(2024)Lee, Wattanawong, Kim, Mangalam, Shen, Anumanchipali,   Mahoney, Keutzer, and Gholami}]{llm2llm} Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng   Shen, Gopala Anumanchipali, Michael~W Mahoney, Kurt Keutzer, and Amir   Gholami. 2024. 
 Llm2llm: Boosting llms with novel iterative data enhancement. 
 \emph{arXiv preprint arXiv:2403.15042}."
2406.14491,GLAN,"[{Li et~al.(2024)Li, Dong, Tang, Wang, Zhang, Huang, Huang, Huang,   Huang, Zhang et~al.}]{GLAN} Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang   Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et~al.   2024.",Synthetic data (almost) from scratch: Generalized instruction tuning   for language models.,Synthetic data (almost) from scratch: Generalized instruction tuning   for language models.,,"[{Li et~al.(2024)Li, Dong, Tang, Wang, Zhang, Huang, Huang, Huang,   Huang, Zhang et~al.}]{GLAN} Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang   Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et~al.   2024. 
 Synthetic data (almost) from scratch: Generalized instruction tuning   for language models. 
 \emph{arXiv preprint arXiv:2402.13064}."
2406.14491,phi1.5,"[{Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno,   Gunasekar, and Lee}]{phi1.5} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee. 2023{\natexlab{b}}.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[{Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno,   Gunasekar, and Lee}]{phi1.5} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya   Gunasekar, and Yin~Tat Lee. 2023{\natexlab{b}}. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2406.14491,syntheticsurvey,"[{Liu et~al.(2024)Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang,   Zhou et~al.}]{syntheticsurvey} Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao,   Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al. 2024.",Best practices and lessons learned on synthetic data for language   models.,Best practices and lessons learned on synthetic data for language   models.,,"[{Liu et~al.(2024)Liu, Wei, Liu, Si, Zhang, Rao, Zheng, Peng, Yang,   Zhou et~al.}]{syntheticsurvey} Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao,   Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et~al. 2024. 
 Best practices and lessons learned on synthetic data for language   models. 
 \emph{arXiv preprint arXiv:2404.07503}."
2406.14491,orca,"[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,   and Awadallah}]{orca} Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid   Palangi, and Ahmed Awadallah. 2023.",Orca: Progressive learning from complex explanation traces of gpt-4.,Orca: Progressive learning from complex explanation traces of gpt-4.,,"[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,   and Awadallah}]{orca} Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid   Palangi, and Ahmed Awadallah. 2023. 
 Orca: Progressive learning from complex explanation traces of gpt-4. 
 \emph{arXiv preprint arXiv:2306.02707}."
2406.14491,gpt4,[{OpenAI(2023)}]{gpt4} OpenAI. 2023.,Gpt-4 technical report.,Gpt-4 technical report.,,"[{OpenAI(2023)}]{gpt4} OpenAI. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.14491,refinedweb,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,   Alobeidli, Pannier, Almazrouei, and Launay}]{refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,   Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,   and Julien Launay. 2023.","The refinedweb dataset for falcon llm: outperforming curated corpora   with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora   with web data, and web data only.",,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,   Alobeidli, Pannier, Almazrouei, and Launay}]{refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,   Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,   and Julien Launay. 2023. 
 The refinedweb dataset for falcon llm: outperforming curated corpora   with web data, and web data only. 
 \emph{arXiv preprint arXiv:2306.01116}."
2406.14491,massiveweb,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,   Aslanides, Henderson, Ring, Young et~al.}]{massiveweb} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,   Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,   et~al. 2021.","Scaling language models: Methods, analysis \& insights from training   gopher.","Scaling language models: Methods, analysis \& insights from training   gopher.",,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,   Aslanides, Henderson, Ring, Young et~al.}]{massiveweb} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,   Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,   et~al. 2021. 
 Scaling language models: Methods, analysis \& insights from training   gopher. 
 \emph{arXiv preprint arXiv:2112.11446}."
2406.14491,llama2,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,   Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,   Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.14491,bAbI,"[{Weston et~al.(2015)Weston, Bordes, Chopra, Rush, Van~Merri{\""e}nboer,   Joulin, and Mikolov}]{bAbI} Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart   Van~Merri{\""e}nboer, Armand Joulin, and Tomas Mikolov. 2015.",Towards ai-complete question answering: A set of prerequisite toy   tasks.,Towards ai-complete question answering: A set of prerequisite toy   tasks.,,"[{Weston et~al.(2015)Weston, Bordes, Chopra, Rush, Van~Merri{\""e}nboer,   Joulin, and Mikolov}]{bAbI} Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart   Van~Merri{\""e}nboer, Armand Joulin, and Tomas Mikolov. 2015. 
 Towards ai-complete question answering: A set of prerequisite toy   tasks. 
 \emph{arXiv preprint arXiv:1502.05698}."
2406.14491,Bloom,"[{Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c},   Hesslow, Castagn{\'e}, Luccioni, Yvon et~al.}]{Bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie   Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha   Luccioni, Fran{\c{c}}ois Yvon, et~al. 2022.",Bloom: A 176b-parameter open-access multilingual language model.,Bloom: A 176b-parameter open-access multilingual language model.,,"[{Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c},   Hesslow, Castagn{\'e}, Luccioni, Yvon et~al.}]{Bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie   Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha   Luccioni, Fran{\c{c}}ois Yvon, et~al. 2022. 
 Bloom: A 176b-parameter open-access multilingual language model. 
 \emph{arXiv preprint arXiv:2211.05100}."
2406.14491,tweetqa,"[{Xiong et~al.(2019)Xiong, Wu, Wang, Kulkarni, Yu, Chang, Guo, and   Wang}]{tweetqa} Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo~Yu, Shiyu Chang,   Xiaoxiao Guo, and William~Yang Wang. 2019.",Tweetqa: A social media focused question answering dataset.,Tweetqa: A social media focused question answering dataset.,,"[{Xiong et~al.(2019)Xiong, Wu, Wang, Kulkarni, Yu, Chang, Guo, and   Wang}]{tweetqa} Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo~Yu, Shiyu Chang,   Xiaoxiao Guo, and William~Yang Wang. 2019. 
 Tweetqa: A social media focused question answering dataset. 
 \emph{arXiv preprint arXiv:1907.06292}."
2406.14491,genie,"[{Yehudai et~al.(2024)Yehudai, Carmeli, Mass, Arviv, Mills, Toledo,   Shnarch, and Choshen}]{genie} Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf   Toledo, Eyal Shnarch, and Leshem Choshen. 2024.",Genie: Achieving human parity in content-grounded datasets   generation.,Genie: Achieving human parity in content-grounded datasets   generation.,,"[{Yehudai et~al.(2024)Yehudai, Carmeli, Mass, Arviv, Mills, Toledo,   Shnarch, and Choshen}]{genie} Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf   Toledo, Eyal Shnarch, and Leshem Choshen. 2024. 
 Genie: Achieving human parity in content-grounded datasets   generation. 
 \emph{arXiv preprint arXiv:2401.14367}."
2406.14491,mammoth2,"[{Yue et~al.(2024)Yue, Zheng, Zhang, and Chen}]{mammoth2} Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen. 2024.",Mammoth2: Scaling instructions from the web.,Mammoth2: Scaling instructions from the web.,,"[{Yue et~al.(2024)Yue, Zheng, Zhang, and Chen}]{mammoth2} Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen. 2024. 
 Mammoth2: Scaling instructions from the web. 
 \emph{arXiv preprint arXiv:2405.03548}."
2406.14491,opt,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin et~al.}]{opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin et~al.}]{opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2406.145,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.145,chen2022toward,"[{Chen et~al.(2022)Chen, Varma, Wan, Langlotz, and   Delbrouck}]{chen2022toward} Zhihong Chen, Maya Varma, Xiang Wan, Curtis Langlotz, and Jean-Benoit   Delbrouck. 2022.",Toward expanding the scope of radiology report summarization to   multiple anatomies and modalities.,Toward expanding the scope of radiology report summarization to   multiple anatomies and modalities.,,"[{Chen et~al.(2022)Chen, Varma, Wan, Langlotz, and   Delbrouck}]{chen2022toward} Zhihong Chen, Maya Varma, Xiang Wan, Curtis Langlotz, and Jean-Benoit   Delbrouck. 2022. 
 Toward expanding the scope of radiology report summarization to   multiple anatomies and modalities. 
 \emph{arXiv preprint arXiv:2211.08584}."
2406.145,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,   Jingjing Xu, and Zhifang Sui. 2022.",A survey for in-context learning.,A survey for in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,   Jingjing Xu, and Zhifang Sui. 2022. 
 A survey for in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2406.145,dosovitskiy2020image,"[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,   Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly   et~al.}]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,   Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg   Heigold, Sylvain Gelly, et~al. 2020.",An image is worth 16x16 words: Transformers for image recognition at   scale.,An image is worth 16x16 words: Transformers for image recognition at   scale.,,"[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,   Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly   et~al.}]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,   Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg   Heigold, Sylvain Gelly, et~al. 2020. 
 An image is worth 16x16 words: Transformers for image recognition at   scale. 
 \emph{arXiv preprint arXiv:2010.11929}."
2406.145,hu2023zero,"[{Hu et~al.(2023)Hu, Ameer, Zuo, Peng, Zhou, Li, Li, Li, Jiang, and   Xu}]{hu2023zero} Yan Hu, Iqra Ameer, Xu~Zuo, Xueqing Peng, Yujia Zhou, Zehan Li, Yiming Li,   Jianfu Li, Xiaoqian Jiang, and Hua Xu. 2023.",Zero-shot clinical entity recognition using chatgpt.,Zero-shot clinical entity recognition using chatgpt.,,"[{Hu et~al.(2023)Hu, Ameer, Zuo, Peng, Zhou, Li, Li, Li, Jiang, and   Xu}]{hu2023zero} Yan Hu, Iqra Ameer, Xu~Zuo, Xueqing Peng, Yujia Zhou, Zehan Li, Yiming Li,   Jianfu Li, Xiaoqian Jiang, and Hua Xu. 2023. 
 Zero-shot clinical entity recognition using chatgpt. 
 \emph{arXiv preprint arXiv:2303.16416}."
2406.145,nori2023can,"[{Nori et~al.(2023)Nori, Lee, Zhang, Carignan, Edgar, Fusi, King,   Larson, Li, Liu et~al.}]{nori2023can} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo   Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023.",Can generalist foundation models outcompete special-purpose tuning?   case study in medicine.,Can generalist foundation models outcompete special-purpose tuning?   case study in medicine.,,"[{Nori et~al.(2023)Nori, Lee, Zhang, Carignan, Edgar, Fusi, King,   Larson, Li, Liu et~al.}]{nori2023can} Harsha Nori, Yin~Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo   Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et~al. 2023. 
 Can generalist foundation models outcompete special-purpose tuning?   case study in medicine. 
 \emph{arXiv preprint arXiv:2311.16452}."
2406.145,sarkar2024identification,"[{Sarkar et~al.(2024)Sarkar, Chuang, Mohammed, and   Jiang}]{sarkar2024identification} Atiquer~Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, and Xiaoqian Jiang.   2024.",De-identification is not always enough.,De-identification is not always enough.,,"[{Sarkar et~al.(2024)Sarkar, Chuang, Mohammed, and   Jiang}]{sarkar2024identification} Atiquer~Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, and Xiaoqian Jiang.   2024. 
 De-identification is not always enough. 
 \emph{arXiv preprint arXiv:2402.00179}."
2406.145,shi2023replug,"[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer,   and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,   Luke Zettlemoyer, and Wen-tau Yih. 2023.",Replug: Retrieval-augmented black-box language models.,Replug: Retrieval-augmented black-box language models.,,"[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer,   and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,   Luke Zettlemoyer, and Wen-tau Yih. 2023. 
 Replug: Retrieval-augmented black-box language models. 
 \emph{arXiv preprint arXiv:2301.12652}."
2406.145,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.   2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.   2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.145,van2023clinical,"[{Van~Veen et~al.(2023{\natexlab{b}})Van~Veen, Van~Uden, Blankemeier,   Delbrouck, Aali, Bluethgen, Pareek, Polacin, Collins, Ahuja   et~al.}]{van2023clinical} Dave Van~Veen, Cara Van~Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad   Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, William Collins,   Neera Ahuja, et~al. 2023{\natexlab{b}}.",Clinical text summarization: adapting large language models can   outperform human experts.,Clinical text summarization: adapting large language models can   outperform human experts.,,"[{Van~Veen et~al.(2023{\natexlab{b}})Van~Veen, Van~Uden, Blankemeier,   Delbrouck, Aali, Bluethgen, Pareek, Polacin, Collins, Ahuja   et~al.}]{van2023clinical} Dave Van~Veen, Cara Van~Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad   Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, William Collins,   Neera Ahuja, et~al. 2023{\natexlab{b}}. 
 Clinical text summarization: adapting large language models can   outperform human experts. 
 \emph{arXiv preprint arXiv:2309.07430}."
2406.145,wang2023openchat,"[{Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Zhan, Li, Song, and   Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.   2023{\natexlab{a}}.",Openchat: Advancing open-source language models with mixed-quality   data.,Openchat: Advancing open-source language models with mixed-quality   data.,,"[{Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Zhan, Li, Song, and   Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.   2023{\natexlab{a}}. 
 Openchat: Advancing open-source language models with mixed-quality   data. 
 \emph{arXiv preprint arXiv:2309.11235}."
2406.145,wang2024llm,"[{Wang et~al.(2024)Wang, Luo, Jiang, Li, and Qiu}]{wang2024llm} Zilong Wang, Xufang Luo, Xinyang Jiang, Dongsheng Li, and Lili Qiu. 2024.",Llm-radjudge: Achieving radiologist-level evaluation for x-ray report   generation.,Llm-radjudge: Achieving radiologist-level evaluation for x-ray report   generation.,,"[{Wang et~al.(2024)Wang, Luo, Jiang, Li, and Qiu}]{wang2024llm} Zilong Wang, Xufang Luo, Xinyang Jiang, Dongsheng Li, and Lili Qiu. 2024. 
 Llm-radjudge: Achieving radiologist-level evaluation for x-ray report   generation. 
 \emph{arXiv preprint arXiv:2404.00998}."
2406.145,yao2023knowledge,"[{Yao et~al.(2023{\natexlab{a}})Yao, Xu, Lian, Wang, Yi, and   Xie}]{yao2023knowledge} Jing Yao, Wei Xu, Jianxun Lian, Xiting Wang, Xiaoyuan Yi, and Xing Xie.   2023{\natexlab{a}}.",Knowledge plugins: Enhancing large language models for   domain-specific recommendations.,Knowledge plugins: Enhancing large language models for   domain-specific recommendations.,,"[{Yao et~al.(2023{\natexlab{a}})Yao, Xu, Lian, Wang, Yi, and   Xie}]{yao2023knowledge} Jing Yao, Wei Xu, Jianxun Lian, Xiting Wang, Xiaoyuan Yi, and Xing Xie.   2023{\natexlab{a}}. 
 Knowledge plugins: Enhancing large language models for   domain-specific recommendations. 
 \emph{arXiv preprint arXiv:2311.10779}."
2406.145,zhang2022automatic,"[{Zhang et~al.(2022)Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022.",Automatic chain of thought prompting in large language models.,Automatic chain of thought prompting in large language models.,,"[{Zhang et~al.(2022)Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022. 
 Automatic chain of thought prompting in large language models. 
 \emph{arXiv preprint arXiv:2210.03493}."
2406.14955,akyurek2022learning,"[{Aky{\""u}rek et~al.(2022)Aky{\""u}rek, Schuurmans, Andreas, Ma, and Zhou}]{akyurek2022learning} Ekin Aky{\""u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022.",What learning algorithm is in-context learning? investigations with linear models.,What learning algorithm is in-context learning? investigations with linear models.,,"[{Aky{\""u}rek et~al.(2022)Aky{\""u}rek, Schuurmans, Andreas, Ma, and Zhou}]{akyurek2022learning} Ekin Aky{\""u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. 
 What learning algorithm is in-context learning? investigations with linear models. 
 \emph{arXiv preprint arXiv:2211.15661}."
2406.14955,austin2021program,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.",Program synthesis with large language models.,Program synthesis with large language models.,,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021. 
 Program synthesis with large language models. 
 \emph{arXiv preprint arXiv:2108.07732}."
2406.14955,bai2023qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang et~al.}]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang et~al.}]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2406.14955,bubeck2023sparks,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}."
2406.14955,chen2021evaluating,"[{Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021{\natexlab{a}}.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021{\natexlab{a}}. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}."
2406.14955,chen2021meta,"[{Chen et~al.(2021{\natexlab{b}})Chen, Zhong, Zha, Karypis, and He}]{chen2021meta} Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He~He. 2021{\natexlab{b}}.",Meta-learning via language model in-context tuning.,Meta-learning via language model in-context tuning.,,"[{Chen et~al.(2021{\natexlab{b}})Chen, Zhong, Zha, Karypis, and He}]{chen2021meta} Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He~He. 2021{\natexlab{b}}. 
 Meta-learning via language model in-context tuning. 
 \emph{arXiv preprint arXiv:2110.07814}."
2406.14955,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.14955,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2406.14955,diao2023active,"[{Diao et~al.(2023)Diao, Wang, Lin, and Zhang}]{diao2023active} Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.",Active prompting with chain-of-thought for large language models.,Active prompting with chain-of-thought for large language models.,,"[{Diao et~al.(2023)Diao, Wang, Lin, and Zhang}]{diao2023active} Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. 
 Active prompting with chain-of-thought for large language models. 
 \emph{arXiv preprint arXiv:2302.12246}."
2406.14955,dua2022successive,"[{Dua et~al.(2022)Dua, Gupta, Singh, and Gardner}]{dua2022successive} Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022.",Successive prompting for decomposing complex questions.,Successive prompting for decomposing complex questions.,,"[{Dua et~al.(2022)Dua, Gupta, Singh, and Gardner}]{dua2022successive} Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. 
 Successive prompting for decomposing complex questions. 
 \emph{arXiv preprint arXiv:2212.04092}."
2406.14955,fu2022complexity,"[{Fu et~al.(2022)Fu, Peng, Sabharwal, Clark, and Khot}]{fu2022complexity} Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022.",Complexity-based prompting for multi-step reasoning.,Complexity-based prompting for multi-step reasoning.,,"[{Fu et~al.(2022)Fu, Peng, Sabharwal, Clark, and Khot}]{fu2022complexity} Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. 
 Complexity-based prompting for multi-step reasoning. 
 \emph{arXiv preprint arXiv:2210.00720}."
2406.14955,han2023context,"[{Han et~al.(2023)Han, Wang, Zhao, and Ji}]{han2023context} Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. 2023.",In-context learning of large language models explained as kernel regression.,In-context learning of large language models explained as kernel regression.,,"[{Han et~al.(2023)Han, Wang, Zhao, and Ji}]{han2023context} Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. 2023. 
 In-context learning of large language models explained as kernel regression. 
 \emph{arXiv preprint arXiv:2305.12766}."
2406.14955,hendrycks2020measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2406.14955,hendrycks2021measuring,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}."
2406.14955,huang2023c,"[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei et~al.}]{huang2023c} Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et~al. 2023.",C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.,C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.,,"[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei et~al.}]{huang2023c} Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et~al. 2023. 
 C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. 
 \emph{arXiv preprint arXiv:2305.08322}."
2406.14955,lai2017race,"[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.",Race: Large-scale reading comprehension dataset from examinations.,Race: Large-scale reading comprehension dataset from examinations.,,"[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. 
 Race: Large-scale reading comprehension dataset from examinations. 
 \emph{arXiv preprint arXiv:1704.04683}."
2406.14955,lai2023chatgpt,"[{Lai et~al.(2023)Lai, Ngo, Veyseh, Man, Dernoncourt, Bui, and Nguyen}]{lai2023chatgpt} Viet~Dac Lai, Nghia~Trung Ngo, Amir Pouran~Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien~Huu Nguyen. 2023.",Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.,Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.,,"[{Lai et~al.(2023)Lai, Ngo, Veyseh, Man, Dernoncourt, Bui, and Nguyen}]{lai2023chatgpt} Viet~Dac Lai, Nghia~Trung Ngo, Amir Pouran~Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien~Huu Nguyen. 2023. 
 Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. 
 \emph{arXiv preprint arXiv:2304.05613}."
2406.14955,liang2022holistic,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}."
2406.14955,lin2021truthfulqa,"[{Lin et~al.(2021)Lin, Hilton, and Evans}]{lin2021truthfulqa} Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.",Truthfulqa: Measuring how models mimic human falsehoods.,Truthfulqa: Measuring how models mimic human falsehoods.,,"[{Lin et~al.(2021)Lin, Hilton, and Evans}]{lin2021truthfulqa} Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. 
 Truthfulqa: Measuring how models mimic human falsehoods. 
 \emph{arXiv preprint arXiv:2109.07958}."
2406.14955,ling2017program,"[{Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom}]{ling2017program} Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.",Program induction by rationale generation: Learning to solve and explain algebraic word problems.,Program induction by rationale generation: Learning to solve and explain algebraic word problems.,,"[{Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom}]{ling2017program} Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. 
 Program induction by rationale generation: Learning to solve and explain algebraic word problems. 
 \emph{arXiv preprint arXiv:1705.04146}."
2406.14955,liu2023lost,"[{Liu et~al.(2023{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}]{liu2023lost} Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023{\natexlab{a}}.",Lost in the middle: How language models use long contexts.,Lost in the middle: How language models use long contexts.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}]{liu2023lost} Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023{\natexlab{a}}. 
 Lost in the middle: How language models use long contexts. 
 \emph{arXiv preprint arXiv:2307.03172}."
2406.14955,lu2021fantastically,"[{Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and Stenetorp}]{lu2021fantastically} Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021.",Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.,Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.,,"[{Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and Stenetorp}]{lu2021fantastically} Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. 
 Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. 
 \emph{arXiv preprint arXiv:2104.08786}."
2406.14955,lyu2023faithful,"[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023.",Faithful chain-of-thought reasoning.,Faithful chain-of-thought reasoning.,,"[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}]{lyu2023faithful} Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. 
 Faithful chain-of-thought reasoning. 
 \emph{arXiv preprint arXiv:2301.13379}."
2406.14955,mihaylov2018can,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.",Can a suit of armor conduct electricity? a new dataset for open book question answering.,Can a suit of armor conduct electricity? a new dataset for open book question answering.,,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. 
 Can a suit of armor conduct electricity? a new dataset for open book question answering. 
 \emph{arXiv preprint arXiv:1809.02789}."
2406.14955,min2021metaicl,"[{Min et~al.(2021)Min, Lewis, Zettlemoyer, and Hajishirzi}]{min2021metaicl} Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021.",Metaicl: Learning to learn in context.,Metaicl: Learning to learn in context.,,"[{Min et~al.(2021)Min, Lewis, Zettlemoyer, and Hajishirzi}]{min2021metaicl} Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. 
 Metaicl: Learning to learn in context. 
 \emph{arXiv preprint arXiv:2110.15943}."
2406.14955,min2022rethinking,"[{Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer}]{min2022rethinking} Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.",Rethinking the role of demonstrations: What makes in-context learning work?,Rethinking the role of demonstrations: What makes in-context learning work?,,"[{Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer}]{min2022rethinking} Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. 
 Rethinking the role of demonstrations: What makes in-context learning work? 
 \emph{arXiv preprint arXiv:2202.12837}."
2406.14955,nie2022improving,"[{Nie et~al.(2022)Nie, Chen, Zhang, and Cheng}]{nie2022improving} Feng Nie, Meixi Chen, Zhirui Zhang, and Xu~Cheng. 2022.",Improving few-shot performance of language models via nearest neighbor calibration.,Improving few-shot performance of language models via nearest neighbor calibration.,,"[{Nie et~al.(2022)Nie, Chen, Zhang, and Cheng}]{nie2022improving} Feng Nie, Meixi Chen, Zhirui Zhang, and Xu~Cheng. 2022. 
 Improving few-shot performance of language models via nearest neighbor calibration. 
 \emph{arXiv preprint arXiv:2212.02216}."
2406.14955,olsson2022context,"[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022.",In-context learning and induction heads.,In-context learning and induction heads.,,"[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022. 
 In-context learning and induction heads. 
 \emph{arXiv preprint arXiv:2209.11895}."
2406.14955,qin2023chatgpt,"[{Qin et~al.(2023{\natexlab{a}})Qin, Zhang, Zhang, Chen, Yasunaga, and Yang}]{qin2023chatgpt} Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023{\natexlab{a}}.",Is chatgpt a general-purpose natural language processing task solver?,Is chatgpt a general-purpose natural language processing task solver?,,"[{Qin et~al.(2023{\natexlab{a}})Qin, Zhang, Zhang, Chen, Yasunaga, and Yang}]{qin2023chatgpt} Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023{\natexlab{a}}. 
 Is chatgpt a general-purpose natural language processing task solver? 
 \emph{arXiv preprint arXiv:2302.06476}."
2406.14955,qin2023tool,"[{Qin et~al.(2023{\natexlab{b}})Qin, Hu, Lin, Chen, Ding, Cui, Zeng, Huang, Xiao, Han et~al.}]{qin2023tool} Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et~al. 2023{\natexlab{b}}.",Tool learning with foundation models.,Tool learning with foundation models.,,"[{Qin et~al.(2023{\natexlab{b}})Qin, Hu, Lin, Chen, Ding, Cui, Zeng, Huang, Xiao, Han et~al.}]{qin2023tool} Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et~al. 2023{\natexlab{b}}. 
 Tool learning with foundation models. 
 \emph{arXiv preprint arXiv:2304.08354}."
2406.14955,rajpurkar2018know,"[{Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang}]{rajpurkar2018know} Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.",Know what you don't know: Unanswerable questions for squad.,Know what you don't know: Unanswerable questions for squad.,,"[{Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang}]{rajpurkar2018know} Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. 
 Know what you don't know: Unanswerable questions for squad. 
 \emph{arXiv preprint arXiv:1806.03822}."
2406.14955,shridhar2022distilling,"[{Shridhar et~al.(2022)Shridhar, Stolfo, and Sachan}]{shridhar2022distilling} Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2022.",Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions.,Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions.,,"[{Shridhar et~al.(2022)Shridhar, Stolfo, and Sachan}]{shridhar2022distilling} Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2022. 
 Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. 
 \emph{arXiv preprint arXiv:2212.00193}."
2406.14955,shum2023automatic,"[{Shum et~al.(2023)Shum, Diao, and Zhang}]{shum2023automatic} KaShun Shum, Shizhe Diao, and Tong Zhang. 2023.",Automatic prompt augmentation and selection with chain-of-thought from labeled data.,Automatic prompt augmentation and selection with chain-of-thought from labeled data.,,"[{Shum et~al.(2023)Shum, Diao, and Zhang}]{shum2023automatic} KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. 
 Automatic prompt augmentation and selection with chain-of-thought from labeled data. 
 \emph{arXiv preprint arXiv:2302.12822}."
2406.14955,srivastava2022beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2406.14955,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.14955,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.14955,wang2018glue,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}."
2406.14955,wang2022self,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}."
2406.14955,wu2023openicl,"[{Wu et~al.(2023)Wu, Wang, Ye, Feng, Xu, Qiao, and Wu}]{wu2023openicl} Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu~Qiao, and Zhiyong Wu. 2023.",Openicl: An open-source framework for in-context learning.,Openicl: An open-source framework for in-context learning.,,"[{Wu et~al.(2023)Wu, Wang, Ye, Feng, Xu, Qiao, and Wu}]{wu2023openicl} Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu~Qiao, and Zhiyong Wu. 2023. 
 Openicl: An open-source framework for in-context learning. 
 \emph{arXiv preprint arXiv:2303.02913}."
2406.14955,xie2021explanation,"[{Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation} Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021.",An explanation of in-context learning as implicit bayesian inference.,An explanation of in-context learning as implicit bayesian inference.,,"[{Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation} Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. 
 An explanation of in-context learning as implicit bayesian inference. 
 \emph{arXiv preprint arXiv:2111.02080}."
2406.14955,yang2023iterative,"[{Yang et~al.(2023)Yang, Hui, Yang, Li, Huang, and Li}]{yang2023iterative} Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.",Iterative forward tuning boosts in-context learning in language models.,Iterative forward tuning boosts in-context learning in language models.,,"[{Yang et~al.(2023)Yang, Hui, Yang, Li, Huang, and Li}]{yang2023iterative} Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. 
 Iterative forward tuning boosts in-context learning in language models. 
 \emph{arXiv preprint arXiv:2305.13016}."
2406.14955,yao2023tree,"[{Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan}]{yao2023tree} Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.",Tree of thoughts: Deliberate problem solving with large language models.,Tree of thoughts: Deliberate problem solving with large language models.,,"[{Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan}]{yao2023tree} Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. 
 Tree of thoughts: Deliberate problem solving with large language models. 
 \emph{arXiv preprint arXiv:2305.10601}."
2406.14955,ye2022complementary,"[{Ye et~al.(2022)Ye, Iyer, Celikyilmaz, Stoyanov, Durrett, and Pasunuru}]{ye2022complementary} Xi~Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022.",Complementary explanations for effective in-context learning.,Complementary explanations for effective in-context learning.,,"[{Ye et~al.(2022)Ye, Iyer, Celikyilmaz, Stoyanov, Durrett, and Pasunuru}]{ye2022complementary} Xi~Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru. 2022. 
 Complementary explanations for effective in-context learning. 
 \emph{arXiv preprint arXiv:2211.13892}."
2406.14955,yuan2023well,"[{Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, and Huang}]{yuan2023well} Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023.",How well do large language models perform in arithmetic tasks?,How well do large language models perform in arithmetic tasks?,,"[{Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, and Huang}]{yuan2023well} Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. 
 How well do large language models perform in arithmetic tasks? 
 \emph{arXiv preprint arXiv:2304.02015}."
2406.14955,zellers2019hellaswag,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}."
2406.14955,zhong2023agieval,"[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval} Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.",Agieval: A human-centric benchmark for evaluating foundation models.,Agieval: A human-centric benchmark for evaluating foundation models.,,"[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval} Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. 
 Agieval: A human-centric benchmark for evaluating foundation models. 
 \emph{arXiv preprint arXiv:2304.06364}."
2406.14955,zhou2022least,"[{Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Bousquet, Le, and Chi}]{zhou2022least} Denny Zhou, Nathanael Sch{\""a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed~Chi. 2022.",Least-to-most prompting enables complex reasoning in large language models.,Least-to-most prompting enables complex reasoning in large language models.,,"[{Zhou et~al.(2022)Zhou, Sch{\""a}rli, Hou, Wei, Scales, Wang, Schuurmans, Bousquet, Le, and Chi}]{zhou2022least} Denny Zhou, Nathanael Sch{\""a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed~Chi. 2022. 
 Least-to-most prompting enables complex reasoning in large language models. 
 \emph{arXiv preprint arXiv:2205.10625}."
2406.15796,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.15796,bai2022training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2406.15796,bhardwaj2024language,"[{Bhardwaj et~al.(2024)Bhardwaj, Anh, and Poria}]{bhardwaj2024language} Rishabh Bhardwaj, Do~Duc Anh, and Soujanya Poria. 2024.",Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic.,Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic.,,"[{Bhardwaj et~al.(2024)Bhardwaj, Anh, and Poria}]{bhardwaj2024language} Rishabh Bhardwaj, Do~Duc Anh, and Soujanya Poria. 2024. 
 Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic. 
 \emph{arXiv preprint arXiv:2402.11746}."
2406.15796,chen2023unlearn,[{Chen and Yang(2023)}]{chen2023unlearn} Jiaao Chen and Diyi Yang. 2023.,Unlearn what you want to forget: Efficient unlearning for llms.,Unlearn what you want to forget: Efficient unlearning for llms.,,"[{Chen and Yang(2023)}]{chen2023unlearn} Jiaao Chen and Diyi Yang. 2023. 
 Unlearn what you want to forget: Efficient unlearning for llms. 
 \emph{arXiv preprint arXiv:2310.20150}."
2406.15796,das2024security,"[{Das et~al.(2024)Das, Amini, and Wu}]{das2024security} Badhan~Chandra Das, M~Hadi Amini, and Yanzhao Wu. 2024.",Security and privacy challenges of large language models: A survey.,Security and privacy challenges of large language models: A survey.,,"[{Das et~al.(2024)Das, Amini, and Wu}]{das2024security} Badhan~Chandra Das, M~Hadi Amini, and Yanzhao Wu. 2024. 
 Security and privacy challenges of large language models: A survey. 
 \emph{arXiv preprint arXiv:2402.00888}."
2406.15796,duan2024negating,"[{Duan et~al.(2024)Duan, Yi, Zhang, Lu, Xie, and Gu}]{duan2024negating} Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. 2024.",Negating negatives: Alignment without human positive samples via distributional dispreference optimization.,Negating negatives: Alignment without human positive samples via distributional dispreference optimization.,,"[{Duan et~al.(2024)Duan, Yi, Zhang, Lu, Xie, and Gu}]{duan2024negating} Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. 2024. 
 Negating negatives: Alignment without human positive samples via distributional dispreference optimization. 
 \emph{arXiv preprint arXiv:2403.03419}."
2406.15796,feng2023trends,"[{Feng et~al.(2023)Feng, Ma, Yu, Huang, Wang, Chen, Peng, Feng, Qin et~al.}]{feng2023trends} Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023.","Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications.","Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications.",,"[{Feng et~al.(2023)Feng, Ma, Yu, Huang, Wang, Chen, Peng, Feng, Qin et~al.}]{feng2023trends} Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023. 
 Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications. 
 \emph{arXiv preprint arXiv:2311.05876}."
2406.15796,gekhman2024does,"[{Gekhman et~al.(2024)Gekhman, Yona, Aharoni, Eyal, Feder, Reichart, and Herzig}]{gekhman2024does} Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024.",Does fine-tuning llms on new knowledge encourage hallucinations?,Does fine-tuning llms on new knowledge encourage hallucinations?,,"[{Gekhman et~al.(2024)Gekhman, Yona, Aharoni, Eyal, Feder, Reichart, and Herzig}]{gekhman2024does} Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. 
 Does fine-tuning llms on new knowledge encourage hallucinations? 
 \emph{arXiv preprint arXiv:2405.05904}."
2406.15796,huang2023survey,"[{Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin et~al.}]{huang2023survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.",,"[{Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin et~al.}]{huang2023survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023. 
 A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 
 \emph{arXiv preprint arXiv:2311.05232}."
2406.15796,jang2022knowledge,"[{Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo}]{jang2022knowledge} Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022.",Knowledge unlearning for mitigating privacy risks in language models.,Knowledge unlearning for mitigating privacy risks in language models.,,"[{Jang et~al.(2022)Jang, Yoon, Yang, Cha, Lee, Logeswaran, and Seo}]{jang2022knowledge} Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. 
 Knowledge unlearning for mitigating privacy risks in language models. 
 \emph{arXiv preprint arXiv:2210.01504}."
2406.15796,joshi2017triviaqa,"[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa} Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017.",Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.,Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.,,"[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi2017triviaqa} Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer. 2017. 
 Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. 
 \emph{arXiv preprint arXiv:1705.03551}."
2406.15796,kamalloo2023evaluating,"[{Kamalloo et~al.(2023)Kamalloo, Dziri, Clarke, and Rafiei}]{kamalloo2023evaluating} Ehsan Kamalloo, Nouha Dziri, Charles~LA Clarke, and Davood Rafiei. 2023.",Evaluating open-domain question answering in the era of large language models.,Evaluating open-domain question answering in the era of large language models.,,"[{Kamalloo et~al.(2023)Kamalloo, Dziri, Clarke, and Rafiei}]{kamalloo2023evaluating} Ehsan Kamalloo, Nouha Dziri, Charles~LA Clarke, and Davood Rafiei. 2023. 
 Evaluating open-domain question answering in the era of large language models. 
 \emph{arXiv preprint arXiv:2305.06984}."
2406.15796,karamolegkou2023copyright,"[{Karamolegkou et~al.(2023)Karamolegkou, Li, Zhou, and S{\o}gaard}]{karamolegkou2023copyright} Antonia Karamolegkou, Jiaang Li, Li~Zhou, and Anders S{\o}gaard. 2023.",Copyright violations and large language models.,Copyright violations and large language models.,,"[{Karamolegkou et~al.(2023)Karamolegkou, Li, Zhou, and S{\o}gaard}]{karamolegkou2023copyright} Antonia Karamolegkou, Jiaang Li, Li~Zhou, and Anders S{\o}gaard. 2023. 
 Copyright violations and large language models. 
 \emph{arXiv preprint arXiv:2310.13771}."
2406.15796,li2024wmdp,"[{Li et~al.(2024)Li, Pan, Gopal, Yue, Berrios, Gatti, Li, Dombrowski, Goel, Phan et~al.}]{li2024wmdp} Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et~al. 2024.",The wmdp benchmark: Measuring and reducing malicious use with unlearning.,The wmdp benchmark: Measuring and reducing malicious use with unlearning.,,"[{Li et~al.(2024)Li, Pan, Gopal, Yue, Berrios, Gatti, Li, Dombrowski, Goel, Phan et~al.}]{li2024wmdp} Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et~al. 2024. 
 The wmdp benchmark: Measuring and reducing malicious use with unlearning. 
 \emph{arXiv preprint arXiv:2403.03218}."
2406.15796,li2023textbooks,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2406.15796,liu2024rethinking,"[{Liu et~al.(2024{\natexlab{b}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Xu, Yao, Li, Varshney et~al.}]{liu2024rethinking} Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush~R Varshney, et~al. 2024{\natexlab{b}}.",Rethinking machine unlearning for large language models.,Rethinking machine unlearning for large language models.,,"[{Liu et~al.(2024{\natexlab{b}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Xu, Yao, Li, Varshney et~al.}]{liu2024rethinking} Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush~R Varshney, et~al. 2024{\natexlab{b}}. 
 Rethinking machine unlearning for large language models. 
 \emph{arXiv preprint arXiv:2402.08787}."
2406.15796,liu2024towards,"[{Liu et~al.(2024{\natexlab{c}})Liu, Dou, Tan, Tian, and Jiang}]{liu2024towards} Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024{\natexlab{c}}.",Towards safer large language models through machine unlearning.,Towards safer large language models through machine unlearning.,,"[{Liu et~al.(2024{\natexlab{c}})Liu, Dou, Tan, Tian, and Jiang}]{liu2024towards} Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024{\natexlab{c}}. 
 Towards safer large language models through machine unlearning. 
 \emph{arXiv preprint arXiv:2402.10058}."
2406.15796,lu2024eraser,"[{Lu et~al.(2024)Lu, Zeng, Wang, Lu, Chen, Zhuang, and Chen}]{lu2024eraser} Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, and Cen Chen. 2024.",Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge.,Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge.,,"[{Lu et~al.(2024)Lu, Zeng, Wang, Lu, Chen, Zhuang, and Chen}]{lu2024eraser} Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, and Cen Chen. 2024. 
 Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge. 
 \emph{arXiv preprint arXiv:2404.05880}."
2406.15796,lynch2024eight,"[{Lynch et~al.(2024)Lynch, Guo, Ewart, Casper, and Hadfield-Menell}]{lynch2024eight} Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024.",Eight methods to evaluate robust unlearning in llms.,Eight methods to evaluate robust unlearning in llms.,,"[{Lynch et~al.(2024)Lynch, Guo, Ewart, Casper, and Hadfield-Menell}]{lynch2024eight} Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024. 
 Eight methods to evaluate robust unlearning in llms. 
 \emph{arXiv preprint arXiv:2402.16835}."
2406.15796,maini2024tofu,"[{Maini et~al.(2024)Maini, Feng, Schwarzschild, Lipton, and Kolter}]{maini2024tofu} Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary~C Lipton, and J~Zico Kolter. 2024.",Tofu: A task of fictitious unlearning for llms.,Tofu: A task of fictitious unlearning for llms.,,"[{Maini et~al.(2024)Maini, Feng, Schwarzschild, Lipton, and Kolter}]{maini2024tofu} Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary~C Lipton, and J~Zico Kolter. 2024. 
 Tofu: A task of fictitious unlearning for llms. 
 \emph{arXiv preprint arXiv:2401.06121}."
2406.15796,seegmiller2024llms,"[{Seegmiller et~al.(2024)Seegmiller, Gatto, Sharif, Basak, and Preum}]{seegmiller2024llms} Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, and Sarah~Masud Preum. 2024.",Do llms find human answers to fact-driven questions perplexing? a case study on reddit.,Do llms find human answers to fact-driven questions perplexing? a case study on reddit.,,"[{Seegmiller et~al.(2024)Seegmiller, Gatto, Sharif, Basak, and Preum}]{seegmiller2024llms} Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, and Sarah~Masud Preum. 2024. 
 Do llms find human answers to fact-driven questions perplexing? a case study on reddit. 
 \emph{arXiv preprint arXiv:2404.01147}."
2406.15796,shi2023detecting,"[{Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer}]{shi2023detecting} Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023.",Detecting pretraining data from large language models.,Detecting pretraining data from large language models.,,"[{Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer}]{shi2023detecting} Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. 
 Detecting pretraining data from large language models. 
 \emph{arXiv preprint arXiv:2310.16789}."
2406.15796,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.15796,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.15796,wang2024detoxifying,"[{Wang et~al.(2024)Wang, Zhang, Xu, Xi, Deng, Yao, Zhang, Yang, Wang, and Chen}]{wang2024detoxifying} Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. 2024.",Detoxifying large language models via knowledge editing.,Detoxifying large language models via knowledge editing.,,"[{Wang et~al.(2024)Wang, Zhang, Xu, Xi, Deng, Yao, Zhang, Yang, Wang, and Chen}]{wang2024detoxifying} Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. 2024. 
 Detoxifying large language models via knowledge editing. 
 \emph{arXiv preprint arXiv:2403.14472}."
2406.15796,weller2023according,"[{Weller et~al.(2023)Weller, Marone, Weir, Lawrie, Khashabi, and Van~Durme}]{weller2023according} Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van~Durme. 2023.",""" according to..."" prompting language models improves quoting from pre-training data.",""" according to..."" prompting language models improves quoting from pre-training data.",,"[{Weller et~al.(2023)Weller, Marone, Weir, Lawrie, Khashabi, and Van~Durme}]{weller2023according} Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van~Durme. 2023. 
 "" according to..."" prompting language models improves quoting from pre-training data. 
 \emph{arXiv preprint arXiv:2305.13252}."
2406.15796,yao2023large,"[{Yao et~al.(2023{\natexlab{a}})Yao, Xu, and Liu}]{yao2023large} Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023{\natexlab{a}}.",Large language model unlearning.,Large language model unlearning.,,"[{Yao et~al.(2023{\natexlab{a}})Yao, Xu, and Liu}]{yao2023large} Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023{\natexlab{a}}. 
 Large language model unlearning. 
 \emph{arXiv preprint arXiv:2310.10683}."
2406.15796,yao2023editing,"[{Yao et~al.(2023{\natexlab{b}})Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang}]{yao2023editing} Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023{\natexlab{b}}.","Editing large language models: Problems, methods, and opportunities.","Editing large language models: Problems, methods, and opportunities.",,"[{Yao et~al.(2023{\natexlab{b}})Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang}]{yao2023editing} Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023{\natexlab{b}}. 
 Editing large language models: Problems, methods, and opportunities. 
 \emph{arXiv preprint arXiv:2305.13172}."
2406.15796,zhang2023right,"[{Zhang et~al.(2023)Zhang, Finckenberg-Broman, Hoang, Pan, Xing, Staples, and Xu}]{zhang2023right} Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, and Xiwei Xu. 2023.","Right to be forgotten in the era of large language models: Implications, challenges, and solutions.","Right to be forgotten in the era of large language models: Implications, challenges, and solutions.",,"[{Zhang et~al.(2023)Zhang, Finckenberg-Broman, Hoang, Pan, Xing, Staples, and Xu}]{zhang2023right} Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, and Xiwei Xu. 2023. 
 Right to be forgotten in the era of large language models: Implications, challenges, and solutions. 
 \emph{arXiv preprint arXiv:2307.03941}."
2406.15796,zhang2024negative,"[{Zhang et~al.(2024)Zhang, Lin, Bai, and Mei}]{zhang2024negative} Ruiqi Zhang, Licong Lin, Yu~Bai, and Song Mei. 2024.",Negative preference optimization: From catastrophic collapse to effective unlearning.,Negative preference optimization: From catastrophic collapse to effective unlearning.,,"[{Zhang et~al.(2024)Zhang, Lin, Bai, and Mei}]{zhang2024negative} Ruiqi Zhang, Licong Lin, Yu~Bai, and Song Mei. 2024. 
 Negative preference optimization: From catastrophic collapse to effective unlearning. 
 \emph{arXiv preprint arXiv:2404.05868}."
2406.15796,zhang2019bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
2406.16061,almazrouei2023falcon,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Étienne Goffinet, Hesslow, Launay, Malartic, Mazzotta, Noune, Pannier, and Penedo}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023.",The falcon series of open language models.,The falcon series of open language models.,,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Étienne Goffinet, Hesslow, Launay, Malartic, Mazzotta, Noune, Pannier, and Penedo}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv: 2311.16867}."
2406.16061,austin2021program,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.",Program synthesis with large language models.,Program synthesis with large language models.,,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021. 
 Program synthesis with large language models. 
 \emph{arXiv preprint arXiv:2108.07732}."
2406.16061,azar2023ipo,"[{Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos}]{azar2023ipo} Mohammad~Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R{\'e}mi Munos. 2023.",A general theoretical paradigm to understand learning from human preferences.,A general theoretical paradigm to understand learning from human preferences.,,"[{Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos}]{azar2023ipo} Mohammad~Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R{\'e}mi Munos. 2023. 
 A general theoretical paradigm to understand learning from human preferences. 
 \emph{arXiv preprint arXiv:2310.12036}."
2406.16061,bai2022training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2406.16061,cai2023ulma,"[{Cai et~al.(2023)Cai, Song, Jiang, Teng, Gu, and Zhang}]{cai2023ulma} Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. 2023.",Ulma: Unified language model alignment with demonstration and point-wise human preference.,Ulma: Unified language model alignment with demonstration and point-wise human preference.,,"[{Cai et~al.(2023)Cai, Song, Jiang, Teng, Gu, and Zhang}]{cai2023ulma} Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. 2023. 
 Ulma: Unified language model alignment with demonstration and point-wise human preference. 
 \emph{arXiv preprint arXiv:2312.02554}."
2406.16061,chen2021evaluating,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}]{chen2021evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv: 2107.03374}."
2406.16061,chowdhury2024provably,"[{Chowdhury et~al.(2024)Chowdhury, Kini, and Natarajan}]{chowdhury2024provably} Sayak~Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. 2024.",Provably robust dpo: Aligning language models with noisy feedback.,Provably robust dpo: Aligning language models with noisy feedback.,,"[{Chowdhury et~al.(2024)Chowdhury, Kini, and Natarajan}]{chowdhury2024provably} Sayak~Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. 2024. 
 Provably robust dpo: Aligning language models with noisy feedback. 
 \emph{arXiv preprint arXiv: 2403.00409}."
2406.16061,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv: 1803.05457}."
2406.16061,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2406.16061,openai2023gpt4,[{et~al.(2023)}]{openai2023gpt4} OpenAI et~al. 2023.,Gpt-4 technical report.,Gpt-4 technical report.,,"[{et~al.(2023)}]{openai2023gpt4} OpenAI et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv: 2303.08774}."
2406.16061,gulcehre2023reinforced,"[{Gulcehre et~al.(2023)Gulcehre, Paine, Srinivasan, Konyushkova, Weerts, Sharma, Siddhant, Ahern, Wang, Gu et~al.}]{gulcehre2023reinforced} Caglar Gulcehre, Tom~Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et~al. 2023.",Reinforced self-training (rest) for language modeling.,Reinforced self-training (rest) for language modeling.,,"[{Gulcehre et~al.(2023)Gulcehre, Paine, Srinivasan, Konyushkova, Weerts, Sharma, Siddhant, Ahern, Wang, Gu et~al.}]{gulcehre2023reinforced} Caglar Gulcehre, Tom~Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et~al. 2023. 
 Reinforced self-training (rest) for language modeling. 
 \emph{arXiv preprint arXiv:2308.08998}."
2406.16061,hosseini2024v,"[{Hosseini et~al.(2024)Hosseini, Yuan, Malkin, Courville, Sordoni, and Agarwal}]{hosseini2024v} Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024.",V-star: Training verifiers for self-taught reasoners.,V-star: Training verifiers for self-taught reasoners.,,"[{Hosseini et~al.(2024)Hosseini, Yuan, Malkin, Courville, Sordoni, and Agarwal}]{hosseini2024v} Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. 
 V-star: Training verifiers for self-taught reasoners. 
 \emph{arXiv preprint arXiv:2402.06457}."
2406.16061,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv: 2106.09685}."
2406.16061,huang2022large,"[{Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han}]{huang2022large} Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.",Large language models can self-improve.,Large language models can self-improve.,,"[{Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han}]{huang2022large} Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. 
 Large language models can self-improve. 
 \emph{arXiv preprint arXiv:2210.11610}."
2406.16061,huang2022towards,[{Huang and Chang(2022)}]{huang2022towards} Jie Huang and Kevin Chen-Chuan Chang. 2022.,Towards reasoning in large language models: A survey.,Towards reasoning in large language models: A survey.,,"[{Huang and Chang(2022)}]{huang2022towards} Jie Huang and Kevin Chen-Chuan Chang. 2022. 
 Towards reasoning in large language models: A survey. 
 \emph{arXiv preprint arXiv:2212.10403}."
2406.16061,ji2023ai,"[{Ji et~al.(2023)Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang et~al.}]{ji2023ai} Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et~al. 2023.",Ai alignment: A comprehensive survey.,Ai alignment: A comprehensive survey.,,"[{Ji et~al.(2023)Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang et~al.}]{ji2023ai} Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et~al. 2023. 
 Ai alignment: A comprehensive survey. 
 \emph{arXiv preprint arXiv:2310.19852}."
2406.16061,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed}]{jiang2023mistral} Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed}]{jiang2023mistral} Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv: 2310.06825}."
2406.16061,rae2021scaling,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021. 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 \emph{arXiv preprint arXiv:2112.11446}."
2406.16061,saeidi2024insights,"[{Saeidi et~al.(2024)Saeidi, Verma, and Baral}]{saeidi2024insights} Amir Saeidi, Shivanshu Verma, and Chitta Baral. 2024.",Insights into alignment: Evaluating dpo and its variants across multiple tasks.,Insights into alignment: Evaluating dpo and its variants across multiple tasks.,,"[{Saeidi et~al.(2024)Saeidi, Verma, and Baral}]{saeidi2024insights} Amir Saeidi, Shivanshu Verma, and Chitta Baral. 2024. 
 Insights into alignment: Evaluating dpo and its variants across multiple tasks. 
 \emph{arXiv preprint arXiv: 2404.14723}."
2406.16061,schulman2017proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}."
2406.16061,singh2023beyond,"[{Singh et~al.(2023)Singh, Co-Reyes, Agarwal, Anand, Patil, Garcia, Liu, Harrison, Lee, Xu, Parisi, Kumar, Alemi, Rizkowsky, Nova, Adlam, Bohnet, Elsayed, Sedghi, Mordatch, Simpson, Gur, Snoek, Pennington, Hron, Kenealy, Swersky, Mahajan, Culp, Xiao, Bileschi, Constant, Novak, Liu, Warkentin, Qian, Bansal, Dyer, Neyshabur, Sohl-Dickstein, and Fiedel}]{singh2023beyond} Avi Singh, John~D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter~J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell~L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2023.",Beyond human data: Scaling self-training for problem-solving with language models.,Beyond human data: Scaling self-training for problem-solving with language models.,,"[{Singh et~al.(2023)Singh, Co-Reyes, Agarwal, Anand, Patil, Garcia, Liu, Harrison, Lee, Xu, Parisi, Kumar, Alemi, Rizkowsky, Nova, Adlam, Bohnet, Elsayed, Sedghi, Mordatch, Simpson, Gur, Snoek, Pennington, Hron, Kenealy, Swersky, Mahajan, Culp, Xiao, Bileschi, Constant, Novak, Liu, Warkentin, Qian, Bansal, Dyer, Neyshabur, Sohl-Dickstein, and Fiedel}]{singh2023beyond} Avi Singh, John~D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter~J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell~L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2023. 
 Beyond human data: Scaling self-training for problem-solving with language models. 
 \emph{arXiv preprint arXiv: 2312.06585}."
2406.16061,taylor2022galactica,"[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic}]{taylor2022galactica} Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022.",Galactica: A large language model for science.,Galactica: A large language model for science.,,"[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic}]{taylor2022galactica} Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. 
 Galactica: A large language model for science. 
 \emph{arXiv preprint arXiv: 2211.09085}."
2406.16061,team2024gemma,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024.",Gemma: Open models based on gemini research and technology.,Gemma: Open models based on gemini research and technology.,,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024. 
 Gemma: Open models based on gemini research and technology. 
 \emph{arXiv preprint arXiv:2403.08295}."
2406.16061,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv: 2307.09288}."
2406.16061,tunstall2023zephyr,"[{Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib et~al.}]{tunstall2023zephyr} Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al. 2023.",Zephyr: Direct distillation of lm alignment.,Zephyr: Direct distillation of lm alignment.,,"[{Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib et~al.}]{tunstall2023zephyr} Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier, Nathan Habib, et~al. 2023. 
 Zephyr: Direct distillation of lm alignment. 
 \emph{arXiv preprint arXiv:2310.16944}."
2406.16061,yuan2023scaling,"[{Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou}]{yuan2023scaling} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.",Scaling relationship on learning mathematical reasoning with large language models.,Scaling relationship on learning mathematical reasoning with large language models.,,"[{Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou}]{yuan2023scaling} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. 
 Scaling relationship on learning mathematical reasoning with large language models. 
 \emph{arXiv preprint arXiv:2308.01825}."
2406.16061,zhao2023slic,"[{Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu}]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu. 2023.",Slic-hf: Sequence likelihood calibration with human feedback.,Slic-hf: Sequence likelihood calibration with human feedback.,,"[{Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu}]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu. 2023. 
 Slic-hf: Sequence likelihood calibration with human feedback. 
 \emph{arXiv preprint arXiv:2305.10425}."
2406.16061,ziegler2019finetuning,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019finetuning} Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019finetuning} Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv: 1909.08593}."
2406.16316,adilazuarda2024measuring,"[{Adilazuarda et~al.(2024)Adilazuarda, Mukherjee, Lavania, Singh, Aji, O'Neill, Modi, and Choudhury}]{adilazuarda2024measuring} Muhammad~Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham~Fikri Aji, Jacki O'Neill, Ashutosh Modi, and Monojit Choudhury. 2024.","Towards measuring and modeling ""culture"" in llms: A survey.","Towards measuring and modeling ""culture"" in llms: A survey.",,"[{Adilazuarda et~al.(2024)Adilazuarda, Mukherjee, Lavania, Singh, Aji, O'Neill, Modi, and Choudhury}]{adilazuarda2024measuring} Muhammad~Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham~Fikri Aji, Jacki O'Neill, Ashutosh Modi, and Monojit Choudhury. 2024. 
 Towards measuring and modeling ""culture"" in llms: A survey. 
 \emph{arXiv preprint arXiv:2403.15412}."
2406.16316,bai2022training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk, Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk, Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2406.16316,chen2023phoenix,"[{Chen et~al.(2023)Chen, Jiang, Chen, Wang, Yu, Chen, Zhang, Liang, Zhang, Zhang, Li, Wan, Wang, and Li}]{chen2023phoenix} Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023.",Phoenix: Democratizing {ChatGPT} across languages.,Phoenix: Democratizing {ChatGPT} across languages.,,"[{Chen et~al.(2023)Chen, Jiang, Chen, Wang, Yu, Chen, Zhang, Liang, Zhang, Zhang, Li, Wan, Wang, and Li}]{chen2023phoenix} Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. 
 Phoenix: Democratizing {ChatGPT} across languages. 
 \emph{arXiv preprint arXiv:2304.10453}."
2406.16316,chiang2024chatbot,"[{Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang, Zhu, Jordan, Gonzalez, and Stoica}]{chiang2024chatbot} Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios~Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph~E. Gonzalez, and Ion Stoica. 2024.",Chatbot arena: An open platform for evaluating {LLMs} by human preference.,Chatbot arena: An open platform for evaluating {LLMs} by human preference.,,"[{Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang, Zhu, Jordan, Gonzalez, and Stoica}]{chiang2024chatbot} Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios~Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph~E. Gonzalez, and Ion Stoica. 2024. 
 Chatbot arena: An open platform for evaluating {LLMs} by human preference. 
 \emph{arXiv preprint arXiv:2403.04132}."
2406.16316,conitzer2024social,"[{Conitzer et~al.(2024)Conitzer, Freedman, Heitzig, Holliday, Jacobs, Lambert, Mossé, Pacuit, Russell, Schoelkopf, Tewolde, and Zwicker}]{conitzer2024social} Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley~H. Holliday, Bob~M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William~S. Zwicker. 2024.",Social choice for {AI} alignment: Dealing with diverse human feedback.,Social choice for {AI} alignment: Dealing with diverse human feedback.,,"[{Conitzer et~al.(2024)Conitzer, Freedman, Heitzig, Holliday, Jacobs, Lambert, Mossé, Pacuit, Russell, Schoelkopf, Tewolde, and Zwicker}]{conitzer2024social} Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley~H. Holliday, Bob~M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William~S. Zwicker. 2024. 
 Social choice for {AI} alignment: Dealing with diverse human feedback. 
 \emph{arXiv preprint arXiv:2404.10271}."
2406.16316,durmus2024measuring,"[{Durmus et~al.(2024)Durmus, Nguyen, Liao, Schiefer, Askell, Bakhtin, Chen, Hatfield-Dodds, Hernandez, Joseph, Lovitt, McCandlish, Sikder, Tamkin, Thamkul, Kaplan, Clark, and Ganguli}]{durmus2024measuring} Esin Durmus, Karina Nguyen, Thomas~I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. 2024.",Towards measuring the representation of subjective global opinions in language models.,Towards measuring the representation of subjective global opinions in language models.,,"[{Durmus et~al.(2024)Durmus, Nguyen, Liao, Schiefer, Askell, Bakhtin, Chen, Hatfield-Dodds, Hernandez, Joseph, Lovitt, McCandlish, Sikder, Tamkin, Thamkul, Kaplan, Clark, and Ganguli}]{durmus2024measuring} Esin Durmus, Karina Nguyen, Thomas~I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. 2024. 
 Towards measuring the representation of subjective global opinions in language models. 
 \emph{arXiv preprint arXiv:2306.16388}."
2406.16316,fujii2024continual,"[{Fujii et~al.(2024)Fujii, Nakamura, Loem, Iida, Ohi, Hattori, Shota, Mizuki, Yokota, and Okazaki}]{fujii2024continual} Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. 2024.",Continual pre-training for cross-lingual {LLM} adaptation: Enhancing {J}apanese language capabilities.,Continual pre-training for cross-lingual {LLM} adaptation: Enhancing {J}apanese language capabilities.,,"[{Fujii et~al.(2024)Fujii, Nakamura, Loem, Iida, Ohi, Hattori, Shota, Mizuki, Yokota, and Okazaki}]{fujii2024continual} Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. 2024. 
 Continual pre-training for cross-lingual {LLM} adaptation: Enhancing {J}apanese language capabilities. 
 \emph{arXiv preprint arXiv:2404.17790}."
2406.16316,guan2024hallusionbench,"[{Guan et~al.(2024)Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, Manocha, and Zhou}]{guan2024hallusionbench} Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2024.",Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models.,Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models.,,"[{Guan et~al.(2024)Guan, Liu, Wu, Xian, Li, Liu, Wang, Chen, Huang, Yacoob, Manocha, and Zhou}]{guan2024hallusionbench} Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2024. 
 Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. 
 \emph{arXiv preprint arXiv:2310.14566}."
2406.16316,jiang2022machines,"[{Jiang et~al.(2022)Jiang, Hwang, Bhagavatula, Bras, Liang, Dodge, Sakaguchi, Forbes, Borchardt, Gabriel, Tsvetkov, Etzioni, Sap, Rini, and Choi}]{jiang2022machines} Liwei Jiang, Jena~D. Hwang, Chandra Bhagavatula, Ronan~Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, and Yejin Choi. 2022.",Can machines learn morality? {T}he {D}elphi experiment.,Can machines learn morality? {T}he {D}elphi experiment.,,"[{Jiang et~al.(2022)Jiang, Hwang, Bhagavatula, Bras, Liang, Dodge, Sakaguchi, Forbes, Borchardt, Gabriel, Tsvetkov, Etzioni, Sap, Rini, and Choi}]{jiang2022machines} Liwei Jiang, Jena~D. Hwang, Chandra Bhagavatula, Ronan~Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, and Yejin Choi. 2022. 
 Can machines learn morality? {T}he {D}elphi experiment. 
 \emph{arXiv preprint arXiv:2110.07574}."
2406.16316,kirk2023personalisation,"[{Kirk et~al.(2023)Kirk, Vidgen, Röttger, and Hale}]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott~A. Hale. 2023.",Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,,"[{Kirk et~al.(2023)Kirk, Vidgen, Röttger, and Hale}]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott~A. Hale. 2023. 
 Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. 
 \emph{arXiv preprint arXiv:2303.05453}."
2406.16316,kirk2024prism,"[{Kirk et~al.(2024)Kirk, Whitefield, Röttger, Bean, Margatina, Ciro, Mosquera, Bartolo, Williams, He, Vidgen, and Hale}]{kirk2024prism} Hannah~Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He~He, Bertie Vidgen, and Scott~A. Hale. 2024.","The {PRISM} alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models.","The {PRISM} alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models.",,"[{Kirk et~al.(2024)Kirk, Whitefield, Röttger, Bean, Margatina, Ciro, Mosquera, Bartolo, Williams, He, Vidgen, and Hale}]{kirk2024prism} Hannah~Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He~He, Bertie Vidgen, and Scott~A. Hale. 2024. 
 The {PRISM} alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. 
 \emph{arXiv preprint arXiv:2404.16019}."
2406.16316,naous2024having,"[{Naous et~al.(2024)Naous, Ryan, Ritter, and Xu}]{naous2024having} Tarek Naous, Michael~J. Ryan, Alan Ritter, and Wei Xu. 2024.",Having beer after prayer? measuring cultural bias in large language models.,Having beer after prayer? measuring cultural bias in large language models.,,"[{Naous et~al.(2024)Naous, Ryan, Ritter, and Xu}]{naous2024having} Tarek Naous, Michael~J. Ryan, Alan Ritter, and Wei Xu. 2024. 
 Having beer after prayer? measuring cultural bias in large language models. 
 \emph{arXiv preprint arXiv:2305.14456}."
2406.16316,openai2024gpt4,"[{OpenAI et~al.(2024)OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk,   Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone,   Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph}]{openai2024gpt4} OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung~Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón~Posada Fishman, Juston Forte, Isabella Fulford, Leo   Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang~Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish~Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong~Wook Kim, Christina Kim, Yongjik Kim, Jan~Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak~Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan   Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott~Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de~Avila Belbute~Peres, Michael Petrov, Henrique~Ponde de~Oliveira~Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr~H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,   Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe~Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine~B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe~Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin~Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ~Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia   Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024.",{GPT-4} technical report.,{GPT-4} technical report.,,"[{OpenAI et~al.(2024)OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk,   Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone,   Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph}]{openai2024gpt4} OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung~Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón~Posada Fishman, Juston Forte, Isabella Fulford, Leo   Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang~Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish~Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong~Wook Kim, Christina Kim, Yongjik Kim, Jan~Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak~Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan   Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott~Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de~Avila Belbute~Peres, Michael Petrov, Henrique~Ponde de~Oliveira~Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr~H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,   Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe~Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine~B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe~Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin~Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ~Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia   Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. 
 {GPT-4} technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.16316,rao2024normad,"[{Rao et~al.(2024)Rao, Yerukola, Shah, Reinecke, and Sap}]{rao2024normad} Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. 2024.",Normad: A benchmark for measuring the cultural adaptability of large language models.,Normad: A benchmark for measuring the cultural adaptability of large language models.,,"[{Rao et~al.(2024)Rao, Yerukola, Shah, Reinecke, and Sap}]{rao2024normad} Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. 2024. 
 Normad: A benchmark for measuring the cultural adaptability of large language models. 
 \emph{arXiv preprint arXiv:2404.12464}."
2406.16316,rodionov2023evaluation,"[{Rodionov et~al.(2023)Rodionov, Goertzel, and Goertzel}]{rodionov2023evaluation} Sergey Rodionov, Zarathustra~Amadeus Goertzel, and Ben Goertzel. 2023.",An evaluation of {GPT-4} on the {ETHICS} dataset.,An evaluation of {GPT-4} on the {ETHICS} dataset.,,"[{Rodionov et~al.(2023)Rodionov, Goertzel, and Goertzel}]{rodionov2023evaluation} Sergey Rodionov, Zarathustra~Amadeus Goertzel, and Ben Goertzel. 2023. 
 An evaluation of {GPT-4} on the {ETHICS} dataset. 
 \emph{arXiv preprint arXiv:2309.10492}."
2406.16316,shaham2024multilingual,"[{Shaham et~al.(2024)Shaham, Herzig, Aharoni, Szpektor, Tsarfaty, and Eyal}]{shaham2024multilingual} Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024.",Multilingual instruction tuning with just a pinch of multilinguality.,Multilingual instruction tuning with just a pinch of multilinguality.,,"[{Shaham et~al.(2024)Shaham, Herzig, Aharoni, Szpektor, Tsarfaty, and Eyal}]{shaham2024multilingual} Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024. 
 Multilingual instruction tuning with just a pinch of multilinguality. 
 \emph{arXiv preprint arXiv:2401.01854}."
2406.16316,sidahmed2024perl,"[{Sidahmed et~al.(2024)Sidahmed, Phatale, Hutcheson, Lin, Chen, Yu, Jin, Komarytsia, Ahlheim, Zhu, Chaudhary, Li, Ganesh, Byrne, Hoffmann, Mansoor, Li, Rastogi, and Dixon}]{sidahmed2024perl} Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Simral Chaudhary, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, and Lucas Dixon. 2024.",Perl: Parameter efficient reinforcement learning from human feedback.,Perl: Parameter efficient reinforcement learning from human feedback.,,"[{Sidahmed et~al.(2024)Sidahmed, Phatale, Hutcheson, Lin, Chen, Yu, Jin, Komarytsia, Ahlheim, Zhu, Chaudhary, Li, Ganesh, Byrne, Hoffmann, Mansoor, Li, Rastogi, and Dixon}]{sidahmed2024perl} Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Simral Chaudhary, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, and Lucas Dixon. 2024. 
 Perl: Parameter efficient reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2403.10704}."
2406.16316,sorensen2024roadmap,"[{Sorensen et~al.(2024{\natexlab{b}})Sorensen, Moore, Fisher, Gordon, Mireshghallah, Rytting, Ye, Jiang, Lu, Dziri, Althoff, and Choi}]{sorensen2024roadmap} Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher~Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. 2024{\natexlab{b}}.",A roadmap to pluralistic alignment.,A roadmap to pluralistic alignment.,,"[{Sorensen et~al.(2024{\natexlab{b}})Sorensen, Moore, Fisher, Gordon, Mireshghallah, Rytting, Ye, Jiang, Lu, Dziri, Althoff, and Choi}]{sorensen2024roadmap} Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher~Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. 2024{\natexlab{b}}. 
 A roadmap to pluralistic alignment. 
 \emph{arXiv preprint arXiv:2402.05070}."
2406.16316,Touvron2023,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Michael, Ranjan, Xiaoqing, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{Touvron2023} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael, Smith Ranjan, Subramanian Xiaoqing, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Michael, Ranjan, Xiaoqing, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{Touvron2023} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael, Smith Ranjan, Subramanian Xiaoqing, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.16316,xu2024exploring,"[{Xu et~al.(2024)Xu, Dong, Guo, Wu, and Xiong}]{xu2024exploring} Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, and Deyi Xiong. 2024.","Exploring multilingual concepts of human value in large language models: Is value alignment consistent, transferable and controllable across languages?","Exploring multilingual concepts of human value in large language models: Is value alignment consistent, transferable and controllable across languages?",,"[{Xu et~al.(2024)Xu, Dong, Guo, Wu, and Xiong}]{xu2024exploring} Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, and Deyi Xiong. 2024. 
 Exploring multilingual concepts of human value in large language models: Is value alignment consistent, transferable and controllable across languages? 
 \emph{arXiv preprint arXiv:2402.18120}."
2406.16316,zhang2023chinese,"[{Zhang et~al.(2023)Zhang, Shi, Liu, Yuan, Li, Dong, Shu, Li, Wang, Lin, Huang, and Fu}]{zhang2023chinese} Ge~Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu~Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023.",Chinese open instruction generalist: A preliminary release.,Chinese open instruction generalist: A preliminary release.,,"[{Zhang et~al.(2023)Zhang, Shi, Liu, Yuan, Li, Dong, Shu, Li, Wang, Lin, Huang, and Fu}]{zhang2023chinese} Ge~Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu~Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023. 
 Chinese open instruction generalist: A preliminary release. 
 \emph{arXiv preprint arXiv:2304.07987}."
2406.16316,ziegler2020finetuning,"[{Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2020finetuning} Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[{Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2020finetuning} Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv:1909.08593}."
2406.16441,multiple,"[{Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman et~al.}]{multiple} Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, et~al. 2022.",Multipl-e: A scalable and extensible approach to benchmarking neural code generation.,Multipl-e: A scalable and extensible approach to benchmarking neural code generation.,,"[{Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman et~al.}]{multiple} Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, et~al. 2022. 
 Multipl-e: A scalable and extensible approach to benchmarking neural code generation. 
 \emph{arXiv preprint arXiv:2208.08227}."
2406.16441,deng2024r2c2,"[{Deng et~al.(2024)Deng, Liu, Zhu, Liu, Li, Wang, Zhao, Zhang, Wu, Yin et~al.}]{deng2024r2c2} Ken Deng, Jiaheng Liu, He~Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, et~al. 2024.",R2c2-coder: Enhancing and benchmarking real-world repository-level code completion abilities of code large language models.,R2c2-coder: Enhancing and benchmarking real-world repository-level code completion abilities of code large language models.,,"[{Deng et~al.(2024)Deng, Liu, Zhu, Liu, Li, Wang, Zhao, Zhang, Wu, Yin et~al.}]{deng2024r2c2} Ken Deng, Jiaheng Liu, He~Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, et~al. 2024. 
 R2c2-coder: Enhancing and benchmarking real-world repository-level code completion abilities of code large language models. 
 \emph{arXiv preprint arXiv:2406.01359}."
2406.16441,guo2024lemur,"[{Guo et~al.(2024{\natexlab{b}})Guo, Zhang, Le, Yang, Liu, Li, Zheng, Xu, Zang, Zheng et~al.}]{guo2024lemur} Hongcheng Guo, Wei Zhang, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, et~al. 2024{\natexlab{b}}.",Lemur: Log parsing with entropy sampling and chain-of-thought merging.,Lemur: Log parsing with entropy sampling and chain-of-thought merging.,,"[{Guo et~al.(2024{\natexlab{b}})Guo, Zhang, Le, Yang, Liu, Li, Zheng, Xu, Zang, Zheng et~al.}]{guo2024lemur} Hongcheng Guo, Wei Zhang, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, et~al. 2024{\natexlab{b}}. 
 Lemur: Log parsing with entropy sampling and chain-of-thought merging. 
 \emph{arXiv preprint arXiv:2402.18205}."
2406.16441,ji2024sevenllm,"[{Ji et~al.(2024)Ji, Yang, Chai, Wei, Yang, Duan, Wang, Sun, Guo, Li et~al.}]{ji2024sevenllm} Hangyuan Ji, Jian Yang, Linzheng Chai, Chaoren Wei, Liqun Yang, Yunlong Duan, Yunli Wang, Tianzhen Sun, Hongcheng Guo, Tongliang Li, et~al. 2024.","Sevenllm: Benchmarking, eliciting, and enhancing abilities of large language models in cyber threat intelligence.","Sevenllm: Benchmarking, eliciting, and enhancing abilities of large language models in cyber threat intelligence.",,"[{Ji et~al.(2024)Ji, Yang, Chai, Wei, Yang, Duan, Wang, Sun, Guo, Li et~al.}]{ji2024sevenllm} Hangyuan Ji, Jian Yang, Linzheng Chai, Chaoren Wei, Liqun Yang, Yunlong Duan, Yunli Wang, Tianzhen Sun, Hongcheng Guo, Tongliang Li, et~al. 2024. 
 Sevenllm: Benchmarking, eliciting, and enhancing abilities of large language models in cyber threat intelligence. 
 \emph{arXiv preprint arXiv:2405.03446}."
2406.16441,que2024d,"[{Que et~al.(2024)Que, Liu, Zhang, Zhang, Qu, Ma, Duan, Bai, Wang, Zhang et~al.}]{que2024d} Haoran Que, Jiaheng Liu, Ge~Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et~al. 2024.",D-cpt law: Domain-specific continual pre-training scaling law for large language models.,D-cpt law: Domain-specific continual pre-training scaling law for large language models.,,"[{Que et~al.(2024)Que, Liu, Zhang, Zhang, Qu, Ma, Duan, Bai, Wang, Zhang et~al.}]{que2024d} Haoran Que, Jiaheng Liu, Ge~Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et~al. 2024. 
 D-cpt law: Domain-specific continual pre-training scaling law for large language models. 
 \emph{arXiv preprint arXiv:2406.01375}."
2406.16441,zhang2024mapneo,"[{Zhang et~al.(2024)Zhang, Qu, Liu, Zhang, Lin, Yu, Pan, Cheng, Liu, Lin, Yuan, Zheng, Pang, Du, Liang, Ma, Li, Ma, Lin, Benetos, Yang, Zhou, Ma, Liu, Niu, Wang, Que, Liu, Liu, Guo, Gao, Zhou, Zhang, Zhou, Wang, Bai, Zhang, Zhang, Wang, Yang, Zhao, Zhang, Ouyang, Huang, and Chen}]{zhang2024mapneo} Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. 2024.",Map-neo: Highly capable and transparent bilingual large language model series.,Map-neo: Highly capable and transparent bilingual large language model series.,,"[{Zhang et~al.(2024)Zhang, Qu, Liu, Zhang, Lin, Yu, Pan, Cheng, Liu, Lin, Yuan, Zheng, Pang, Du, Liang, Ma, Li, Ma, Lin, Benetos, Yang, Zhou, Ma, Liu, Niu, Wang, Que, Liu, Liu, Guo, Gao, Zhou, Zhang, Zhou, Wang, Bai, Zhang, Zhang, Wang, Yang, Zhao, Zhang, Ouyang, Huang, and Chen}]{zhang2024mapneo} Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. 2024. 
 Map-neo: Highly capable and transparent bilingual large language model series. 
 \emph{arXiv preprint arXiv: 2405.19327}."
2406.16554,allenai_arc,"[{Clark et~al.(2018{\natexlab{a}})Clark, Cowhey, Etzioni, Khot,   Sabharwal, Schoenick, and Tafjord}]{allenai_arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018{\natexlab{a}}.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[{Clark et~al.(2018{\natexlab{a}})Clark, Cowhey, Etzioni, Khot,   Sabharwal, Schoenick, and Tafjord}]{allenai_arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018{\natexlab{a}}. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv:1803.05457v1}."
2406.16554,clark2018think,"[{Clark et~al.(2018{\natexlab{b}})Clark, Cowhey, Etzioni, Khot,   Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018{\natexlab{b}}.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[{Clark et~al.(2018{\natexlab{b}})Clark, Cowhey, Etzioni, Khot,   Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 2018{\natexlab{b}}. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.16554,dai2024deepseekmoe,"[{Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu   et~al.}]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi   Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al. 2024.",Deepseekmoe: Towards ultimate expert specialization in   mixture-of-experts language models.,Deepseekmoe: Towards ultimate expert specialization in   mixture-of-experts language models.,,"[{Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu   et~al.}]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi   Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al. 2024. 
 Deepseekmoe: Towards ultimate expert specialization in   mixture-of-experts language models. 
 \emph{arXiv preprint arXiv:2401.06066}."
2406.16554,devlin2018bert,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and   Toutanova}]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",Bert: Pre-training of deep bidirectional transformers for language   understanding.,Bert: Pre-training of deep bidirectional transformers for language   understanding.,,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and   Toutanova}]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. 
 Bert: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}."
2406.16554,fedus2022review,"[{Fedus et~al.(2022{\natexlab{a}})Fedus, Dean, and   Zoph}]{fedus2022review} William Fedus, Jeff Dean, and Barret Zoph. 2022{\natexlab{a}}.",A review of sparse expert models in deep learning.,A review of sparse expert models in deep learning.,,"[{Fedus et~al.(2022{\natexlab{a}})Fedus, Dean, and   Zoph}]{fedus2022review} William Fedus, Jeff Dean, and Barret Zoph. 2022{\natexlab{a}}. 
 A review of sparse expert models in deep learning. 
 \emph{arXiv preprint arXiv:2209.01667}."
2406.16554,hendrycks2020measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,   and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn   Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,   and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn   Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2406.16554,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2406.16554,komatsuzaki2022sparse,"[{Komatsuzaki et~al.(2022)Komatsuzaki, Puigcerver, Lee-Thorp, Ruiz,   Mustafa, Ainslie, Tay, Dehghani, and Houlsby}]{komatsuzaki2022sparse} Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos~Riquelme Ruiz, Basil   Mustafa, Joshua Ainslie, Yi~Tay, Mostafa Dehghani, and Neil Houlsby. 2022.",Sparse upcycling: Training mixture-of-experts from dense checkpoints.,Sparse upcycling: Training mixture-of-experts from dense checkpoints.,,"[{Komatsuzaki et~al.(2022)Komatsuzaki, Puigcerver, Lee-Thorp, Ruiz,   Mustafa, Ainslie, Tay, Dehghani, and Houlsby}]{komatsuzaki2022sparse} Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos~Riquelme Ruiz, Basil   Mustafa, Joshua Ainslie, Yi~Tay, Mostafa Dehghani, and Neil Houlsby. 2022. 
 Sparse upcycling: Training mixture-of-experts from dense checkpoints. 
 \emph{arXiv preprint arXiv:2212.05055}."
2406.16554,lepikhin2020gshard,"[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,   Shazeer, and Chen}]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping   Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.",Gshard: Scaling giant models with conditional computation and   automatic sharding.,Gshard: Scaling giant models with conditional computation and   automatic sharding.,,"[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,   Shazeer, and Chen}]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping   Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. 
 Gshard: Scaling giant models with conditional computation and   automatic sharding. 
 \emph{arXiv preprint arXiv:2006.16668}."
2406.16554,liu2020logiqa,"[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and   Zhang}]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.   2020.",Logiqa: A challenge dataset for machine reading comprehension with   logical reasoning.,Logiqa: A challenge dataset for machine reading comprehension with   logical reasoning.,,"[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and   Zhang}]{liu2020logiqa} Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.   2020. 
 Logiqa: A challenge dataset for machine reading comprehension with   logical reasoning. 
 \emph{arXiv preprint arXiv:2007.08124}."
2406.16554,lu2024mitigating,"[{Lu et~al.(2024)Lu, Tian, Wei, Qu, Cheng, Chen   et~al.}]{lu2024mitigating} Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu~Cheng, Dangyang Chen, et~al. 2024.",Mitigating boundary ambiguity and inherent bias for text   classification in the era of large language models.,Mitigating boundary ambiguity and inherent bias for text   classification in the era of large language models.,,"[{Lu et~al.(2024)Lu, Tian, Wei, Qu, Cheng, Chen   et~al.}]{lu2024mitigating} Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu~Cheng, Dangyang Chen, et~al. 2024. 
 Mitigating boundary ambiguity and inherent bias for text   classification in the era of large language models. 
 \emph{arXiv preprint arXiv:2406.07001}."
2406.16554,paperno2016lambada,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,   Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,   Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel   Fern{\'a}ndez. 2016.",The lambada dataset: Word prediction requiring a broad discourse   context.,The lambada dataset: Word prediction requiring a broad discourse   context.,,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,   Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,   Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel   Fern{\'a}ndez. 2016. 
 The lambada dataset: Word prediction requiring a broad discourse   context. 
 \emph{arXiv preprint arXiv:1606.06031}."
2406.16554,ramachandran2017swish,"[{Ramachandran et~al.(2017)Ramachandran, Zoph, and   Le}]{ramachandran2017swish} Prajit Ramachandran, Barret Zoph, and Quoc~V Le. 2017.",Searching for activation functions.,Searching for activation functions.,,"[{Ramachandran et~al.(2017)Ramachandran, Zoph, and   Le}]{ramachandran2017swish} Prajit Ramachandran, Barret Zoph, and Quoc~V Le. 2017. 
 Searching for activation functions. 
 \emph{arXiv preprint arXiv:1710.05941}."
2406.16554,shazeer2020glu,[{Shazeer(2020)}]{shazeer2020glu} Noam Shazeer. 2020.,Glu variants improve transformer.,Glu variants improve transformer.,,"[{Shazeer(2020)}]{shazeer2020glu} Noam Shazeer. 2020. 
 Glu variants improve transformer. 
 \emph{arXiv preprint arXiv:2002.05202}."
2406.16554,shazeer2017outrageously,"[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,   and Dean}]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,   Geoffrey Hinton, and Jeff Dean. 2017.",Outrageously large neural networks: The sparsely-gated   mixture-of-experts layer.,Outrageously large neural networks: The sparsely-gated   mixture-of-experts layer.,,"[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,   and Dean}]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,   Geoffrey Hinton, and Jeff Dean. 2017. 
 Outrageously large neural networks: The sparsely-gated   mixture-of-experts layer. 
 \emph{arXiv preprint arXiv:1701.06538}."
2406.16554,su2024living,"[{Su et~al.(2024{\natexlab{a}})Su, Li, Zhang, Zhu, Qu, Zhou, Bowen,   Cheng et~al.}]{su2024living} Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen,   Yu~Cheng, et~al. 2024{\natexlab{a}}.",Living in the moment: Can large language models grasp co-temporal   reasoning?,Living in the moment: Can large language models grasp co-temporal   reasoning?,,"[{Su et~al.(2024{\natexlab{a}})Su, Li, Zhang, Zhu, Qu, Zhou, Bowen,   Cheng et~al.}]{su2024living} Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen,   Yu~Cheng, et~al. 2024{\natexlab{a}}. 
 Living in the moment: Can large language models grasp co-temporal   reasoning? 
 \emph{arXiv preprint arXiv:2406.09072}."
2406.16554,tao2024magis,"[{Tao et~al.(2024)Tao, Zhou, Zhang, and Cheng}]{tao2024magis} Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu~Cheng. 2024.",Magis: Llm-based multi-agent framework for github issue resolution.,Magis: Llm-based multi-agent framework for github issue resolution.,,"[{Tao et~al.(2024)Tao, Zhou, Zhang, and Cheng}]{tao2024magis} Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu~Cheng. 2024. 
 Magis: Llm-based multi-agent framework for github issue resolution. 
 \emph{arXiv preprint arXiv:2403.17927}."
2406.16554,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar   et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar   et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.16554,wei2022emergent,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,   Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian   Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.   2022.",Emergent abilities of large language models.,Emergent abilities of large language models.,,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,   Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian   Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.   2022. 
 Emergent abilities of large language models. 
 \emph{arXiv preprint arXiv:2206.07682}."
2406.16554,welbl2017crowdsourcing,"[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl2017crowdsourcing} Johannes Welbl, Nelson~F Liu, and Matt Gardner. 2017.",Crowdsourcing multiple choice science questions.,Crowdsourcing multiple choice science questions.,,"[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl2017crowdsourcing} Johannes Welbl, Nelson~F Liu, and Matt Gardner. 2017. 
 Crowdsourcing multiple choice science questions. 
 \emph{arXiv preprint arXiv:1707.06209}."
2406.16554,xue2024openmoe,"[{Xue et~al.(2024)Xue, Zheng, Fu, Ni, Zheng, Zhou, and   You}]{xue2024openmoe} Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and   Yang You. 2024.",Openmoe: An early effort on open mixture-of-experts language models.,Openmoe: An early effort on open mixture-of-experts language models.,,"[{Xue et~al.(2024)Xue, Zheng, Fu, Ni, Zheng, Zhou, and   You}]{xue2024openmoe} Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and   Yang You. 2024. 
 Openmoe: An early effort on open mixture-of-experts language models. 
 \emph{arXiv preprint arXiv:2402.01739}."
2406.16554,zellers2019hellaswag,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}."
2406.16554,zhang2021moefication,"[{Zhang et~al.(2021)Zhang, Lin, Liu, Li, Sun, and   Zhou}]{zhang2021moefication} Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.   2021.",Moefication: Transformer feed-forward layers are mixtures of experts.,Moefication: Transformer feed-forward layers are mixtures of experts.,,"[{Zhang et~al.(2021)Zhang, Lin, Liu, Li, Sun, and   Zhou}]{zhang2021moefication} Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.   2021. 
 Moefication: Transformer feed-forward layers are mixtures of experts. 
 \emph{arXiv preprint arXiv:2110.01786}."
2406.16554,zoph2022st,"[{Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and   Fedus}]{zoph2022st} Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam   Shazeer, and William Fedus. 2022.",St-moe: Designing stable and transferable sparse expert models.,St-moe: Designing stable and transferable sparse expert models.,,"[{Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and   Fedus}]{zoph2022st} Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam   Shazeer, and William Fedus. 2022. 
 St-moe: Designing stable and transferable sparse expert models. 
 \emph{arXiv preprint arXiv:2202.08906}."
2406.16554,zuo2022moebert,"[{Zuo et~al.(2022)Zuo, Zhang, Liang, He, Zhao, and   Chen}]{zuo2022moebert} Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen.   2022.",Moebert: from bert to mixture-of-experts via importance-guided   adaptation.,Moebert: from bert to mixture-of-experts via importance-guided   adaptation.,,"[{Zuo et~al.(2022)Zuo, Zhang, Liang, He, Zhao, and   Chen}]{zuo2022moebert} Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen.   2022. 
 Moebert: from bert to mixture-of-experts via importance-guided   adaptation. 
 \emph{arXiv preprint arXiv:2204.07675}."
2406.16635,frankle2018lottery,[{Frankle and Carbin(2018)}]{frankle2018lottery} Jonathan Frankle and Michael Carbin. 2018.,"The lottery ticket hypothesis: Finding sparse, trainable neural networks.","The lottery ticket hypothesis: Finding sparse, trainable neural networks.",,"[{Frankle and Carbin(2018)}]{frankle2018lottery} Jonathan Frankle and Michael Carbin. 2018. 
 The lottery ticket hypothesis: Finding sparse, trainable neural networks. 
 \emph{arXiv preprint arXiv:1803.03635}."
2406.16635,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2406.16635,lee2018snip,"[{Lee et~al.(2018)Lee, Ajanthan, and Torr}]{lee2018snip} Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr. 2018.",Snip: Single-shot network pruning based on connection sensitivity.,Snip: Single-shot network pruning based on connection sensitivity.,,"[{Lee et~al.(2018)Lee, Ajanthan, and Torr}]{lee2018snip} Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr. 2018. 
 Snip: Single-shot network pruning based on connection sensitivity. 
 \emph{arXiv preprint arXiv:1810.02340}."
2406.16635,liang2022holistic,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}."
2406.16635,merity2016pointer,"[{Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.",Pointer sentinel mixture models.,Pointer sentinel mixture models.,,"[{Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. 
 Pointer sentinel mixture models. 
 \emph{arXiv preprint arXiv:1609.07843}."
2406.16635,mihaylov2018can,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.",Can a suit of armor conduct electricity? a new dataset for open book question answering.,Can a suit of armor conduct electricity? a new dataset for open book question answering.,,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. 
 Can a suit of armor conduct electricity? a new dataset for open book question answering. 
 \emph{arXiv preprint arXiv:1809.02789}."
2406.16635,min2022rethinking,"[{Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer}]{min2022rethinking} Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.",Rethinking the role of demonstrations: What makes in-context learning work?,Rethinking the role of demonstrations: What makes in-context learning work?,,"[{Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer}]{min2022rethinking} Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. 
 Rethinking the role of demonstrations: What makes in-context learning work? 
 \emph{arXiv preprint arXiv:2202.12837}."
2406.16635,molchanov2016pruning,"[{Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and Kautz}]{molchanov2016pruning} Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2016.",Pruning convolutional neural networks for resource efficient inference.,Pruning convolutional neural networks for resource efficient inference.,,"[{Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and Kautz}]{molchanov2016pruning} Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2016. 
 Pruning convolutional neural networks for resource efficient inference. 
 \emph{arXiv preprint arXiv:1611.06440}."
2406.16635,turner2019blockswap,"[{Turner et~al.(2019)Turner, Crowley, O'Boyle, Storkey, and Gray}]{turner2019blockswap} Jack Turner, Elliot~J Crowley, Michael O'Boyle, Amos Storkey, and Gavin Gray. 2019.",Blockswap: Fisher-guided block substitution for network compression on a budget.,Blockswap: Fisher-guided block substitution for network compression on a budget.,,"[{Turner et~al.(2019)Turner, Crowley, O'Boyle, Storkey, and Gray}]{turner2019blockswap} Jack Turner, Elliot~J Crowley, Michael O'Boyle, Amos Storkey, and Gavin Gray. 2019. 
 Blockswap: Fisher-guided block substitution for network compression on a budget. 
 \emph{arXiv preprint arXiv:1906.04113}."
2406.16635,wang2020picking,"[{Wang et~al.(2020)Wang, Zhang, and Grosse}]{wang2020picking} Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020.",Picking winning tickets before training by preserving gradient flow.,Picking winning tickets before training by preserving gradient flow.,,"[{Wang et~al.(2020)Wang, Zhang, and Grosse}]{wang2020picking} Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020. 
 Picking winning tickets before training by preserving gradient flow. 
 \emph{arXiv preprint arXiv:2002.07376}."
2406.1669,Bi2024deepseek,"[{Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, and Du}]{Bi2024deepseek} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, and Qiushi Du. 2024.",Deepseek llm: Scaling open-source language models with longtermism.,Deepseek llm: Scaling open-source language models with longtermism.,,"[{Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, and Du}]{Bi2024deepseek} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, and Qiushi Du. 2024. 
 Deepseek llm: Scaling open-source language models with longtermism. 
 \emph{arXiv preprint arXiv:2401.02954}."
2406.1669,flashattention2_2023,[{Dao(2023)}]{flashattention2_2023} Tri Dao. 2023.,Flashattention-2: Faster attention with better parallelism and work partitioning.,Flashattention-2: Faster attention with better parallelism and work partitioning.,,"[{Dao(2023)}]{flashattention2_2023} Tri Dao. 2023. 
 Flashattention-2: Faster attention with better parallelism and work partitioning. 
 \emph{arXiv preprint arXiv:2307.08691}."
2406.1669,fu2022hungry,"[{Fu et~al.(2022)Fu, Dao, Saab, Thomas, Rudra, and R{\'e}}]{fu2022hungry} Daniel~Y Fu, Tri Dao, Khaled~K Saab, Armin~W Thomas, Atri Rudra, and Christopher R{\'e}. 2022.",Hungry hungry hippos: Towards language modeling with state space models.,Hungry hungry hippos: Towards language modeling with state space models.,,"[{Fu et~al.(2022)Fu, Dao, Saab, Thomas, Rudra, and R{\'e}}]{fu2022hungry} Daniel~Y Fu, Tri Dao, Khaled~K Saab, Armin~W Thomas, Atri Rudra, and Christopher R{\'e}. 2022. 
 Hungry hungry hippos: Towards language modeling with state space models. 
 \emph{arXiv preprint arXiv:2212.14052}."
2406.1669,gao2024sparse,"[{Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}]{gao2024sparse} Leo Gao, Tom~Dupré la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024.",Scaling and evaluating sparse autoencoders.,Scaling and evaluating sparse autoencoders.,,"[{Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu}]{gao2024sparse} Leo Gao, Tom~Dupré la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024. 
 Scaling and evaluating sparse autoencoders. 
 \emph{arXiv preprint arXiv:2406.04093}."
2406.1669,mamba,[{Gu and Dao(2023)}]{mamba} Albert Gu and Tri Dao. 2023.,Mamba: Linear-time sequence modeling with selective state spaces.,Mamba: Linear-time sequence modeling with selective state spaces.,,"[{Gu and Dao(2023)}]{mamba} Albert Gu and Tri Dao. 2023. 
 Mamba: Linear-time sequence modeling with selective state spaces. 
 \emph{arXiv preprint arXiv:2312.00752}."
2406.1669,gu2021efficiently,"[{Gu et~al.(2021{\natexlab{a}})Gu, Goel, and R{\'e}}]{gu2021efficiently} Albert Gu, Karan Goel, and Christopher R{\'e}. 2021{\natexlab{a}}.",Efficiently modeling long sequences with structured state spaces.,Efficiently modeling long sequences with structured state spaces.,,"[{Gu et~al.(2021{\natexlab{a}})Gu, Goel, and R{\'e}}]{gu2021efficiently} Albert Gu, Karan Goel, and Christopher R{\'e}. 2021{\natexlab{a}}. 
 Efficiently modeling long sequences with structured state spaces. 
 \emph{arXiv preprint arXiv:2111.00396}."
2406.1669,Henighan_scaling_2020,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{Henighan_scaling_2020} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{Henighan_scaling_2020} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}."
2406.1669,danny_scaling_transfer_openai_2021,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{danny_scaling_transfer_openai_2021} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.",Scaling laws for transfer.,Scaling laws for transfer.,,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{danny_scaling_transfer_openai_2021} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. 
 Scaling laws for transfer. 
 \emph{arXiv preprint arXiv:2102.01293}."
2406.1669,jacob_scaling_law_rl_openai_2023,"[{Hilton et~al.(2023)Hilton, Tang, and Schulman}]{jacob_scaling_law_rl_openai_2023} Jacob Hilton, Jie Tang, and John Schulman. 2023.",Scaling laws for single-agent reinforcement learning.,Scaling laws for single-agent reinforcement learning.,,"[{Hilton et~al.(2023)Hilton, Tang, and Schulman}]{jacob_scaling_law_rl_openai_2023} Jacob Hilton, Jie Tang, and John Schulman. 2023. 
 Scaling laws for single-agent reinforcement learning. 
 \emph{arXiv preprint arXiv:2301.13442}."
2406.1669,jordan_chinchilla_2022,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark et~al.}]{jordan_chinchilla_2022} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark et~al.}]{jordan_chinchilla_2022} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2406.1669,ruler2024,"[{Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, and Ginsburg}]{ruler2024} Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024.",Ruler: What's the real context size of your long-context language models?,Ruler: What's the real context size of your long-context language models?,,"[{Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, and Ginsburg}]{ruler2024} Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. 
 Ruler: What's the real context size of your long-context language models? 
 \emph{arXiv preprint arXiv:2404.06654}."
2406.1669,hua2022transformer,"[{Hua et~al.(2022)Hua, Dai, Liu, and Le}]{hua2022transformer} Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc~V Le. 2022.",Transformer quality in linear time.,Transformer quality in linear time.,,"[{Hua et~al.(2022)Hua, Dai, Liu, and Le}]{hua2022transformer} Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc~V Le. 2022. 
 Transformer quality in linear time. 
 \emph{arXiv preprint arXiv:2202.10447}."
2406.1669,Isik2024downstream,"[{Isik et~al.(2024)Isik, Ponomareva, Hazimeh, Paparas, Vassilvitskii, and Koyejo}]{Isik2024downstream} Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. 2024.",Scaling laws for downstream task performance of large language models.,Scaling laws for downstream task performance of large language models.,,"[{Isik et~al.(2024)Isik, Ponomareva, Hazimeh, Paparas, Vassilvitskii, and Koyejo}]{Isik2024downstream} Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. 2024. 
 Scaling laws for downstream task performance of large language models. 
 \emph{arXiv preprint arXiv:2402.04177}."
2406.1669,jared_scaling_law_openai_2020,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{jared_scaling_law_openai_2020} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{jared_scaling_law_openai_2020} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2406.1669,liu2022neural,"[{Liu et~al.(2022)Liu, Li, Lu, Qin, Sun, Xu, and Zhong}]{liu2022neural} Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. 2022.",Neural architecture search on efficient transformers and beyond.,Neural architecture search on efficient transformers and beyond.,,"[{Liu et~al.(2022)Liu, Li, Lu, Qin, Sun, Xu, and Zhong}]{liu2022neural} Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. 2022. 
 Neural architecture search on efficient transformers and beyond. 
 \emph{arXiv preprint arXiv:2207.13955}."
2406.1669,gpt4,[{OpanAI(2023)}]{gpt4} OpanAI. 2023.,Gpt-4 technical report.,Gpt-4 technical report.,,"[{OpanAI(2023)}]{gpt4} OpanAI. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.1669,paperno2016lambada,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez. 2016.",The lambada dataset: Word prediction requiring a broad discourse context.,The lambada dataset: Word prediction requiring a broad discourse context.,,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez. 2016. 
 The lambada dataset: Word prediction requiring a broad discourse context. 
 \emph{arXiv preprint arXiv:1606.06031}."
2406.1669,qin2023toeplitz,"[{Qin et~al.(2023{\natexlab{a}})Qin, Han, Sun, He, Li, Li, Dai, Kong, and Zhong}]{qin2023toeplitz} Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. 2023{\natexlab{a}}.",Toeplitz neural network for sequence modeling.,Toeplitz neural network for sequence modeling.,,"[{Qin et~al.(2023{\natexlab{a}})Qin, Han, Sun, He, Li, Li, Dai, Kong, and Zhong}]{qin2023toeplitz} Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. 2023{\natexlab{a}}. 
 Toeplitz neural network for sequence modeling. 
 \emph{arXiv preprint arXiv:2305.04749}."
2406.1669,qin2024you,"[{Qin et~al.(2024{\natexlab{a}})Qin, Mao, Shen, Li, Zhang, Dai, and Zhong}]{qin2024you} Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. 2024{\natexlab{a}}.",You only scan once: Efficient multi-dimension sequential modeling with lightnet.,You only scan once: Efficient multi-dimension sequential modeling with lightnet.,,"[{Qin et~al.(2024{\natexlab{a}})Qin, Mao, Shen, Li, Zhang, Dai, and Zhong}]{qin2024you} Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. 2024{\natexlab{a}}. 
 You only scan once: Efficient multi-dimension sequential modeling with lightnet. 
 \emph{arXiv preprint arXiv:2405.21022}."
2406.1669,qin2024unlocking,"[{Qin et~al.(2024{\natexlab{b}})Qin, Shen, Sun, Li, Birchfield, Hartley, and Zhong}]{qin2024unlocking} Zhen Qin, Xuyang Shen, Weigao Sun, Dong Li, Stan Birchfield, Richard Hartley, and Yiran Zhong. 2024{\natexlab{b}}.",Unlocking the secrets of linear complexity sequence model from a unified perspective.,Unlocking the secrets of linear complexity sequence model from a unified perspective.,,"[{Qin et~al.(2024{\natexlab{b}})Qin, Shen, Sun, Li, Birchfield, Hartley, and Zhong}]{qin2024unlocking} Zhen Qin, Xuyang Shen, Weigao Sun, Dong Li, Stan Birchfield, Richard Hartley, and Yiran Zhong. 2024{\natexlab{b}}. 
 Unlocking the secrets of linear complexity sequence model from a unified perspective. 
 In \emph{arXiv preprint arXiv:2405.17383}."
2406.1669,qin2024lightning,"[{Qin et~al.(2024{\natexlab{c}})Qin, Sun, Li, Shen, Sun, and Zhong}]{qin2024lightning} Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024{\natexlab{c}}.",Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models.,Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models.,,"[{Qin et~al.(2024{\natexlab{c}})Qin, Sun, Li, Shen, Sun, and Zhong}]{qin2024lightning} Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024{\natexlab{c}}. 
 Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. 
 \emph{arXiv preprint arXiv:2401.04658}."
2406.1669,qin2024various,"[{Qin et~al.(2024{\natexlab{d}})Qin, Sun, Li, Shen, Sun, and Zhong}]{qin2024various} Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024{\natexlab{d}}.","Various lengths, constant speed: Efficient language modeling with lightning attention.","Various lengths, constant speed: Efficient language modeling with lightning attention.",,"[{Qin et~al.(2024{\natexlab{d}})Qin, Sun, Li, Shen, Sun, and Zhong}]{qin2024various} Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024{\natexlab{d}}. 
 Various lengths, constant speed: Efficient language modeling with lightning attention. 
 \emph{arXiv preprint arXiv:2405.17381}."
2406.1669,qin2024hgrn2,"[{Qin et~al.(2024{\natexlab{e}})Qin, Yang, Sun, Shen, Li, Sun, and Zhong}]{qin2024hgrn2} Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024{\natexlab{e}}.",Hgrn2: Gated linear rnns with state expansion.,Hgrn2: Gated linear rnns with state expansion.,,"[{Qin et~al.(2024{\natexlab{e}})Qin, Yang, Sun, Shen, Li, Sun, and Zhong}]{qin2024hgrn2} Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024{\natexlab{e}}. 
 Hgrn2: Gated linear rnns with state expansion. 
 \emph{arXiv preprint arXiv:2404.07904}."
2406.1669,shaham2022scrolls,"[{Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta, Xiong, Geva, Berant et~al.}]{shaham2022scrolls} Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et~al. 2022.",Scrolls: Standardized comparison over long language sequences.,Scrolls: Standardized comparison over long language sequences.,,"[{Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta, Xiong, Geva, Berant et~al.}]{shaham2022scrolls} Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et~al. 2022. 
 Scrolls: Standardized comparison over long language sequences. 
 \emph{arXiv preprint arXiv:2201.03533}."
2406.1669,hui_unraveling_2024,"[{Su et~al.(2024)Su, Tian, Shen, and Cai}]{hui_unraveling_2024} Hui Su, Zhi Tian, Xiaoyu Shen, and Xunliang Cai. 2024.",Unraveling the mystery of scaling laws: Part i.,Unraveling the mystery of scaling laws: Part i.,,"[{Su et~al.(2024)Su, Tian, Shen, and Cai}]{hui_unraveling_2024} Hui Su, Zhi Tian, Xiaoyu Shen, and Xunliang Cai. 2024. 
 Unraveling the mystery of scaling laws: Part i. 
 \emph{arXiv preprint arXiv:2403.06563}."
2406.1669,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.1669,llama2_2023,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2_2023} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2_2023} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.1669,yang2023gated,"[{Yang et~al.(2023)Yang, Wang, Shen, Panda, and Kim}]{yang2023gated} Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023.",Gated linear attention transformers with hardware-efficient training.,Gated linear attention transformers with hardware-efficient training.,,"[{Yang et~al.(2023)Yang, Wang, Shen, Panda, and Kim}]{yang2023gated} Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023. 
 Gated linear attention transformers with hardware-efficient training. 
 \emph{arXiv preprint arXiv:2312.06635}."
2406.16743,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.16743,bai2022training,"[{Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022{\natexlab{a}}.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022{\natexlab{a}}. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2406.16743,bai2022constitutional,"[{Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 2022{\natexlab{b}}.",Constitutional ai: Harmlessness from ai feedback.,Constitutional ai: Harmlessness from ai feedback.,,"[{Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 2022{\natexlab{b}}. 
 Constitutional ai: Harmlessness from ai feedback. 
 \emph{arXiv preprint arXiv:2212.08073}."
2406.16743,bhardwaj2023red,[{Bhardwaj and Poria(2023)}]{bhardwaj2023red} Rishabh Bhardwaj and Soujanya Poria. 2023.,Red-teaming large language models using chain of utterances for safety-alignment.,Red-teaming large language models using chain of utterances for safety-alignment.,,"[{Bhardwaj and Poria(2023)}]{bhardwaj2023red} Rishabh Bhardwaj and Soujanya Poria. 2023. 
 Red-teaming large language models using chain of utterances for safety-alignment. 
 \emph{arXiv preprint arXiv:2308.09662}."
2406.16743,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.16743,shen2023anything,"[{Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang}]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023.",""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",,"[{Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang}]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. 
 "" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. 
 \emph{arXiv preprint arXiv:2308.03825}."
2406.16743,sun2024trustllm,"[{Sun et~al.(2024)Sun, Huang, Wang, Wu, Zhang, Gao, Huang, Lyu, Zhang, Li et~al.}]{sun2024trustllm} Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al. 2024.",Trustllm: Trustworthiness in large language models.,Trustllm: Trustworthiness in large language models.,,"[{Sun et~al.(2024)Sun, Huang, Wang, Wu, Zhang, Gao, Huang, Lyu, Zhang, Li et~al.}]{sun2024trustllm} Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al. 2024. 
 Trustllm: Trustworthiness in large language models. 
 \emph{arXiv preprint arXiv:2401.05561}."
2406.16743,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.16743,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.16743,zou2023universal,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023.",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023. 
 Universal and transferable adversarial attacks on aligned language models. 
 \emph{arXiv preprint arXiv:2307.15043}."
2406.17224,gpt4,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2406.17224,summary1,"[Adams et~al.(2023)Adams, Fabbri, Ladhak, Lehman, and Elhadad]{summary1} Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, and No{\'e}mie Elhadad.",From sparse to dense: Gpt-4 summarization with chain of density prompting.,From sparse to dense: Gpt-4 summarization with chain of density prompting.,,"[Adams et~al.(2023)Adams, Fabbri, Ladhak, Lehman, and Elhadad]{summary1} Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, and No{\'e}mie Elhadad. 
 From sparse to dense: Gpt-4 summarization with chain of density prompting. 
 \emph{arXiv preprint arXiv:2309.04269}, 2023."
2406.17224,ancona2017towards,"[Ancona et~al.(2017)Ancona, Ceolini, {\""O}ztireli, and Gross]{ancona2017towards} Marco Ancona, Enea Ceolini, Cengiz {\""O}ztireli, and Markus Gross.",Towards better understanding of gradient-based attribution methods for deep neural networks.,Towards better understanding of gradient-based attribution methods for deep neural networks.,,"[Ancona et~al.(2017)Ancona, Ceolini, {\""O}ztireli, and Gross]{ancona2017towards} Marco Ancona, Enea Ceolini, Cengiz {\""O}ztireli, and Markus Gross. 
 Towards better understanding of gradient-based attribution methods for deep neural networks. 
 \emph{arXiv preprint arXiv:1711.06104}, 2017."
2406.17224,rlprompt,"[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and Hu]{rlprompt} Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric~P Xing, and Zhiting Hu.",Rlprompt: Optimizing discrete text prompts with reinforcement learning.,Rlprompt: Optimizing discrete text prompts with reinforcement learning.,,"[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and Hu]{rlprompt} Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric~P Xing, and Zhiting Hu. 
 Rlprompt: Optimizing discrete text prompts with reinforcement learning. 
 \emph{arXiv preprint arXiv:2205.12548}, 2022."
2406.17224,promptbreeder,"[Fernando et~al.(2023)Fernando, Banarse, Michalewski, Osindero, and Rockt{\""a}schel]{promptbreeder} Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt{\""a}schel.",Promptbreeder: Self-referential self-improvement via prompt evolution.,Promptbreeder: Self-referential self-improvement via prompt evolution.,,"[Fernando et~al.(2023)Fernando, Banarse, Michalewski, Osindero, and Rockt{\""a}schel]{promptbreeder} Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt{\""a}schel. 
 Promptbreeder: Self-referential self-improvement via prompt evolution. 
 \emph{arXiv preprint arXiv:2309.16797}, 2023."
2406.17224,distill,[Frosst and Hinton(2017)]{distill} Nicholas Frosst and Geoffrey Hinton.,Distilling a neural network into a soft decision tree.,Distilling a neural network into a soft decision tree.,,"[Frosst and Hinton(2017)]{distill} Nicholas Frosst and Geoffrey Hinton. 
 Distilling a neural network into a soft decision tree. 
 \emph{arXiv preprint arXiv:1711.09784}, 2017."
2406.17224,summary2,"[Goyal et~al.(2022)Goyal, Li, and Durrett]{summary2} Tanya Goyal, Junyi~Jessy Li, and Greg Durrett.",News summarization and evaluation in the era of gpt-3.,News summarization and evaluation in the era of gpt-3.,,"[Goyal et~al.(2022)Goyal, Li, and Durrett]{summary2} Tanya Goyal, Junyi~Jessy Li, and Greg Durrett. 
 News summarization and evaluation in the era of gpt-3. 
 \emph{arXiv preprint arXiv:2209.12356}, 2022."
2406.17224,genetic,"[Guo et~al.(2023)Guo, Wang, Guo, Li, Song, Tan, Liu, Bian, and Yang]{genetic} Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu~Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.",Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.,Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.,,"[Guo et~al.(2023)Guo, Wang, Guo, Li, Song, Tan, Liu, Bian, and Yang]{genetic} Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu~Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. 
 Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. 
 \emph{arXiv preprint arXiv:2309.08532}, 2023."
2406.17224,long_prompt,"[Hsieh et~al.(2023)Hsieh, Si, Yu, and Dhillon]{long_prompt} Cho-Jui Hsieh, Si~Si, Felix~X Yu, and Inderjit~S Dhillon.",Automatic engineering of long prompts.,Automatic engineering of long prompts.,,"[Hsieh et~al.(2023)Hsieh, Si, Yu, and Dhillon]{long_prompt} Cho-Jui Hsieh, Si~Si, Felix~X Yu, and Inderjit~S Dhillon. 
 Automatic engineering of long prompts. 
 \emph{arXiv preprint arXiv:2311.10117}, 2023."
2406.17224,losch2019interpretability,"[Losch et~al.(2019)Losch, Fritz, and Schiele]{losch2019interpretability} Max Losch, Mario Fritz, and Bernt Schiele.",Interpretability beyond classification output: Semantic bottleneck networks.,Interpretability beyond classification output: Semantic bottleneck networks.,,"[Losch et~al.(2019)Losch, Fritz, and Schiele]{losch2019interpretability} Max Losch, Mario Fritz, and Bernt Schiele. 
 Interpretability beyond classification output: Semantic bottleneck networks. 
 \emph{arXiv preprint arXiv:1907.10882}, 2019."
2406.17224,fgvc,"[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{fgvc} Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.",Fine-grained visual classification of aircraft.,Fine-grained visual classification of aircraft.,,"[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{fgvc} Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. 
 Fine-grained visual classification of aircraft. 
 \emph{arXiv preprint arXiv:1306.5151}, 2013."
2406.17224,oikarinen2023label,"[Oikarinen et~al.(2023)Oikarinen, Das, Nguyen, and Weng]{oikarinen2023label} Tuomas Oikarinen, Subhro Das, Lam~M Nguyen, and Tsui-Wei Weng.",Label-free concept bottleneck models.,Label-free concept bottleneck models.,,"[Oikarinen et~al.(2023)Oikarinen, Das, Nguyen, and Weng]{oikarinen2023label} Tuomas Oikarinen, Subhro Das, Lam~M Nguyen, and Tsui-Wei Weng. 
 Label-free concept bottleneck models. 
 \emph{arXiv preprint arXiv:2304.06129}, 2023."
2406.17224,petsiuk2018rise,"[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{petsiuk2018rise} Vitali Petsiuk, Abir Das, and Kate Saenko.",Rise: Randomized input sampling for explanation of black-box models.,Rise: Randomized input sampling for explanation of black-box models.,,"[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{petsiuk2018rise} Vitali Petsiuk, Abir Das, and Kate Saenko. 
 Rise: Randomized input sampling for explanation of black-box models. 
 \emph{arXiv preprint arXiv:1806.07421}, 2018."
2406.17224,apo,"[Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and Zeng]{apo} Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng.","Automatic prompt optimization with"" gradient descent"" and beam search.","Automatic prompt optimization with"" gradient descent"" and beam search.",,"[Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and Zeng]{apo} Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng. 
 Automatic prompt optimization with"" gradient descent"" and beam search. 
 \emph{arXiv preprint arXiv:2305.03495}, 2023."
2406.17224,summary4,[Pu and Demberg(2023)]{summary4} Dongqi Pu and Vera Demberg.,Chatgpt vs human-authored text: Insights into controllable text summarization and sentence style transfer.,Chatgpt vs human-authored text: Insights into controllable text summarization and sentence style transfer.,,"[Pu and Demberg(2023)]{summary4} Dongqi Pu and Vera Demberg. 
 Chatgpt vs human-authored text: Insights into controllable text summarization and sentence style transfer. 
 \emph{arXiv preprint arXiv:2306.07799}, 2023."
2406.17224,autoprompt,"[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh.",Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,,"[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 
 Autoprompt: Eliciting knowledge from language models with automatically generated prompts. 
 \emph{arXiv preprint arXiv:2010.15980}, 2020."
2406.17224,gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2406.17224,promptagent,"[Wang et~al.(2023)Wang, Li, Wang, Bai, Luo, Zhang, Jojic, Xing, and Hu]{promptagent} Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric~P Xing, and Zhiting Hu.",Promptagent: Strategic planning with language models enables expert-level prompt optimization.,Promptagent: Strategic planning with language models enables expert-level prompt optimization.,,"[Wang et~al.(2023)Wang, Li, Wang, Bai, Luo, Zhang, Jojic, Xing, and Hu]{promptagent} Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric~P Xing, and Zhiting Hu. 
 Promptagent: Strategic planning with language models enables expert-level prompt optimization. 
 \emph{arXiv preprint arXiv:2310.16427}, 2023."
2406.17224,gps,"[Xu et~al.(2022)Xu, Chen, Du, Shao, Wang, Li, and Yang]{gps} Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang.",Gps: Genetic prompt search for efficient few-shot learning.,Gps: Genetic prompt search for efficient few-shot learning.,,"[Xu et~al.(2022)Xu, Chen, Du, Shao, Wang, Li, and Yang]{gps} Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 
 Gps: Genetic prompt search for efficient few-shot learning. 
 \emph{arXiv preprint arXiv:2210.17041}, 2022."
2406.17224,opro,"[Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{opro} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen.",Large language models as optimizers.,Large language models as optimizers.,,"[Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{opro} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 
 Large language models as optimizers. 
 \emph{arXiv preprint arXiv:2309.03409}, 2023."
2406.17224,yuksekgonul2022post,"[Yuksekgonul et~al.(2022)Yuksekgonul, Wang, and Zou]{yuksekgonul2022post} Mert Yuksekgonul, Maggie Wang, and James Zou.",Post-hoc concept bottleneck models.,Post-hoc concept bottleneck models.,,"[Yuksekgonul et~al.(2022)Yuksekgonul, Wang, and Zou]{yuksekgonul2022post} Mert Yuksekgonul, Maggie Wang, and James Zou. 
 Post-hoc concept bottleneck models. 
 \emph{arXiv preprint arXiv:2205.15480}, 2022."
2406.17224,tempera,"[Zhang et~al.(2022)Zhang, Wang, Zhou, Schuurmans, and Gonzalez]{tempera} Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph~E Gonzalez.",Tempera: Test-time prompting via reinforcement learning.,Tempera: Test-time prompting via reinforcement learning.,,"[Zhang et~al.(2022)Zhang, Wang, Zhou, Schuurmans, and Gonzalez]{tempera} Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph~E Gonzalez. 
 Tempera: Test-time prompting via reinforcement learning. 
 \emph{arXiv preprint arXiv:2211.11890}, 2022."
2406.17224,ape,"[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba]{ape} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.",Large language models are human-level prompt engineers.,Large language models are human-level prompt engineers.,,"[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba]{ape} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 
 Large language models are human-level prompt engineers. 
 \emph{arXiv preprint arXiv:2211.01910}, 2022."
2406.17224,zintgraf2017visualizing,"[Zintgraf et~al.(2017)Zintgraf, Cohen, Adel, and Welling]{zintgraf2017visualizing} Luisa~M Zintgraf, Taco~S Cohen, Tameem Adel, and Max Welling.",Visualizing deep neural network decisions: Prediction difference analysis.,Visualizing deep neural network decisions: Prediction difference analysis.,,"[Zintgraf et~al.(2017)Zintgraf, Cohen, Adel, and Welling]{zintgraf2017visualizing} Luisa~M Zintgraf, Taco~S Cohen, Tameem Adel, and Max Welling. 
 Visualizing deep neural network decisions: Prediction difference analysis. 
 \emph{arXiv preprint arXiv:1702.04595}, 2017."
2406.17245,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.17245,buzzega2020dark,"[{Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and Calderara}]{buzzega2020dark} Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. 2020.","Dark experience for general continual learning: a strong, simple baseline.","Dark experience for general continual learning: a strong, simple baseline.",,"[{Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and Calderara}]{buzzega2020dark} Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. 2020. 
 Dark experience for general continual learning: a strong, simple baseline. 
 \emph{arXiv preprint arXiv:2004.07211}."
2406.17245,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.17245,gururangan2021demix,"[{Gururangan et~al.(2021)Gururangan, Lewis, Holtzman, Smith, and Zettlemoyer}]{gururangan2021demix} Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah~A Smith, and Luke Zettlemoyer. 2021.",Demix layers: Disentangling domains for modular language modeling.,Demix layers: Disentangling domains for modular language modeling.,,"[{Gururangan et~al.(2021)Gururangan, Lewis, Holtzman, Smith, and Zettlemoyer}]{gururangan2021demix} Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah~A Smith, and Luke Zettlemoyer. 2021. 
 Demix layers: Disentangling domains for modular language modeling. 
 \emph{arXiv preprint arXiv:2108.05036}."
2406.17245,jiang2024mixtral,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2406.17245,ke2023continual,"[{Ke et~al.(2023)Ke, Shao, Lin, Konishi, Kim, and Liu}]{ke2023continual} Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023.",Continual pre-training of language models.,Continual pre-training of language models.,,"[{Ke et~al.(2023)Ke, Shao, Lin, Konishi, Kim, and Liu}]{ke2023continual} Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023. 
 Continual pre-training of language models. 
 \emph{arXiv preprint arXiv:2302.03241}."
2406.17245,liu2019RoBERTa,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019RoBERTa} Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu2019RoBERTa} Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. 
 Roberta: A robustly optimized bert pretraining approach. 
 \emph{arXiv preprint arXiv:1907.11692}."
2406.17245,luo2024moelora,"[{Luo et~al.(2024)Luo, Lei, Lei, Liu, He, Zhao, and Liu}]{luo2024moelora} Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. 2024.",Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models.,Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models.,,"[{Luo et~al.(2024)Luo, Lei, Lei, Liu, He, Zhao, and Liu}]{luo2024moelora} Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. 2024. 
 Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. 
 \emph{arXiv preprint arXiv:2402.12851}."
2406.17245,qin2021lfpt5,[{Qin and Joty(2021)}]{qin2021lfpt5} Chengwei Qin and Shafiq Joty. 2021.,Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5.,Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5.,,"[{Qin and Joty(2021)}]{qin2021lfpt5} Chengwei Qin and Shafiq Joty. 2021. 
 Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5. 
 \emph{arXiv preprint arXiv:2110.07298}."
2406.17245,qin2022elle,"[{Qin et~al.(2022)Qin, Zhang, Lin, Liu, Li, Sun, and Zhou}]{qin2022elle} Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022.",Elle: Efficient lifelong pre-training for emerging data.,Elle: Efficient lifelong pre-training for emerging data.,,"[{Qin et~al.(2022)Qin, Zhang, Lin, Liu, Li, Sun, and Zhou}]{qin2022elle} Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022. 
 Elle: Efficient lifelong pre-training for emerging data. 
 \emph{arXiv preprint arXiv:2203.06311}."
2406.17245,qiu2023emergent,"[{Qiu et~al.(2023)Qiu, Huang, and Fu}]{qiu2023emergent} Zihan Qiu, Zeyu Huang, and Jie Fu. 2023.",Emergent mixture-of-experts: Can dense pre-trained transformers benefit from emergent modular structures?,Emergent mixture-of-experts: Can dense pre-trained transformers benefit from emergent modular structures?,,"[{Qiu et~al.(2023)Qiu, Huang, and Fu}]{qiu2023emergent} Zihan Qiu, Zeyu Huang, and Jie Fu. 2023. 
 Emergent mixture-of-experts: Can dense pre-trained transformers benefit from emergent modular structures? 
 \emph{arXiv preprint arXiv:2310.10908}."
2406.17245,sakaguchi2019winogrande,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.",Winogrande: An adversarial winograd schema challenge at scale.,Winogrande: An adversarial winograd schema challenge at scale.,,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. 
 Winogrande: An adversarial winograd schema challenge at scale. 
 \emph{arXiv preprint arXiv:1907.10641}."
2406.17245,shi2024continual,"[{Shi et~al.(2024)Shi, Xu, Wang, Qin, Wang, Wang, and Wang}]{shi2024continual} Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. 2024.",Continual learning of large language models: A comprehensive survey.,Continual learning of large language models: A comprehensive survey.,,"[{Shi et~al.(2024)Shi, Xu, Wang, Qin, Wang, Wang, and Wang}]{shi2024continual} Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. 2024. 
 Continual learning of large language models: A comprehensive survey. 
 \emph{arXiv preprint arXiv:2404.16789}."
2406.17245,touvron2023Llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023Llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023Llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.17245,wang2018glue,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}."
2406.17245,wang2024rehearsal,"[{Wang et~al.(2024{\natexlab{b}})Wang, Adel, Lange, Str{\""o}tgen, and Sch{\""u}tze}]{wang2024rehearsal} Mingyang Wang, Heike Adel, Lukas Lange, Jannik Str{\""o}tgen, and Hinrich Sch{\""u}tze. 2024{\natexlab{b}}.",Rehearsal-free modular and compositional continual learning for language models.,Rehearsal-free modular and compositional continual learning for language models.,,"[{Wang et~al.(2024{\natexlab{b}})Wang, Adel, Lange, Str{\""o}tgen, and Sch{\""u}tze}]{wang2024rehearsal} Mingyang Wang, Heike Adel, Lukas Lange, Jannik Str{\""o}tgen, and Hinrich Sch{\""u}tze. 2024{\natexlab{b}}. 
 Rehearsal-free modular and compositional continual learning for language models. 
 \emph{arXiv preprint arXiv:2404.00790}."
2406.17245,wang2023trace,"[{Wang et~al.(2023{\natexlab{c}})Wang, Zhang, Chen, Gao, Jin, Yang, Xi, Zheng, Zou, Gui et~al.}]{wang2023trace} Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, et~al. 2023{\natexlab{c}}.",Trace: A comprehensive benchmark for continual learning in large language models.,Trace: A comprehensive benchmark for continual learning in large language models.,,"[{Wang et~al.(2023{\natexlab{c}})Wang, Zhang, Chen, Gao, Jin, Yang, Xi, Zheng, Zou, Gui et~al.}]{wang2023trace} Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, et~al. 2023{\natexlab{c}}. 
 Trace: A comprehensive benchmark for continual learning in large language models. 
 \emph{arXiv preprint arXiv:2310.06762}."
2406.17245,wang2024inscl,"[{Wang et~al.(2024{\natexlab{d}})Wang, Liu, Shi, Li, Chen, Lu, and Yang}]{wang2024inscl} Yifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu, and Yujiu Yang. 2024{\natexlab{d}}.",Inscl: A data-efficient continual learning paradigm for fine-tuning large language models with instructions.,Inscl: A data-efficient continual learning paradigm for fine-tuning large language models with instructions.,,"[{Wang et~al.(2024{\natexlab{d}})Wang, Liu, Shi, Li, Chen, Lu, and Yang}]{wang2024inscl} Yifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu, and Yujiu Yang. 2024{\natexlab{d}}. 
 Inscl: A data-efficient continual learning paradigm for fine-tuning large language models with instructions. 
 \emph{arXiv preprint arXiv:2403.11435}."
2406.17245,wu2024continual,"[{Wu et~al.(2024)Wu, Luo, Li, Pan, Vu, and Haffari}]{wu2024continual} Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. 2024.",Continual learning for large language models: A survey.,Continual learning for large language models: A survey.,,"[{Wu et~al.(2024)Wu, Luo, Li, Pan, Vu, and Haffari}]{wu2024continual} Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. 2024. 
 Continual learning for large language models: A survey. 
 \emph{arXiv preprint arXiv:2402.01364}."
2406.17245,zhao2024sapt,"[{Zhao et~al.(2024)Zhao, Wang, Hu, Zhao, Qin, Zhang, Yang, Xu, and Che}]{zhao2024sapt} Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang, Dongliang Xu, and Wanxiang Che. 2024.",Sapt: A shared attention framework for parameter-efficient continual learning of large language models.,Sapt: A shared attention framework for parameter-efficient continual learning of large language models.,,"[{Zhao et~al.(2024)Zhao, Wang, Hu, Zhao, Qin, Zhang, Yang, Xu, and Che}]{zhao2024sapt} Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang, Dongliang Xu, and Wanxiang Che. 2024. 
 Sapt: A shared attention framework for parameter-efficient continual learning of large language models. 
 \emph{arXiv preprint arXiv:2401.08295}."
2406.17245,zheng2023learn,"[{Zheng et~al.(2023)Zheng, Qiu, and Ma}]{zheng2023learn} Junhao Zheng, Shengjie Qiu, and Qianli Ma. 2023.",Learn or recall? revisiting incremental learning with pre-trained language models.,Learn or recall? revisiting incremental learning with pre-trained language models.,,"[{Zheng et~al.(2023)Zheng, Qiu, and Ma}]{zheng2023learn} Junhao Zheng, Shengjie Qiu, and Qianli Ma. 2023. 
 Learn or recall? revisiting incremental learning with pre-trained language models. 
 \emph{arXiv preprint arXiv:2312.07887}."
2406.17245,zhu2024model,"[{Zhu et~al.(2024)Zhu, Sun, Li, Shen, Yan, Ding, Kuang, and Wu}]{zhu2024model} Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke~Yan, Shouhong Ding, Kun Kuang, and Chao Wu. 2024.",Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.,Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.,,"[{Zhu et~al.(2024)Zhu, Sun, Li, Shen, Yan, Ding, Kuang, and Wu}]{zhu2024model} Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke~Yan, Shouhong Ding, Kun Kuang, and Chao Wu. 2024. 
 Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. 
 \emph{arXiv preprint arXiv:2402.12048}."
2406.17519,bubeck2023sparks,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}."
2406.17519,izacard2020leveraging,[{Izacard and Grave(2020)}]{izacard2020leveraging} Gautier Izacard and Edouard Grave. 2020.,Leveraging passage retrieval with generative models for open domain question answering.,Leveraging passage retrieval with generative models for open domain question answering.,,"[{Izacard and Grave(2020)}]{izacard2020leveraging} Gautier Izacard and Edouard Grave. 2020. 
 Leveraging passage retrieval with generative models for open domain question answering. 
 \emph{arXiv preprint arXiv:2007.01282}."
2406.17519,leng2023mitigating,"[{Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing}]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.",Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,,"[{Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing}]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. 
 Mitigating object hallucinations in large vision-language models through visual contrastive decoding. 
 \emph{arXiv preprint arXiv:2311.16922}."
2406.17519,li2022contrastive,"[{Li et~al.(2022)Li, Holtzman, Fried, Liang, Eisner, Hashimoto, Zettlemoyer, and Lewis}]{li2022contrastive} Xiang~Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022.",Contrastive decoding: Open-ended text generation as optimization.,Contrastive decoding: Open-ended text generation as optimization.,,"[{Li et~al.(2022)Li, Holtzman, Fried, Liang, Eisner, Hashimoto, Zettlemoyer, and Lewis}]{li2022contrastive} Xiang~Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022. 
 Contrastive decoding: Open-ended text generation as optimization. 
 \emph{arXiv preprint arXiv:2210.15097}."
2406.17519,lin2023ra,"[{Lin et~al.(2023)Lin, Chen, Chen, Shi, Lomeli, James, Rodriguez, Kahn, Szilvasy, Lewis et~al.}]{lin2023ra} Xi~Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et~al. 2023.",Ra-dit: Retrieval-augmented dual instruction tuning.,Ra-dit: Retrieval-augmented dual instruction tuning.,,"[{Lin et~al.(2023)Lin, Chen, Chen, Shi, Lomeli, James, Rodriguez, Kahn, Szilvasy, Lewis et~al.}]{lin2023ra} Xi~Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et~al. 2023. 
 Ra-dit: Retrieval-augmented dual instruction tuning. 
 \emph{arXiv preprint arXiv:2310.01352}."
2406.17519,longpre2021entity,"[{Longpre et~al.(2021)Longpre, Perisetla, Chen, Ramesh, DuBois, and Singh}]{longpre2021entity} Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021.",Entity-based knowledge conflicts in question answering.,Entity-based knowledge conflicts in question answering.,,"[{Longpre et~al.(2021)Longpre, Perisetla, Chen, Ramesh, DuBois, and Singh}]{longpre2021entity} Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. 
 Entity-based knowledge conflicts in question answering. 
 \emph{arXiv preprint arXiv:2109.05052}."
2406.17519,o2023contrastive,[{O'Brien and Lewis(2023)}]{o2023contrastive} Sean O'Brien and Mike Lewis. 2023.,Contrastive decoding improves reasoning in large language models.,Contrastive decoding improves reasoning in large language models.,,"[{O'Brien and Lewis(2023)}]{o2023contrastive} Sean O'Brien and Mike Lewis. 2023. 
 Contrastive decoding improves reasoning in large language models. 
 \emph{arXiv preprint arXiv:2309.09117}."
2406.17519,petroni2020kilt,"[{Petroni et~al.(2020)Petroni, Piktus, Fan, Lewis, Yazdani, De~Cao, Thorne, Jernite, Karpukhin, Maillard et~al.}]{petroni2020kilt} Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De~Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et~al. 2020.",Kilt: a benchmark for knowledge intensive language tasks.,Kilt: a benchmark for knowledge intensive language tasks.,,"[{Petroni et~al.(2020)Petroni, Piktus, Fan, Lewis, Yazdani, De~Cao, Thorne, Jernite, Karpukhin, Maillard et~al.}]{petroni2020kilt} Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De~Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et~al. 2020. 
 Kilt: a benchmark for knowledge intensive language tasks. 
 \emph{arXiv preprint arXiv:2009.02252}."
2406.17519,qiu2024clongeval,"[{Qiu et~al.(2024)Qiu, Li, Huang, Zhong, and King}]{qiu2024clongeval} Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, and Irwin King. 2024.",Clongeval: A chinese benchmark for evaluating long-context large language models.,Clongeval: A chinese benchmark for evaluating long-context large language models.,,"[{Qiu et~al.(2024)Qiu, Li, Huang, Zhong, and King}]{qiu2024clongeval} Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, and Irwin King. 2024. 
 Clongeval: A chinese benchmark for evaluating long-context large language models. 
 \emph{arXiv preprint arXiv:2403.03514}."
2406.17519,shi2023trusting,"[{Shi et~al.(2023{\natexlab{b}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and Yih}]{shi2023trusting} Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023{\natexlab{b}}.",Trusting your evidence: Hallucinate less with context-aware decoding.,Trusting your evidence: Hallucinate less with context-aware decoding.,,"[{Shi et~al.(2023{\natexlab{b}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and Yih}]{shi2023trusting} Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023{\natexlab{b}}. 
 Trusting your evidence: Hallucinate less with context-aware decoding. 
 \emph{arXiv preprint arXiv:2305.14739}."
2406.17519,shi2023replug,"[{Shi et~al.(2023{\natexlab{c}})Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023{\natexlab{c}}.",Replug: Retrieval-augmented black-box language models.,Replug: Retrieval-augmented black-box language models.,,"[{Shi et~al.(2023{\natexlab{c}})Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug} Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023{\natexlab{c}}. 
 Replug: Retrieval-augmented black-box language models. 
 \emph{arXiv preprint arXiv:2301.12652}."
2406.17519,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.17519,van2022mutual,"[{Van~der Poel et~al.(2022)Van~der Poel, Cotterell, and Meister}]{van2022mutual} Liam Van~der Poel, Ryan Cotterell, and Clara Meister. 2022.",Mutual information alleviates hallucinations in abstractive summarization.,Mutual information alleviates hallucinations in abstractive summarization.,,"[{Van~der Poel et~al.(2022)Van~der Poel, Cotterell, and Meister}]{van2022mutual} Liam Van~der Poel, Ryan Cotterell, and Clara Meister. 2022. 
 Mutual information alleviates hallucinations in abstractive summarization. 
 \emph{arXiv preprint arXiv:2210.13210}."
2406.17519,varshney2023stitch,"[{Varshney et~al.(2023)Varshney, Yao, Zhang, Chen, and Yu}]{varshney2023stitch} Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023.",A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation.,A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation.,,"[{Varshney et~al.(2023)Varshney, Yao, Zhang, Chen, and Yu}]{varshney2023stitch} Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. 
 A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. 
 \emph{arXiv preprint arXiv:2307.03987}."
2406.17519,yen2024long,"[{Yen et~al.(2024)Yen, Gao, and Chen}]{yen2024long} Howard Yen, Tianyu Gao, and Danqi Chen. 2024.",Long-context language modeling with parallel context encoding.,Long-context language modeling with parallel context encoding.,,"[{Yen et~al.(2024)Yen, Gao, and Chen}]{yen2024long} Howard Yen, Tianyu Gao, and Danqi Chen. 2024. 
 Long-context language modeling with parallel context encoding. 
 \emph{arXiv preprint arXiv:2402.16617}."
2406.17519,zhang2024raft,"[{Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez}]{zhang2024raft} Tianjun Zhang, Shishir~G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph~E Gonzalez. 2024.",Raft: Adapting language model to domain specific rag.,Raft: Adapting language model to domain specific rag.,,"[{Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez}]{zhang2024raft} Tianjun Zhang, Shishir~G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph~E Gonzalez. 2024. 
 Raft: Adapting language model to domain specific rag. 
 \emph{arXiv preprint arXiv:2403.10131}."
2406.17642,quantifying-memorization,"[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang]{quantifying-memorization} Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.",Quantifying memorization across neural language models.,Quantifying memorization across neural language models.,,"[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang]{quantifying-memorization} Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 
 Quantifying memorization across neural language models. 
 \emph{arXiv preprint arXiv:2202.07646}, 2022."
2406.17642,continued-pretraining,"[Gupta et~al.(2023)Gupta, Th{\'e}rien, Ibrahim, Richter, Anthony, Belilovsky, Rish, and Lesort]{continued-pretraining} Kshitij Gupta, Benjamin Th{\'e}rien, Adam Ibrahim, Mats~L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth{\'e}e Lesort.",Continual pre-training of large language models: How to (re) warm your model?,Continual pre-training of large language models: How to (re) warm your model?,,"[Gupta et~al.(2023)Gupta, Th{\'e}rien, Ibrahim, Richter, Anthony, Belilovsky, Rish, and Lesort]{continued-pretraining} Kshitij Gupta, Benjamin Th{\'e}rien, Adam Ibrahim, Mats~L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth{\'e}e Lesort. 
 Continual pre-training of large language models: How to (re) warm your model? 
 \emph{arXiv preprint arXiv:2308.04014}, 2023."
2406.17642,mmlu,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}, 2020."
2406.17642,scaling-laws,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{scaling-laws} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{scaling-laws} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}, 2017."
2406.17642,chinchilla,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{chinchilla} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{chinchilla} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2406.17642,nucleus-sampling,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{nucleus-sampling} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.",The curious case of neural text degeneration.,The curious case of neural text degeneration.,,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{nucleus-sampling} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 
 The curious case of neural text degeneration. 
 \emph{arXiv preprint arXiv:1904.09751}, 2019."
2406.17642,lora,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}, 2021."
2406.17642,hallucination-survey,"[Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin, et~al.]{hallucination-survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.",,"[Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin, et~al.]{hallucination-survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 
 A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 
 \emph{arXiv preprint arXiv:2311.05232}, 2023."
2406.17642,mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2406.17642,openai-scaling-laws,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{openai-scaling-laws} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{openai-scaling-laws} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2406.17642,rag,"[Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and Lewis]{rag} Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.",Generalization through memorization: Nearest neighbor language models.,Generalization through memorization: Nearest neighbor language models.,,"[Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and Lewis]{rag} Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 
 Generalization through memorization: Nearest neighbor language models. 
 \emph{arXiv preprint arXiv:1911.00172}, 2019."
2406.17642,data-parallelism,[Krizhevsky(2014)]{data-parallelism} Alex Krizhevsky.,One weird trick for parallelizing convolutional neural networks.,One weird trick for parallelizing convolutional neural networks.,,"[Krizhevsky(2014)]{data-parallelism} Alex Krizhevsky. 
 One weird trick for parallelizing convolutional neural networks. 
 \emph{arXiv preprint arXiv:1404.5997}, 2014."
2406.17642,truthful-qa,"[Lin et~al.(2021)Lin, Hilton, and Evans]{truthful-qa} Stephanie Lin, Jacob Hilton, and Owain Evans.",Truthfulqa: Measuring how models mimic human falsehoods.,Truthfulqa: Measuring how models mimic human falsehoods.,,"[Lin et~al.(2021)Lin, Hilton, and Evans]{truthful-qa} Stephanie Lin, Jacob Hilton, and Owain Evans. 
 Truthfulqa: Measuring how models mimic human falsehoods. 
 \emph{arXiv preprint arXiv:2109.07958}, 2021."
2406.17642,retrieval-failures,"[Mallen et~al.(2022)Mallen, Asai, Zhong, Das, Khashabi, and Hajishirzi]{retrieval-failures} Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.",When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.,When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.,,"[Mallen et~al.(2022)Mallen, Asai, Zhong, Das, Khashabi, and Hajishirzi]{retrieval-failures} Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 
 When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. 
 \emph{arXiv preprint arXiv:2212.10511}, 2022."
2406.17642,memit,"[Meng et~al.(2022)Meng, Sharma, Andonian, Belinkov, and Bau]{memit} Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.",Mass-editing memory in a transformer.,Mass-editing memory in a transformer.,,"[Meng et~al.(2022)Meng, Sharma, Andonian, Belinkov, and Bau]{memit} Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 
 Mass-editing memory in a transformer. 
 \emph{arXiv preprint arXiv:2210.07229}, 2022."
2406.17642,mixed-precision-training,"[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{mixed-precision-training} Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al.",Mixed precision training.,Mixed precision training.,,"[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{mixed-precision-training} Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al. 
 Mixed precision training. 
 \emph{arXiv preprint arXiv:1710.03740}, 2017."
2406.17642,ift,"[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{ift} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al.",Multitask prompted training enables zero-shot task generalization.,Multitask prompted training enables zero-shot task generalization.,,"[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{ift} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 
 Multitask prompted training enables zero-shot task generalization. 
 \emph{arXiv preprint arXiv:2110.08207}, 2021."
2406.17642,moe,"[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{moe} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.",Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,,"[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{moe} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 
 \emph{arXiv preprint arXiv:1701.06538}, 2017."
2406.17642,llama2,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.17642,hallucinations-guaranteed,"[Xu et~al.(2024)Xu, Jain, and Kankanhalli]{hallucinations-guaranteed} Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli.",Hallucination is inevitable: An innate limitation of large language models.,Hallucination is inevitable: An innate limitation of large language models.,,"[Xu et~al.(2024)Xu, Jain, and Kankanhalli]{hallucinations-guaranteed} Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 
 Hallucination is inevitable: An innate limitation of large language models. 
 \emph{arXiv preprint arXiv:2401.11817}, 2024."
2406.17642,softmax-bottleneck,"[Yang et~al.(2017)Yang, Dai, Salakhutdinov, and Cohen]{softmax-bottleneck} Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen.",Breaking the softmax bottleneck: A high-rank rnn language model.,Breaking the softmax bottleneck: A high-rank rnn language model.,,"[Yang et~al.(2017)Yang, Dai, Salakhutdinov, and Cohen]{softmax-bottleneck} Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen. 
 Breaking the softmax bottleneck: A high-rank rnn language model. 
 \emph{arXiv preprint arXiv:1711.03953}, 2017."
2406.17642,rethinking-gen,"[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{rethinking-gen} Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.",Understanding deep learning requires rethinking generalization.,Understanding deep learning requires rethinking generalization.,,"[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{rethinking-gen} Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 
 Understanding deep learning requires rethinking generalization. 
 \emph{arXiv preprint arXiv:1611.03530}, 2017."
2406.17659,lester2021power,"[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power} B.~Lester, R.~Al-Rfou, and N.~Constant.",The power of scale for parameter-efficient prompt tuning.,The power of scale for parameter-efficient prompt tuning.,,"[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power} B.~Lester, R.~Al-Rfou, and N.~Constant. 
 The power of scale for parameter-efficient prompt tuning. 
 \emph{arXiv preprint arXiv:2104.08691}, 2021."
2406.17659,driess2020deep,"[Driess et~al.(2020{\natexlab{a}})Driess, Ha, and   Toussaint]{driess2020deep} D.~Driess, J.-S. Ha, and M.~Toussaint.",Deep visual reasoning: Learning to predict action sequences for task   and motion planning from an initial scene image.,Deep visual reasoning: Learning to predict action sequences for task   and motion planning from an initial scene image.,,"[Driess et~al.(2020{\natexlab{a}})Driess, Ha, and   Toussaint]{driess2020deep} D.~Driess, J.-S. Ha, and M.~Toussaint. 
 Deep visual reasoning: Learning to predict action sequences for task   and motion planning from an initial scene image. 
 \emph{arXiv preprint arXiv:2006.05398}, 2020{\natexlab{a}}."
2406.17659,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.",Bert: Pre-training of deep bidirectional transformers for language   understanding.,Bert: Pre-training of deep bidirectional transformers for language   understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2406.17659,chen2021evaluating,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen2021evaluating} M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,   Y.~Burda, N.~Joseph, G.~Brockman, et~al.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen2021evaluating} M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,   Y.~Burda, N.~Joseph, G.~Brockman, et~al. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}, 2021."
2406.17659,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,   X.~Li, X.~V. Lin, et~al.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,   X.~Li, X.~V. Lin, et~al. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2406.17659,ahn2022can,"[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,   Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can} M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn,   K.~Gopalakrishnan, K.~Hausman, A.~Herzog, et~al.","Do as i can, not as i say: Grounding language in robotic affordances.","Do as i can, not as i say: Grounding language in robotic affordances.",,"[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,   Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can} M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn,   K.~Gopalakrishnan, K.~Hausman, A.~Herzog, et~al. 
 Do as i can, not as i say: Grounding language in robotic affordances. 
 \emph{arXiv preprint arXiv:2204.01691}, 2022."
2406.17659,huang2022inner,"[Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng,   Tompson, Mordatch, Chebotar, et~al.]{huang2022inner} W.~Huang, F.~Xia, T.~Xiao, H.~Chan, J.~Liang, P.~Florence, A.~Zeng, J.~Tompson,   I.~Mordatch, Y.~Chebotar, et~al.",Inner monologue: Embodied reasoning through planning with language   models.,Inner monologue: Embodied reasoning through planning with language   models.,,"[Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng,   Tompson, Mordatch, Chebotar, et~al.]{huang2022inner} W.~Huang, F.~Xia, T.~Xiao, H.~Chan, J.~Liang, P.~Florence, A.~Zeng, J.~Tompson,   I.~Mordatch, Y.~Chebotar, et~al. 
 Inner monologue: Embodied reasoning through planning with language   models. 
 \emph{arXiv preprint arXiv:2207.05608}, 2022."
2406.17659,singh2022progprompt,"[Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,   Thomason, and Garg]{singh2022progprompt} I.~Singh, V.~Blukis, A.~Mousavian, A.~Goyal, D.~Xu, J.~Tremblay, D.~Fox,   J.~Thomason, and A.~Garg.",Progprompt: Generating situated robot task plans using large language   models.,Progprompt: Generating situated robot task plans using large language   models.,,"[Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,   Thomason, and Garg]{singh2022progprompt} I.~Singh, V.~Blukis, A.~Mousavian, A.~Goyal, D.~Xu, J.~Tremblay, D.~Fox,   J.~Thomason, and A.~Garg. 
 Progprompt: Generating situated robot task plans using large language   models. 
 \emph{arXiv preprint arXiv:2209.11302}, 2022."
2406.17659,rana2023sayplan,"[Rana et~al.(2023)Rana, Haviland, Garg, Abou-Chakra, Reid, and   Suenderhauf]{rana2023sayplan} K.~Rana, J.~Haviland, S.~Garg, J.~Abou-Chakra, I.~Reid, and N.~Suenderhauf.",Sayplan: Grounding large language models using 3d scene graphs for   scalable task planning.,Sayplan: Grounding large language models using 3d scene graphs for   scalable task planning.,,"[Rana et~al.(2023)Rana, Haviland, Garg, Abou-Chakra, Reid, and   Suenderhauf]{rana2023sayplan} K.~Rana, J.~Haviland, S.~Garg, J.~Abou-Chakra, I.~Reid, and N.~Suenderhauf. 
 Sayplan: Grounding large language models using 3d scene graphs for   scalable task planning. 
 \emph{arXiv preprint arXiv:2307.06135}, 2023."
2406.17659,valmeekam2022large,"[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and   Kambhampati]{valmeekam2022large} K.~Valmeekam, A.~Olmo, S.~Sreedharan, and S.~Kambhampati.",Large language models still can't plan (a benchmark for llms on   planning and reasoning about change).,Large language models still can't plan (a benchmark for llms on   planning and reasoning about change).,,"[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and   Kambhampati]{valmeekam2022large} K.~Valmeekam, A.~Olmo, S.~Sreedharan, and S.~Kambhampati. 
 Large language models still can't plan (a benchmark for llms on   planning and reasoning about change). 
 \emph{arXiv preprint arXiv:2206.10498}, 2022."
2406.17659,pallagani2022plansformer,"[Pallagani et~al.(2022)Pallagani, Muppasani, Murugesan, Rossi, Horesh,   Srivastava, Fabiano, and Loreggia]{pallagani2022plansformer} V.~Pallagani, B.~Muppasani, K.~Murugesan, F.~Rossi, L.~Horesh, B.~Srivastava,   F.~Fabiano, and A.~Loreggia.",Plansformer: Generating symbolic plans using transformers.,Plansformer: Generating symbolic plans using transformers.,,"[Pallagani et~al.(2022)Pallagani, Muppasani, Murugesan, Rossi, Horesh,   Srivastava, Fabiano, and Loreggia]{pallagani2022plansformer} V.~Pallagani, B.~Muppasani, K.~Murugesan, F.~Rossi, L.~Horesh, B.~Srivastava,   F.~Fabiano, and A.~Loreggia. 
 Plansformer: Generating symbolic plans using transformers. 
 \emph{arXiv preprint arXiv:2212.08681}, 2022."
2406.17659,arora2023learning,[Arora and Kambhampati(2023)]{arora2023learning} D.~Arora and S.~Kambhampati.,Learning and leveraging verifiers to improve planning capabilities of   pre-trained language models.,Learning and leveraging verifiers to improve planning capabilities of   pre-trained language models.,,"[Arora and Kambhampati(2023)]{arora2023learning} D.~Arora and S.~Kambhampati. 
 Learning and leveraging verifiers to improve planning capabilities of   pre-trained language models. 
 \emph{arXiv preprint arXiv:2305.17077}, 2023."
2406.17659,chen2023autotamp,"[Chen et~al.(2023)Chen, Arkin, Zhang, Roy, and Fan]{chen2023autotamp} Y.~Chen, J.~Arkin, Y.~Zhang, N.~Roy, and C.~Fan.",Autotamp: Autoregressive task and motion planning with llms as   translators and checkers.,Autotamp: Autoregressive task and motion planning with llms as   translators and checkers.,,"[Chen et~al.(2023)Chen, Arkin, Zhang, Roy, and Fan]{chen2023autotamp} Y.~Chen, J.~Arkin, Y.~Zhang, N.~Roy, and C.~Fan. 
 Autotamp: Autoregressive task and motion planning with llms as   translators and checkers. 
 \emph{arXiv preprint arXiv:2306.06531}, 2023."
2406.17659,wang2024llm,"[Wang et~al.(2024)Wang, Han, Jiao, Zhang, Wu, Zhu, and   Liu]{wang2024llm} S.~Wang, M.~Han, Z.~Jiao, Z.~Zhang, Y.~N. Wu, S.-C. Zhu, and H.~Liu.",Llm\^{} 3: Large language model-based task and motion planning with   motion failure reasoning.,Llm\^{} 3: Large language model-based task and motion planning with   motion failure reasoning.,,"[Wang et~al.(2024)Wang, Han, Jiao, Zhang, Wu, Zhu, and   Liu]{wang2024llm} S.~Wang, M.~Han, Z.~Jiao, Z.~Zhang, Y.~N. Wu, S.-C. Zhu, and H.~Liu. 
 Llm\^{} 3: Large language model-based task and motion planning with   motion failure reasoning. 
 \emph{arXiv preprint arXiv:2403.11552}, 2024."
2406.17659,liu2023llm+,"[Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and   Stone]{liu2023llm+} B.~Liu, Y.~Jiang, X.~Zhang, Q.~Liu, S.~Zhang, J.~Biswas, and P.~Stone.",Llm+ p: Empowering large language models with optimal planning   proficiency.,Llm+ p: Empowering large language models with optimal planning   proficiency.,,"[Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and   Stone]{liu2023llm+} B.~Liu, Y.~Jiang, X.~Zhang, Q.~Liu, S.~Zhang, J.~Biswas, and P.~Stone. 
 Llm+ p: Empowering large language models with optimal planning   proficiency. 
 \emph{arXiv preprint arXiv:2304.11477}, 2023."
2406.17659,stein2023autoplanbench,[Stein and Koller(2023)]{stein2023autoplanbench} K.~Stein and A.~Koller.,Autoplanbench:: Automatically generating benchmarks for llm planners   from pddl.,Autoplanbench:: Automatically generating benchmarks for llm planners   from pddl.,,"[Stein and Koller(2023)]{stein2023autoplanbench} K.~Stein and A.~Koller. 
 Autoplanbench:: Automatically generating benchmarks for llm planners   from pddl. 
 \emph{arXiv preprint arXiv:2311.09830}, 2023."
2406.17659,ding2023task,"[Ding et~al.(2023)Ding, Zhang, Paxton, and Zhang]{ding2023task} Y.~Ding, X.~Zhang, C.~Paxton, and S.~Zhang.",Task and motion planning with large language models for object   rearrangement.,Task and motion planning with large language models for object   rearrangement.,,"[Ding et~al.(2023)Ding, Zhang, Paxton, and Zhang]{ding2023task} Y.~Ding, X.~Zhang, C.~Paxton, and S.~Zhang. 
 Task and motion planning with large language models for object   rearrangement. 
 \emph{arXiv preprint arXiv:2303.06247}, 2023."
2406.17659,zhang2024mm,"[Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu]{zhang2024mm} D.~Zhang, Y.~Yu, C.~Li, J.~Dong, D.~Su, C.~Chu, and D.~Yu.",Mm-llms: Recent advances in multimodal large language models.,Mm-llms: Recent advances in multimodal large language models.,,"[Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu]{zhang2024mm} D.~Zhang, Y.~Yu, C.~Li, J.~Dong, D.~Su, C.~Chu, and D.~Yu. 
 Mm-llms: Recent advances in multimodal large language models. 
 \emph{arXiv preprint arXiv:2401.13601}, 2024."
2406.17659,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,   J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,   J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2406.17659,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut,   J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut,   J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2406.17659,wake2023gpt,"[Wake et~al.(2023)Wake, Kanehira, Sasabuchi, Takamatsu, and   Ikeuchi]{wake2023gpt} N.~Wake, A.~Kanehira, K.~Sasabuchi, J.~Takamatsu, and K.~Ikeuchi.",Gpt-4v (ision) for robotics: Multimodal task planning from human   demonstration.,Gpt-4v (ision) for robotics: Multimodal task planning from human   demonstration.,,"[Wake et~al.(2023)Wake, Kanehira, Sasabuchi, Takamatsu, and   Ikeuchi]{wake2023gpt} N.~Wake, A.~Kanehira, K.~Sasabuchi, J.~Takamatsu, and K.~Ikeuchi. 
 Gpt-4v (ision) for robotics: Multimodal task planning from human   demonstration. 
 \emph{arXiv preprint arXiv:2311.12015}, 2023."
2406.17659,guan2024task,"[Guan et~al.(2024)Guan, Zhou, Liu, Zha, Amor, and   Kambhampati]{guan2024task} L.~Guan, Y.~Zhou, D.~Liu, Y.~Zha, H.~B. Amor, and S.~Kambhampati.",""" task success"" is not enough: Investigating the use of   video-language models as behavior critics for catching undesirable agent   behaviors.",""" task success"" is not enough: Investigating the use of   video-language models as behavior critics for catching undesirable agent   behaviors.",,"[Guan et~al.(2024)Guan, Zhou, Liu, Zha, Amor, and   Kambhampati]{guan2024task} L.~Guan, Y.~Zhou, D.~Liu, Y.~Zha, H.~B. Amor, and S.~Kambhampati. 
 "" task success"" is not enough: Investigating the use of   video-language models as behavior critics for catching undesirable agent   behaviors. 
 \emph{arXiv preprint arXiv:2402.04210}, 2024."
2406.17659,sermanet2023robovqa,"[Sermanet et~al.(2023)Sermanet, Ding, Zhao, Xia, Dwibedi,   Gopalakrishnan, Chan, Dulac-Arnold, Maddineni, Joshi,   et~al.]{sermanet2023robovqa} P.~Sermanet, T.~Ding, J.~Zhao, F.~Xia, D.~Dwibedi, K.~Gopalakrishnan, C.~Chan,   G.~Dulac-Arnold, S.~Maddineni, N.~J. Joshi, et~al.",Robovqa: Multimodal long-horizon reasoning for robotics.,Robovqa: Multimodal long-horizon reasoning for robotics.,,"[Sermanet et~al.(2023)Sermanet, Ding, Zhao, Xia, Dwibedi,   Gopalakrishnan, Chan, Dulac-Arnold, Maddineni, Joshi,   et~al.]{sermanet2023robovqa} P.~Sermanet, T.~Ding, J.~Zhao, F.~Xia, D.~Dwibedi, K.~Gopalakrishnan, C.~Chan,   G.~Dulac-Arnold, S.~Maddineni, N.~J. Joshi, et~al. 
 Robovqa: Multimodal long-horizon reasoning for robotics. 
 \emph{arXiv preprint arXiv:2311.00899}, 2023."
2406.17659,fan2022minedojo,"[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang,   Zhu, and Anandkumar]{fan2022minedojo} L.~Fan, G.~Wang, Y.~Jiang, A.~Mandlekar, Y.~Yang, H.~Zhu, A.~Tang, D.-A. Huang,   Y.~Zhu, and A.~Anandkumar.",Minedojo: Building open-ended embodied agents with internet-scale   knowledge.,Minedojo: Building open-ended embodied agents with internet-scale   knowledge.,,"[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang,   Zhu, and Anandkumar]{fan2022minedojo} L.~Fan, G.~Wang, Y.~Jiang, A.~Mandlekar, Y.~Yang, H.~Zhu, A.~Tang, D.-A. Huang,   Y.~Zhu, and A.~Anandkumar. 
 Minedojo: Building open-ended embodied agents with internet-scale   knowledge. 
 \emph{arXiv preprint arXiv:2206.08853}, 2022."
2406.17659,shafiullah2022clipfields,"[Shafiullah et~al.(2022)Shafiullah, Paxton, Pinto, Chintala, and   Szlam]{shafiullah2022clipfields} N.~M.~M. Shafiullah, C.~Paxton, L.~Pinto, S.~Chintala, and A.~Szlam.",Clip-fields: Weakly supervised semantic fields for robotic memory.,Clip-fields: Weakly supervised semantic fields for robotic memory.,,"[Shafiullah et~al.(2022)Shafiullah, Paxton, Pinto, Chintala, and   Szlam]{shafiullah2022clipfields} N.~M.~M. Shafiullah, C.~Paxton, L.~Pinto, S.~Chintala, and A.~Szlam. 
 Clip-fields: Weakly supervised semantic fields for robotic memory. 
 \emph{arXiv preprint arXiv: Arxiv-2210.05663}, 2022."
2406.17659,ren2023robots,"[Ren et~al.(2023)Ren, Dixit, Bodrova, Singh, Tu, Brown, Xu, Takayama,   Xia, Varley, et~al.]{ren2023robots} A.~Z. Ren, A.~Dixit, A.~Bodrova, S.~Singh, S.~Tu, N.~Brown, P.~Xu, L.~Takayama,   F.~Xia, J.~Varley, et~al.",Robots that ask for help: Uncertainty alignment for large language   model planners.,Robots that ask for help: Uncertainty alignment for large language   model planners.,,"[Ren et~al.(2023)Ren, Dixit, Bodrova, Singh, Tu, Brown, Xu, Takayama,   Xia, Varley, et~al.]{ren2023robots} A.~Z. Ren, A.~Dixit, A.~Bodrova, S.~Singh, S.~Tu, N.~Brown, P.~Xu, L.~Takayama,   F.~Xia, J.~Varley, et~al. 
 Robots that ask for help: Uncertainty alignment for large language   model planners. 
 \emph{arXiv preprint arXiv:2307.01928}, 2023."
2406.17659,du2023vision,"[Du et~al.(2023)Du, Konyushkova, Denil, Raju, Landon, Hill, de~Freitas,   and Cabi]{du2023vision} Y.~Du, K.~Konyushkova, M.~Denil, A.~Raju, J.~Landon, F.~Hill, N.~de~Freitas,   and S.~Cabi.",Vision-language models as success detectors.,Vision-language models as success detectors.,,"[Du et~al.(2023)Du, Konyushkova, Denil, Raju, Landon, Hill, de~Freitas,   and Cabi]{du2023vision} Y.~Du, K.~Konyushkova, M.~Denil, A.~Raju, J.~Landon, F.~Hill, N.~de~Freitas,   and S.~Cabi. 
 Vision-language models as success detectors. 
 \emph{arXiv preprint arXiv:2303.07280}, 2023."
2406.17659,driess2023palm,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,   Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid,   J.~Tompson, Q.~Vuong, T.~Yu, et~al.",Palm-e: An embodied multimodal language model.,Palm-e: An embodied multimodal language model.,,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,   Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid,   J.~Tompson, Q.~Vuong, T.~Yu, et~al. 
 Palm-e: An embodied multimodal language model. 
 \emph{arXiv preprint arXiv:2303.03378}, 2023."
2406.17659,guo2023doremi,"[Guo et~al.(2023)Guo, Wang, Zha, Jiang, and Chen]{guo2023doremi} Y.~Guo, Y.-J. Wang, L.~Zha, Z.~Jiang, and J.~Chen.",Doremi: Grounding language model by detecting and recovering from   plan-execution misalignment.,Doremi: Grounding language model by detecting and recovering from   plan-execution misalignment.,,"[Guo et~al.(2023)Guo, Wang, Zha, Jiang, and Chen]{guo2023doremi} Y.~Guo, Y.-J. Wang, L.~Zha, Z.~Jiang, and J.~Chen. 
 Doremi: Grounding language model by detecting and recovering from   plan-execution misalignment. 
 \emph{arXiv preprint arXiv:2307.00329}, 2023."
2406.17659,reid2024gemini,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap,   Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini} M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac,   R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al.",Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context.,Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context.,,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap,   Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini} M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac,   R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al. 
 Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context. 
 \emph{arXiv preprint arXiv:2403.05530}, 2024."
2406.17659,morrison2018closing,"[Morrison et~al.(2018)Morrison, Corke, and   Leitner]{morrison2018closing} D.~Morrison, P.~Corke, and J.~Leitner.","Closing the loop for robotic grasping: A real-time, generative grasp   synthesis approach.","Closing the loop for robotic grasping: A real-time, generative grasp   synthesis approach.",,"[Morrison et~al.(2018)Morrison, Corke, and   Leitner]{morrison2018closing} D.~Morrison, P.~Corke, and J.~Leitner. 
 Closing the loop for robotic grasping: A real-time, generative grasp   synthesis approach. 
 \emph{arXiv preprint arXiv:1804.05172}, 2018."
2406.17681,abdin2024phi,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.",Phi-3 technical report: A highly capable language model locally on your phone.,Phi-3 technical report: A highly capable language model locally on your phone.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024. 
 Phi-3 technical report: A highly capable language model locally on your phone. 
 \emph{arXiv preprint arXiv:2404.14219}."
2406.17681,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.17681,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2406.17681,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2406.17681,cobbe2021gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2406.17681,dekoninck2024evading,"[{Dekoninck et~al.(2024)Dekoninck, M{\""u}ller, Baader, Fischer, and Vechev}]{dekoninck2024evading} Jasper Dekoninck, Mark~Niklas M{\""u}ller, Maximilian Baader, Marc Fischer, and Martin Vechev. 2024.",Evading data contamination detection for language models is (too) easy.,Evading data contamination detection for language models is (too) easy.,,"[{Dekoninck et~al.(2024)Dekoninck, M{\""u}ller, Baader, Fischer, and Vechev}]{dekoninck2024evading} Jasper Dekoninck, Mark~Niklas M{\""u}ller, Maximilian Baader, Marc Fischer, and Martin Vechev. 2024. 
 Evading data contamination detection for language models is (too) easy. 
 \emph{arXiv preprint arXiv:2402.02823}."
2406.17681,team2023gemini,"[{{Gemini Team} et~al.(2023){Gemini Team}, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{{Gemini Team} et~al.(2023){Gemini Team}, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2406.17681,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2406.17681,jiang2023mistral,"[{Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023{\natexlab{a}}.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023{\natexlab{a}}. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.17681,jiang2024investigating,"[{Jiang et~al.(2024)Jiang, Liu, Zhong, Schaeffer, Ouyang, Han, and Koyejo}]{jiang2024investigating} Minhao Jiang, Ken~Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. 2024.",Investigating data contamination for pre-training language models.,Investigating data contamination for pre-training language models.,,"[{Jiang et~al.(2024)Jiang, Liu, Zhong, Schaeffer, Ouyang, Han, and Koyejo}]{jiang2024investigating} Minhao Jiang, Ken~Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. 2024. 
 Investigating data contamination for pre-training language models. 
 \emph{arXiv preprint arXiv:2401.06059}."
2406.17681,nguyen2023seallms,"[{Nguyen et~al.(2023{\natexlab{b}})Nguyen, Zhang, Li, Aljunied, Tan, Cheng, Chen, Deng, Yang, Liu et~al.}]{nguyen2023seallms} Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, et~al. 2023{\natexlab{b}}.",Seallms--large language models for southeast asia.,Seallms--large language models for southeast asia.,,"[{Nguyen et~al.(2023{\natexlab{b}})Nguyen, Zhang, Li, Aljunied, Tan, Cheng, Chen, Deng, Yang, Liu et~al.}]{nguyen2023seallms} Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, et~al. 2023{\natexlab{b}}. 
 Seallms--large language models for southeast asia. 
 \emph{arXiv preprint arXiv:2312.00738}."
2406.17681,riddell2024quantifying,"[{Riddell et~al.(2024)Riddell, Ni, and Cohan}]{riddell2024quantifying} Martin Riddell, Ansong Ni, and Arman Cohan. 2024.",Quantifying contamination in evaluating code generation capabilities of language models.,Quantifying contamination in evaluating code generation capabilities of language models.,,"[{Riddell et~al.(2024)Riddell, Ni, and Cohan}]{riddell2024quantifying} Martin Riddell, Ansong Ni, and Arman Cohan. 2024. 
 Quantifying contamination in evaluating code generation capabilities of language models. 
 \emph{arXiv preprint arXiv:2403.04811}."
2406.17681,shao2024deepseekmath,"[{Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo}]{shao2024deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK~Li, Y~Wu, and Daya Guo. 2024.",Deepseekmath: Pushing the limits of mathematical reasoning in open language models.,Deepseekmath: Pushing the limits of mathematical reasoning in open language models.,,"[{Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo}]{shao2024deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK~Li, Y~Wu, and Daya Guo. 2024. 
 Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 
 \emph{arXiv preprint arXiv:2402.03300}."
2406.17681,singh2024tokenization,[{Singh and Strouse(2024)}]{singh2024tokenization} Aaditya~K Singh and DJ~Strouse. 2024.,Tokenization counts: the impact of tokenization on arithmetic in frontier llms.,Tokenization counts: the impact of tokenization on arithmetic in frontier llms.,,"[{Singh and Strouse(2024)}]{singh2024tokenization} Aaditya~K Singh and DJ~Strouse. 2024. 
 Tokenization counts: the impact of tokenization on arithmetic in frontier llms. 
 \emph{arXiv preprint arXiv:2402.14903}."
2406.17681,srivastava2024functional,"[{Srivastava et~al.(2024)Srivastava, PV, Menon, Sukumar, Philipose, Prince, Thomas et~al.}]{srivastava2024functional} Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et~al. 2024.","Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap.","Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap.",,"[{Srivastava et~al.(2024)Srivastava, PV, Menon, Sukumar, Philipose, Prince, Thomas et~al.}]{srivastava2024functional} Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et~al. 2024. 
 Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. 
 \emph{arXiv preprint arXiv:2402.19450}."
2406.17681,team2024gemma,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024.",Gemma: Open models based on gemini research and technology.,Gemma: Open models based on gemini research and technology.,,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024. 
 Gemma: Open models based on gemini research and technology. 
 \emph{arXiv preprint arXiv:2403.08295}."
2406.17681,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.17681,xu2024benchmark,"[{Xu et~al.(2024)Xu, Guan, Greene, Kechadi et~al.}]{xu2024benchmark} Cheng Xu, Shuhao Guan, Derek Greene, M~Kechadi, et~al. 2024.",Benchmark data contamination of large language models: A survey.,Benchmark data contamination of large language models: A survey.,,"[{Xu et~al.(2024)Xu, Guan, Greene, Kechadi et~al.}]{xu2024benchmark} Cheng Xu, Shuhao Guan, Derek Greene, M~Kechadi, et~al. 2024. 
 Benchmark data contamination of large language models: A survey. 
 \emph{arXiv preprint arXiv:2406.04244}."
2406.17681,yang2023rethinking,"[{Yang et~al.(2023)Yang, Chiang, Zheng, Gonzalez, and Stoica}]{yang2023rethinking} Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph~E Gonzalez, and Ion Stoica. 2023.",Rethinking benchmark and contamination for language models with rephrased samples.,Rethinking benchmark and contamination for language models with rephrased samples.,,"[{Yang et~al.(2023)Yang, Chiang, Zheng, Gonzalez, and Stoica}]{yang2023rethinking} Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph~E Gonzalez, and Ion Stoica. 2023. 
 Rethinking benchmark and contamination for language models with rephrased samples. 
 \emph{arXiv preprint arXiv:2311.04850}."
2406.17681,young2024yi,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}."
2406.17681,zhou2023don,"[{Zhou et~al.(2023)Zhou, Zhu, Chen, Chen, Zhao, Chen, Lin, Wen, and Han}]{zhou2023don} Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne~Xin Zhao, Xu~Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023.",Don't make your llm an evaluation benchmark cheater.,Don't make your llm an evaluation benchmark cheater.,,"[{Zhou et~al.(2023)Zhou, Zhu, Chen, Chen, Zhao, Chen, Lin, Wen, and Han}]{zhou2023don} Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne~Xin Zhao, Xu~Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. 
 Don't make your llm an evaluation benchmark cheater. 
 \emph{arXiv preprint arXiv:2311.01964}."
2406.17803,salemi2024optimization,"[{Salemi et~al.(2024)Salemi, Kallumadi, and Zamani}]{salemi2024optimization} Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024.",Optimization methods for personalizing large language models through retrieval augmentation.,Optimization methods for personalizing large language models through retrieval augmentation.,,"[{Salemi et~al.(2024)Salemi, Kallumadi, and Zamani}]{salemi2024optimization} Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. 
 Optimization methods for personalizing large language models through retrieval augmentation. 
 \emph{arXiv preprint arXiv:2404.05970}."
2406.17803,shi2023dept,[{Shi and Lipani(2023{\natexlab{a}})}]{shi2023dept} Zhengxiang Shi and Aldo Lipani. 2023{\natexlab{a}}.,Dept: Decomposed prompt tuning for parameter-efficient fine-tuning.,Dept: Decomposed prompt tuning for parameter-efficient fine-tuning.,,"[{Shi and Lipani(2023{\natexlab{a}})}]{shi2023dept} Zhengxiang Shi and Aldo Lipani. 2023{\natexlab{a}}. 
 Dept: Decomposed prompt tuning for parameter-efficient fine-tuning. 
 \emph{arXiv preprint arXiv:2309.05173}."
2406.17803,shi2024instruction,"[{Shi et~al.(2024)Shi, Yang, Wu, Aitchison, Yilmaz, and Lipani}]{shi2024instruction} Zhengyan Shi, Adam~X Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. 2024.",Instruction tuning with loss over instructions.,Instruction tuning with loss over instructions.,,"[{Shi et~al.(2024)Shi, Yang, Wu, Aitchison, Yilmaz, and Lipani}]{shi2024instruction} Zhengyan Shi, Adam~X Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. 2024. 
 Instruction tuning with loss over instructions. 
 \emph{arXiv preprint arXiv:2405.14394}."
2406.18505,saycan2022arxiv,"[{Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman et~al.}]{saycan2022arxiv} Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al. 2022.",Do as i can and not as i say: Grounding language in robotic affordances.,Do as i can and not as i say: Grounding language in robotic affordances.,,"[{Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman et~al.}]{saycan2022arxiv} Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al. 2022. 
 Do as i can and not as i say: Grounding language in robotic affordances. 
 In \emph{arXiv preprint arXiv:2204.01691}."
2406.18505,bachmann2024pitfalls,[{Bachmann and Nagarajan(2024)}]{bachmann2024pitfalls} Gregor Bachmann and Vaishnavh Nagarajan. 2024.,The pitfalls of next-token prediction.,The pitfalls of next-token prediction.,,"[{Bachmann and Nagarajan(2024)}]{bachmann2024pitfalls} Gregor Bachmann and Vaishnavh Nagarajan. 2024. 
 The pitfalls of next-token prediction. 
 \emph{arXiv preprint arXiv:2403.06963}."
2406.18505,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2406.18505,laskin2022context,"[{Laskin et~al.(2022)Laskin, Wang, Oh, Parisotto, Spencer, Steigerwald, Strouse, Hansen, Filos, Brooks et~al.}]{laskin2022context} Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ~Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et~al. 2022.",In-context reinforcement learning with algorithm distillation.,In-context reinforcement learning with algorithm distillation.,,"[{Laskin et~al.(2022)Laskin, Wang, Oh, Parisotto, Spencer, Steigerwald, Strouse, Hansen, Filos, Brooks et~al.}]{laskin2022context} Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ~Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et~al. 2022. 
 In-context reinforcement learning with algorithm distillation. 
 \emph{arXiv preprint arXiv:2210.14215}."
2406.18505,lillicrap2015continuous,"[{Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}]{lillicrap2015continuous} Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015.",Continuous control with deep reinforcement learning.,Continuous control with deep reinforcement learning.,,"[{Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}]{lillicrap2015continuous} Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. 
 Continuous control with deep reinforcement learning. 
 \emph{arXiv preprint arXiv:1509.02971}."
2406.18505,liu2022mind,"[{Liu et~al.(2022)Liu, Wei, Gu, Wu, Vosoughi, Cui, Zhou, and Dai}]{liu2022mind} Ruibo Liu, Jason Wei, Shixiang~Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew~M Dai. 2022.",Mind's eye: Grounded language model reasoning through simulation.,Mind's eye: Grounded language model reasoning through simulation.,,"[{Liu et~al.(2022)Liu, Wei, Gu, Wu, Vosoughi, Cui, Zhou, and Dai}]{liu2022mind} Ruibo Liu, Jason Wei, Shixiang~Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew~M Dai. 2022. 
 Mind's eye: Grounded language model reasoning through simulation. 
 \emph{arXiv preprint arXiv:2210.05359}."
2406.18505,schulman2017proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}."
2406.18505,wang2024transformers,"[{Wang et~al.(2024)Wang, Blaser, Daneshmand, and Zhang}]{wang2024transformers} Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong Zhang. 2024.",Transformers learn temporal difference methods for in-context reinforcement learning.,Transformers learn temporal difference methods for in-context reinforcement learning.,,"[{Wang et~al.(2024)Wang, Blaser, Daneshmand, and Zhang}]{wang2024transformers} Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong Zhang. 2024. 
 Transformers learn temporal difference methods for in-context reinforcement learning. 
 \emph{arXiv preprint arXiv:2405.13861}."
2406.19486,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2406.19486,asai2022attempt,"[{Asai et~al.(2022)Asai, Salehi, Peters, and Hajishirzi}]{asai2022attempt} Akari Asai, Mohammadreza Salehi, Matthew~E Peters, and Hannaneh Hajishirzi. 2022.",Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts.,Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts.,,"[{Asai et~al.(2022)Asai, Salehi, Peters, and Hajishirzi}]{asai2022attempt} Akari Asai, Mohammadreza Salehi, Matthew~E Peters, and Hannaneh Hajishirzi. 2022. 
 Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. 
 \emph{arXiv preprint arXiv:2205.11961}."
2406.19486,clark2019boolq,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}."
2406.19486,clevert2015fast,"[{Clevert et~al.(2015)Clevert, Unterthiner, and Hochreiter}]{clevert2015fast} Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2015.",Fast and accurate deep network learning by exponential linear units (elus).,Fast and accurate deep network learning by exponential linear units (elus).,,"[{Clevert et~al.(2015)Clevert, Unterthiner, and Hochreiter}]{clevert2015fast} Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2015. 
 Fast and accurate deep network learning by exponential linear units (elus). 
 \emph{arXiv preprint arXiv:1511.07289}."
2406.19486,ding2021openprompt,"[{Ding et~al.(2021)Ding, Hu, Zhao, Chen, Liu, Zheng, and Sun}]{ding2021openprompt} Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. 2021.",Openprompt: An open-source framework for prompt-learning.,Openprompt: An open-source framework for prompt-learning.,,"[{Ding et~al.(2021)Ding, Hu, Zhao, Chen, Liu, Zheng, and Sun}]{ding2021openprompt} Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. 2021. 
 Openprompt: An open-source framework for prompt-learning. 
 \emph{arXiv preprint arXiv:2111.01998}."
2406.19486,hendrycks2016gaussian,[{Hendrycks and Gimpel(2016)}]{hendrycks2016gaussian} Dan Hendrycks and Kevin Gimpel. 2016.,Gaussian error linear units (gelus).,Gaussian error linear units (gelus).,,"[{Hendrycks and Gimpel(2016)}]{hendrycks2016gaussian} Dan Hendrycks and Kevin Gimpel. 2016. 
 Gaussian error linear units (gelus). 
 \emph{arXiv preprint arXiv:1606.08415}."
2406.19486,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2406.19486,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2406.19486,khashabi2021prompt,"[{Khashabi et~al.(2021)Khashabi, Lyu, Min, Qin, Richardson, Welleck, Hajishirzi, Khot, Sabharwal, Singh et~al.}]{khashabi2021prompt} Daniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, et~al. 2021.",Prompt waywardness: The curious case of discretized interpretation of continuous prompts.,Prompt waywardness: The curious case of discretized interpretation of continuous prompts.,,"[{Khashabi et~al.(2021)Khashabi, Lyu, Min, Qin, Richardson, Welleck, Hajishirzi, Khot, Sabharwal, Singh et~al.}]{khashabi2021prompt} Daniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, et~al. 2021. 
 Prompt waywardness: The curious case of discretized interpretation of continuous prompts. 
 \emph{arXiv preprint arXiv:2112.08348}."
2406.19486,lester2021power,"[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power} Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",The power of scale for parameter-efficient prompt tuning.,The power of scale for parameter-efficient prompt tuning.,,"[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power} Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. 
 The power of scale for parameter-efficient prompt tuning. 
 \emph{arXiv preprint arXiv:2104.08691}."
2406.19486,li2021prefix,[{Li and Liang(2021)}]{li2021prefix} Xiang~Lisa Li and Percy Liang. 2021.,Prefix-tuning: Optimizing continuous prompts for generation.,Prefix-tuning: Optimizing continuous prompts for generation.,,"[{Li and Liang(2021)}]{li2021prefix} Xiang~Lisa Li and Percy Liang. 2021. 
 Prefix-tuning: Optimizing continuous prompts for generation. 
 \emph{arXiv preprint arXiv:2101.00190}."
2406.19486,pilehvar2018wic,[{Pilehvar and Camacho-Collados(2018)}]{pilehvar2018wic} Mohammad~Taher Pilehvar and Jose Camacho-Collados. 2018.,Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.,Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.,,"[{Pilehvar and Camacho-Collados(2018)}]{pilehvar2018wic} Mohammad~Taher Pilehvar and Jose Camacho-Collados. 2018. 
 Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. 
 \emph{arXiv preprint arXiv:1808.09121}."
2406.19486,sanh2021multitask,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 2021.",Multitask prompted training enables zero-shot task generalization.,Multitask prompted training enables zero-shot task generalization.,,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 2021. 
 Multitask prompted training enables zero-shot task generalization. 
 \emph{arXiv preprint arXiv:2110.08207}."
2406.19486,shi2022toward,"[{Shi et~al.(2022)Shi, Han, Gonen, Holtzman, Tsvetkov, and Zettlemoyer}]{shi2022toward} Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 2022.","Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?","Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?",,"[{Shi et~al.(2022)Shi, Han, Gonen, Holtzman, Tsvetkov, and Zettlemoyer}]{shi2022toward} Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 2022. 
 Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? 
 \emph{arXiv preprint arXiv:2212.10539}."
2406.19486,shi2023dept,[{Shi and Lipani(2023)}]{shi2023dept} Zhengxiang Shi and Aldo Lipani. 2023.,Dept: Decomposed prompt tuning for parameter-efficient fine-tuning.,Dept: Decomposed prompt tuning for parameter-efficient fine-tuning.,,"[{Shi and Lipani(2023)}]{shi2023dept} Zhengxiang Shi and Aldo Lipani. 2023. 
 Dept: Decomposed prompt tuning for parameter-efficient fine-tuning. 
 \emph{arXiv preprint arXiv:2309.05173}."
2406.19486,shin2020autoprompt,"[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh}]{shin2020autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 2020.",Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,,"[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh}]{shin2020autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 2020. 
 Autoprompt: Eliciting knowledge from language models with automatically generated prompts. 
 \emph{arXiv preprint arXiv:2010.15980}."
2406.19486,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2406.19486,wang2023multitask,"[{Wang et~al.(2023)Wang, Panda, Karlinsky, Feris, Sun, and Kim}]{wang2023multitask} Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2023.",Multitask prompt tuning enables parameter-efficient transfer learning.,Multitask prompt tuning enables parameter-efficient transfer learning.,,"[{Wang et~al.(2023)Wang, Panda, Karlinsky, Feris, Sun, and Kim}]{wang2023multitask} Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2023. 
 Multitask prompt tuning enables parameter-efficient transfer learning. 
 \emph{arXiv preprint arXiv:2303.02861}."
2406.19501,overthinking,"[Halawi et~al.(2023)Halawi, Denain, and Steinhardt]{overthinking} Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt.",Overthinking the truth: Understanding how language models process false demonstrations.,Overthinking the truth: Understanding how language models process false demonstrations.,,"[Halawi et~al.(2023)Halawi, Denain, and Steinhardt]{overthinking} Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. 
 Overthinking the truth: Understanding how language models process false demonstrations. 
 \emph{arXiv preprint arXiv:2307.09476}, 2023."
2406.19501,sharma2023towards,"[Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.]{sharma2023towards} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R Johnston, et~al.",Towards understanding sycophancy in language models.,Towards understanding sycophancy in language models.,,"[Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.]{sharma2023towards} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R Johnston, et~al. 
 Towards understanding sycophancy in language models. 
 \emph{arXiv preprint arXiv:2310.13548}, 2023."
2406.19501,blodgett2020language,"[Blodgett et~al.(2020)Blodgett, Barocas, Daum{\'e}~III, and Wallach]{blodgett2020language} Su~Lin Blodgett, Solon Barocas, Hal Daum{\'e}~III, and Hanna Wallach.","Language (technology) is power: A critical survey of"" bias"" in nlp.","Language (technology) is power: A critical survey of"" bias"" in nlp.",,"[Blodgett et~al.(2020)Blodgett, Barocas, Daum{\'e}~III, and Wallach]{blodgett2020language} Su~Lin Blodgett, Solon Barocas, Hal Daum{\'e}~III, and Hanna Wallach. 
 Language (technology) is power: A critical survey of"" bias"" in nlp. 
 \emph{arXiv preprint arXiv:2005.14050}, 2020."
2406.19501,liang2022holistic,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}, 2022."
2406.19501,perez2022ignore,[Perez and Ribeiro(2022)]{perez2022ignore} F{\'a}bio Perez and Ian Ribeiro.,Ignore previous prompt: Attack techniques for language models.,Ignore previous prompt: Attack techniques for language models.,,"[Perez and Ribeiro(2022)]{perez2022ignore} F{\'a}bio Perez and Ian Ribeiro. 
 Ignore previous prompt: Attack techniques for language models. 
 \emph{arXiv preprint arXiv:2211.09527}, 2022."
2406.19501,wallace2020concealed,"[Wallace et~al.(2020)Wallace, Zhao, Feng, and Singh]{wallace2020concealed} Eric Wallace, Tony~Z Zhao, Shi Feng, and Sameer Singh.",Concealed data poisoning attacks on nlp models.,Concealed data poisoning attacks on nlp models.,,"[Wallace et~al.(2020)Wallace, Zhao, Feng, and Singh]{wallace2020concealed} Eric Wallace, Tony~Z Zhao, Shi Feng, and Sameer Singh. 
 Concealed data poisoning attacks on nlp models. 
 \emph{arXiv preprint arXiv:2010.12563}, 2020."
2406.19501,hubinger2024sleeper,"[Hubinger et~al.(2024)Hubinger, Denison, Mu, Lambert, Tong, MacDiarmid, Lanham, Ziegler, Maxwell, Cheng, et~al.]{hubinger2024sleeper} Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel~M Ziegler, Tim Maxwell, Newton Cheng, et~al.",Sleeper agents: Training deceptive llms that persist through safety training.,Sleeper agents: Training deceptive llms that persist through safety training.,,"[Hubinger et~al.(2024)Hubinger, Denison, Mu, Lambert, Tong, MacDiarmid, Lanham, Ziegler, Maxwell, Cheng, et~al.]{hubinger2024sleeper} Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel~M Ziegler, Tim Maxwell, Newton Cheng, et~al. 
 Sleeper agents: Training deceptive llms that persist through safety training. 
 \emph{arXiv preprint arXiv:2401.05566}, 2024."
2406.19501,viegas2023system,[Vi{\'e}gas and Wattenberg(2023)]{viegas2023system} Fernanda Vi{\'e}gas and Martin Wattenberg.,The system model and the user model: Exploring ai dashboard design.,The system model and the user model: Exploring ai dashboard design.,,"[Vi{\'e}gas and Wattenberg(2023)]{viegas2023system} Fernanda Vi{\'e}gas and Martin Wattenberg. 
 The system model and the user model: Exploring ai dashboard design. 
 \emph{arXiv preprint arXiv:2305.02469}, 2023."
2406.19501,belinda-alchemy,"[Li et~al.(2021)Li, Nye, and Andreas]{belinda-alchemy} Belinda~Z Li, Maxwell Nye, and Jacob Andreas.",Implicit representations of meaning in neural language models.,Implicit representations of meaning in neural language models.,,"[Li et~al.(2021)Li, Nye, and Andreas]{belinda-alchemy} Belinda~Z Li, Maxwell Nye, and Jacob Andreas. 
 Implicit representations of meaning in neural language models. 
 \emph{arXiv preprint arXiv:2106.00737}, 2021."
2406.19501,othello,"[Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg]{othello} Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg.",Emergent world representations: Exploring a sequence model trained on a synthetic task.,Emergent world representations: Exploring a sequence model trained on a synthetic task.,,"[Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg]{othello} Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg. 
 Emergent world representations: Exploring a sequence model trained on a synthetic task. 
 \emph{arXiv preprint arXiv:2210.13382}, 2022."
2406.19501,feng2023language,[Feng and Steinhardt(2023)]{feng2023language} Jiahai Feng and Jacob Steinhardt.,How do language models bind entities in context?,How do language models bind entities in context?,,"[Feng and Steinhardt(2023)]{feng2023language} Jiahai Feng and Jacob Steinhardt. 
 How do language models bind entities in context? 
 \emph{arXiv preprint arXiv:2310.17191}, 2023."
2406.19501,abdou2021can,"[Abdou et~al.(2021)Abdou, Kulmizev, Hershcovich, Frank, Pavlick, and S{\o}gaard]{abdou2021can} Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders S{\o}gaard.",Can language models encode perceptual structure without grounding? a case study in color.,Can language models encode perceptual structure without grounding? a case study in color.,,"[Abdou et~al.(2021)Abdou, Kulmizev, Hershcovich, Frank, Pavlick, and S{\o}gaard]{abdou2021can} Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders S{\o}gaard. 
 Can language models encode perceptual structure without grounding? a case study in color. 
 \emph{arXiv preprint arXiv:2109.06129}, 2021."
2406.19501,gurnee2023language,[Gurnee and Tegmark(2023)]{gurnee2023language} Wes Gurnee and Max Tegmark.,Language models represent space and time.,Language models represent space and time.,,"[Gurnee and Tegmark(2023)]{gurnee2023language} Wes Gurnee and Max Tegmark. 
 Language models represent space and time. 
 \emph{arXiv preprint arXiv:2310.02207}, 2023."
2406.19501,burns2022discovering,"[Burns et~al.(2022)Burns, Ye, Klein, and Steinhardt]{burns2022discovering} Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.",Discovering latent knowledge in language models without supervision.,Discovering latent knowledge in language models without supervision.,,"[Burns et~al.(2022)Burns, Ye, Klein, and Steinhardt]{burns2022discovering} Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 
 Discovering latent knowledge in language models without supervision. 
 \emph{arXiv preprint arXiv:2212.03827}, 2022."
2406.19501,marks2023geometry,[Marks and Tegmark(2023)]{marks2023geometry} Samuel Marks and Max Tegmark.,The geometry of truth: Emergent linear structure in large language model representations of true/false datasets.,The geometry of truth: Emergent linear structure in large language model representations of true/false datasets.,,"[Marks and Tegmark(2023)]{marks2023geometry} Samuel Marks and Max Tegmark. 
 The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. 
 \emph{arXiv preprint arXiv:2310.06824}, 2023."
2406.19501,quirkylm,[Mallen and Belrose(2023)]{quirkylm} Alex Mallen and Nora Belrose.,Eliciting latent knowledge from quirky language models.,Eliciting latent knowledge from quirky language models.,,"[Mallen and Belrose(2023)]{quirkylm} Alex Mallen and Nora Belrose. 
 Eliciting latent knowledge from quirky language models. 
 \emph{arXiv preprint arXiv:2312.01037}, 2023."
2406.19501,tenney2019you,"[Tenney et~al.(2019)Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Van~Durme, Bowman, Das, et~al.]{tenney2019you} Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R~Thomas McCoy, Najoung Kim, Benjamin Van~Durme, Samuel~R Bowman, Dipanjan Das, et~al.",What do you learn from context? probing for sentence structure in contextualized word representations.,What do you learn from context? probing for sentence structure in contextualized word representations.,,"[Tenney et~al.(2019)Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Van~Durme, Bowman, Das, et~al.]{tenney2019you} Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R~Thomas McCoy, Najoung Kim, Benjamin Van~Durme, Samuel~R Bowman, Dipanjan Das, et~al. 
 What do you learn from context? probing for sentence structure in contextualized word representations. 
 \emph{arXiv preprint arXiv:1905.06316}, 2019."
2406.19501,peters2018dissecting,"[Peters et~al.(2018)Peters, Neumann, Zettlemoyer, and Yih]{peters2018dissecting} Matthew~E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih.",Dissecting contextual word embeddings: Architecture and representation.,Dissecting contextual word embeddings: Architecture and representation.,,"[Peters et~al.(2018)Peters, Neumann, Zettlemoyer, and Yih]{peters2018dissecting} Matthew~E Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 
 Dissecting contextual word embeddings: Architecture and representation. 
 \emph{arXiv preprint arXiv:1808.08949}, 2018."
2406.19501,prakash2024fine,"[Prakash et~al.(2024)Prakash, Shaham, Haklay, Belinkov, and Bau]{prakash2024fine} Nikhil Prakash, Tamar~Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau.",Fine-tuning enhances existing mechanisms: A case study on entity tracking.,Fine-tuning enhances existing mechanisms: A case study on entity tracking.,,"[Prakash et~al.(2024)Prakash, Shaham, Haklay, Belinkov, and Bau]{prakash2024fine} Nikhil Prakash, Tamar~Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. 
 Fine-tuning enhances existing mechanisms: A case study on entity tracking. 
 \emph{arXiv preprint arXiv:2402.14811}, 2024."
2406.19501,ivison2023camels,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.",Camels in a changing climate: Enhancing lm adaptation with tulu 2.,Camels in a changing climate: Enhancing lm adaptation with tulu 2.,,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al. 
 Camels in a changing climate: Enhancing lm adaptation with tulu 2. 
 \emph{arXiv preprint arXiv:2311.10702}, 2023."
2406.19501,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2406.19501,lre,"[Hernandez et~al.(2023)Hernandez, Sharma, Haklay, Meng, Wattenberg, Andreas, Belinkov, and Bau]{lre} Evan Hernandez, Arnab~Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau.",Linearity of relation decoding in transformer language models.,Linearity of relation decoding in transformer language models.,,"[Hernandez et~al.(2023)Hernandez, Sharma, Haklay, Meng, Wattenberg, Andreas, Belinkov, and Bau]{lre} Evan Hernandez, Arnab~Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. 
 Linearity of relation decoding in transformer language models. 
 \emph{arXiv preprint arXiv:2308.09124}, 2023."
2406.19501,orgad2022choose,[Orgad and Belinkov(2022)]{orgad2022choose} Hadas Orgad and Yonatan Belinkov.,Choose your lenses: Flaws in gender bias evaluation.,Choose your lenses: Flaws in gender bias evaluation.,,"[Orgad and Belinkov(2022)]{orgad2022choose} Hadas Orgad and Yonatan Belinkov. 
 Choose your lenses: Flaws in gender bias evaluation. 
 \emph{arXiv preprint arXiv:2210.11471}, 2022."
2406.19501,zhao2018gender,"[Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and Chang]{zhao2018gender} Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.",Gender bias in coreference resolution: Evaluation and debiasing methods.,Gender bias in coreference resolution: Evaluation and debiasing methods.,,"[Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and Chang]{zhao2018gender} Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 
 Gender bias in coreference resolution: Evaluation and debiasing methods. 
 \emph{arXiv preprint arXiv:1804.06876}, 2018."
2406.19501,rudinger2018gender,"[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{rudinger2018gender} Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.",Gender bias in coreference resolution.,Gender bias in coreference resolution.,,"[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{rudinger2018gender} Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme. 
 Gender bias in coreference resolution. 
 \emph{arXiv preprint arXiv:1804.09301}, 2018."
2406.19501,parrish2021bbq,"[Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{parrish2021bbq} Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman.",Bbq: A hand-built bias benchmark for question answering.,Bbq: A hand-built bias benchmark for question answering.,,"[Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang, Thompson, Htut, and Bowman]{parrish2021bbq} Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman. 
 Bbq: A hand-built bias benchmark for question answering. 
 \emph{arXiv preprint arXiv:2110.08193}, 2021."
2406.19501,kim2023entity,[Kim and Schuster(2023)]{kim2023entity} Najoung Kim and Sebastian Schuster.,Entity tracking in language models.,Entity tracking in language models.,,"[Kim and Schuster(2023)]{kim2023entity} Najoung Kim and Sebastian Schuster. 
 Entity tracking in language models. 
 \emph{arXiv preprint arXiv:2305.02363}, 2023."
2406.19501,pan2022effects,"[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{pan2022effects} Alexander Pan, Kush Bhatia, and Jacob Steinhardt.",The effects of reward misspecification: Mapping and mitigating misaligned models.,The effects of reward misspecification: Mapping and mitigating misaligned models.,,"[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{pan2022effects} Alexander Pan, Kush Bhatia, and Jacob Steinhardt. 
 The effects of reward misspecification: Mapping and mitigating misaligned models. 
 \emph{arXiv preprint arXiv:2201.03544}, 2022."
2406.19501,wolf2019huggingface,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al.",Huggingface's transformers: State-of-the-art natural language processing.,Huggingface's transformers: State-of-the-art natural language processing.,,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al. 
 Huggingface's transformers: State-of-the-art natural language processing. 
 \emph{arXiv preprint arXiv:1910.03771}, 2019."
2406.19501,kingma2014adam,[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2406.19501,llmmcq,"[Robinson et~al.(2022)Robinson, Rytting, and Wingate]{llmmcq} Joshua Robinson, Christopher~Michael Rytting, and David Wingate.",Leveraging large language models for multiple choice question answering.,Leveraging large language models for multiple choice question answering.,,"[Robinson et~al.(2022)Robinson, Rytting, and Wingate]{llmmcq} Joshua Robinson, Christopher~Michael Rytting, and David Wingate. 
 Leveraging large language models for multiple choice question answering. 
 \emph{arXiv preprint arXiv:2210.12353}, 2022."
2406.19774,agarwal2023gkd,"[{Agarwal et~al.(2023)Agarwal, Vieillard, Stanczyk, Ramos, Geist, and Bachem}]{agarwal2023gkd} Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023.",Gkd: Generalized knowledge distillation for auto-regressive sequence models.,Gkd: Generalized knowledge distillation for auto-regressive sequence models.,,"[{Agarwal et~al.(2023)Agarwal, Vieillard, Stanczyk, Ramos, Geist, and Bachem}]{agarwal2023gkd} Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. 
 Gkd: Generalized knowledge distillation for auto-regressive sequence models. 
 \emph{arXiv preprint arXiv:2306.13649}."
2406.19774,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2406.19774,hinton2015distilling,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.",Distilling the knowledge in a neural network.,Distilling the knowledge in a neural network.,,"[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling} Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. 
 Distilling the knowledge in a neural network. 
 \emph{arXiv preprint arXiv:1503.02531}."
2406.19774,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2406.19774,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2406.19774,kim2024promptkd,"[{Kim et~al.(2024)Kim, Jang, and Yang}]{kim2024promptkd} Gyeongman Kim, Doohyuk Jang, and Eunho Yang. 2024.",Promptkd: Distilling student-friendly knowledge for generative language models via prompt tuning.,Promptkd: Distilling student-friendly knowledge for generative language models via prompt tuning.,,"[{Kim et~al.(2024)Kim, Jang, and Yang}]{kim2024promptkd} Gyeongman Kim, Doohyuk Jang, and Eunho Yang. 2024. 
 Promptkd: Distilling student-friendly knowledge for generative language models via prompt tuning. 
 \emph{arXiv preprint arXiv:2402.12842}."
2406.19774,meng2024simpo,"[{Meng et~al.(2024)Meng, Xia, and Chen}]{meng2024simpo} Yu~Meng, Mengzhou Xia, and Danqi Chen. 2024.",Simpo: Simple preference optimization with a reference-free reward.,Simpo: Simple preference optimization with a reference-free reward.,,"[{Meng et~al.(2024)Meng, Xia, and Chen}]{meng2024simpo} Yu~Meng, Mengzhou Xia, and Danqi Chen. 2024. 
 Simpo: Simple preference optimization with a reference-free reward. 
 \emph{arXiv preprint arXiv:2405.14734}."
2406.19774,DPOP,"[{Pal et~al.(2024)Pal, Karkhanis, Dooley, Roberts, Naidu, and White}]{DPOP} Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. 2024.",Smaug: Fixing failure modes of preference optimisation with dpo-positive.,Smaug: Fixing failure modes of preference optimisation with dpo-positive.,,"[{Pal et~al.(2024)Pal, Karkhanis, Dooley, Roberts, Naidu, and White}]{DPOP} Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. 2024. 
 Smaug: Fixing failure modes of preference optimisation with dpo-positive. 
 \emph{arXiv preprint arXiv:2402.13228}."
2406.19774,rafailov2024r,"[{Rafailov et~al.(2024{\natexlab{a}})Rafailov, Hejna, Park, and Finn}]{rafailov2024r} Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. 2024{\natexlab{a}}.",From $ r $ to $ q $: Your language model is secretly a q-function.,From $ r $ to $ q $: Your language model is secretly a q-function.,,"[{Rafailov et~al.(2024{\natexlab{a}})Rafailov, Hejna, Park, and Finn}]{rafailov2024r} Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. 2024{\natexlab{a}}. 
 From $ r $ to $ q $: Your language model is secretly a q-function. 
 \emph{arXiv preprint arXiv:2404.12358}."
2406.19774,song2020lightpaff,"[{Song et~al.(2020)Song, Sun, Tan, Qin, Lu, Liu, and Liu}]{song2020lightpaff} Kaitao Song, Hao Sun, Xu~Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. 2020.",Lightpaff: A two-stage distillation framework for pre-training and fine-tuning.,Lightpaff: A two-stage distillation framework for pre-training and fine-tuning.,,"[{Song et~al.(2020)Song, Sun, Tan, Qin, Lu, Liu, and Liu}]{song2020lightpaff} Kaitao Song, Hao Sun, Xu~Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. 2020. 
 Lightpaff: A two-stage distillation framework for pre-training and fine-tuning. 
 \emph{arXiv preprint arXiv:2004.12817}."
2406.19774,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2406.19774,wen2023f,"[{Wen et~al.(2023)Wen, Li, Du, and Mou}]{wen2023f} Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. 2023.",f-divergence minimization for sequence-level knowledge distillation.,f-divergence minimization for sequence-level knowledge distillation.,,"[{Wen et~al.(2023)Wen, Li, Du, and Mou}]{wen2023f} Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. 2023. 
 f-divergence minimization for sequence-level knowledge distillation. 
 \emph{arXiv preprint arXiv:2307.15190}."
2406.19774,wu2024rethinking,"[{Wu et~al.(2024)Wu, Tao, Wang, Zhao, and Wong}]{wu2024rethinking} Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. 2024.",Rethinking kullback-leibler divergence in knowledge distillation for large language models.,Rethinking kullback-leibler divergence in knowledge distillation for large language models.,,"[{Wu et~al.(2024)Wu, Tao, Wang, Zhao, and Wong}]{wu2024rethinking} Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. 2024. 
 Rethinking kullback-leibler divergence in knowledge distillation for large language models. 
 \emph{arXiv preprint arXiv:2404.02657}."
2406.19774,CPO,"[{Xu et~al.(2024)Xu, Sharaf, Chen, Tan, Shen, Van~Durme, Murray, and Kim}]{CPO} Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van~Durme, Kenton Murray, and Young~Jin Kim. 2024.",Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.,Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.,,"[{Xu et~al.(2024)Xu, Sharaf, Chen, Tan, Shen, Van~Durme, Murray, and Kim}]{CPO} Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van~Durme, Kenton Murray, and Young~Jin Kim. 2024. 
 Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. 
 \emph{arXiv preprint arXiv:2401.08417}."
2406.19774,yuan2024self,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston}]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024.",Self-rewarding language models.,Self-rewarding language models.,,"[{Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston}]{yuan2024self} Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. 
 Self-rewarding language models. 
 \emph{arXiv preprint arXiv:2401.10020}."
2406.19774,zhang2023not,"[{Zhang et~al.(2023)Zhang, Shen, Liu, Liu, Bendersky, Najork, and Zhang}]{zhang2023not} Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. 2023.",Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation.,Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation.,,"[{Zhang et~al.(2023)Zhang, Shen, Liu, Liu, Bendersky, Najork, and Zhang}]{zhang2023not} Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. 2023. 
 Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation. 
 \emph{arXiv preprint arXiv:2305.05010}."
2406.19774,zhang2022opt,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022{\natexlab{a}}. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2407.00653,bang2023multitask,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.","A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.",,"[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung et~al.}]{bang2023multitask} Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et~al. 2023. 
 A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 
 \emph{arXiv preprint arXiv:2302.04023}."
2407.00653,chen2022program,"[{Chen et~al.(2022)Chen, Ma, Wang, and Cohen}]{chen2022program} Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen. 2022.",Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.,Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.,,"[{Chen et~al.(2022)Chen, Ma, Wang, and Cohen}]{chen2022program} Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen. 2022. 
 Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. 
 \emph{arXiv preprint arXiv:2211.12588}."
2407.00653,allenai:arc,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv:1803.05457v1}."
2407.00653,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.00653,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.00653,srivastava2022beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2407.00653,suzgun2022challenging,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, , and Wei}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and Jason Wei. 2022.",Challenging big-bench tasks and whether chain-of-thought can solve them.,Challenging big-bench tasks and whether chain-of-thought can solve them.,,"[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\""a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, , and Wei}]{suzgun2022challenging} Mirac Suzgun, Nathan Scales, Nathanael Sch{\""a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and Jason Wei. 2022. 
 Challenging big-bench tasks and whether chain-of-thought can solve them. 
 \emph{arXiv preprint arXiv:2210.09261}."
2407.00653,talmor2018commonsenseqa,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.",Commonsenseqa: A question answering challenge targeting commonsense knowledge.,Commonsenseqa: A question answering challenge targeting commonsense knowledge.,,"[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa} Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. 
 Commonsenseqa: A question answering challenge targeting commonsense knowledge. 
 \emph{arXiv preprint arXiv:1811.00937}."
2407.00653,xiong2017deeppath,"[{Xiong et~al.(2017)Xiong, Hoang, and Wang}]{xiong2017deeppath} Wenhan Xiong, Thien Hoang, and William~Yang Wang. 2017.",Deeppath: A reinforcement learning method for knowledge graph reasoning.,Deeppath: A reinforcement learning method for knowledge graph reasoning.,,"[{Xiong et~al.(2017)Xiong, Hoang, and Wang}]{xiong2017deeppath} Wenhan Xiong, Thien Hoang, and William~Yang Wang. 2017. 
 Deeppath: A reinforcement learning method for knowledge graph reasoning. 
 \emph{arXiv preprint arXiv:1707.06690}."
2407.00653,yao2019kg,"[{Yao et~al.(2019)Yao, Mao, and Luo}]{yao2019kg} Liang Yao, Chengsheng Mao, and Yuan Luo. 2019.",Kg-bert: Bert for knowledge graph completion.,Kg-bert: Bert for knowledge graph completion.,,"[{Yao et~al.(2019)Yao, Mao, and Luo}]{yao2019kg} Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. 
 Kg-bert: Bert for knowledge graph completion. 
 \emph{arXiv preprint arXiv:1909.03193}."
2407.00945,akiba2024evolutionary,"[Akiba et~al.(2024)Akiba, Shing, Tang, Sun, and Ha]{akiba2024evolutionary} Takuya Akiba, Makoto Shing, Yujin Tang, Qi~Sun, and David Ha.",Evolutionary optimization of model merging recipes.,Evolutionary optimization of model merging recipes.,,"[Akiba et~al.(2024)Akiba, Shing, Tang, Sun, and Ha]{akiba2024evolutionary} Takuya Akiba, Makoto Shing, Yujin Tang, Qi~Sun, and David Ha. 
 Evolutionary optimization of model merging recipes. 
 \emph{arXiv preprint arXiv:2403.13187}, 2024."
2407.00945,bai2023qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2407.00945,beltagy2020longformer,"[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer} Iz~Beltagy, Matthew~E Peters, and Arman Cohan.",Longformer: The long-document transformer.,Longformer: The long-document transformer.,,"[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer} Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 
 Longformer: The long-document transformer. 
 \emph{arXiv preprint arXiv:2004.05150}, 2020."
2407.00945,chen2022task,"[Chen et~al.(2022)Chen, Huang, Xie, Jiao, Jiang, Zhou, Li, and Wei]{chen2022task} Tianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei.",Task-specific expert pruning for sparse mixture-of-experts.,Task-specific expert pruning for sparse mixture-of-experts.,,"[Chen et~al.(2022)Chen, Huang, Xie, Jiao, Jiang, Zhou, Li, and Wei]{chen2022task} Tianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei. 
 Task-specific expert pruning for sparse mixture-of-experts. 
 \emph{arXiv preprint arXiv:2206.00277}, 2022."
2407.00945,child2019generating,"[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating} Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.",Generating long sequences with sparse transformers.,Generating long sequences with sparse transformers.,,"[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating} Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 
 Generating long sequences with sparse transformers. 
 \emph{arXiv preprint arXiv:1904.10509}, 2019."
2407.00945,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2407.00945,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2407.00945,liu2024linear,"[Liu et~al.(2024{\natexlab{b}})Liu, Zhu, Lin, Ning, Blaschko, Yekhanin, Yan, Dai, Yang, and Wang]{liu2024linear} Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew~B Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu~Wang.",Linear combination of saved checkpoints makes consistency and diffusion models better.,Linear combination of saved checkpoints makes consistency and diffusion models better.,,"[Liu et~al.(2024{\natexlab{b}})Liu, Zhu, Lin, Ning, Blaschko, Yekhanin, Yan, Dai, Yang, and Wang]{liu2024linear} Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew~B Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu~Wang. 
 Linear combination of saved checkpoints makes consistency and diffusion models better. 
 \emph{arXiv preprint arXiv:2404.02241}, 2024{\natexlab{b}}."
2407.00945,lu2024not,"[Lu et~al.(2024)Lu, Liu, Xu, Zhou, Huang, Zhang, Yan, and Li]{lu2024not} Xudong Lu, Qi~Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo~Zhang, Junchi Yan, and Hongsheng Li.",Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models.,Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models.,,"[Lu et~al.(2024)Lu, Liu, Xu, Zhou, Huang, Zhang, Yan, and Li]{lu2024not} Xudong Lu, Qi~Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo~Zhang, Junchi Yan, and Hongsheng Li. 
 Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. 
 \emph{arXiv preprint arXiv:2402.14800}, 2024."
2407.00945,muzio2024seer,"[Muzio et~al.(2024)Muzio, Sun, and He]{muzio2024seer} Alexandre Muzio, Alex Sun, and Churan He.",Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts.,Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts.,,"[Muzio et~al.(2024)Muzio, Sun, and He]{muzio2024seer} Alexandre Muzio, Alex Sun, and Churan He. 
 Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts. 
 \emph{arXiv preprint arXiv:2404.05089}, 2024."
2407.00945,polino2018model,"[Polino et~al.(2018)Polino, Pascanu, and Alistarh]{polino2018model} Antonio Polino, Razvan Pascanu, and Dan Alistarh.",Model compression via distillation and quantization.,Model compression via distillation and quantization.,,"[Polino et~al.(2018)Polino, Pascanu, and Alistarh]{polino2018model} Antonio Polino, Razvan Pascanu, and Dan Alistarh. 
 Model compression via distillation and quantization. 
 \emph{arXiv preprint arXiv:1802.05668}, 2018."
2407.00945,salimans2017evolution,"[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and Sutskever]{salimans2017evolution} Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.",Evolution strategies as a scalable alternative to reinforcement learning.,Evolution strategies as a scalable alternative to reinforcement learning.,,"[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and Sutskever]{salimans2017evolution} Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever. 
 Evolution strategies as a scalable alternative to reinforcement learning. 
 \emph{arXiv preprint arXiv:1703.03864}, 2017."
2407.00945,shazeer2020glu,[Shazeer(2020)]{shazeer2020glu} Noam Shazeer.,Glu variants improve transformer.,Glu variants improve transformer.,,"[Shazeer(2020)]{shazeer2020glu} Noam Shazeer. 
 Glu variants improve transformer. 
 \emph{arXiv preprint arXiv:2002.05202}, 2020."
2407.00945,touvron2023llama2,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2407.00945,trofin2021mlgo,"[Trofin et~al.(2021)Trofin, Qian, Brevdo, Lin, Choromanski, and Li]{trofin2021mlgo} Mircea Trofin, Yundi Qian, Eugene Brevdo, Zinan Lin, Krzysztof Choromanski, and David Li.",Mlgo: a machine learning guided compiler optimizations framework.,Mlgo: a machine learning guided compiler optimizations framework.,,"[Trofin et~al.(2021)Trofin, Qian, Brevdo, Lin, Choromanski, and Li]{trofin2021mlgo} Mircea Trofin, Yundi Qian, Eugene Brevdo, Zinan Lin, Krzysztof Choromanski, and David Li. 
 Mlgo: a machine learning guided compiler optimizations framework. 
 \emph{arXiv preprint arXiv:2101.04808}, 2021."
2407.00945,wan2023efficient,"[Wan et~al.(2023)Wan, Wang, Liu, Alam, Zheng, Qu, Yan, Zhu, Zhang, Chowdhury, et~al.]{wan2023efficient} Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu~Zheng, Zhongnan Qu, Shen Yan, Yi~Zhu, Quanlu Zhang, Mosharaf Chowdhury, et~al.",Efficient large language models: A survey.,Efficient large language models: A survey.,,"[Wan et~al.(2023)Wan, Wang, Liu, Alam, Zheng, Qu, Yan, Zhu, Zhang, Chowdhury, et~al.]{wan2023efficient} Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu~Zheng, Zhongnan Qu, Shen Yan, Yi~Zhu, Quanlu Zhang, Mosharaf Chowdhury, et~al. 
 Efficient large language models: A survey. 
 \emph{arXiv preprint arXiv:2312.03863}, 1, 2023."
2407.00945,zhou2024survey,"[Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li, et~al.]{zhou2024survey} Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al.",A survey on efficient inference for large language models.,A survey on efficient inference for large language models.,,"[Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li, et~al.]{zhou2024survey} Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al. 
 A survey on efficient inference for large language models. 
 \emph{arXiv preprint arXiv:2404.14294}, 2024."
2407.00958,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F.~L.;   Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F.~L.;   Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.00958,chen2021evaluating,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman et~al.}]{chen2021evaluating} Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d.~O.; Kaplan, J.;   Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et~al. 2021.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman et~al.}]{chen2021evaluating} Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d.~O.; Kaplan, J.;   Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et~al. 2021. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}."
2407.00958,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui}]{dong2022survey} Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; and   Sui, Z. 2022.",A survey on in-context learning.,A survey on in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui}]{dong2022survey} Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; and   Sui, Z. 2022. 
 A survey on in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2407.00958,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and   Chen}]{hu2021lora} Hu, E.~J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and   Chen, W. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and   Chen}]{hu2021lora} Hu, E.~J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and   Chen, W. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2407.00958,sanh2021multitask,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,   Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Sanh, V.; Webson, A.; Raffel, C.; Bach, S.~H.; Sutawika, L.; Alyafeai, Z.;   Chaffin, A.; Stiegler, A.; Scao, T.~L.; Raja, A.; et~al. 2021.",Multitask prompted training enables zero-shot task generalization.,Multitask prompted training enables zero-shot task generalization.,,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,   Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Sanh, V.; Webson, A.; Raffel, C.; Bach, S.~H.; Sutawika, L.; Alyafeai, Z.;   Chaffin, A.; Stiegler, A.; Scao, T.~L.; Raja, A.; et~al. 2021. 
 Multitask prompted training enables zero-shot task generalization. 
 \emph{arXiv preprint arXiv:2110.08207}."
2407.00958,sun2023simple,"[{Sun et~al.(2023)Sun, Liu, Bair, and Kolter}]{sun2023simple} Sun, M.; Liu, Z.; Bair, A.; and Kolter, J.~Z. 2023.",A simple and effective pruning approach for large language models.,A simple and effective pruning approach for large language models.,,"[{Sun et~al.(2023)Sun, Liu, Bair, and Kolter}]{sun2023simple} Sun, M.; Liu, Z.; Bair, A.; and Kolter, J.~Z. 2023. 
 A simple and effective pruning approach for large language models. 
 \emph{arXiv preprint arXiv:2306.11695}."
2407.00958,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,   Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix,   T.; Rozi{\`e}re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,   Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix,   T.; Rozi{\`e}re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2407.00958,wang2024universalapproximationtheoryfoundations,"[{Wang and   Li(2024{\natexlab{a}})}]{wang2024universalapproximationtheoryfoundations} Wang, W.; and Li, Q. 2024{\natexlab{a}}.",Universal Approximation Theory: Foundations for Parallelism in Neural   Networks.,Universal Approximation Theory: Foundations for Parallelism in Neural   Networks.,,"[{Wang and   Li(2024{\natexlab{a}})}]{wang2024universalapproximationtheoryfoundations} Wang, W.; and Li, Q. 2024{\natexlab{a}}. 
 Universal Approximation Theory: Foundations for Parallelism in Neural   Networks. 
 arXiv:2407.21670."
2407.00958,wang2024universalapproximationtheorybasic,"[{Wang and   Li(2024{\natexlab{b}})}]{wang2024universalapproximationtheorybasic} Wang, W.; and Li, Q. 2024{\natexlab{b}}.",Universal Approximation Theory: The basic theory for deep   learning-based computer vision models.,Universal Approximation Theory: The basic theory for deep   learning-based computer vision models.,,"[{Wang and   Li(2024{\natexlab{b}})}]{wang2024universalapproximationtheorybasic} Wang, W.; and Li, Q. 2024{\natexlab{b}}. 
 Universal Approximation Theory: The basic theory for deep   learning-based computer vision models. 
 arXiv:2407.17480."
2407.00958,wei2021finetuned,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and   Le}]{wei2021finetuned} Wei, J.; Bosma, M.; Zhao, V.~Y.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai,   A.~M.; and Le, Q.~V. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and   Le}]{wei2021finetuned} Wei, J.; Bosma, M.; Zhao, V.~Y.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai,   A.~M.; and Le, Q.~V. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2407.00958,zeng2022glm,"[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia   et~al.}]{zeng2022glm} Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.;   Zheng, W.; Xia, X.; et~al. 2022.",Glm-130b: An open bilingual pre-trained model.,Glm-130b: An open bilingual pre-trained model.,,"[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia   et~al.}]{zeng2022glm} Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.;   Zheng, W.; Xia, X.; et~al. 2022. 
 Glm-130b: An open bilingual pre-trained model. 
 \emph{arXiv preprint arXiv:2210.02414}."
2407.00958,zhao2023survey,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,   Dong et~al.}]{zhao2023survey} Zhao, W.~X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.;   Zhang, J.; Dong, Z.; et~al. 2023.",A survey of large language models.,A survey of large language models.,,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,   Dong et~al.}]{zhao2023survey} Zhao, W.~X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.;   Zhang, J.; Dong, Z.; et~al. 2023. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}."
2407.01009,cobbe2021gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.01009,wang2022self,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, and Denny Zhou. 2022.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, and Denny Zhou. 2022. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}."
2407.01009,zhou2022large,"[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba}]{zhou2022large} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.",Large language models are human-level prompt engineers.,Large language models are human-level prompt engineers.,,"[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba}]{zhou2022large} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. 
 Large language models are human-level prompt engineers. 
 \emph{arXiv preprint arXiv:2211.01910}."
2407.02043,chevalier2023adapting,"[{Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen}]{chevalier2023adapting} Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023.",Adapting language models to compress contexts.,Adapting language models to compress contexts.,,"[{Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen}]{chevalier2023adapting} Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. 
 Adapting language models to compress contexts. 
 \emph{arXiv preprint arXiv:2305.14788}."
2407.02043,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.02043,ge2023context,"[{Ge et~al.(2023)Ge, Hu, Wang, Chen, and Wei}]{ge2023context} Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.",In-context autoencoder for context compression in a large language model.,In-context autoencoder for context compression in a large language model.,,"[{Ge et~al.(2023)Ge, Hu, Wang, Chen, and Wei}]{ge2023context} Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. 
 In-context autoencoder for context compression in a large language model. 
 \emph{arXiv preprint arXiv:2307.06945}."
2407.02043,huang2022inner,"[{Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar et~al.}]{huang2022inner} Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et~al. 2022.",Inner monologue: Embodied reasoning through planning with language models.,Inner monologue: Embodied reasoning through planning with language models.,,"[{Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar et~al.}]{huang2022inner} Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et~al. 2022. 
 Inner monologue: Embodied reasoning through planning with language models. 
 \emph{arXiv preprint arXiv:2207.05608}."
2407.02043,jiang2023llmlingua,"[{Jiang et~al.(2023{\natexlab{a}})Jiang, Wu, Lin, Yang, and Qiu}]{jiang2023llmlingua} Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023{\natexlab{a}}.",Llmlingua: Compressing prompts for accelerated inference of large language models.,Llmlingua: Compressing prompts for accelerated inference of large language models.,,"[{Jiang et~al.(2023{\natexlab{a}})Jiang, Wu, Lin, Yang, and Qiu}]{jiang2023llmlingua} Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023{\natexlab{a}}. 
 Llmlingua: Compressing prompts for accelerated inference of large language models. 
 \emph{arXiv preprint arXiv:2310.05736}."
2407.02043,jiang2023longllmlingua,"[{Jiang et~al.(2023{\natexlab{b}})Jiang, Wu, Luo, Li, Lin, Yang, and Qiu}]{jiang2023longllmlingua} Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023{\natexlab{b}}.",Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.,Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.,,"[{Jiang et~al.(2023{\natexlab{b}})Jiang, Wu, Luo, Li, Lin, Yang, and Qiu}]{jiang2023longllmlingua} Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023{\natexlab{b}}. 
 Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. 
 \emph{arXiv preprint arXiv:2310.06839}."
2407.02043,li2023api,"[{Li et~al.(2023{\natexlab{a}})Li, Song, Yu, Yu, Li, Huang, and Li}]{li2023api} Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023{\natexlab{a}}.",Api-bank: A benchmark for tool-augmented llms.,Api-bank: A benchmark for tool-augmented llms.,,"[{Li et~al.(2023{\natexlab{a}})Li, Song, Yu, Yu, Li, Huang, and Li}]{li2023api} Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023{\natexlab{a}}. 
 Api-bank: A benchmark for tool-augmented llms. 
 \emph{arXiv preprint arXiv:2304.08244}."
2407.02043,li2023compressing,"[{Li et~al.(2023{\natexlab{b}})Li, Dong, Lin, and Guerin}]{li2023compressing} Yucheng Li, Bo~Dong, Chenghua Lin, and Frank Guerin. 2023{\natexlab{b}}.",Compressing context to enhance inference efficiency of large language models.,Compressing context to enhance inference efficiency of large language models.,,"[{Li et~al.(2023{\natexlab{b}})Li, Dong, Lin, and Guerin}]{li2023compressing} Yucheng Li, Bo~Dong, Chenghua Lin, and Frank Guerin. 2023{\natexlab{b}}. 
 Compressing context to enhance inference efficiency of large language models. 
 \emph{arXiv preprint arXiv:2310.06201}."
2407.02043,patil2023gorilla,"[{Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez}]{patil2023gorilla} Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez. 2023.",Gorilla: Large language model connected with massive apis.,Gorilla: Large language model connected with massive apis.,,"[{Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez}]{patil2023gorilla} Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez. 2023. 
 Gorilla: Large language model connected with massive apis. 
 \emph{arXiv preprint arXiv:2305.15334}."
2407.02043,qin2023toolllm,"[{Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian et~al.}]{qin2023toolllm} Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et~al. 2023.",Toolllm: Facilitating large language models to master 16000+ real-world apis.,Toolllm: Facilitating large language models to master 16000+ real-world apis.,,"[{Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian et~al.}]{qin2023toolllm} Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et~al. 2023. 
 Toolllm: Facilitating large language models to master 16000+ real-world apis. 
 \emph{arXiv preprint arXiv:2307.16789}."
2407.02043,schick2023toolformer,"[{Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom}]{schick2023toolformer} Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.",Toolformer: Language models can teach themselves to use tools.,Toolformer: Language models can teach themselves to use tools.,,"[{Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom}]{schick2023toolformer} Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. 
 Toolformer: Language models can teach themselves to use tools. 
 \emph{arXiv preprint arXiv:2302.04761}."
2407.02043,tang2023toolalpaca,"[{Tang et~al.(2023)Tang, Deng, Lin, Han, Liang, and Sun}]{tang2023toolalpaca} Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le~Sun. 2023.",Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.,Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.,,"[{Tang et~al.(2023)Tang, Deng, Lin, Han, Liang, and Sun}]{tang2023toolalpaca} Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le~Sun. 2023. 
 Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. 
 \emph{arXiv preprint arXiv:2306.05301}."
2407.02043,wang2023openchat,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023.",Openchat: Advancing open-source language models with mixed-quality data.,Openchat: Advancing open-source language models with mixed-quality data.,,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{wang2023openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. 
 Openchat: Advancing open-source language models with mixed-quality data. 
 \emph{arXiv preprint arXiv:2309.11235}."
2407.02043,wang2024loma,[{Wang and Xiao(2024)}]{wang2024loma} Yumeng Wang and Zhenyang Xiao. 2024.,Loma: Lossless compressed memory attention.,Loma: Lossless compressed memory attention.,,"[{Wang and Xiao(2024)}]{wang2024loma} Yumeng Wang and Zhenyang Xiao. 2024. 
 Loma: Lossless compressed memory attention. 
 \emph{arXiv preprint arXiv:2401.09486}."
2407.02043,xu2023tool,"[{Xu et~al.(2023)Xu, Hong, Li, Hu, Chen, and Zhang}]{xu2023tool} Qiantong Xu, Fenglu Hong, Bo~Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023.",On the tool manipulation capability of open-source large language models.,On the tool manipulation capability of open-source large language models.,,"[{Xu et~al.(2023)Xu, Hong, Li, Hu, Chen, and Zhang}]{xu2023tool} Qiantong Xu, Fenglu Hong, Bo~Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023. 
 On the tool manipulation capability of open-source large language models. 
 \emph{arXiv preprint arXiv:2305.16504}."
2407.02043,yang2023gpt4tools,"[{Yang et~al.(2023)Yang, Song, Li, Zhao, Ge, Li, and Shan}]{yang2023gpt4tools} Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023.",Gpt4tools: Teaching large language model to use tools via self-instruction.,Gpt4tools: Teaching large language model to use tools via self-instruction.,,"[{Yang et~al.(2023)Yang, Song, Li, Zhao, Ge, Li, and Shan}]{yang2023gpt4tools} Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. 
 Gpt4tools: Teaching large language model to use tools via self-instruction. 
 \emph{arXiv preprint arXiv:2305.18752}."
2407.02056,gpt4,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.02056,humaneval,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}."
2407.02056,GSM8K,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{GSM8K} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{GSM8K} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.02056,MATH,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{MATH} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{MATH} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}."
2407.02056,multi,"[{Huang et~al.(2023)Huang, Lu, Chen, Wan, and Duan}]{multi} Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2023.",Enhancing large language models in coding through multi-perspective self-consistency.,Enhancing large language models in coding through multi-perspective self-consistency.,,"[{Huang et~al.(2023)Huang, Lu, Chen, Wan, and Duan}]{multi} Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2023. 
 Enhancing large language models in coding through multi-perspective self-consistency. 
 \emph{arXiv preprint arXiv:2309.17272}."
2407.02056,birdsql,"[{Li et~al.(2023{\natexlab{a}})Li, Hui, Qu, Li, Yang, Li, Wang, Qin, Cao, Geng et~al.}]{birdsql} Jinyang Li, Binyuan Hui, Ge~Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et~al. 2023{\natexlab{a}}.",Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls.,Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls.,,"[{Li et~al.(2023{\natexlab{a}})Li, Hui, Qu, Li, Yang, Li, Wang, Qin, Cao, Geng et~al.}]{birdsql} Jinyang Li, Binyuan Hui, Ge~Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et~al. 2023{\natexlab{a}}. 
 Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. 
 \emph{arXiv preprint arXiv:2305.03111}."
2407.02056,humanevalplus,"[{Liu et~al.(2023{\natexlab{a}})Liu, Xia, Wang, and Zhang}]{humanevalplus} Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023{\natexlab{a}}.",Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.,Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Xia, Wang, and Zhang}]{humanevalplus} Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023{\natexlab{a}}. 
 Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. 
 \emph{arXiv preprint arXiv:2305.01210}."
2407.02056,yang2023large,"[{Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{yang2023large} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2023.",Large language models as optimizers.,Large language models as optimizers.,,"[{Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{yang2023large} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2023. 
 Large language models as optimizers. 
 \emph{arXiv preprint arXiv:2309.03409}."
2407.02056,yoran2023answering,"[{Yoran et~al.(2023)Yoran, Wolfson, Bogin, Katz, Deutch, and Berant}]{yoran2023answering} Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023.",Answering questions by meta-reasoning over multiple chains of thought.,Answering questions by meta-reasoning over multiple chains of thought.,,"[{Yoran et~al.(2023)Yoran, Wolfson, Bogin, Katz, Deutch, and Berant}]{yoran2023answering} Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. 
 Answering questions by meta-reasoning over multiple chains of thought. 
 \emph{arXiv preprint arXiv:2304.13007}."
2407.02056,yuan2023batcheval,"[{Yuan et~al.(2023)Yuan, Feng, Li, Wang, Pan, Wang, and Li}]{yuan2023batcheval} Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, and Kan Li. 2023.",Batcheval: Towards human-like text evaluation.,Batcheval: Towards human-like text evaluation.,,"[{Yuan et~al.(2023)Yuan, Feng, Li, Wang, Pan, Wang, and Li}]{yuan2023batcheval} Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, and Kan Li. 2023. 
 Batcheval: Towards human-like text evaluation. 
 \emph{arXiv preprint arXiv:2401.00437}."
2407.02056,yue2023large,"[{Yue et~al.(2023)Yue, Zhao, Zhang, Du, and Yao}]{yue2023large} Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. 2023.",Large language model cascades with mixture of thoughts representations for cost-efficient reasoning.,Large language model cascades with mixture of thoughts representations for cost-efficient reasoning.,,"[{Yue et~al.(2023)Yue, Zhao, Zhang, Du, and Yao}]{yue2023large} Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. 2023. 
 Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. 
 \emph{arXiv preprint arXiv:2310.03094}."
2407.02056,bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
2407.02056,self-contrast,"[{Zhang et~al.(2024)Zhang, Shen, Wu, Peng, Wang, Zhuang, and Lu}]{self-contrast} Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. 2024.",Self-contrast: Better reflection through inconsistent solving perspectives.,Self-contrast: Better reflection through inconsistent solving perspectives.,,"[{Zhang et~al.(2024)Zhang, Shen, Wu, Peng, Wang, Zhuang, and Lu}]{self-contrast} Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. 2024. 
 Self-contrast: Better reflection through inconsistent solving perspectives. 
 \emph{arXiv preprint arXiv:2401.02009}."
2407.02056,zheng2023progressive,"[{Zheng et~al.(2023)Zheng, Liu, Xie, Li, and Li}]{zheng2023progressive} Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu~Li. 2023.",Progressive-hint prompting improves reasoning in large language models.,Progressive-hint prompting improves reasoning in large language models.,,"[{Zheng et~al.(2023)Zheng, Liu, Xie, Li, and Li}]{zheng2023progressive} Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu~Li. 2023. 
 Progressive-hint prompting improves reasoning in large language models. 
 \emph{arXiv preprint arXiv:2304.09797}."
2407.02118,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.02118,frantar2023scaling,"[{Frantar et~al.(2023)Frantar, Riquelme, Houlsby, Alistarh, and Evci}]{frantar2023scaling} Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. 2023.",Scaling laws for sparsely-connected foundation models.,Scaling laws for sparsely-connected foundation models.,,"[{Frantar et~al.(2023)Frantar, Riquelme, Houlsby, Alistarh, and Evci}]{frantar2023scaling} Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. 2023. 
 Scaling laws for sparsely-connected foundation models. 
 \emph{arXiv preprint arXiv:2309.08520}."
2407.02118,gupta2023continual,"[{Gupta et~al.(2023)Gupta, Th{\'e}rien, Ibrahim, Richter, Anthony, Belilovsky, Rish, and Lesort}]{gupta2023continual} Kshitij Gupta, Benjamin Th{\'e}rien, Adam Ibrahim, Mats~L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth{\'e}e Lesort. 2023.",Continual pre-training of large language models: How to (re) warm your model?,Continual pre-training of large language models: How to (re) warm your model?,,"[{Gupta et~al.(2023)Gupta, Th{\'e}rien, Ibrahim, Richter, Anthony, Belilovsky, Rish, and Lesort}]{gupta2023continual} Kshitij Gupta, Benjamin Th{\'e}rien, Adam Ibrahim, Mats~L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth{\'e}e Lesort. 2023. 
 Continual pre-training of large language models: How to (re) warm your model? 
 \emph{arXiv preprint arXiv:2308.04014}."
2407.02118,hernandez2022scaling,"[{Hernandez et~al.(2022)Hernandez, Brown, Conerly, DasSarma, Drain, El-Showk, Elhage, Hatfield-Dodds, Henighan, Hume et~al.}]{hernandez2022scaling} Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et~al. 2022.",Scaling laws and interpretability of learning from repeated data.,Scaling laws and interpretability of learning from repeated data.,,"[{Hernandez et~al.(2022)Hernandez, Brown, Conerly, DasSarma, Drain, El-Showk, Elhage, Hatfield-Dodds, Henighan, Hume et~al.}]{hernandez2022scaling} Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et~al. 2022. 
 Scaling laws and interpretability of learning from repeated data. 
 \emph{arXiv preprint arXiv:2205.10487}."
2407.02118,hernandez2021scaling,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{hernandez2021scaling} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.",Scaling laws for transfer.,Scaling laws for transfer.,,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{hernandez2021scaling} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. 
 Scaling laws for transfer. 
 \emph{arXiv preprint arXiv:2102.01293}."
2407.02118,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2407.02118,ibrahim2024simple,"[{Ibrahim et~al.(2024)Ibrahim, Th{\'e}rien, Gupta, Richter, Anthony, Lesort, Belilovsky, and Rish}]{ibrahim2024simple} Adam Ibrahim, Benjamin Th{\'e}rien, Kshitij Gupta, Mats~L Richter, Quentin Anthony, Timoth{\'e}e Lesort, Eugene Belilovsky, and Irina Rish. 2024.",Simple and scalable strategies to continually pre-train large language models.,Simple and scalable strategies to continually pre-train large language models.,,"[{Ibrahim et~al.(2024)Ibrahim, Th{\'e}rien, Gupta, Richter, Anthony, Lesort, Belilovsky, and Rish}]{ibrahim2024simple} Adam Ibrahim, Benjamin Th{\'e}rien, Kshitij Gupta, Mats~L Richter, Quentin Anthony, Timoth{\'e}e Lesort, Eugene Belilovsky, and Irina Rish. 2024. 
 Simple and scalable strategies to continually pre-train large language models. 
 \emph{arXiv preprint arXiv:2403.08763}."
2407.02118,kalajdzievski2024scaling,[{Kalajdzievski(2024)}]{kalajdzievski2024scaling} Damjan Kalajdzievski. 2024.,Scaling laws for forgetting when fine-tuning large language models.,Scaling laws for forgetting when fine-tuning large language models.,,"[{Kalajdzievski(2024)}]{kalajdzievski2024scaling} Damjan Kalajdzievski. 2024. 
 Scaling laws for forgetting when fine-tuning large language models. 
 \emph{arXiv preprint arXiv:2401.05605}."
2407.02118,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2407.02118,krajewski2024scaling,"[{Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro, Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski et~al.}]{krajewski2024scaling} Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi{\'o}ro, Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr{\'o}l, Tomasz Odrzyg{\'o}{\'z}d{\'z}, Piotr Sankowski, et~al. 2024.",Scaling laws for fine-grained mixture of experts.,Scaling laws for fine-grained mixture of experts.,,"[{Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro, Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski et~al.}]{krajewski2024scaling} Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi{\'o}ro, Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr{\'o}l, Tomasz Odrzyg{\'o}{\'z}d{\'z}, Piotr Sankowski, et~al. 2024. 
 Scaling laws for fine-grained mixture of experts. 
 \emph{arXiv preprint arXiv:2402.07871}."
2407.02118,muennighoff2023scaling,"[{Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel}]{muennighoff2023scaling} Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023.",Scaling data-constrained language models.,Scaling data-constrained language models.,,"[{Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi, Pyysalo, Wolf, and Raffel}]{muennighoff2023scaling} Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. 
 Scaling data-constrained language models. 
 \emph{arXiv preprint arXiv:2305.16264}."
2407.02118,tang2020multilingual,"[{Tang et~al.(2020)Tang, Tran, Li, Chen, Goyal, Chaudhary, Gu, and Fan}]{tang2020multilingual} Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020.",Multilingual translation with extensible multilingual pretraining and finetuning.,Multilingual translation with extensible multilingual pretraining and finetuning.,,"[{Tang et~al.(2020)Tang, Tran, Li, Chen, Goyal, Chaudhary, Gu, and Fan}]{tang2020multilingual} Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. 
 Multilingual translation with extensible multilingual pretraining and finetuning. 
 \emph{arXiv preprint arXiv:2008.00401}."
2407.02118,tay2022scaling,"[{Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler}]{tay2022scaling} Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh~Q Tran, Dani Yogatama, and Donald Metzler. 2022.",Scaling laws vs model architectures: How does inductive bias influence scaling?,Scaling laws vs model architectures: How does inductive bias influence scaling?,,"[{Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler}]{tay2022scaling} Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh~Q Tran, Dani Yogatama, and Donald Metzler. 2022. 
 Scaling laws vs model architectures: How does inductive bias influence scaling? 
 \emph{arXiv preprint arXiv:2207.10551}."
2407.02118,touvron2023llama2,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.02118,wei2022emergent,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al. 2022.",Emergent abilities of large language models.,Emergent abilities of large language models.,,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al. 2022. 
 Emergent abilities of large language models. 
 \emph{arXiv preprint arXiv:2206.07682}."
2407.02118,winata2023overcoming,"[{Winata et~al.(2023)Winata, Xie, Radhakrishnan, Wu, Jin, Cheng, Kulkarni, and Preotiuc-Pietro}]{winata2023overcoming} Genta~Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2023.",Overcoming catastrophic forgetting in massively multilingual continual learning.,Overcoming catastrophic forgetting in massively multilingual continual learning.,,"[{Winata et~al.(2023)Winata, Xie, Radhakrishnan, Wu, Jin, Cheng, Kulkarni, and Preotiuc-Pietro}]{winata2023overcoming} Genta~Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2023. 
 Overcoming catastrophic forgetting in massively multilingual continual learning. 
 \emph{arXiv preprint arXiv:2305.16252}."
2407.02118,wu2019emerging,"[{Wu et~al.(2019)Wu, Conneau, Li, Zettlemoyer, and Stoyanov}]{wu2019emerging} Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",Emerging cross-lingual structure in pretrained language models.,Emerging cross-lingual structure in pretrained language models.,,"[{Wu et~al.(2019)Wu, Conneau, Li, Zettlemoyer, and Stoyanov}]{wu2019emerging} Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2019. 
 Emerging cross-lingual structure in pretrained language models. 
 \emph{arXiv preprint arXiv:1911.01464}."
2407.02118,zhang2024scaling,"[{Zhang et~al.(2024)Zhang, Liu, Cherry, and Firat}]{zhang2024scaling} Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. 2024.","When scaling meets llm finetuning: The effect of data, model and finetuning method.","When scaling meets llm finetuning: The effect of data, model and finetuning method.",,"[{Zhang et~al.(2024)Zhang, Liu, Cherry, and Firat}]{zhang2024scaling} Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. 2024. 
 When scaling meets llm finetuning: The effect of data, model and finetuning method. 
 \emph{arXiv preprint arXiv:2402.17193}."
2407.02666,ahn2022can,"[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman, et~al.]{ahn2022can} M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn, C.~Fu, K.~Gopalakrishnan, K.~Hausman, et~al.","Do as i can, not as i say: Grounding language in robotic affordances.","Do as i can, not as i say: Grounding language in robotic affordances.",,"[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman, et~al.]{ahn2022can} M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn, C.~Fu, K.~Gopalakrishnan, K.~Hausman, et~al. 
 Do as i can, not as i say: Grounding language in robotic affordances. 
 \emph{arXiv preprint arXiv:2204.01691}, 2022."
2407.02666,driess2023palm,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, et~al.",Palm-e: An embodied multimodal language model.,Palm-e: An embodied multimodal language model.,,"[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm} D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, et~al. 
 Palm-e: An embodied multimodal language model. 
 \emph{arXiv preprint arXiv:2303.03378}, 2023."
2407.02666,ebert2018visual,"[Ebert et~al.(2018)Ebert, Finn, Dasari, Xie, Lee, and Levine]{ebert2018visual} F.~Ebert, C.~Finn, S.~Dasari, A.~Xie, A.~Lee, and S.~Levine.",Visual foresight: Model-based deep reinforcement learning for vision-based robotic control.,Visual foresight: Model-based deep reinforcement learning for vision-based robotic control.,,"[Ebert et~al.(2018)Ebert, Finn, Dasari, Xie, Lee, and Levine]{ebert2018visual} F.~Ebert, C.~Finn, S.~Dasari, A.~Xie, A.~Lee, and S.~Levine. 
 Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. 
 \emph{arXiv preprint arXiv:1812.00568}, 2018."
2407.02666,haarnoja2018learning,"[Haarnoja et~al.(2018)Haarnoja, Ha, Zhou, Tan, Tucker, and Levine]{haarnoja2018learning} T.~Haarnoja, S.~Ha, A.~Zhou, J.~Tan, G.~Tucker, and S.~Levine.",Learning to walk via deep reinforcement learning.,Learning to walk via deep reinforcement learning.,,"[Haarnoja et~al.(2018)Haarnoja, Ha, Zhou, Tan, Tucker, and Levine]{haarnoja2018learning} T.~Haarnoja, S.~Ha, A.~Zhou, J.~Tan, G.~Tucker, and S.~Levine. 
 Learning to walk via deep reinforcement learning. 
 \emph{arXiv preprint arXiv:1812.11103}, 2018."
2407.02666,tan2018sim,"[Tan et~al.(2018)Tan, Zhang, Coumans, Iscen, Bai, Hafner, Bohez, and Vanhoucke]{tan2018sim} J.~Tan, T.~Zhang, E.~Coumans, A.~Iscen, Y.~Bai, D.~Hafner, S.~Bohez, and V.~Vanhoucke.",Sim-to-real: Learning agile locomotion for quadruped robots.,Sim-to-real: Learning agile locomotion for quadruped robots.,,"[Tan et~al.(2018)Tan, Zhang, Coumans, Iscen, Bai, Hafner, Bohez, and Vanhoucke]{tan2018sim} J.~Tan, T.~Zhang, E.~Coumans, A.~Iscen, Y.~Bai, D.~Hafner, S.~Bohez, and V.~Vanhoucke. 
 Sim-to-real: Learning agile locomotion for quadruped robots. 
 \emph{arXiv preprint arXiv:1804.10332}, 2018."
2407.02666,yang2021learning,"[Yang et~al.(2021)Yang, Zhang, Hansen, Xu, and Wang]{yang2021learning} R.~Yang, M.~Zhang, N.~Hansen, H.~Xu, and X.~Wang.",Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers.,Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers.,,"[Yang et~al.(2021)Yang, Zhang, Hansen, Xu, and Wang]{yang2021learning} R.~Yang, M.~Zhang, N.~Hansen, H.~Xu, and X.~Wang. 
 Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. 
 \emph{arXiv preprint arXiv:2107.03996}, 2021."
2407.02666,peng2020learning,"[Peng et~al.(2020)Peng, Coumans, Zhang, Lee, Tan, and Levine]{peng2020learning} X.~B. Peng, E.~Coumans, T.~Zhang, T.-W. Lee, J.~Tan, and S.~Levine.",Learning agile robotic locomotion skills by imitating animals.,Learning agile robotic locomotion skills by imitating animals.,,"[Peng et~al.(2020)Peng, Coumans, Zhang, Lee, Tan, and Levine]{peng2020learning} X.~B. Peng, E.~Coumans, T.~Zhang, T.-W. Lee, J.~Tan, and S.~Levine. 
 Learning agile robotic locomotion skills by imitating animals. 
 \emph{arXiv preprint arXiv:2004.00784}, 2020."
2407.02666,caluwaerts2023barkour,"[Caluwaerts et~al.(2023)Caluwaerts, Iscen, Kew, Yu, Zhang, Freeman, Lee, Lee, Saliceti, Zhuang, et~al.]{caluwaerts2023barkour} K.~Caluwaerts, A.~Iscen, J.~C. Kew, W.~Yu, T.~Zhang, D.~Freeman, K.-H. Lee, L.~Lee, S.~Saliceti, V.~Zhuang, et~al.",Barkour: Benchmarking animal-level agility with quadruped robots.,Barkour: Benchmarking animal-level agility with quadruped robots.,,"[Caluwaerts et~al.(2023)Caluwaerts, Iscen, Kew, Yu, Zhang, Freeman, Lee, Lee, Saliceti, Zhuang, et~al.]{caluwaerts2023barkour} K.~Caluwaerts, A.~Iscen, J.~C. Kew, W.~Yu, T.~Zhang, D.~Freeman, K.-H. Lee, L.~Lee, S.~Saliceti, V.~Zhuang, et~al. 
 Barkour: Benchmarking animal-level agility with quadruped robots. 
 \emph{arXiv preprint arXiv:2305.14654}, 2023."
2407.02666,he2024agile,"[He et~al.(2024)He, Zhang, Xiao, He, Liu, and Shi]{he2024agile} T.~He, C.~Zhang, W.~Xiao, G.~He, C.~Liu, and G.~Shi.",Agile but safe: Learning collision-free high-speed legged locomotion.,Agile but safe: Learning collision-free high-speed legged locomotion.,,"[He et~al.(2024)He, Zhang, Xiao, He, Liu, and Shi]{he2024agile} T.~He, C.~Zhang, W.~Xiao, G.~He, C.~Liu, and G.~Shi. 
 Agile but safe: Learning collision-free high-speed legged locomotion. 
 \emph{arXiv preprint arXiv:2401.17583}, 2024."
2407.02666,cheng2023extreme,"[Cheng et~al.(2023)Cheng, Shi, Agarwal, and Pathak]{cheng2023extreme} X.~Cheng, K.~Shi, A.~Agarwal, and D.~Pathak.",Extreme parkour with legged robots.,Extreme parkour with legged robots.,,"[Cheng et~al.(2023)Cheng, Shi, Agarwal, and Pathak]{cheng2023extreme} X.~Cheng, K.~Shi, A.~Agarwal, and D.~Pathak. 
 Extreme parkour with legged robots. 
 \emph{arXiv preprint arXiv:2309.14341}, 2023."
2407.02666,rajeswaran2016epopt,"[Rajeswaran et~al.(2016)Rajeswaran, Ghotra, Ravindran, and Levine]{rajeswaran2016epopt} A.~Rajeswaran, S.~Ghotra, B.~Ravindran, and S.~Levine.",Epopt: Learning robust neural network policies using model ensembles.,Epopt: Learning robust neural network policies using model ensembles.,,"[Rajeswaran et~al.(2016)Rajeswaran, Ghotra, Ravindran, and Levine]{rajeswaran2016epopt} A.~Rajeswaran, S.~Ghotra, B.~Ravindran, and S.~Levine. 
 Epopt: Learning robust neural network policies using model ensembles. 
 \emph{arXiv preprint arXiv:1610.01283}, 2016."
2407.02666,sadeghi2016cad2rl,[Sadeghi and Levine(2016)]{sadeghi2016cad2rl} F.~Sadeghi and S.~Levine.,Cad2rl: Real single-image flight without a single real image.,Cad2rl: Real single-image flight without a single real image.,,"[Sadeghi and Levine(2016)]{sadeghi2016cad2rl} F.~Sadeghi and S.~Levine. 
 Cad2rl: Real single-image flight without a single real image. 
 \emph{arXiv preprint arXiv:1611.04201}, 2016."
2407.02666,akkaya2019solving,"[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew, Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving} I.~Akkaya, M.~Andrychowicz, M.~Chociej, M.~Litwin, B.~McGrew, A.~Petron, A.~Paino, M.~Plappert, G.~Powell, R.~Ribas, et~al.",Solving rubik's cube with a robot hand.,Solving rubik's cube with a robot hand.,,"[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew, Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving} I.~Akkaya, M.~Andrychowicz, M.~Chociej, M.~Litwin, B.~McGrew, A.~Petron, A.~Paino, M.~Plappert, G.~Powell, R.~Ribas, et~al. 
 Solving rubik's cube with a robot hand. 
 \emph{arXiv preprint arXiv:1910.07113}, 2019."
2407.02666,margolis2022rapid,"[Margolis et~al.(2022)Margolis, Yang, Paigwar, Chen, and Agrawal]{margolis2022rapid} G.~B. Margolis, G.~Yang, K.~Paigwar, T.~Chen, and P.~Agrawal.",Rapid locomotion via reinforcement learning.,Rapid locomotion via reinforcement learning.,,"[Margolis et~al.(2022)Margolis, Yang, Paigwar, Chen, and Agrawal]{margolis2022rapid} G.~B. Margolis, G.~Yang, K.~Paigwar, T.~Chen, and P.~Agrawal. 
 Rapid locomotion via reinforcement learning. 
 \emph{arXiv preprint arXiv:2205.02824}, 2022."
2407.02666,haarnoja2023learning,"[Haarnoja et~al.(2023)Haarnoja, Moran, Lever, Huang, Tirumala, Wulfmeier, Humplik, Tunyasuvunakool, Siegel, Hafner, et~al.]{haarnoja2023learning} T.~Haarnoja, B.~Moran, G.~Lever, S.~H. Huang, D.~Tirumala, M.~Wulfmeier, J.~Humplik, S.~Tunyasuvunakool, N.~Y. Siegel, R.~Hafner, et~al.",Learning agile soccer skills for a bipedal robot with deep reinforcement learning.,Learning agile soccer skills for a bipedal robot with deep reinforcement learning.,,"[Haarnoja et~al.(2023)Haarnoja, Moran, Lever, Huang, Tirumala, Wulfmeier, Humplik, Tunyasuvunakool, Siegel, Hafner, et~al.]{haarnoja2023learning} T.~Haarnoja, B.~Moran, G.~Lever, S.~H. Huang, D.~Tirumala, M.~Wulfmeier, J.~Humplik, S.~Tunyasuvunakool, N.~Y. Siegel, R.~Hafner, et~al. 
 Learning agile soccer skills for a bipedal robot with deep reinforcement learning. 
 \emph{arXiv preprint arXiv:2304.13653}, 2023."
2407.02666,kumar2021rma,"[Kumar et~al.(2021)Kumar, Fu, Pathak, and Malik]{kumar2021rma} A.~Kumar, Z.~Fu, D.~Pathak, and J.~Malik.",Rma: Rapid motor adaptation for legged robots.,Rma: Rapid motor adaptation for legged robots.,,"[Kumar et~al.(2021)Kumar, Fu, Pathak, and Malik]{kumar2021rma} A.~Kumar, Z.~Fu, D.~Pathak, and J.~Malik. 
 Rma: Rapid motor adaptation for legged robots. 
 \emph{arXiv preprint arXiv:2107.04034}, 2021."
2407.02666,chen2023adapt,"[Chen et~al.(2023)Chen, Chada, Smith, Sharma, Fu, Levine, and Finn]{chen2023adapt} A.~S. Chen, G.~Chada, L.~Smith, A.~Sharma, Z.~Fu, S.~Levine, and C.~Finn.",Adapt on-the-go: Behavior modulation for single-life robot deployment.,Adapt on-the-go: Behavior modulation for single-life robot deployment.,,"[Chen et~al.(2023)Chen, Chada, Smith, Sharma, Fu, Levine, and Finn]{chen2023adapt} A.~S. Chen, G.~Chada, L.~Smith, A.~Sharma, Z.~Fu, S.~Levine, and C.~Finn. 
 Adapt on-the-go: Behavior modulation for single-life robot deployment. 
 \emph{arXiv preprint arXiv:2311.01059}, 2023."
2407.02666,sharma2020learning,"[Sharma et~al.(2020)Sharma, Liang, Zhao, LaGrassa, and Kroemer]{sharma2020learning} M.~Sharma, J.~Liang, J.~Zhao, A.~LaGrassa, and O.~Kroemer.",Learning to compose hierarchical object-centric controllers for robotic manipulation.,Learning to compose hierarchical object-centric controllers for robotic manipulation.,,"[Sharma et~al.(2020)Sharma, Liang, Zhao, LaGrassa, and Kroemer]{sharma2020learning} M.~Sharma, J.~Liang, J.~Zhao, A.~LaGrassa, and O.~Kroemer. 
 Learning to compose hierarchical object-centric controllers for robotic manipulation. 
 \emph{arXiv preprint arXiv:2011.04627}, 2020."
2407.02666,pertsch2021guided,"[Pertsch et~al.(2021)Pertsch, Lee, Wu, and Lim]{pertsch2021guided} K.~Pertsch, Y.~Lee, Y.~Wu, and J.~J. Lim.",Guided reinforcement learning with learned skills.,Guided reinforcement learning with learned skills.,,"[Pertsch et~al.(2021)Pertsch, Lee, Wu, and Lim]{pertsch2021guided} K.~Pertsch, Y.~Lee, Y.~Wu, and J.~J. Lim. 
 Guided reinforcement learning with learned skills. 
 \emph{arXiv preprint arXiv:2107.10253}, 2021."
2407.02666,huang2022inner,"[Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, et~al.]{huang2022inner} W.~Huang, F.~Xia, T.~Xiao, H.~Chan, J.~Liang, P.~Florence, A.~Zeng, J.~Tompson, I.~Mordatch, Y.~Chebotar, et~al.",Inner monologue: Embodied reasoning through planning with language models.,Inner monologue: Embodied reasoning through planning with language models.,,"[Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, et~al.]{huang2022inner} W.~Huang, F.~Xia, T.~Xiao, H.~Chan, J.~Liang, P.~Florence, A.~Zeng, J.~Tompson, I.~Mordatch, Y.~Chebotar, et~al. 
 Inner monologue: Embodied reasoning through planning with language models. 
 \emph{arXiv preprint arXiv:2207.05608}, 2022."
2407.02666,yu2023language,"[Yu et~al.(2023)Yu, Gileadi, Fu, Kirmani, Lee, Arenas, Chiang, Erez, Hasenclever, Humplik, et~al.]{yu2023language} W.~Yu, N.~Gileadi, C.~Fu, S.~Kirmani, K.-H. Lee, M.~G. Arenas, H.-T.~L. Chiang, T.~Erez, L.~Hasenclever, J.~Humplik, et~al.",Language to rewards for robotic skill synthesis.,Language to rewards for robotic skill synthesis.,,"[Yu et~al.(2023)Yu, Gileadi, Fu, Kirmani, Lee, Arenas, Chiang, Erez, Hasenclever, Humplik, et~al.]{yu2023language} W.~Yu, N.~Gileadi, C.~Fu, S.~Kirmani, K.-H. Lee, M.~G. Arenas, H.-T.~L. Chiang, T.~Erez, L.~Hasenclever, J.~Humplik, et~al. 
 Language to rewards for robotic skill synthesis. 
 \emph{arXiv preprint arXiv:2306.08647}, 2023."
2407.02666,sha2023languagempc,"[Sha et~al.(2023)Sha, Mu, Jiang, Chen, Xu, Luo, Li, Tomizuka, Zhan, and Ding]{sha2023languagempc} H.~Sha, Y.~Mu, Y.~Jiang, L.~Chen, C.~Xu, P.~Luo, S.~E. Li, M.~Tomizuka, W.~Zhan, and M.~Ding.",Languagempc: Large language models as decision makers for autonomous driving.,Languagempc: Large language models as decision makers for autonomous driving.,,"[Sha et~al.(2023)Sha, Mu, Jiang, Chen, Xu, Luo, Li, Tomizuka, Zhan, and Ding]{sha2023languagempc} H.~Sha, Y.~Mu, Y.~Jiang, L.~Chen, C.~Xu, P.~Luo, S.~E. Li, M.~Tomizuka, W.~Zhan, and M.~Ding. 
 Languagempc: Large language models as decision makers for autonomous driving. 
 \emph{arXiv preprint arXiv:2310.03026}, 2023."
2407.02666,mirchandani2023large,"[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess, Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large} S.~Mirchandani, F.~Xia, P.~Florence, B.~Ichter, D.~Driess, M.~G. Arenas, K.~Rao, D.~Sadigh, and A.~Zeng.",Large language models as general pattern machines.,Large language models as general pattern machines.,,"[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess, Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large} S.~Mirchandani, F.~Xia, P.~Florence, B.~Ichter, D.~Driess, M.~G. Arenas, K.~Rao, D.~Sadigh, and A.~Zeng. 
 Large language models as general pattern machines. 
 \emph{arXiv preprint arXiv:2307.04721}, 2023."
2407.02666,zha2023distilling,"[Zha et~al.(2023)Zha, Cui, Lin, Kwon, Arenas, Zeng, Xia, and Sadigh]{zha2023distilling} L.~Zha, Y.~Cui, L.-H. Lin, M.~Kwon, M.~G. Arenas, A.~Zeng, F.~Xia, and D.~Sadigh.",Distilling and retrieving generalizable knowledge for robot manipulation via language corrections.,Distilling and retrieving generalizable knowledge for robot manipulation via language corrections.,,"[Zha et~al.(2023)Zha, Cui, Lin, Kwon, Arenas, Zeng, Xia, and Sadigh]{zha2023distilling} L.~Zha, Y.~Cui, L.-H. Lin, M.~Kwon, M.~G. Arenas, A.~Zeng, F.~Xia, and D.~Sadigh. 
 Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. 
 \emph{arXiv preprint arXiv:2311.10678}, 2023."
2407.02666,liang2024learning,"[Liang et~al.(2024)Liang, Xia, Yu, Zeng, Arenas, Attarian, Bauza, Bennice, Bewley, Dostmohamed, et~al.]{liang2024learning} J.~Liang, F.~Xia, W.~Yu, A.~Zeng, M.~G. Arenas, M.~Attarian, M.~Bauza, M.~Bennice, A.~Bewley, A.~Dostmohamed, et~al.",Learning to learn faster from human feedback with language model predictive control.,Learning to learn faster from human feedback with language model predictive control.,,"[Liang et~al.(2024)Liang, Xia, Yu, Zeng, Arenas, Attarian, Bauza, Bennice, Bewley, Dostmohamed, et~al.]{liang2024learning} J.~Liang, F.~Xia, W.~Yu, A.~Zeng, M.~G. Arenas, M.~Attarian, M.~Bauza, M.~Bennice, A.~Bewley, A.~Dostmohamed, et~al. 
 Learning to learn faster from human feedback with language model predictive control. 
 \emph{arXiv preprint arXiv:2402.11450}, 2024."
2407.02666,huang2023voxposer,"[Huang et~al.(2023)Huang, Wang, Zhang, Li, Wu, and Fei-Fei]{huang2023voxposer} W.~Huang, C.~Wang, R.~Zhang, Y.~Li, J.~Wu, and L.~Fei-Fei.",Voxposer: Composable 3d value maps for robotic manipulation with language models.,Voxposer: Composable 3d value maps for robotic manipulation with language models.,,"[Huang et~al.(2023)Huang, Wang, Zhang, Li, Wu, and Fei-Fei]{huang2023voxposer} W.~Huang, C.~Wang, R.~Zhang, Y.~Li, J.~Wu, and L.~Fei-Fei. 
 Voxposer: Composable 3d value maps for robotic manipulation with language models. 
 \emph{arXiv preprint arXiv:2307.05973}, 2023."
2407.02666,nasiriany2024pivot,"[Nasiriany et~al.(2024)Nasiriany, Xia, Yu, Xiao, Liang, Dasgupta, Xie, Driess, Wahid, Xu, et~al.]{nasiriany2024pivot} S.~Nasiriany, F.~Xia, W.~Yu, T.~Xiao, J.~Liang, I.~Dasgupta, A.~Xie, D.~Driess, A.~Wahid, Z.~Xu, et~al.",Pivot: Iterative visual prompting elicits actionable knowledge for vlms.,Pivot: Iterative visual prompting elicits actionable knowledge for vlms.,,"[Nasiriany et~al.(2024)Nasiriany, Xia, Yu, Xiao, Liang, Dasgupta, Xie, Driess, Wahid, Xu, et~al.]{nasiriany2024pivot} S.~Nasiriany, F.~Xia, W.~Yu, T.~Xiao, J.~Liang, I.~Dasgupta, A.~Xie, D.~Driess, A.~Wahid, Z.~Xu, et~al. 
 Pivot: Iterative visual prompting elicits actionable knowledge for vlms. 
 \emph{arXiv preprint arXiv:2402.07872}, 2024."
2407.02666,belkhale2024rt,"[Belkhale et~al.(2024)Belkhale, Ding, Xiao, Sermanet, Vuong, Tompson, Chebotar, Dwibedi, and Sadigh]{belkhale2024rt} S.~Belkhale, T.~Ding, T.~Xiao, P.~Sermanet, Q.~Vuong, J.~Tompson, Y.~Chebotar, D.~Dwibedi, and D.~Sadigh.",Rt-h: Action hierarchies using language.,Rt-h: Action hierarchies using language.,,"[Belkhale et~al.(2024)Belkhale, Ding, Xiao, Sermanet, Vuong, Tompson, Chebotar, Dwibedi, and Sadigh]{belkhale2024rt} S.~Belkhale, T.~Ding, T.~Xiao, P.~Sermanet, Q.~Vuong, J.~Tompson, Y.~Chebotar, D.~Dwibedi, and D.~Sadigh. 
 Rt-h: Action hierarchies using language. 
 \emph{arXiv preprint arXiv:2403.01823}, 2024."
2407.02666,tang2023saytap,"[Tang et~al.(2023)Tang, Yu, Tan, Zen, Faust, and Harada]{tang2023saytap} Y.~Tang, W.~Yu, J.~Tan, H.~Zen, A.~Faust, and T.~Harada.",Saytap: Language to quadrupedal locomotion.,Saytap: Language to quadrupedal locomotion.,,"[Tang et~al.(2023)Tang, Yu, Tan, Zen, Faust, and Harada]{tang2023saytap} Y.~Tang, W.~Yu, J.~Tan, H.~Zen, A.~Faust, and T.~Harada. 
 Saytap: Language to quadrupedal locomotion. 
 \emph{arXiv preprint arXiv:2306.07580}, 2023."
2407.02666,ouyang2024long,"[Ouyang et~al.(2024)Ouyang, Li, Li, Li, Yu, Sreenath, and Wu]{ouyang2024long} Y.~Ouyang, J.~Li, Y.~Li, Z.~Li, C.~Yu, K.~Sreenath, and Y.~Wu.",Long-horizon locomotion and manipulation on a quadrupedal robot with large language models.,Long-horizon locomotion and manipulation on a quadrupedal robot with large language models.,,"[Ouyang et~al.(2024)Ouyang, Li, Li, Li, Yu, Sreenath, and Wu]{ouyang2024long} Y.~Ouyang, J.~Li, Y.~Li, Z.~Li, C.~Yu, K.~Sreenath, and Y.~Wu. 
 Long-horizon locomotion and manipulation on a quadrupedal robot with large language models. 
 \emph{arXiv preprint arXiv:2404.05291}, 2024."
2407.02666,zeng2022socratic,"[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker, Tombari, Purohit, Ryoo, Sindhwani, et~al.]{zeng2022socratic} A.~Zeng, M.~Attarian, B.~Ichter, K.~Choromanski, A.~Wong, S.~Welker, F.~Tombari, A.~Purohit, M.~Ryoo, V.~Sindhwani, et~al.",Socratic models: Composing zero-shot multimodal reasoning with language.,Socratic models: Composing zero-shot multimodal reasoning with language.,,"[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker, Tombari, Purohit, Ryoo, Sindhwani, et~al.]{zeng2022socratic} A.~Zeng, M.~Attarian, B.~Ichter, K.~Choromanski, A.~Wong, S.~Welker, F.~Tombari, A.~Purohit, M.~Ryoo, V.~Sindhwani, et~al. 
 Socratic models: Composing zero-shot multimodal reasoning with language. 
 \emph{arXiv preprint arXiv:2204.00598}, 2022."
2407.02666,min2022rethinking,"[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking} S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer.",Rethinking the role of demonstrations: What makes in-context learning work?,Rethinking the role of demonstrations: What makes in-context learning work?,,"[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking} S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer. 
 Rethinking the role of demonstrations: What makes in-context learning work? 
 \emph{arXiv preprint arXiv:2202.12837}, 2022."
2407.02666,dong2022survey,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui.",A survey on in-context learning.,A survey on in-context learning.,,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey} Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui. 
 A survey on in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}, 2022."
2407.02694,cot-chatgpt,"[Chen et~al.(2023)Chen, Chen, Huang, and Zhou]{cot-chatgpt} Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou.",{When Do You Need Chain-of-Thought Prompting for ChatGPT?},{When Do You Need Chain-of-Thought Prompting for ChatGPT?},,"[Chen et~al.(2023)Chen, Chen, Huang, and Zhou]{cot-chatgpt} Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 
 {When Do You Need Chain-of-Thought Prompting for ChatGPT?} 
 \emph{arXiv preprint arXiv:2304.03262}, 2023."
2407.02694,palm,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.",{PaLM: Scaling Language Modeling with Pathways}.,{PaLM: Scaling Language Modeling with Pathways}.,,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 
 {PaLM: Scaling Language Modeling with Pathways}. 
 \emph{arXiv preprint arXiv:2204.02311}, 2022."
2407.02694,group-lasso-nn,[Feng \& Simon(2017)Feng and Simon]{group-lasso-nn} Jean Feng and Noah Simon.,{Sparse-input Neural Networks for High-dimensional Nonparametric Regression and Classification}.,{Sparse-input Neural Networks for High-dimensional Nonparametric Regression and Classification}.,,"[Feng \& Simon(2017)Feng and Simon]{group-lasso-nn} Jean Feng and Noah Simon. 
 {Sparse-input Neural Networks for High-dimensional Nonparametric Regression and Classification}. 
 \emph{arXiv preprint arXiv:1711.07592}, 2017."
2407.02694,llm-bias-fairness,"[Gallegos et~al.(2024)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{llm-bias-fairness} Isabel~O. Gallegos, Ryan~A. Rossi, Joe Barrow, Md~Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen~K. Ahmed.",{Bias and Fairness in Large Language Models: A Survey}.,{Bias and Fairness in Large Language Models: A Survey}.,,"[Gallegos et~al.(2024)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{llm-bias-fairness} Isabel~O. Gallegos, Ryan~A. Rossi, Joe Barrow, Md~Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen~K. Ahmed. 
 {Bias and Fairness in Large Language Models: A Survey}. 
 \emph{arXiv:2309.00770}, 2024."
2407.02694,llm-clinical-lievin,"[Liévin et~al.(2022)Liévin, Hother, and Winther]{llm-clinical-lievin} Valentin Liévin, Christoffer~Egeberg Hother, and Ole Winther.",{Can Large Language Models Reason About Medical Questions?},{Can Large Language Models Reason About Medical Questions?},,"[Liévin et~al.(2022)Liévin, Hother, and Winther]{llm-clinical-lievin} Valentin Liévin, Christoffer~Egeberg Hother, and Ole Winther. 
 {Can Large Language Models Reason About Medical Questions?} 
 \emph{arXiv preprint arXiv:2207.08143}, 2022."
2407.02694,llm-weak,"[Manikandan et~al.(2023)Manikandan, Jiang, and Kolter]{llm-weak} Hariharan Manikandan, Yiding Jiang, and J~Zico Kolter.",{Language Models Are Weak Learners}.,{Language Models Are Weak Learners}.,,"[Manikandan et~al.(2023)Manikandan, Jiang, and Kolter]{llm-weak} Hariharan Manikandan, Yiding Jiang, and J~Zico Kolter. 
 {Language Models Are Weak Learners}. 
 \emph{arXiv preprint arXiv:2306.14101}, 2023."
2407.02694,gpt4,[OpenAI(2023)]{gpt4} OpenAI.,{GPT-4 Technical Report}.,{GPT-4 Technical Report}.,,"[OpenAI(2023)]{gpt4} OpenAI. 
 {GPT-4 Technical Report}. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.02694,rlhf-2,"[Stiennon et~al.(2022)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{rlhf-2} Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano.",{Learning to Summarize from Human Feedback}.,{Learning to Summarize from Human Feedback}.,,"[Stiennon et~al.(2022)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{rlhf-2} Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 
 {Learning to Summarize from Human Feedback}. 
 \emph{arXiv:2009.01325}, 2022."
2407.02694,llama2,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom.",{Llama 2: Open Foundation and Fine-Tuned Chat Models}.,{Llama 2: Open Foundation and Fine-Tuned Chat Models}.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 
 {Llama 2: Open Foundation and Fine-Tuned Chat Models}. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2407.02819,dettmers2023qlora,"[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer}]{dettmers2023qlora} Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.",Qlora: Efficient finetuning of quantized llms.,Qlora: Efficient finetuning of quantized llms.,,"[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer}]{dettmers2023qlora} Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. 
 Qlora: Efficient finetuning of quantized llms. 
 \emph{arXiv preprint arXiv:2305.14314}."
2407.02819,gao2020pile,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima et~al.}]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020.",The {P}ile: An 800{GB} dataset of diverse text for language modeling.,The {P}ile: An 800{GB} dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima et~al.}]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020. 
 The {P}ile: An 800{GB} dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2407.02819,kaddour2023minipile,[{Kaddour(2023)}]{kaddour2023minipile} Jean Kaddour. 2023.,The minipile challenge for data-efficient language models.,The minipile challenge for data-efficient language models.,,"[{Kaddour(2023)}]{kaddour2023minipile} Jean Kaddour. 2023. 
 The minipile challenge for data-efficient language models. 
 \emph{arXiv preprint arXiv:2304.08442}."
2407.03876,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.03876,yao2023dschat,"[{Yao et~al.(2023)Yao, Aminabadi, Ruwase, Rajbhandari, Wu, Awan, Rasley, Zhang, Li, Holmes, Zhou, Wyatt, Smith, Kurilenko, Qin, Tanaka, Che, Song, and He}]{yao2023dschat} Zhewei Yao, Reza~Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar~Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen~Leon Song, and Yuxiong He. 2023.","{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}.","{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}.",,"[{Yao et~al.(2023)Yao, Aminabadi, Ruwase, Rajbhandari, Wu, Awan, Rasley, Zhang, Li, Holmes, Zhou, Wyatt, Smith, Kurilenko, Qin, Tanaka, Che, Song, and He}]{yao2023dschat} Zhewei Yao, Reza~Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar~Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen~Leon Song, and Yuxiong He. 2023. 
 {DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}. 
 \emph{arXiv preprint arXiv:2308.01320}."
2407.03876,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.03876,yao2023dschat,"[{Yao et~al.(2023)Yao, Aminabadi, Ruwase, Rajbhandari, Wu, Awan, Rasley, Zhang, Li, Holmes, Zhou, Wyatt, Smith, Kurilenko, Qin, Tanaka, Che, Song, and He}]{yao2023dschat} Zhewei Yao, Reza~Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar~Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen~Leon Song, and Yuxiong He. 2023.","{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}.","{DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}.",,"[{Yao et~al.(2023)Yao, Aminabadi, Ruwase, Rajbhandari, Wu, Awan, Rasley, Zhang, Li, Holmes, Zhou, Wyatt, Smith, Kurilenko, Qin, Tanaka, Che, Song, and He}]{yao2023dschat} Zhewei Yao, Reza~Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar~Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen~Leon Song, and Yuxiong He. 2023. 
 {DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales}. 
 \emph{arXiv preprint arXiv:2308.01320}."
2407.04449,bardes2021vicreg,"[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg} Adrien Bardes, Jean Ponce, and Yann LeCun.",Vicreg: Variance-invariance-covariance regularization for self-supervised learning.,Vicreg: Variance-invariance-covariance regularization for self-supervised learning.,,"[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg} Adrien Bardes, Jean Ponce, and Yann LeCun. 
 Vicreg: Variance-invariance-covariance regularization for self-supervised learning. 
 \emph{arXiv preprint arXiv:2105.04906}, 2021."
2407.04449,dosovitskiy2020image,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.",An image is worth 16x16 words: Transformers for image recognition at scale.,An image is worth 16x16 words: Transformers for image recognition at scale.,,"[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al. 
 An image is worth 16x16 words: Transformers for image recognition at scale. 
 \emph{arXiv preprint arXiv:2010.11929}, 2020."
2407.04449,gidaris2018unsupervised,"[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{gidaris2018unsupervised} Spyros Gidaris, Praveer Singh, and Nikos Komodakis.",Unsupervised representation learning by predicting image rotations.,Unsupervised representation learning by predicting image rotations.,,"[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{gidaris2018unsupervised} Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 
 Unsupervised representation learning by predicting image rotations. 
 \emph{arXiv preprint arXiv:1803.07728}, 2018."
2407.04449,grill2020bootstrap,"[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap} Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires, Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.",Bootstrap your own latent: A new approach to self-supervised learning.,Bootstrap your own latent: A new approach to self-supervised learning.,,"[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap} Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires, Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al. 
 Bootstrap your own latent: A new approach to self-supervised learning. 
 \emph{arXiv preprint arXiv:2006.07733}, 2020."
2407.04449,hayat2022medfuse,"[Hayat et~al.(2022)Hayat, Geras, and Shamout]{hayat2022medfuse} Nasir Hayat, Krzysztof~J Geras, and Farah~E Shamout.",Medfuse: Multi-modal fusion with clinical time-series data and chest x-ray images.,Medfuse: Multi-modal fusion with clinical time-series data and chest x-ray images.,,"[Hayat et~al.(2022)Hayat, Geras, and Shamout]{hayat2022medfuse} Nasir Hayat, Krzysztof~J Geras, and Farah~E Shamout. 
 Medfuse: Multi-modal fusion with clinical time-series data and chest x-ray images. 
 \emph{arXiv preprint arXiv:2207.07027}, 2022."
2407.04449,johnson2019mimic,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.",,"[Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng. 
 Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. 
 \emph{arXiv preprint arXiv:1901.07042}, 2019."
2407.04449,kingma2014adam,[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2407.04449,lan2019albert,"[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut]{lan2019albert} Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.",Albert: A lite bert for self-supervised learning of language representations.,Albert: A lite bert for self-supervised learning of language representations.,,"[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut]{lan2019albert} Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 
 Albert: A lite bert for self-supervised learning of language representations. 
 \emph{arXiv preprint arXiv:1909.11942}, 2019."
2407.04449,loshchilov2017decoupled,[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter.,Decoupled weight decay regularization.,Decoupled weight decay regularization.,,"[Loshchilov and Hutter(2017)]{loshchilov2017decoupled} Ilya Loshchilov and Frank Hutter. 
 Decoupled weight decay regularization. 
 \emph{arXiv preprint arXiv:1711.05101}, 2017."
2407.04449,micikevicius2017mixed,"[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{micikevicius2017mixed} Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al.",Mixed precision training.,Mixed precision training.,,"[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{micikevicius2017mixed} Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al. 
 Mixed precision training. 
 \emph{arXiv preprint arXiv:1710.03740}, 2017."
2407.04449,sriram2021covid,"[Sriram et~al.(2021)Sriram, Muckley, Sinha, Shamout, Pineau, Geras, Azour, Aphinyanaphongs, Yakubova, and Moore]{sriram2021covid} Anuroop Sriram, Matthew Muckley, Koustuv Sinha, Farah Shamout, Joelle Pineau, Krzysztof~J Geras, Lea Azour, Yindalon Aphinyanaphongs, Nafissa Yakubova, and William Moore.",Covid-19 prognosis via self-supervised representation learning and multi-image prediction.,Covid-19 prognosis via self-supervised representation learning and multi-image prediction.,,"[Sriram et~al.(2021)Sriram, Muckley, Sinha, Shamout, Pineau, Geras, Azour, Aphinyanaphongs, Yakubova, and Moore]{sriram2021covid} Anuroop Sriram, Matthew Muckley, Koustuv Sinha, Farah Shamout, Joelle Pineau, Krzysztof~J Geras, Lea Azour, Yindalon Aphinyanaphongs, Nafissa Yakubova, and William Moore. 
 Covid-19 prognosis via self-supervised representation learning and multi-image prediction. 
 \emph{arXiv preprint arXiv:2101.04909}, 2021."
2407.04449,vu2021medaug,"[Vu et~al.(2021)Vu, Wang, Balachandar, Liu, Ng, and Rajpurkar]{vu2021medaug} Yen Nhi~Truong Vu, Richard Wang, Niranjan Balachandar, Can Liu, Andrew~Y Ng, and Pranav Rajpurkar.",Medaug: Contrastive learning leveraging patient metadata improves representations for chest x-ray interpretation.,Medaug: Contrastive learning leveraging patient metadata improves representations for chest x-ray interpretation.,,"[Vu et~al.(2021)Vu, Wang, Balachandar, Liu, Ng, and Rajpurkar]{vu2021medaug} Yen Nhi~Truong Vu, Richard Wang, Niranjan Balachandar, Can Liu, Andrew~Y Ng, and Pranav Rajpurkar. 
 Medaug: Contrastive learning leveraging patient metadata improves representations for chest x-ray interpretation. 
 \emph{arXiv preprint arXiv:2102.10663}, 2021."
2407.04449,xie2020pgl,"[Xie et~al.(2020)Xie, Zhang, Liao, Xia, and Shen]{xie2020pgl} Yutong Xie, Jianpeng Zhang, Zehui Liao, Yong Xia, and Chunhua Shen.",Pgl: prior-guided local self-supervised learning for 3d medical image segmentation.,Pgl: prior-guided local self-supervised learning for 3d medical image segmentation.,,"[Xie et~al.(2020)Xie, Zhang, Liao, Xia, and Shen]{xie2020pgl} Yutong Xie, Jianpeng Zhang, Zehui Liao, Yong Xia, and Chunhua Shen. 
 Pgl: prior-guided local self-supervised learning for 3d medical image segmentation. 
 \emph{arXiv preprint arXiv:2011.12640}, 2020."
2407.04449,zhang2022self,"[Zhang et~al.(2022{\natexlab{a}})Zhang, Zhao, Tsiligkaridis, and Zitnik]{zhang2022self} Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik.",Self-supervised contrastive pre-training for time series via time-frequency consistency.,Self-supervised contrastive pre-training for time series via time-frequency consistency.,,"[Zhang et~al.(2022{\natexlab{a}})Zhang, Zhao, Tsiligkaridis, and Zitnik]{zhang2022self} Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. 
 Self-supervised contrastive pre-training for time series via time-frequency consistency. 
 \emph{arXiv preprint arXiv:2206.08496}, 2022{\natexlab{a}}."
2407.05131,alsentzer2019publicly,"[{Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott}]{alsentzer2019publicly} Emily Alsentzer, John~R Murphy, Willie Boag, Wei-Hung Weng, Di~Jin, Tristan Naumann, and Matthew McDermott. 2019.",Publicly available clinical bert embeddings.,Publicly available clinical bert embeddings.,,"[{Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott}]{alsentzer2019publicly} Emily Alsentzer, John~R Murphy, Willie Boag, Wei-Hung Weng, Di~Jin, Tristan Naumann, and Matthew McDermott. 2019. 
 Publicly available clinical bert embeddings. 
 \emph{arXiv preprint arXiv:1904.03323}."
2407.05131,angelopoulos2021learn,"[{Angelopoulos et~al.(2021)Angelopoulos, Bates, Cand{\`e}s, Jordan, and Lei}]{angelopoulos2021learn} Anastasios~N. Angelopoulos, Stephen Bates, Emmanuel~J. Cand{\`e}s, Michael~I. Jordan, and Lihua Lei. 2021.",Learn then {{Test}}: {{Calibrating Predictive Algorithms}} to {{Achieve Risk Control}}.,Learn then {{Test}}: {{Calibrating Predictive Algorithms}} to {{Achieve Risk Control}}.,,"[{Angelopoulos et~al.(2021)Angelopoulos, Bates, Cand{\`e}s, Jordan, and Lei}]{angelopoulos2021learn} Anastasios~N. Angelopoulos, Stephen Bates, Emmanuel~J. Cand{\`e}s, Michael~I. Jordan, and Lihua Lei. 2021. 
 Learn then {{Test}}: {{Calibrating Predictive Algorithms}} to {{Achieve Risk Control}}. 
 \emph{arXiv:2110.01052}."
2407.05131,chuang2023dola,"[{Chuang et~al.(2023)Chuang, Xie, Luo, Kim, Glass, and He}]{chuang2023dola} Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023.",Dola: Decoding by contrasting layers improves factuality in large language models.,Dola: Decoding by contrasting layers improves factuality in large language models.,,"[{Chuang et~al.(2023)Chuang, Xie, Luo, Kim, Glass, and He}]{chuang2023dola} Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. 
 Dola: Decoding by contrasting layers improves factuality in large language models. 
 \emph{arXiv preprint arXiv:2309.03883}."
2407.05131,gao2023retrieval,"[{Gao et~al.(2023)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, and Wang}]{gao2023retrieval} Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, and Haofen Wang. 2023.",Retrieval-augmented generation for large language models: A survey.,Retrieval-augmented generation for large language models: A survey.,,"[{Gao et~al.(2023)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, and Wang}]{gao2023retrieval} Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, and Haofen Wang. 2023. 
 Retrieval-augmented generation for large language models: A survey. 
 \emph{arXiv preprint arXiv:2312.10997}."
2407.05131,he2024meddr,"[{He et~al.(2024)He, Nie, Chen, Cai, Wang, Yang, and Chen}]{he2024meddr} Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. 2024.",Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning.,Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning.,,"[{He et~al.(2024)He, Nie, Chen, Cai, Wang, Yang, and Chen}]{he2024meddr} Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. 2024. 
 Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. 
 \emph{arXiv preprint arXiv:2404.15127}."
2407.05131,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2407.05131,huang2023opera,"[{Huang et~al.(2023)Huang, Dong, Zhang, Wang, He, Wang, Lin, Zhang, and Yu}]{huang2023opera} Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023.",Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.,Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.,,"[{Huang et~al.(2023)Huang, Dong, Zhang, Wang, He, Wang, Lin, Zhang, and Yu}]{huang2023opera} Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023. 
 Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. 
 \emph{arXiv preprint arXiv:2311.17911}."
2407.05131,johnson2019mimic,"[{Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng}]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng. 2019.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.","Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.",,"[{Johnson et~al.(2019)Johnson, Pollard, Greenbaum, Lungren, Deng, Peng, Lu, Mark, Berkowitz, and Horng}]{johnson2019mimic} Alistair~EW Johnson, Tom~J Pollard, Nathaniel~R Greenbaum, Matthew~P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger~G Mark, Seth~J Berkowitz, and Steven Horng. 2019. 
 Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. 
 \emph{arXiv preprint arXiv:1901.07042}."
2407.05131,kumar2024improving,[{Kumar and Marttinen(2024)}]{kumar2024improving} Yogesh Kumar and Pekka Marttinen. 2024.,Improving medical multi-modal contrastive learning with expert annotations.,Improving medical multi-modal contrastive learning with expert annotations.,,"[{Kumar and Marttinen(2024)}]{kumar2024improving} Yogesh Kumar and Pekka Marttinen. 2024. 
 Improving medical multi-modal contrastive learning with expert annotations. 
 \emph{arXiv preprint arXiv:2403.10153}."
2407.05131,leng2023mitigating,"[{Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing}]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.",Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,,"[{Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing}]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. 
 Mitigating object hallucinations in large vision-language models through visual contrastive decoding. 
 \emph{arXiv preprint arXiv:2311.16922}."
2407.05131,li2024tp,"[{Li et~al.(2024)Li, Xiong, Xia, Ju, and Ge}]{li2024tp} Wenxue Li, Xinyu Xiong, Peng Xia, Lie Ju, and Zongyuan Ge. 2024.",Tp-drseg: Improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted sam.,Tp-drseg: Improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted sam.,,"[{Li et~al.(2024)Li, Xiong, Xia, Ju, and Ge}]{li2024tp} Wenxue Li, Xinyu Xiong, Peng Xia, Lie Ju, and Zongyuan Ge. 2024. 
 Tp-drseg: Improving diabetic retinopathy lesion segmentation with explicit text-prompts assisted sam. 
 \emph{arXiv preprint arXiv:2406.15764}."
2407.05131,liu2023improved,"[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee}]{liu2023improved} Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee. 2023{\natexlab{a}}.",Improved baselines with visual instruction tuning.,Improved baselines with visual instruction tuning.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee}]{liu2023improved} Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee. 2023{\natexlab{a}}. 
 Improved baselines with visual instruction tuning. 
 \emph{arXiv preprint arXiv:2310.03744}."
2407.05131,liu2023visual,"[{Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee}]{liu2023visual} Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023{\natexlab{b}}.",Visual instruction tuning.,Visual instruction tuning.,,"[{Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee}]{liu2023visual} Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023{\natexlab{b}}. 
 Visual instruction tuning. 
 \emph{arXiv preprint arXiv:2304.08485}."
2407.05131,luo2024fairclip,"[{Luo et~al.(2024)Luo, Shi, Khan, Afzal, Huang, Yuan, Tian, Song, Kouhana, Elze et~al.}]{luo2024fairclip} Yan Luo, Min Shi, Muhammad~Osama Khan, Muhammad~Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu~Tian, Luo Song, Ava Kouhana, Tobias Elze, et~al. 2024.",Fairclip: Harnessing fairness in vision-language learning.,Fairclip: Harnessing fairness in vision-language learning.,,"[{Luo et~al.(2024)Luo, Shi, Khan, Afzal, Huang, Yuan, Tian, Song, Kouhana, Elze et~al.}]{luo2024fairclip} Yan Luo, Min Shi, Muhammad~Osama Khan, Muhammad~Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu~Tian, Luo Song, Ava Kouhana, Tobias Elze, et~al. 2024. 
 Fairclip: Harnessing fairness in vision-language learning. 
 \emph{arXiv preprint arXiv:2403.19949}."
2407.05131,royer2024multimedeval,"[{Royer et~al.(2024)Royer, Menze, and Sekuboyina}]{royer2024multimedeval} Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. 2024.",Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models.,Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models.,,"[{Royer et~al.(2024)Royer, Menze, and Sekuboyina}]{royer2024multimedeval} Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. 2024. 
 Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. 
 \emph{arXiv preprint arXiv:2402.09262}."
2407.05131,wu2023towards,"[{Wu et~al.(2023)Wu, Zhang, Zhang, Wang, and Xie}]{wu2023towards} Chaoyi Wu, Xiaoman Zhang, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 2023.",Towards generalist foundation model for radiology.,Towards generalist foundation model for radiology.,,"[{Wu et~al.(2023)Wu, Zhang, Zhang, Wang, and Xie}]{wu2023towards} Chaoyi Wu, Xiaoman Zhang, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 2023. 
 Towards generalist foundation model for radiology. 
 \emph{arXiv preprint arXiv:2308.02463}."
2407.05131,xia2024cares,"[{Xia et~al.(2024{\natexlab{a}})Xia, Chen, Tian, Gong, Hou, Xu, Wu, Fan, Zhou, Zhu et~al.}]{xia2024cares} Peng Xia, Ze~Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et~al. 2024{\natexlab{a}}.",Cares: A comprehensive benchmark of trustworthiness in medical vision language models.,Cares: A comprehensive benchmark of trustworthiness in medical vision language models.,,"[{Xia et~al.(2024{\natexlab{a}})Xia, Chen, Tian, Gong, Hou, Xu, Wu, Fan, Zhou, Zhu et~al.}]{xia2024cares} Peng Xia, Ze~Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et~al. 2024{\natexlab{a}}. 
 Cares: A comprehensive benchmark of trustworthiness in medical vision language models. 
 \emph{arXiv preprint arXiv:2406.06007}."
2407.05131,xia2024generalizing,"[{Xia et~al.(2024{\natexlab{b}})Xia, Hu, Tang, Li, Zheng, Ju, Duan, Yao, and Ge}]{xia2024generalizing} Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. 2024{\natexlab{b}}.",Generalizing to unseen domains in diabetic retinopathy with disentangled representations.,Generalizing to unseen domains in diabetic retinopathy with disentangled representations.,,"[{Xia et~al.(2024{\natexlab{b}})Xia, Hu, Tang, Li, Zheng, Ju, Duan, Yao, and Ge}]{xia2024generalizing} Peng Xia, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, and Zongyuan Ge. 2024{\natexlab{b}}. 
 Generalizing to unseen domains in diabetic retinopathy with disentangled representations. 
 In \emph{arXiv preprint arXiv:2406.06384}."
2407.05131,zhang2023pmc,"[{Zhang et~al.(2023)Zhang, Wu, Zhao, Lin, Zhang, Wang, and Xie}]{zhang2023pmc} Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 2023.",Pmc-vqa: Visual instruction tuning for medical visual question answering.,Pmc-vqa: Visual instruction tuning for medical visual question answering.,,"[{Zhang et~al.(2023)Zhang, Wu, Zhao, Lin, Zhang, Wang, and Xie}]{zhang2023pmc} Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang, and Weidi Xie. 2023. 
 Pmc-vqa: Visual instruction tuning for medical visual question answering. 
 \emph{arXiv preprint arXiv:2305.10415}."
2407.05131,zhou2024aligning,"[{Zhou et~al.(2024{\natexlab{a}})Zhou, Cui, Rafailov, Finn, and Yao}]{zhou2024aligning} Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. 2024{\natexlab{a}}.",Aligning modalities in vision large language models via preference fine-tuning.,Aligning modalities in vision large language models via preference fine-tuning.,,"[{Zhou et~al.(2024{\natexlab{a}})Zhou, Cui, Rafailov, Finn, and Yao}]{zhou2024aligning} Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. 2024{\natexlab{a}}. 
 Aligning modalities in vision large language models via preference fine-tuning. 
 \emph{arXiv preprint arXiv:2402.11411}."
2407.05131,zhou2024calibrated,"[{Zhou et~al.(2024{\natexlab{b}})Zhou, Fan, Cheng, Yang, Chen, Cui, Wang, Li, Zhang, and Yao}]{zhou2024calibrated} Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 2024{\natexlab{b}}.",Calibrated self-rewarding vision language models.,Calibrated self-rewarding vision language models.,,"[{Zhou et~al.(2024{\natexlab{b}})Zhou, Fan, Cheng, Yang, Chen, Cui, Wang, Li, Zhang, and Yao}]{zhou2024calibrated} Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 2024{\natexlab{b}}. 
 Calibrated self-rewarding vision language models. 
 \emph{arXiv preprint arXiv:2405.14622}."
2407.05131,zhu2023minigpt,"[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",Minigpt-4: Enhancing vision-language understanding with advanced large language models.,Minigpt-4: Enhancing vision-language understanding with advanced large language models.,,"[{Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny}]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. 
 Minigpt-4: Enhancing vision-language understanding with advanced large language models. 
 \emph{arXiv preprint arXiv:2304.10592}."
2407.05213,alsentzer2019publicly,"[Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott]{alsentzer2019publicly} Emily Alsentzer, John~R Murphy, Willie Boag, Wei-Hung Weng, Di~Jin, Tristan Naumann, and Matthew McDermott.",Publicly available clinical bert embeddings.,Publicly available clinical bert embeddings.,,"[Alsentzer et~al.(2019)Alsentzer, Murphy, Boag, Weng, Jin, Naumann, and McDermott]{alsentzer2019publicly} Emily Alsentzer, John~R Murphy, Willie Boag, Wei-Hung Weng, Di~Jin, Tristan Naumann, and Matthew McDermott. 
 Publicly available clinical bert embeddings. 
 \emph{arXiv preprint arXiv:1904.03323}, 2019."
2407.05213,gu2017badnets,"[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets} Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.",Badnets: Identifying vulnerabilities in the machine learning model supply chain.,Badnets: Identifying vulnerabilities in the machine learning model supply chain.,,"[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets} Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 
 Badnets: Identifying vulnerabilities in the machine learning model supply chain. 
 \emph{arXiv preprint arXiv:1708.06733}, 2017."
2407.05213,guan2023badsam,"[Guan et~al.(2023)Guan, Hu, Zhou, Zhang, Li, and Liu]{guan2023badsam} Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, and Ninghao Liu.",Badsam: Exploring security vulnerabilities of sam via backdoor attacks.,Badsam: Exploring security vulnerabilities of sam via backdoor attacks.,,"[Guan et~al.(2023)Guan, Hu, Zhou, Zhang, Li, and Liu]{guan2023badsam} Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, and Ninghao Liu. 
 Badsam: Exploring security vulnerabilities of sam via backdoor attacks. 
 \emph{arXiv preprint arXiv:2305.03289}, 2023."
2407.05213,gururangan2020don,"[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith]{gururangan2020don} Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A Smith.",Don't stop pretraining: adapt language models to domains and tasks.,Don't stop pretraining: adapt language models to domains and tasks.,,"[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith]{gururangan2020don} Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A Smith. 
 Don't stop pretraining: adapt language models to domains and tasks. 
 \emph{arXiv preprint arXiv:2004.10964}, 2020."
2407.05213,jin2023prometheus,"[Jin et~al.(2023)Jin, Katsis, Sang, Sun, Bertino, Kompella, and Kundu]{jin2023prometheus} Xin Jin, Charalampos Katsis, Fan Sang, Jiahao Sun, Elisa Bertino, Ramana~Rao Kompella, and Ashish Kundu.",Prometheus: Infrastructure security posture analysis with ai-generated attack graphs.,Prometheus: Infrastructure security posture analysis with ai-generated attack graphs.,,"[Jin et~al.(2023)Jin, Katsis, Sang, Sun, Bertino, Kompella, and Kundu]{jin2023prometheus} Xin Jin, Charalampos Katsis, Fan Sang, Jiahao Sun, Elisa Bertino, Ramana~Rao Kompella, and Ashish Kundu. 
 Prometheus: Infrastructure security posture analysis with ai-generated attack graphs. 
 \emph{arXiv preprint arXiv:2312.13119}, 2023."
2407.05213,khadanga2019using,"[Khadanga et~al.(2019)Khadanga, Aggarwal, Joty, and Srivastava]{khadanga2019using} Swaraj Khadanga, Karan Aggarwal, Shafiq Joty, and Jaideep Srivastava.",Using clinical notes with time series data for icu management.,Using clinical notes with time series data for icu management.,,"[Khadanga et~al.(2019)Khadanga, Aggarwal, Joty, and Srivastava]{khadanga2019using} Swaraj Khadanga, Karan Aggarwal, Shafiq Joty, and Jaideep Srivastava. 
 Using clinical notes with time series data for icu management. 
 \emph{arXiv preprint arXiv:1909.09702}, 2019."
2407.05213,liu2024beyond,"[Liu et~al.(2024)Liu, Zhou, Zeng, Xiao, Cheng, Zhang, Lee, Zhang, and Chen]{liu2024beyond} Wanlong Liu, Li~Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, and Wenyu Chen.",Beyond single-event extraction: Towards efficient document-level multi-event argument extraction.,Beyond single-event extraction: Towards efficient document-level multi-event argument extraction.,,"[Liu et~al.(2024)Liu, Zhou, Zeng, Xiao, Cheng, Zhang, Lee, Zhang, and Chen]{liu2024beyond} Wanlong Liu, Li~Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, and Wenyu Chen. 
 Beyond single-event extraction: Towards efficient document-level multi-event argument extraction. 
 \emph{arXiv preprint arXiv:2405.01884}, 2024."
2407.05213,lo2019s2orc,"[Lo et~al.(2019)Lo, Wang, Neumann, Kinney, and Weld]{lo2019s2orc} Kyle Lo, Lucy~Lu Wang, Mark Neumann, Rodney Kinney, and Dan~S Weld.",S2orc: The semantic scholar open research corpus.,S2orc: The semantic scholar open research corpus.,,"[Lo et~al.(2019)Lo, Wang, Neumann, Kinney, and Weld]{lo2019s2orc} Kyle Lo, Lucy~Lu Wang, Mark Neumann, Rodney Kinney, and Dan~S Weld. 
 S2orc: The semantic scholar open research corpus. 
 \emph{arXiv preprint arXiv:1911.02782}, 2019."
2407.05213,lyu2022attention,"[Lyu et~al.(2022{\natexlab{c}})Lyu, Zheng, Ma, Ling, and Chen]{lyu2022attention} Weimin Lyu, Songzhu Zheng, Tengfei Ma, Haibin Ling, and Chao Chen.",Attention hijacking in trojan transformers.,Attention hijacking in trojan transformers.,,"[Lyu et~al.(2022{\natexlab{c}})Lyu, Zheng, Ma, Ling, and Chen]{lyu2022attention} Weimin Lyu, Songzhu Zheng, Tengfei Ma, Haibin Ling, and Chao Chen. 
 Attention hijacking in trojan transformers. 
 \emph{arXiv preprint arXiv:2208.04946}, 2022{\natexlab{c}}."
2407.05213,tian2023knowledge,"[Tian et~al.(2023)Tian, Pei, Zhang, Zhang, and Chawla]{tian2023knowledge} Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh~V Chawla.",Knowledge distillation on graphs: A survey.,Knowledge distillation on graphs: A survey.,,"[Tian et~al.(2023)Tian, Pei, Zhang, Zhang, and Chawla]{tian2023knowledge} Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh~V Chawla. 
 Knowledge distillation on graphs: A survey. 
 \emph{arXiv preprint arXiv:2302.00219}, 2023."
2407.05213,wolf2019huggingface,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al.",Huggingface's transformers: State-of-the-art natural language processing.,Huggingface's transformers: State-of-the-art natural language processing.,,"[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface} Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, et~al. 
 Huggingface's transformers: State-of-the-art natural language processing. 
 \emph{arXiv preprint arXiv:1910.03771}, 2019."
2407.05213,yang2021leverage,[Yang \& Wu(2021)Yang and Wu]{yang2021leverage} Bo~Yang and Lijun Wu.,How to leverage multimodal ehr data for better medical predictions?,How to leverage multimodal ehr data for better medical predictions?,,"[Yang \& Wu(2021)Yang and Wu]{yang2021leverage} Bo~Yang and Lijun Wu. 
 How to leverage multimodal ehr data for better medical predictions? 
 \emph{arXiv preprint arXiv:2110.15763}, 2021."
2407.05213,zhu2024model,"[Zhu et~al.(2024)Zhu, Sun, Li, Shen, Yan, Ding, Kuang, and Wu]{zhu2024model} Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke~Yan, Shouhong Ding, Kun Kuang, and Chao Wu.",Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.,Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.,,"[Zhu et~al.(2024)Zhu, Sun, Li, Shen, Yan, Ding, Kuang, and Wu]{zhu2024model} Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke~Yan, Shouhong Ding, Kun Kuang, and Chao Wu. 
 Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. 
 \emph{arXiv preprint arXiv:2402.12048}, 2024."
2407.05563,austin-etal-2021-program,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin-etal-2021-program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.",Program synthesis with large language models.,Program synthesis with large language models.,,"[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin-etal-2021-program} Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021. 
 Program synthesis with large language models. 
 \emph{arXiv preprint arXiv:2108.07732}."
2407.05563,bai-etal-2023-qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang et~al.}]{bai-etal-2023-qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang et~al.}]{bai-etal-2023-qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2407.05563,bai-etal-2022-training,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai-etal-2022-training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai-etal-2022-training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2407.05563,chen-etal-2021-evaluating,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen-etal-2021-evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen-etal-2021-evaluating} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}."
2407.05563,chen-etal-2016-training,"[{Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin}]{chen-etal-2016-training} Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.",Training deep nets with sublinear memory cost.,Training deep nets with sublinear memory cost.,,"[{Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin}]{chen-etal-2016-training} Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. 
 Training deep nets with sublinear memory cost. 
 \emph{arXiv preprint arXiv:1604.06174}."
2407.05563,chung-etal-2022-scaling,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung-etal-2022-scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung-etal-2022-scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2407.05563,clark-etal-2018-think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark-etal-2018-think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark-etal-2018-think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2407.05563,cobbe-etal-2021-training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe-etal-2021-training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe-etal-2021-training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.05563,costa-etal-2022-no,"[{Costa-juss{\`a} et~al.(2022)Costa-juss{\`a}, Cross, {\c{C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard et~al.}]{costa-etal-2022-no} Marta~R Costa-juss{\`a}, James Cross, Onur {\c{C}}elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et~al. 2022.",No language left behind: Scaling human-centered machine translation.,No language left behind: Scaling human-centered machine translation.,,"[{Costa-juss{\`a} et~al.(2022)Costa-juss{\`a}, Cross, {\c{C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard et~al.}]{costa-etal-2022-no} Marta~R Costa-juss{\`a}, James Cross, Onur {\c{C}}elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et~al. 2022. 
 No language left behind: Scaling human-centered machine translation. 
 \emph{arXiv preprint arXiv:2207.04672}."
2407.05563,jiang-etal-2023-mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang-etal-2023-mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang-etal-2023-mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.05563,li-etal-2023-cmmlu,"[{Li et~al.(2023{\natexlab{a}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin}]{li-etal-2023-cmmlu} Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023{\natexlab{a}}.",Cmmlu: Measuring massive multitask language understanding in chinese.,Cmmlu: Measuring massive multitask language understanding in chinese.,,"[{Li et~al.(2023{\natexlab{a}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin}]{li-etal-2023-cmmlu} Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023{\natexlab{a}}. 
 Cmmlu: Measuring massive multitask language understanding in chinese. 
 \emph{arXiv preprint arXiv:2306.09212}."
2407.05563,patil-etal-2023-gorilla,"[{Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez}]{patil-etal-2023-gorilla} Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez. 2023.",Gorilla: Large language model connected with massive apis.,Gorilla: Large language model connected with massive apis.,,"[{Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez}]{patil-etal-2023-gorilla} Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez. 2023. 
 Gorilla: Large language model connected with massive apis. 
 \emph{arXiv preprint arXiv:2305.15334}."
2407.05563,schulman-etal-2017-proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman-etal-2017-proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman-etal-2017-proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}."
2407.05563,shao-etal-2024-deepseekmath,"[{Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo}]{shao-etal-2024-deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK~Li, Y~Wu, and Daya Guo. 2024.",Deepseekmath: Pushing the limits of mathematical reasoning in open language models.,Deepseekmath: Pushing the limits of mathematical reasoning in open language models.,,"[{Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo}]{shao-etal-2024-deepseekmath} Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK~Li, Y~Wu, and Daya Guo. 2024. 
 Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 
 \emph{arXiv preprint arXiv:2402.03300}."
2407.05563,touvron-etal-2023-llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron-etal-2023-llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron-etal-2023-llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2407.05563,touvron-etal-2023-llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron-etal-2023-llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron-etal-2023-llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.05563,yang-etal-2023-baichuan,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan et~al.}]{yang-etal-2023-baichuan} Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, et~al. 2023.",Baichuan 2: Open large-scale language models.,Baichuan 2: Open large-scale language models.,,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan et~al.}]{yang-etal-2023-baichuan} Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, et~al. 2023. 
 Baichuan 2: Open large-scale language models. 
 \emph{arXiv preprint arXiv:2309.10305}."
2407.05563,young-etal-2024-yi,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young-etal-2024-yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young-etal-2024-yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}."
2407.05563,zeng-etal-2022-glm,"[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia et~al.}]{zeng-etal-2022-glm} Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al. 2022.",Glm-130b: An open bilingual pre-trained model.,Glm-130b: An open bilingual pre-trained model.,,"[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia et~al.}]{zeng-etal-2022-glm} Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al. 2022. 
 Glm-130b: An open bilingual pre-trained model. 
 \emph{arXiv preprint arXiv:2210.02414}."
2407.05563,zhang-etal-2022-opt,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang-etal-2022-opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang-etal-2022-opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2407.05563,zhao-etal-2023-survey,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao-etal-2023-survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023.",A survey of large language models.,A survey of large language models.,,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao-etal-2023-survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}."
2407.05563,zhou-etal-2023-instruction,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{zhou-etal-2023-instruction} Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023{\natexlab{b}}.",Instruction-following evaluation for large language models.,Instruction-following evaluation for large language models.,,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{zhou-etal-2023-instruction} Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023{\natexlab{b}}. 
 Instruction-following evaluation for large language models. 
 \emph{arXiv preprint arXiv:2311.07911}."
2407.0574,ali2023tokenizer,"[{Ali et~al.(2023)Ali, Fromm, Thellmann, Rutmann, L{\""u}bbering, Leveling, Klug, Ebert, Doll, Buschhoff et~al.}]{ali2023tokenizer} Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max L{\""u}bbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper~Schulze Buschhoff, et~al. 2023.",Tokenizer choice for llm training: Negligible or crucial?,Tokenizer choice for llm training: Negligible or crucial?,,"[{Ali et~al.(2023)Ali, Fromm, Thellmann, Rutmann, L{\""u}bbering, Leveling, Klug, Ebert, Doll, Buschhoff et~al.}]{ali2023tokenizer} Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max L{\""u}bbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper~Schulze Buschhoff, et~al. 2023. 
 Tokenizer choice for llm training: Negligible or crucial? 
 \emph{arXiv preprint arXiv:2310.08754}."
2407.0574,almazrouei2023falcon,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.",The falcon series of open language models.,The falcon series of open language models.,,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv:2311.16867}."
2407.0574,bandarkar2023belebele,"[{Bandarkar et~al.(2023)Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Goyal, Krishnan, Zettlemoyer, and Khabsa}]{bandarkar2023belebele} Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya~Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023.",The belebele benchmark: a parallel reading comprehension dataset in 122 language variants.,The belebele benchmark: a parallel reading comprehension dataset in 122 language variants.,,"[{Bandarkar et~al.(2023)Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Goyal, Krishnan, Zettlemoyer, and Khabsa}]{bandarkar2023belebele} Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya~Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. 
 The belebele benchmark: a parallel reading comprehension dataset in 122 language variants. 
 \emph{arXiv preprint arXiv:2308.16884}."
2407.0574,pile2020,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}]{pile2020} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.",The {P}ile: An 800gb dataset of diverse text for language modeling.,The {P}ile: An 800gb dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}]{pile2020} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. 
 The {P}ile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2407.0574,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.0574,jin2023kobbq,"[{Jin et~al.(2023)Jin, Kim, Lee, Yoo, Oh, and Lee}]{jin2023kobbq} Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, and Hwaran Lee. 2023.",Kobbq: Korean bias benchmark for question answering.,Kobbq: Korean bias benchmark for question answering.,,"[{Jin et~al.(2023)Jin, Kim, Lee, Yoo, Oh, and Lee}]{jin2023kobbq} Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, and Hwaran Lee. 2023. 
 Kobbq: Korean bias benchmark for question answering. 
 \emph{arXiv preprint arXiv:2307.16778}."
2407.0574,kudo2018sentencepiece,[{Kudo and Richardson(2018)}]{kudo2018sentencepiece} Taku Kudo and John Richardson. 2018.,Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.,Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.,,"[{Kudo and Richardson(2018)}]{kudo2018sentencepiece} Taku Kudo and John Richardson. 2018. 
 Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. 
 \emph{arXiv preprint arXiv:1808.06226}."
2407.0574,lai2017race,"[{Lai et~al.(2017{\natexlab{b}})Lai, Xie, Liu, Yang, and Hovy}]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017{\natexlab{b}}.",Race: Large-scale reading comprehension dataset from examinations.,Race: Large-scale reading comprehension dataset from examinations.,,"[{Lai et~al.(2017{\natexlab{b}})Lai, Xie, Liu, Yang, and Hovy}]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017{\natexlab{b}}. 
 Race: Large-scale reading comprehension dataset from examinations. 
 \emph{arXiv preprint arXiv:1704.04683}."
2407.0574,penedo2023refinedweb,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.",,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{penedo2023refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. 
 The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 
 \emph{arXiv preprint arXiv:2306.01116}."
2407.0574,shaikh2022second,"[{Shaikh et~al.(2022)Shaikh, Zhang, Held, Bernstein, and Yang}]{shaikh2022second} Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. 2022.","On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning.","On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning.",,"[{Shaikh et~al.(2022)Shaikh, Zhang, Held, Bernstein, and Yang}]{shaikh2022second} Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. 2022. 
 On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. 
 \emph{arXiv preprint arXiv:2212.08061}."
2407.0574,si2022prompting,"[{Si et~al.(2022)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and Wang}]{si2022prompting} Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022.",Prompting gpt-3 to be reliable.,Prompting gpt-3 to be reliable.,,"[{Si et~al.(2022)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and Wang}]{si2022prompting} Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022. 
 Prompting gpt-3 to be reliable. 
 \emph{arXiv preprint arXiv:2210.09150}."
2407.0574,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.06172,chiang2024chatbot,"[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang, Zhu, Jordan, Gonzalez, et~al.]{chiang2024chatbot} W.-L. Chiang, L.~Zheng, Y.~Sheng, A.~N. Angelopoulos, T.~Li, D.~Li, H.~Zhang, B.~Zhu, M.~Jordan, J.~E. Gonzalez, et~al.",Chatbot arena: An open platform for evaluating llms by human preference.,Chatbot arena: An open platform for evaluating llms by human preference.,,"[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang, Zhu, Jordan, Gonzalez, et~al.]{chiang2024chatbot} W.-L. Chiang, L.~Zheng, Y.~Sheng, A.~N. Angelopoulos, T.~Li, D.~Li, H.~Zhang, B.~Zhu, M.~Jordan, J.~E. Gonzalez, et~al. 
 Chatbot arena: An open platform for evaluating llms by human preference. 
 \emph{arXiv preprint arXiv:2403.04132}, 2024."
2407.06172,clark2018think,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and O.~Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and O.~Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2407.06172,cobbe2021gsm8k,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k} K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k} K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2407.06172,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2407.06172,hendrycks2021measuring,"[Hendrycks et~al.(2021{\natexlab{c}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring} D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and J.~Steinhardt.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[Hendrycks et~al.(2021{\natexlab{c}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021measuring} D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and J.~Steinhardt. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}, 2021{\natexlab{c}}."
2407.06172,jin2019pubmedqa,"[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa} Q.~Jin, B.~Dhingra, Z.~Liu, W.~W. Cohen, and X.~Lu.",Pubmedqa: A dataset for biomedical research question answering.,Pubmedqa: A dataset for biomedical research question answering.,,"[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa} Q.~Jin, B.~Dhingra, Z.~Liu, W.~W. Cohen, and X.~Lu. 
 Pubmedqa: A dataset for biomedical research question answering. 
 \emph{arXiv preprint arXiv:1909.06146}, 2019."
2407.06172,2017arXivtriviaqa,"[{Joshi} et~al.(2017){Joshi}, {Choi}, {Weld}, and {Zettlemoyer}]{2017arXivtriviaqa} M.~{Joshi}, E.~{Choi}, D.~{Weld}, and L.~{Zettlemoyer}.",{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}.,{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}.,,"[{Joshi} et~al.(2017){Joshi}, {Choi}, {Weld}, and {Zettlemoyer}]{2017arXivtriviaqa} M.~{Joshi}, E.~{Choi}, D.~{Weld}, and L.~{Zettlemoyer}. 
 {triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}. 
 \emph{arXiv e-prints}, art. arXiv:1705.03551, 2017."
2407.06172,liang2024mapping,"[Liang et~al.(2024)Liang, Zhang, Wu, Lepp, Ji, Zhao, Cao, Liu, He, Huang, et~al.]{liang2024mapping} W.~Liang, Y.~Zhang, Z.~Wu, H.~Lepp, W.~Ji, X.~Zhao, H.~Cao, S.~Liu, S.~He, Z.~Huang, et~al.",Mapping the increasing use of llms in scientific papers.,Mapping the increasing use of llms in scientific papers.,,"[Liang et~al.(2024)Liang, Zhang, Wu, Lepp, Ji, Zhao, Cao, Liu, He, Huang, et~al.]{liang2024mapping} W.~Liang, Y.~Zhang, Z.~Wu, H.~Lepp, W.~Ji, X.~Zhao, H.~Cao, S.~Liu, S.~He, Z.~Huang, et~al. 
 Mapping the increasing use of llms in scientific papers. 
 \emph{arXiv preprint arXiv:2404.01268}, 2024."
2407.06172,cnndm,"[Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang, et~al.]{cnndm} R.~Nallapati, B.~Zhou, C.~Gulcehre, B.~Xiang, et~al.",Abstractive text summarization using sequence-to-sequence rnns and beyond.,Abstractive text summarization using sequence-to-sequence rnns and beyond.,,"[Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang, et~al.]{cnndm} R.~Nallapati, B.~Zhou, C.~Gulcehre, B.~Xiang, et~al. 
 Abstractive text summarization using sequence-to-sequence rnns and beyond. 
 \emph{arXiv preprint arXiv:1602.06023}, 2016."
2407.06172,xsum,"[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{xsum} S.~Narayan, S.~B. Cohen, and M.~Lapata.","Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.","Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.",,"[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{xsum} S.~Narayan, S.~B. Cohen, and M.~Lapata. 
 Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. 
 \emph{arXiv preprint arXiv:1808.08745}, 2018."
2407.06172,sellam2020bleurt,"[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt} T.~Sellam, D.~Das, and A.~P. Parikh.",Bleurt: Learning robust metrics for text generation.,Bleurt: Learning robust metrics for text generation.,,"[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt} T.~Sellam, D.~Das, and A.~P. Parikh. 
 Bleurt: Learning robust metrics for text generation. 
 \emph{arXiv preprint arXiv:2004.04696}, 2020."
2407.06172,shi2024best,"[Shi et~al.(2024)Shi, Yang, Yang, and Shen]{shi2024best} C.~Shi, K.~Yang, J.~Yang, and C.~Shen.",Best arm identification for prompt learning under a limited budget.,Best arm identification for prompt learning under a limited budget.,,"[Shi et~al.(2024)Shi, Yang, Yang, and Shen]{shi2024best} C.~Shi, K.~Yang, J.~Yang, and C.~Shen. 
 Best arm identification for prompt learning under a limited budget. 
 \emph{arXiv preprint arXiv:2402.09723}, 2024."
2407.06172,yang2023large,"[Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{yang2023large} C.~Yang, X.~Wang, Y.~Lu, H.~Liu, Q.~V. Le, D.~Zhou, and X.~Chen.",Large language models as optimizers.,Large language models as optimizers.,,"[Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen]{yang2023large} C.~Yang, X.~Wang, Y.~Lu, H.~Liu, Q.~V. Le, D.~Zhou, and X.~Chen. 
 Large language models as optimizers. 
 \emph{arXiv preprint arXiv:2309.03409}, 2023."
2407.06172,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2407.06172,zhang2019bertscore,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}, 2019."
2407.06438,flamingo,"[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{flamingo} Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et~al.",Flamingo: a visual language model for few-shot learning.,Flamingo: a visual language model for few-shot learning.,,"[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{flamingo} Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et~al. 
 Flamingo: a visual language model for few-shot learning. 
 \emph{arXiv preprint arXiv:2204.14198}, 2022."
2407.06438,awadalla2023openflamingo,"[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, et~al.]{awadalla2023openflamingo} Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.",Openflamingo: An open-source framework for training large autoregressive vision-language models.,Openflamingo: An open-source framework for training large autoregressive vision-language models.,,"[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, et~al.]{awadalla2023openflamingo} Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al. 
 Openflamingo: An open-source framework for training large autoregressive vision-language models. 
 \emph{arXiv preprint arXiv:2308.01390}, 2023."
2407.06438,bahri2021explaining,"[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.",Explaining neural scaling laws.,Explaining neural scaling laws.,,"[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma]{bahri2021explaining} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 
 Explaining neural scaling laws. 
 \emph{arXiv preprint arXiv:2102.06701}, 2021."
2407.06438,chen2024allava,"[Chen et~al.(2024{\natexlab{a}})Chen, Chen, Zhang, Chen, Wu, Zhang, Chen, Li, Wan, and Wang]{chen2024allava} Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.",Allava: Harnessing gpt4v-synthesized data for a lite vision-language model.,Allava: Harnessing gpt4v-synthesized data for a lite vision-language model.,,"[Chen et~al.(2024{\natexlab{a}})Chen, Chen, Zhang, Chen, Wu, Zhang, Chen, Li, Wan, and Wang]{chen2024allava} Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. 
 Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. 
 \emph{arXiv preprint arXiv:2402.11684}, 2024{\natexlab{a}}."
2407.06438,chen2023minigpt,"[Chen et~al.(2023{\natexlab{a}})Chen, Zhu, Shen, Li, Liu, Zhang, Krishnamoorthi, Chandra, Xiong, and Elhoseiny]{chen2023minigpt} Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.",Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.,Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.,,"[Chen et~al.(2023{\natexlab{a}})Chen, Zhu, Shen, Li, Liu, Zhang, Krishnamoorthi, Chandra, Xiong, and Elhoseiny]{chen2023minigpt} Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 
 Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 
 \emph{arXiv preprint arXiv:2310.09478}, 2023{\natexlab{a}}."
2407.06438,chen2023sharegpt4v,"[Chen et~al.(2023{\natexlab{b}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{chen2023sharegpt4v} Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.",Sharegpt4v: Improving large multi-modal models with better captions.,Sharegpt4v: Improving large multi-modal models with better captions.,,"[Chen et~al.(2023{\natexlab{b}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{chen2023sharegpt4v} Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 
 Sharegpt4v: Improving large multi-modal models with better captions. 
 \emph{arXiv preprint arXiv:2311.12793}, 2023{\natexlab{b}}."
2407.06438,chen2024we,"[Chen et~al.(2024{\natexlab{b}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin, et~al.]{chen2024we} Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, et~al.",Are we on the right way for evaluating large vision-language models?,Are we on the right way for evaluating large vision-language models?,,"[Chen et~al.(2024{\natexlab{b}})Chen, Li, Dong, Zhang, Zang, Chen, Duan, Wang, Qiao, Lin, et~al.]{chen2024we} Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, et~al. 
 Are we on the right way for evaluating large vision-language models? 
 \emph{arXiv preprint arXiv:2403.20330}, 2024{\natexlab{b}}."
2407.06438,chen2023measuring,"[Chen et~al.(2023{\natexlab{c}})Chen, Sikka, Cogswell, Ji, and Divakaran]{chen2023measuring} Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran.",Measuring and improving chain-of-thought reasoning in vision-language models.,Measuring and improving chain-of-thought reasoning in vision-language models.,,"[Chen et~al.(2023{\natexlab{c}})Chen, Sikka, Cogswell, Ji, and Divakaran]{chen2023measuring} Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 
 Measuring and improving chain-of-thought reasoning in vision-language models. 
 \emph{arXiv preprint arXiv:2309.04461}, 2023{\natexlab{c}}."
2407.06438,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2407.06438,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2407.06438,ding2023enhancing,"[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.",Enhancing chat language models by scaling high-quality instructional conversations.,Enhancing chat language models by scaling high-quality instructional conversations.,,"[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 
 Enhancing chat language models by scaling high-quality instructional conversations. 
 \emph{arXiv preprint arXiv:2305.14233}, 2023."
2407.06438,dong2024internlm,"[Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Zhang, Duan, Zhang, Li, et~al.]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et~al.",Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.,Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.,,"[Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Zhang, Duan, Zhang, Li, et~al.]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et~al. 
 Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. 
 \emph{arXiv preprint arXiv:2404.06512}, 2024."
2407.06438,dosovitskiyImageWorth16x162021,"[Dosovitskiy et~al.(2021{\natexlab{a}})Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiyImageWorth16x162021} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.",An {{Image}} is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}.,An {{Image}} is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}.,,"[Dosovitskiy et~al.(2021{\natexlab{a}})Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiyImageWorth16x162021} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 
 An {{Image}} is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}. 
 \emph{arXiv:2010.11929 [cs]}, 2021{\natexlab{a}}."
2407.06438,du2024understanding,"[Du et~al.(2024)Du, Zeng, Dong, and Tang]{du2024understanding} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang.",Understanding emergent abilities of language models from the loss perspective.,Understanding emergent abilities of language models from the loss perspective.,,"[Du et~al.(2024)Du, Zeng, Dong, and Tang]{du2024understanding} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. 
 Understanding emergent abilities of language models from the loss perspective. 
 \emph{arXiv preprint arXiv:2403.15796}, 2024."
2407.06438,ferraro2015survey,"[Ferraro et~al.(2015)Ferraro, Mostafazadeh, Vanderwende, Devlin, Galley, Mitchell, et~al.]{ferraro2015survey} Francis Ferraro, Nasrin Mostafazadeh, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell, et~al.",A survey of current datasets for vision and language research.,A survey of current datasets for vision and language research.,,"[Ferraro et~al.(2015)Ferraro, Mostafazadeh, Vanderwende, Devlin, Galley, Mitchell, et~al.]{ferraro2015survey} Francis Ferraro, Nasrin Mostafazadeh, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell, et~al. 
 A survey of current datasets for vision and language research. 
 \emph{arXiv preprint arXiv:1506.06833}, 2015."
2407.06438,gao2023llama,"[Gao et~al.(2023)Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, et~al.]{gao2023llama} Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al.",Llama-adapter v2: Parameter-efficient visual instruction model.,Llama-adapter v2: Parameter-efficient visual instruction model.,,"[Gao et~al.(2023)Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, et~al.]{gao2023llama} Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al. 
 Llama-adapter v2: Parameter-efficient visual instruction model. 
 \emph{arXiv preprint arXiv:2304.15010}, 2023."
2407.06438,ge2023planting,"[Ge et~al.(2023)Ge, Ge, Zeng, Wang, and Shan]{ge2023planting} Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan.",Planting a seed of vision in large language model.,Planting a seed of vision in large language model.,,"[Ge et~al.(2023)Ge, Ge, Zeng, Wang, and Shan]{ge2023planting} Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 
 Planting a seed of vision in large language model. 
 \emph{arXiv preprint arXiv:2307.08041}, 2023."
2407.06438,gong2023multimodal,"[Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang, Luo, and Chen]{gong2023multimodal} Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.",Multimodal-gpt: A vision and language model for dialogue with humans.,Multimodal-gpt: A vision and language model for dialogue with humans.,,"[Gong et~al.(2023)Gong, Lyu, Zhang, Wang, Zheng, Zhao, Liu, Zhang, Luo, and Chen]{gong2023multimodal} Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 
 Multimodal-gpt: A vision and language model for dialogue with humans. 
 \emph{arXiv preprint arXiv:2305.04790}, 2023."
2407.06438,hendrycks2020measuring,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}, 2020."
2407.06438,VisCPM,"[Hu et~al.(2023)Hu, Yao, Wang, Wang, Pan, Chen, Yu, Wu, Zhao, Zhang, et~al.]{VisCPM} Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et~al.",Large multilingual models pivot zero-shot multimodal learning across languages.,Large multilingual models pivot zero-shot multimodal learning across languages.,,"[Hu et~al.(2023)Hu, Yao, Wang, Wang, Pan, Chen, Yu, Wu, Zhao, Zhang, et~al.]{VisCPM} Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et~al. 
 Large multilingual models pivot zero-shot multimodal learning across languages. 
 \emph{arXiv preprint arXiv:2308.12038}, 2023."
2407.06438,hu2024minicpm,"[Hu et~al.(2024{\natexlab{b}})Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{hu2024minicpm} Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al.",Minicpm: Unveiling the potential of small language models with scalable training strategies.,Minicpm: Unveiling the potential of small language models with scalable training strategies.,,"[Hu et~al.(2024{\natexlab{b}})Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{hu2024minicpm} Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al. 
 Minicpm: Unveiling the potential of small language models with scalable training strategies. 
 \emph{arXiv preprint arXiv:2404.06395}, 2024{\natexlab{b}}."
2407.06438,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2407.06438,kahou2017figureqa,"[Kahou et~al.(2017)Kahou, Michalski, Atkinson, K{\'a}d{\'a}r, Trischler, and Bengio]{kahou2017figureqa} Samira~Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, {\'A}kos K{\'a}d{\'a}r, Adam Trischler, and Yoshua Bengio.",Figureqa: An annotated figure dataset for visual reasoning.,Figureqa: An annotated figure dataset for visual reasoning.,,"[Kahou et~al.(2017)Kahou, Michalski, Atkinson, K{\'a}d{\'a}r, Trischler, and Bengio]{kahou2017figureqa} Samira~Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, {\'A}kos K{\'a}d{\'a}r, Adam Trischler, and Yoshua Bengio. 
 Figureqa: An annotated figure dataset for visual reasoning. 
 \emph{arXiv preprint arXiv:1710.07300}, 2017."
2407.06438,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2407.06438,lai2017race,"[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.",Race: Large-scale reading comprehension dataset from examinations.,Race: Large-scale reading comprehension dataset from examinations.,,"[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race} Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 
 Race: Large-scale reading comprehension dataset from examinations. 
 \emph{arXiv preprint arXiv:1704.04683}, 2017."
2407.06438,laurenccon2024matters,"[Lauren{\c{c}}on et~al.(2024)Lauren{\c{c}}on, Tronchon, Cord, and Sanh]{laurenccon2024matters} Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.",What matters when building vision-language models?,What matters when building vision-language models?,,"[Lauren{\c{c}}on et~al.(2024)Lauren{\c{c}}on, Tronchon, Cord, and Sanh]{laurenccon2024matters} Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh. 
 What matters when building vision-language models? 
 \emph{arXiv preprint arXiv:2405.02246}, 2024."
2407.06438,li2023otter,"[Li et~al.(2023{\natexlab{a}})Li, Zhang, Chen, Wang, Yang, and Liu]{li2023otter} Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.",Otter: A multi-modal model with in-context instruction tuning.,Otter: A multi-modal model with in-context instruction tuning.,,"[Li et~al.(2023{\natexlab{a}})Li, Zhang, Chen, Wang, Yang, and Liu]{li2023otter} Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 
 Otter: A multi-modal model with in-context instruction tuning. 
 \emph{arXiv preprint arXiv:2305.03726}, 2023{\natexlab{a}}."
2407.06438,li2023silkie,"[Li et~al.(2023{\natexlab{d}})Li, Xie, Li, Chen, Wang, Chen, Yang, Wang, and Kong]{li2023silkie} Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.",Silkie: Preference distillation for large visual language models.,Silkie: Preference distillation for large visual language models.,,"[Li et~al.(2023{\natexlab{d}})Li, Xie, Li, Chen, Wang, Chen, Yang, Wang, and Kong]{li2023silkie} Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. 
 Silkie: Preference distillation for large visual language models. 
 \emph{arXiv preprint arXiv:2312.10665}, 2023{\natexlab{d}}."
2407.06438,li2019visualbert,"[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert} Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.",Visualbert: A simple and performant baseline for vision and language.,Visualbert: A simple and performant baseline for vision and language.,,"[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert} Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 
 Visualbert: A simple and performant baseline for vision and language. 
 \emph{arXiv preprint arXiv:1908.03557}, 2019."
2407.06438,li2023evaluating,"[Li et~al.(2023{\natexlab{e}})Li, Du, Zhou, Wang, Zhao, and Wen]{li2023evaluating} Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.",Evaluating object hallucination in large vision-language models.,Evaluating object hallucination in large vision-language models.,,"[Li et~al.(2023{\natexlab{e}})Li, Du, Zhou, Wang, Zhao, and Wen]{li2023evaluating} Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen. 
 Evaluating object hallucination in large vision-language models. 
 \emph{arXiv preprint arXiv:2305.10355}, 2023{\natexlab{e}}."
2407.06438,lin2023sphinx,"[Lin et~al.(2023)Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, et~al.]{lin2023sphinx} Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al.","Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.","Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.",,"[Lin et~al.(2023)Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, et~al.]{lin2023sphinx} Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al. 
 Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. 
 \emph{arXiv preprint arXiv:2311.07575}, 2023."
2407.06438,liu2023aligning,"[Liu et~al.(2023{\natexlab{b}})Liu, Lin, Li, Wang, Yacoob, and Wang]{liu2023aligning} Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.",Aligning large multi-modal model with robust instruction tuning.,Aligning large multi-modal model with robust instruction tuning.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Lin, Li, Wang, Yacoob, and Wang]{liu2023aligning} Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 
 Aligning large multi-modal model with robust instruction tuning. 
 \emph{arXiv preprint arXiv:2306.14565}, 2023{\natexlab{b}}."
2407.06438,liu2023hidden,"[Liu et~al.(2023{\natexlab{d}})Liu, Li, Li, Yu, Huang, Peng, Liu, Chen, Li, Jin, et~al.]{liu2023hidden} Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et~al.",On the hidden mystery of ocr in large multimodal models.,On the hidden mystery of ocr in large multimodal models.,,"[Liu et~al.(2023{\natexlab{d}})Liu, Li, Li, Yu, Huang, Peng, Liu, Chen, Li, Jin, et~al.]{liu2023hidden} Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et~al. 
 On the hidden mystery of ocr in large multimodal models. 
 \emph{arXiv preprint arXiv:2305.07895}, 2023{\natexlab{d}}."
2407.06438,lu2024deepseek,"[Lu et~al.(2024)Lu, Liu, Zhang, Wang, Dong, Liu, Sun, Ren, Li, Sun, et~al.]{lu2024deepseek} Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.",Deepseek-vl: towards real-world vision-language understanding.,Deepseek-vl: towards real-world vision-language understanding.,,"[Lu et~al.(2024)Lu, Liu, Zhang, Wang, Dong, Liu, Sun, Ren, Li, Sun, et~al.]{lu2024deepseek} Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al. 
 Deepseek-vl: towards real-world vision-language understanding. 
 \emph{arXiv preprint arXiv:2403.05525}, 2024."
2407.06438,lu2021iconqa,"[Lu et~al.(2021)Lu, Qiu, Chen, Xia, Zhao, Zhang, Yu, Liang, and Zhu]{lu2021iconqa} Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.",Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning.,Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning.,,"[Lu et~al.(2021)Lu, Qiu, Chen, Xia, Zhao, Zhang, Yu, Liang, and Zhu]{lu2021iconqa} Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 
 Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. 
 \emph{arXiv preprint arXiv:2110.13214}, 2021."
2407.06438,lu2022dynamic,"[Lu et~al.(2022{\natexlab{b}})Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and Kalyan]{lu2022dynamic} Pan Lu, Liang Qiu, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.",Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.,Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.,,"[Lu et~al.(2022{\natexlab{b}})Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and Kalyan]{lu2022dynamic} Pan Lu, Liang Qiu, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 
 Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. 
 \emph{arXiv preprint arXiv:2209.14610}, 2022{\natexlab{b}}."
2407.06438,lu2023mathvista,"[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.",Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,,"[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 
 Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 
 \emph{arXiv preprint arXiv:2310.02255}, 2023."
2407.06438,masry2022chartqa,"[Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque]{masry2022chartqa} Ahmed Masry, Do~Xuan Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.",Chartqa: A benchmark for question answering about charts with visual and logical reasoning.,Chartqa: A benchmark for question answering about charts with visual and logical reasoning.,,"[Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque]{masry2022chartqa} Ahmed Masry, Do~Xuan Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque. 
 Chartqa: A benchmark for question answering about charts with visual and logical reasoning. 
 \emph{arXiv preprint arXiv:2203.10244}, 2022."
2407.06438,obeid2020chart,[Obeid \& Hoque(2020)Obeid and Hoque]{obeid2020chart} Jason Obeid and Enamul Hoque.,Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model.,Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model.,,"[Obeid \& Hoque(2020)Obeid and Hoque]{obeid2020chart} Jason Obeid and Enamul Hoque. 
 Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. 
 \emph{arXiv preprint arXiv:2010.09142}, 2020."
2407.06438,ridnik2021imagenet,"[Ridnik et~al.(2021{\natexlab{a}})Ridnik, Ben-Baruch, Noy, and Zelnik-Manor]{ridnik2021imagenet} Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.",Imagenet-21k pretraining for the masses.,Imagenet-21k pretraining for the masses.,,"[Ridnik et~al.(2021{\natexlab{a}})Ridnik, Ben-Baruch, Noy, and Zelnik-Manor]{ridnik2021imagenet} Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. 
 Imagenet-21k pretraining for the masses. 
 \emph{arXiv preprint arXiv:2104.10972}, 2021{\natexlab{a}}."
2407.06438,schuhmann2021laion,"[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion} Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.",Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.,Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.,,"[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion} Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 
 Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 
 \emph{arXiv preprint arXiv:2111.02114}, 2021."
2407.06438,shoeybi2019megatron,"[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron} Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.",Megatron-lm: Training multi-billion parameter language models using model parallelism.,Megatron-lm: Training multi-billion parameter language models using model parallelism.,,"[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron} Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 
 Megatron-lm: Training multi-billion parameter language models using model parallelism. 
 \emph{arXiv preprint arXiv:1909.08053}, 2019."
2407.06438,su2023pandagpt,"[Su et~al.(2023)Su, Lan, Li, Xu, Wang, and Cai]{su2023pandagpt} Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.",Pandagpt: One model to instruction-follow them all.,Pandagpt: One model to instruction-follow them all.,,"[Su et~al.(2023)Su, Lan, Li, Xu, Wang, and Cai]{su2023pandagpt} Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 
 Pandagpt: One model to instruction-follow them all. 
 \emph{arXiv preprint arXiv:2305.16355}, 2023."
2407.06438,sun2023generative,"[Sun et~al.(2023{\natexlab{a}})Sun, Yu, Cui, Zhang, Zhang, Wang, Gao, Liu, Huang, and Wang]{sun2023generative} Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.",Generative pretraining in multimodality.,Generative pretraining in multimodality.,,"[Sun et~al.(2023{\natexlab{a}})Sun, Yu, Cui, Zhang, Zhang, Wang, Gao, Liu, Huang, and Wang]{sun2023generative} Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 
 Generative pretraining in multimodality. 
 \emph{arXiv preprint arXiv:2307.05222}, 2023{\natexlab{a}}."
2407.06438,sun2023aligning,"[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, et~al.]{sun2023aligning} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et~al.",Aligning large multimodal models with factually augmented rlhf.,Aligning large multimodal models with factually augmented rlhf.,,"[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, et~al.]{sun2023aligning} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et~al. 
 Aligning large multimodal models with factually augmented rlhf. 
 \emph{arXiv preprint arXiv:2309.14525}, 2023{\natexlab{b}}."
2407.06438,tang2023vistext,"[Tang et~al.(2023)Tang, Boggust, and Satyanarayan]{tang2023vistext} Benny~J Tang, Angie Boggust, and Arvind Satyanarayan.",Vistext: A benchmark for semantically rich chart captioning.,Vistext: A benchmark for semantically rich chart captioning.,,"[Tang et~al.(2023)Tang, Boggust, and Satyanarayan]{tang2023vistext} Benny~J Tang, Angie Boggust, and Arvind Satyanarayan. 
 Vistext: A benchmark for semantically rich chart captioning. 
 \emph{arXiv preprint arXiv:2307.05356}, 2023."
2407.06438,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2407.06438,wang2023see,"[Wang et~al.(2023)Wang, Meng, Weng, He, Wu, and Jiang]{wang2023see} Junke Wang, Lingchen Meng, Zejia Weng, Bo~He, Zuxuan Wu, and Yu-Gang Jiang.",To see is to believe: Prompting gpt-4v for better visual instruction tuning.,To see is to believe: Prompting gpt-4v for better visual instruction tuning.,,"[Wang et~al.(2023)Wang, Meng, Weng, He, Wu, and Jiang]{wang2023see} Junke Wang, Lingchen Meng, Zejia Weng, Bo~He, Zuxuan Wu, and Yu-Gang Jiang. 
 To see is to believe: Prompting gpt-4v for better visual instruction tuning. 
 \emph{arXiv preprint arXiv:2311.07574}, 2023."
2407.06438,wang2024measuring,"[Wang et~al.(2024{\natexlab{a}})Wang, Pan, Shi, Lu, Zhan, and Li]{wang2024measuring} Ke~Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li.",Measuring multimodal mathematical reasoning with math-vision dataset.,Measuring multimodal mathematical reasoning with math-vision dataset.,,"[Wang et~al.(2024{\natexlab{a}})Wang, Pan, Shi, Lu, Zhan, and Li]{wang2024measuring} Ke~Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 
 Measuring multimodal mathematical reasoning with math-vision dataset. 
 \emph{arXiv preprint arXiv:2402.14804}, 2024{\natexlab{a}}."
2407.06438,wang2024executable,"[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Yuan, Zhang, Li, Peng, and Ji]{wang2024executable} Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji.",Executable code actions elicit better llm agents.,Executable code actions elicit better llm agents.,,"[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Yuan, Zhang, Li, Peng, and Ji]{wang2024executable} Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 
 Executable code actions elicit better llm agents. 
 \emph{arXiv preprint arXiv:2402.01030}, 2024{\natexlab{b}}."
2407.06438,wang2021simvlm,"[Wang et~al.(2021)Wang, Yu, Yu, Dai, Tsvetkov, and Cao]{wang2021simvlm} Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.",Simvlm: Simple visual language model pretraining with weak supervision.,Simvlm: Simple visual language model pretraining with weak supervision.,,"[Wang et~al.(2021)Wang, Yu, Yu, Dai, Tsvetkov, and Cao]{wang2021simvlm} Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 
 Simvlm: Simple visual language model pretraining with weak supervision. 
 \emph{arXiv preprint arXiv:2108.10904}, 2021."
2407.06438,wei2023instructiongpt,"[Wei et~al.(2023)Wei, Jiang, Huang, and Sun]{wei2023instructiongpt} Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun.",Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4.,Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4.,,"[Wei et~al.(2023)Wei, Jiang, Huang, and Sun]{wei2023instructiongpt} Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. 
 Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4. 
 \emph{arXiv preprint arXiv:2308.12067}, 2023."
2407.06438,wu2024macaroon,"[Wu et~al.(2024)Wu, Fung, Li, Wan, Chang, and Ji]{wu2024macaroon} Shujin Wu, Yi~R Fung, Sha Li, Yixin Wan, Kai-Wei Chang, and Heng Ji.",Macaroon: Training vision-language models to be your engaged partners.,Macaroon: Training vision-language models to be your engaged partners.,,"[Wu et~al.(2024)Wu, Fung, Li, Wan, Chang, and Ji]{wu2024macaroon} Shujin Wu, Yi~R Fung, Sha Li, Yixin Wan, Kai-Wei Chang, and Heng Ji. 
 Macaroon: Training vision-language models to be your engaged partners. 
 \emph{arXiv preprint arXiv:2406.14137}, 2024."
2407.06438,xu2024llava,"[Xu et~al.(2024{\natexlab{a}})Xu, Yao, Guo, Cui, Ni, Ge, Chua, Liu, Sun, and Huang]{xu2024llava} Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang.",Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.,Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.,,"[Xu et~al.(2024{\natexlab{a}})Xu, Yao, Guo, Cui, Ni, Ge, Chua, Liu, Sun, and Huang]{xu2024llava} Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. 
 Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. 
 \emph{arXiv preprint arXiv:2403.11703}, 2024{\natexlab{a}}."
2407.06438,xu2024vision,"[Xu et~al.(2024{\natexlab{b}})Xu, Feng, Shao, Ashby, Shen, Jin, Cheng, Wang, and Huang]{xu2024vision} Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di~Jin, Yu~Cheng, Qifan Wang, and Lifu Huang.",Vision-flan: Scaling human-labeled tasks in visual instruction tuning.,Vision-flan: Scaling human-labeled tasks in visual instruction tuning.,,"[Xu et~al.(2024{\natexlab{b}})Xu, Feng, Shao, Ashby, Shen, Jin, Cheng, Wang, and Huang]{xu2024vision} Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di~Jin, Yu~Cheng, Qifan Wang, and Lifu Huang. 
 Vision-flan: Scaling human-labeled tasks in visual instruction tuning. 
 \emph{arXiv preprint arXiv:2402.11690}, 2024{\natexlab{b}}."
2407.06438,yu2023mm,"[Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang]{yu2023mm} Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.",Mm-vet: Evaluating large multimodal models for integrated capabilities.,Mm-vet: Evaluating large multimodal models for integrated capabilities.,,"[Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang]{yu2023mm} Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 
 Mm-vet: Evaluating large multimodal models for integrated capabilities. 
 \emph{arXiv preprint arXiv:2308.02490}, 2023."
2407.06438,yuan2024advancing,"[Yuan et~al.(2024)Yuan, Cui, Wang, Ding, Wang, Deng, Shan, Chen, Xie, Lin, et~al.]{yuan2024advancing} Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et~al.",Advancing llm reasoning generalists with preference trees.,Advancing llm reasoning generalists with preference trees.,,"[Yuan et~al.(2024)Yuan, Cui, Wang, Ding, Wang, Deng, Shan, Chen, Xie, Lin, et~al.]{yuan2024advancing} Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et~al. 
 Advancing llm reasoning generalists with preference trees. 
 \emph{arXiv preprint arXiv:2404.02078}, 2024."
2407.06438,yue2023mmmu,"[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, et~al.]{yue2023mmmu} Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.",Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.,Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.,,"[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, et~al.]{yue2023mmmu} Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al. 
 Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 
 \emph{arXiv preprint arXiv:2311.16502}, 2023."
2407.06438,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2407.06438,zeng2023matters,"[Zeng et~al.(2023)Zeng, Zhang, Zheng, Xia, Wei, Wei, Zhang, and Kong]{zeng2023matters} Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.",What matters in training a gpt4-style language model with multimodal inputs?,What matters in training a gpt4-style language model with multimodal inputs?,,"[Zeng et~al.(2023)Zeng, Zhang, Zheng, Xia, Wei, Wei, Zhang, and Kong]{zeng2023matters} Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong. 
 What matters in training a gpt4-style language model with multimodal inputs? 
 \emph{arXiv preprint arXiv:2307.02469}, 2023."
2407.06438,zhang2024mm,"[Zhang et~al.(2024{\natexlab{a}})Zhang, Yu, Li, Dong, Su, Chu, and Yu]{zhang2024mm} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.",Mm-llms: Recent advances in multimodal large language models.,Mm-llms: Recent advances in multimodal large language models.,,"[Zhang et~al.(2024{\natexlab{a}})Zhang, Yu, Li, Dong, Su, Chu, and Yu]{zhang2024mm} Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 
 Mm-llms: Recent advances in multimodal large language models. 
 \emph{arXiv preprint arXiv:2401.13601}, 2024{\natexlab{a}}."
2407.06438,internlmxcomposer,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Dong, Wang, Cao, Xu, Ouyang, Zhao, Ding, Zhang, Duan, Zhang, Yan, Zhang, Li, Li, Chen, He, Zhang, Qiao, Lin, and Wang]{internlmxcomposer} Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang.",Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.,Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.,,"[Zhang et~al.(2023{\natexlab{a}})Zhang, Dong, Wang, Cao, Xu, Ouyang, Zhao, Ding, Zhang, Duan, Zhang, Yan, Zhang, Li, Li, Chen, He, Zhang, Qiao, Lin, and Wang]{internlmxcomposer} Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang. 
 Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. 
 \emph{arXiv preprint arXiv:2309.15112}, 2023{\natexlab{a}}."
2407.06438,zhang2023llavar,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhang, Gu, Zhou, Lipka, Yang, and Sun]{zhang2023llavar} Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.",Llavar: Enhanced visual instruction tuning for text-rich image understanding.,Llavar: Enhanced visual instruction tuning for text-rich image understanding.,,"[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhang, Gu, Zhou, Lipka, Yang, and Sun]{zhang2023llavar} Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 
 Llavar: Enhanced visual instruction tuning for text-rich image understanding. 
 \emph{arXiv preprint arXiv:2306.17107}, 2023{\natexlab{b}}."
2407.06542,cobbe2021gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.06542,ding2023enhancing,"[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou}]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.",Enhancing chat language models by scaling high-quality instructional conversations.,Enhancing chat language models by scaling high-quality instructional conversations.,,"[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou}]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. 
 Enhancing chat language models by scaling high-quality instructional conversations. 
 \emph{arXiv preprint arXiv:2305.14233}."
2407.06542,dubois2024length,"[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto}]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 2024.",Length-controlled alpacaeval: A simple way to debias automatic evaluators.,Length-controlled alpacaeval: A simple way to debias automatic evaluators.,,"[{Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto}]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 2024. 
 Length-controlled alpacaeval: A simple way to debias automatic evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}."
2407.06542,yu2023metamath,"[{Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu}]{yu2023metamath} Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.",Metamath: Bootstrap your own mathematical questions for large language models.,Metamath: Bootstrap your own mathematical questions for large language models.,,"[{Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu}]{yu2023metamath} Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. 
 Metamath: Bootstrap your own mathematical questions for large language models. 
 \emph{arXiv preprint arXiv:2309.12284}."
2407.06654,sakaguchi2019winogrande,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.",Winogrande: An adversarial winograd schema challenge at scale.,Winogrande: An adversarial winograd schema challenge at scale.,,"[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2019winogrande} Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. 
 Winogrande: An adversarial winograd schema challenge at scale. 
 \emph{arXiv preprint arXiv:1907.10641}."
2407.07263,ainslie2023gqa,"[{Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai}]{ainslie2023gqa} Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'o}n, and Sumit Sanghai. 2023.",{GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}.,{GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}.,,"[{Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai}]{ainslie2023gqa} Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'o}n, and Sumit Sanghai. 2023. 
 {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}. 
 \emph{arXiv preprint arXiv:2305.13245}."
2407.07263,chowdhery2022palm,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022.",{PaLM: Scaling Language Modeling with Pathways}.,{PaLM: Scaling Language Modeling with Pathways}.,,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022. 
 {PaLM: Scaling Language Modeling with Pathways}. 
 \emph{arXiv preprint arXiv:2204.02311}."
2407.07263,el2019ccaligned,"[{El-Kishky et~al.(2019)El-Kishky, Chaudhary, Guzm{\'a}n, and Koehn}]{el2019ccaligned} Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzm{\'a}n, and Philipp Koehn. 2019.",Ccaligned: A massive collection of cross-lingual web-document pairs.,Ccaligned: A massive collection of cross-lingual web-document pairs.,,"[{El-Kishky et~al.(2019)El-Kishky, Chaudhary, Guzm{\'a}n, and Koehn}]{el2019ccaligned} Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzm{\'a}n, and Philipp Koehn. 2019. 
 Ccaligned: A massive collection of cross-lingual web-document pairs. 
 \emph{arXiv preprint arXiv:1911.06154}."
2407.07263,pile-dataset-2020,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}]{pile-dataset-2020} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.",The {P}ile: An 800gb dataset of diverse text for language modeling.,The {P}ile: An 800gb dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy}]{pile-dataset-2020} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. 
 The {P}ile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2407.07263,hendrycks2020measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",{Measuring Massive Multitask Language Understanding}.,{Measuring Massive Multitask Language Understanding}.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 {Measuring Massive Multitask Language Understanding}. 
 \emph{arXiv preprint arXiv:2009.03300}."
2407.07263,kudo2018sentencepiece,[{Kudo and Richardson(2018)}]{kudo2018sentencepiece} Taku Kudo and John Richardson. 2018.,{Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing}.,{Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing}.,,"[{Kudo and Richardson(2018)}]{kudo2018sentencepiece} Taku Kudo and John Richardson. 2018. 
 {Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing}. 
 \emph{arXiv preprint arXiv:1808.06226}."
2407.07263,schwenk2019ccmatrix,"[{Schwenk et~al.(2019)Schwenk, Wenzek, Edunov, Grave, and Joulin}]{schwenk2019ccmatrix} Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin. 2019.",Ccmatrix: Mining billions of high-quality parallel sentences on the web.,Ccmatrix: Mining billions of high-quality parallel sentences on the web.,,"[{Schwenk et~al.(2019)Schwenk, Wenzek, Edunov, Grave, and Joulin}]{schwenk2019ccmatrix} Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin. 2019. 
 Ccmatrix: Mining billions of high-quality parallel sentences on the web. 
 \emph{arXiv preprint arXiv:1911.04944}."
2407.07263,touvron2023llama2,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",{Llama 2: Open Foundation and Fine-tuned Chat Models}.,{Llama 2: Open Foundation and Fine-tuned Chat Models}.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 {Llama 2: Open Foundation and Fine-tuned Chat Models}. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.07263,wang2022text,"[{Wang et~al.(2022)Wang, Yang, Huang, Jiao, Yang, Jiang, Majumder, and Wei}]{wang2022text} Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022.",Text embeddings by weakly-supervised contrastive pre-training.,Text embeddings by weakly-supervised contrastive pre-training.,,"[{Wang et~al.(2022)Wang, Yang, Huang, Jiao, Yang, Jiang, Majumder, and Wei}]{wang2022text} Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. 
 Text embeddings by weakly-supervised contrastive pre-training. 
 \emph{arXiv preprint arXiv:2212.03533}."
2407.0788,lam2021orthounimodal,"[Lam et~al.(2021)Lam, Liu, and Zhang]{lam2021orthounimodal} H.~Lam, Z.~Liu, and X.~Zhang.","Orthounimodal distributionally robust optimization: Representation, computation and multivariate extreme event applications.","Orthounimodal distributionally robust optimization: Representation, computation and multivariate extreme event applications.",,"[Lam et~al.(2021)Lam, Liu, and Zhang]{lam2021orthounimodal} H.~Lam, Z.~Liu, and X.~Zhang. 
 Orthounimodal distributionally robust optimization: Representation, computation and multivariate extreme event applications. 
 \emph{arXiv preprint arXiv:2111.07894}, 2021."
2407.0788,liu2019roberta,"[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta} Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta} Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov. 
 Roberta: A robustly optimized bert pretraining approach. 
 \emph{arXiv preprint arXiv:1907.11692}, 2019."
2407.0844,abdin2024phi,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.",Phi-3 technical report: A highly capable language model locally on your phone.,Phi-3 technical report: A highly capable language model locally on your phone.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024. 
 Phi-3 technical report: A highly capable language model locally on your phone. 
 \emph{arXiv preprint arXiv:2404.14219}."
2407.0844,choi2023kcts,"[{Choi et~al.(2023)Choi, Fang, Wang, and Song}]{choi2023kcts} Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023.",Kcts: knowledge-constrained tree search decoding with token-level hallucination detection.,Kcts: knowledge-constrained tree search decoding with token-level hallucination detection.,,"[{Choi et~al.(2023)Choi, Fang, Wang, and Song}]{choi2023kcts} Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023. 
 Kcts: knowledge-constrained tree search decoding with token-level hallucination detection. 
 \emph{arXiv preprint arXiv:2310.09044}."
2407.0844,hu2024case,"[{Hu et~al.(2024)Hu, Tang, Yang, and Zhang}]{hu2024case} Yi~Hu, Xiaojuan Tang, Haotong Yang, and Muhan Zhang. 2024.",Case-based or rule-based: How do transformers do the math?,Case-based or rule-based: How do transformers do the math?,,"[{Hu et~al.(2024)Hu, Tang, Yang, and Zhang}]{hu2024case} Yi~Hu, Xiaojuan Tang, Haotong Yang, and Muhan Zhang. 2024. 
 Case-based or rule-based: How do transformers do the math? 
 \emph{arXiv preprint arXiv:2402.17709}."
2407.0844,mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.0844,salad,"[{Li et~al.(2024)Li, Dong, Wang, Hu, Zuo, Lin, Qiao, and Shao}]{salad} Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu~Qiao, and Jing Shao. 2024.",Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.,,"[{Li et~al.(2024)Li, Dong, Wang, Hu, Zuo, Lin, Qiao, and Shao}]{salad} Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu~Qiao, and Jing Shao. 2024. 
 Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. 
 \emph{arXiv preprint arXiv:2402.05044}."
2407.0844,luo2023chatrule,"[{Luo et~al.(2023)Luo, Ju, Xiong, Li, Haffari, and Pan}]{luo2023chatrule} Linhao Luo, Jiaxin Ju, Bo~Xiong, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023.",Chatrule: Mining logical rules with large language models for knowledge graph reasoning.,Chatrule: Mining logical rules with large language models for knowledge graph reasoning.,,"[{Luo et~al.(2023)Luo, Ju, Xiong, Li, Haffari, and Pan}]{luo2023chatrule} Linhao Luo, Jiaxin Ju, Bo~Xiong, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. 
 Chatrule: Mining logical rules with large language models for knowledge graph reasoning. 
 \emph{arXiv preprint arXiv:2309.01538}."
2407.0844,self-refine,"[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang et~al.}]{self-refine} Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al. 2023.",Self-refine: Iterative refinement with self-feedback.,Self-refine: Iterative refinement with self-feedback.,,"[{Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang et~al.}]{self-refine} Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al. 2023. 
 Self-refine: Iterative refinement with self-feedback. 
 \emph{arXiv preprint arXiv:2303.17651}."
2407.0844,instruction2,"[{Mishra et~al.(2021)Mishra, Khashabi, Baral, and Hajishirzi}]{instruction2} Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021.",Cross-task generalization via natural language crowdsourcing instructions.,Cross-task generalization via natural language crowdsourcing instructions.,,"[{Mishra et~al.(2021)Mishra, Khashabi, Baral, and Hajishirzi}]{instruction2} Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. 
 Cross-task generalization via natural language crowdsourcing instructions. 
 \emph{arXiv preprint arXiv:2104.08773}."
2407.0844,rules,"[{Mu et~al.(2023)Mu, Chen, Wang, Chen, Karamardian, Aljeraisy, Hendrycks, and Wagner}]{rules} Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2023.",Can llms follow simple rules?,Can llms follow simple rules?,,"[{Mu et~al.(2023)Mu, Chen, Wang, Chen, Karamardian, Aljeraisy, Hendrycks, and Wagner}]{rules} Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2023. 
 Can llms follow simple rules? 
 \emph{arXiv preprint arXiv:2311.04235}."
2407.0844,qin2024infobench,"[{Qin et~al.(2024)Qin, Song, Hu, Yao, Cho, Wang, Wu, Liu, Liu, and Yu}]{qin2024infobench} Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024.",Infobench: Evaluating instruction following ability in large language models.,Infobench: Evaluating instruction following ability in large language models.,,"[{Qin et~al.(2024)Qin, Song, Hu, Yao, Cho, Wang, Wu, Liu, Liu, and Yu}]{qin2024infobench} Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. 
 Infobench: Evaluating instruction following ability in large language models. 
 \emph{arXiv preprint arXiv:2401.03601}."
2407.0844,self-reflection,"[{Shinn et~al.(2023)Shinn, Labash, and Gopinath}]{self-reflection} Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023.",Reflexion: an autonomous agent with dynamic memory and self-reflection.,Reflexion: an autonomous agent with dynamic memory and self-reflection.,,"[{Shinn et~al.(2023)Shinn, Labash, and Gopinath}]{self-reflection} Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. 
 Reflexion: an autonomous agent with dynamic memory and self-reflection. 
 \emph{arXiv preprint arXiv:2303.11366}."
2407.0844,clutrr,"[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and Hamilton}]{clutrr} Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L Hamilton. 2019.",Clutrr: A diagnostic benchmark for inductive reasoning from text.,Clutrr: A diagnostic benchmark for inductive reasoning from text.,,"[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and Hamilton}]{clutrr} Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L Hamilton. 2019. 
 Clutrr: A diagnostic benchmark for inductive reasoning from text. 
 \emph{arXiv preprint arXiv:1908.06177}."
2407.0844,sun2023expnote,"[{Sun et~al.(2023)Sun, Yu, He, Zhao, and Liu}]{sun2023expnote} Wangtao Sun, Xuanqing Yu, Shizhu He, Jun Zhao, and Kang Liu. 2023.",Expnote: Black-box large language models are better task solvers with experience notebook.,Expnote: Black-box large language models are better task solvers with experience notebook.,,"[{Sun et~al.(2023)Sun, Yu, He, Zhao, and Liu}]{sun2023expnote} Wangtao Sun, Xuanqing Yu, Shizhu He, Jun Zhao, and Kang Liu. 2023. 
 Expnote: Black-box large language models are better task solvers with experience notebook. 
 \emph{arXiv preprint arXiv:2311.07032}."
2407.0844,llama2,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.0844,ulogic,"[{Wang et~al.(2024)Wang, Wei, Choi, and Ren}]{ulogic} Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024.",Can llms reason with rules? logic scaffolding for stress-testing and improving llms.,Can llms reason with rules? logic scaffolding for stress-testing and improving llms.,,"[{Wang et~al.(2024)Wang, Wei, Choi, and Ren}]{ulogic} Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024. 
 Can llms reason with rules? logic scaffolding for stress-testing and improving llms. 
 \emph{arXiv preprint arXiv:2402.11442}."
2407.0844,instruction3,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{instruction3} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{instruction3} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2407.0844,cot,"[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and Zhou}]{cot} Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and Denny Zhou. 2022.",Chain of thought prompting elicits reasoning in large language models.,Chain of thought prompting elicits reasoning in large language models.,,"[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and Zhou}]{cot} Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and Denny Zhou. 2022. 
 Chain of thought prompting elicits reasoning in large language models. 
 \emph{arXiv preprint arXiv:2201.11903}."
2407.0844,cail1,"[{Xiao et~al.(2018)Xiao, Zhong, Guo, Tu, Liu, Sun, Feng, Han, Hu, Wang et~al.}]{cail1} Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et~al. 2018.",Cail2018: A large-scale legal dataset for judgment prediction.,Cail2018: A large-scale legal dataset for judgment prediction.,,"[{Xiao et~al.(2018)Xiao, Zhong, Guo, Tu, Liu, Sun, Feng, Han, Hu, Wang et~al.}]{cail1} Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et~al. 2018. 
 Cail2018: A large-scale legal dataset for judgment prediction. 
 \emph{arXiv preprint arXiv:1807.02478}."
2407.0844,yang2023failures,"[{Yang et~al.(2023)Yang, Li, and Liu}]{yang2023failures} Zeyuan Yang, Peng Li, and Yang Liu. 2023.",Failures pave the way: Enhancing large language models through tuning-free rule accumulation.,Failures pave the way: Enhancing large language models through tuning-free rule accumulation.,,"[{Yang et~al.(2023)Yang, Li, and Liu}]{yang2023failures} Zeyuan Yang, Peng Li, and Yang Liu. 2023. 
 Failures pave the way: Enhancing large language models through tuning-free rule accumulation. 
 \emph{arXiv preprint arXiv:2310.15746}."
2407.0844,deer,"[{Yang et~al.(2022)Yang, Dong, Du, Cheng, Cambria, Liu, Gao, and Wei}]{deer} Zonglin Yang, Li~Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022.",Language models as inductive reasoners.,Language models as inductive reasoners.,,"[{Yang et~al.(2022)Yang, Dong, Du, Cheng, Cambria, Liu, Gao, and Wei}]{deer} Zonglin Yang, Li~Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022. 
 Language models as inductive reasoners. 
 \emph{arXiv preprint arXiv:2212.10923}."
2407.0844,young2024yi,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang et~al.}]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 2024. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}."
2407.0844,zhao2023expel,"[{Zhao et~al.(2023)Zhao, Huang, Xu, Lin, Liu, and Huang}]{zhao2023expel} Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2023.",Expel: Llm agents are experiential learners.,Expel: Llm agents are experiential learners.,,"[{Zhao et~al.(2023)Zhao, Huang, Xu, Lin, Liu, and Huang}]{zhao2023expel} Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2023. 
 Expel: Llm agents are experiential learners. 
 \emph{arXiv preprint arXiv:2308.10144}."
2407.0844,cail2,"[{Zhong et~al.(2018)Zhong, Xiao, Guo, Tu, Liu, Sun, Feng, Han, Hu, Wang et~al.}]{cail2} Haoxi Zhong, Chaojun Xiao, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et~al. 2018.",Overview of cail2018: Legal judgment prediction competition.,Overview of cail2018: Legal judgment prediction competition.,,"[{Zhong et~al.(2018)Zhong, Xiao, Guo, Tu, Liu, Sun, Feng, Han, Hu, Wang et~al.}]{cail2} Haoxi Zhong, Chaojun Xiao, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xianpei Han, Zhen Hu, Heng Wang, et~al. 2018. 
 Overview of cail2018: Legal judgment prediction competition. 
 \emph{arXiv preprint arXiv:1810.05851}."
2407.0844,instruction1,"[{Zhong et~al.(2021)Zhong, Lee, Zhang, and Klein}]{instruction1} Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021.",Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections.,Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections.,,"[{Zhong et~al.(2021)Zhong, Lee, Zhang, and Klein}]{instruction1} Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. 
 Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. 
 \emph{arXiv preprint arXiv:2104.04670}."
2407.0844,instruction-following-eval,"[{Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{instruction-following-eval} Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023.",Instruction-following evaluation for large language models.,Instruction-following evaluation for large language models.,,"[{Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{instruction-following-eval} Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023. 
 Instruction-following evaluation for large language models. 
 \emph{arXiv preprint arXiv:2311.07911}."
2407.0844,hit,"[{Zhu et~al.(2023)Zhu, Xue, Chen, Zhou, Tang, Schuurmans, and Dai}]{hit} Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023.",Large language models can learn rules.,Large language models can learn rules.,,"[{Zhu et~al.(2023)Zhu, Xue, Chen, Zhou, Tang, Schuurmans, and Dai}]{hit} Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023. 
 Large language models can learn rules. 
 \emph{arXiv preprint arXiv:2310.07064}."
2407.09429,chowdhery2022palm,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022.",{PaLM: Scaling language modeling with pathways}.,{PaLM: Scaling language modeling with pathways}.,,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022. 
 {PaLM: Scaling language modeling with pathways}. 
 \emph{arXiv preprint arXiv:2204.02311}."
2407.09429,chung2022scaling,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{chung2022scaling} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2407.09429,han2023medalpaca,"[{Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem}]{han2023medalpaca} Tianyu Han, Lisa~C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L{\""o}ser, Daniel Truhn, and Keno~K Bressem. 2023.",{MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data}.,{MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data}.,,"[{Han et~al.(2023)Han, Adams, Papaioannou, Grundmann, Oberhauser, L{\""o}ser, Truhn, and Bressem}]{han2023medalpaca} Tianyu Han, Lisa~C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L{\""o}ser, Daniel Truhn, and Keno~K Bressem. 2023. 
 {MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data}. 
 \emph{arXiv preprint arXiv:2304.08247}."
2407.09429,iyer2022opt,"[{Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster, Wang, Liu, Koura et~al.}]{iyer2022opt} Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh Koura, et~al. 2022.",{OPT-IML}: Scaling language model instruction meta learning through the lens of generalization.,{OPT-IML}: Scaling language model instruction meta learning through the lens of generalization.,,"[{Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster, Wang, Liu, Koura et~al.}]{iyer2022opt} Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh Koura, et~al. 2022. 
 {OPT-IML}: Scaling language model instruction meta learning through the lens of generalization. 
 \emph{arXiv preprint arXiv:2212.12017}."
2407.09429,kweon2023publicly,"[{Kweon et~al.(2023)Kweon, Kim, Kim, Im, Cho, Bae, Oh, Lee, Moon, You et~al.}]{kweon2023publicly} Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong~Hak Moon, Seng~Chan You, et~al. 2023.",Publicly shareable clinical large language model built on synthetic clinical notes.,Publicly shareable clinical large language model built on synthetic clinical notes.,,"[{Kweon et~al.(2023)Kweon, Kim, Kim, Im, Cho, Bae, Oh, Lee, Moon, You et~al.}]{kweon2023publicly} Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong~Hak Moon, Seng~Chan You, et~al. 2023. 
 Publicly shareable clinical large language model built on synthetic clinical notes. 
 \emph{arXiv preprint arXiv:2309.00237}."
2407.09429,ouyang2022training,"[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training} Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.",Training language models to follow instructions with human feedback.,Training language models to follow instructions with human feedback.,,"[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training} Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022. 
 Training language models to follow instructions with human feedback. 
 \emph{arXiv preprint arXiv:2203.02155}."
2407.09429,sanh2021multitask,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 2021.",Multitask prompted training enables zero-shot task generalization.,Multitask prompted training enables zero-shot task generalization.,,"[{Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja et~al.}]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 2021. 
 Multitask prompted training enables zero-shot task generalization. 
 \emph{arXiv preprint arXiv:2110.08207}."
2407.09429,toma2023clinical,"[{Toma et~al.(2023)Toma, Lawler, Ba, Krishnan, Rubin, and Wang}]{toma2023clinical} Augustin Toma, Patrick~R Lawler, Jimmy Ba, Rahul~G Krishnan, Barry~B Rubin, and Bo~Wang. 2023.",{Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding}.,{Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding}.,,"[{Toma et~al.(2023)Toma, Lawler, Ba, Krishnan, Rubin, and Wang}]{toma2023clinical} Augustin Toma, Patrick~R Lawler, Jimmy Ba, Rahul~G Krishnan, Barry~B Rubin, and Bo~Wang. 2023. 
 {Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding}. 
 \emph{arXiv preprint arXiv:2305.12031}."
2407.09429,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2407.09429,wei2021finetuned,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2407.09652,ali2023tokenizer,"[{Ali et~al.(2023)Ali, Fromm, Thellmann, Rutmann, L{\""u}bbering, Leveling, Klug, Ebert, Doll, Buschhoff et~al.}]{ali2023tokenizer} Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max L{\""u}bbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper~Schulze Buschhoff, et~al. 2023.",Tokenizer choice for llm training: Negligible or crucial?,Tokenizer choice for llm training: Negligible or crucial?,,"[{Ali et~al.(2023)Ali, Fromm, Thellmann, Rutmann, L{\""u}bbering, Leveling, Klug, Ebert, Doll, Buschhoff et~al.}]{ali2023tokenizer} Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max L{\""u}bbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper~Schulze Buschhoff, et~al. 2023. 
 Tokenizer choice for llm training: Negligible or crucial? 
 \emph{arXiv preprint arXiv:2310.08754}."
2407.09652,qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2407.09652,bandarkar2023belebele,"[{Bandarkar et~al.(2023)Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Goyal, Krishnan, Zettlemoyer, and Khabsa}]{bandarkar2023belebele} Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya~Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023.",The belebele benchmark: a parallel reading comprehension dataset in 122 language variants.,The belebele benchmark: a parallel reading comprehension dataset in 122 language variants.,,"[{Bandarkar et~al.(2023)Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Goyal, Krishnan, Zettlemoyer, and Khabsa}]{bandarkar2023belebele} Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya~Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. 
 The belebele benchmark: a parallel reading comprehension dataset in 122 language variants. 
 \emph{arXiv preprint arXiv:2308.16884}."
2407.09652,cai2024internlm2,"[{Cai et~al.(2024)Cai, Cao, Chen, Chen, Chen, Chen, Chen, Chen, Chen, Chu et~al.}]{cai2024internlm2} Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et~al. 2024.",Internlm2 technical report.,Internlm2 technical report.,,"[{Cai et~al.(2024)Cai, Cao, Chen, Chen, Chen, Chen, Chen, Chen, Chen, Chu et~al.}]{cai2024internlm2} Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et~al. 2024. 
 Internlm2 technical report. 
 \emph{arXiv preprint arXiv:2403.17297}."
2407.09652,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.09652,nllb-22,"[{{NLLB Team} et~al.(2022){NLLB Team}, Costa-jussà, Cross, Çelebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood, Akula, Barrault, Mejia-Gonzalez, Hansanti, Hoffman, Jarrett, Sadagopan, Rowe, Spruit, Tran, Andrews, Ayan, Bhosale, Edunov, Fan, Gao, Goswami, Guzmán, Koehn, Mourachko, Ropers, Saleem, Schwenk, and Wang}]{nllb-22} {NLLB Team}, Marta~R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al~Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik~Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip~Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",No language left behind: Scaling human-centered machine translation.,No language left behind: Scaling human-centered machine translation.,,"[{{NLLB Team} et~al.(2022){NLLB Team}, Costa-jussà, Cross, Çelebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood, Akula, Barrault, Mejia-Gonzalez, Hansanti, Hoffman, Jarrett, Sadagopan, Rowe, Spruit, Tran, Andrews, Ayan, Bhosale, Edunov, Fan, Gao, Goswami, Guzmán, Koehn, Mourachko, Ropers, Saleem, Schwenk, and Wang}]{nllb-22} {NLLB Team}, Marta~R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al~Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik~Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip~Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. 
 No language left behind: Scaling human-centered machine translation. 
 \emph{arXiv:1902.01382}."
2407.09652,workshop2022bloom,"[{Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon et~al.}]{workshop2022bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, et~al. 2022.",Bloom: A 176b-parameter open-access multilingual language model.,Bloom: A 176b-parameter open-access multilingual language model.,,"[{Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon et~al.}]{workshop2022bloom} BigScience Workshop, Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, et~al. 2022. 
 Bloom: A 176b-parameter open-access multilingual language model. 
 \emph{arXiv preprint arXiv:2211.05100}."
2407.09652,yang2023baichuan,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan et~al.}]{yang2023baichuan} Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, et~al. 2023.",Baichuan 2: Open large-scale language models.,Baichuan 2: Open large-scale language models.,,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan et~al.}]{yang2023baichuan} Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, et~al. 2023. 
 Baichuan 2: Open large-scale language models. 
 \emph{arXiv preprint arXiv:2309.10305}."
2407.09985,abbas2023semdedup,"[{Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos}]{abbas2023semdedup} Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos. 2023.",Semdedup: Data-efficient learning at web-scale through semantic deduplication.,Semdedup: Data-efficient learning at web-scale through semantic deduplication.,,"[{Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos}]{abbas2023semdedup} Amro Abbas, Kushal Tirumala, D{\'a}niel Simig, Surya Ganguli, and Ari~S Morcos. 2023. 
 Semdedup: Data-efficient learning at web-scale through semantic deduplication. 
 \emph{arXiv preprint arXiv:2303.09540}."
2407.09985,chen2024tree,"[{Chen et~al.(2024)Chen, White, Mooney, Payani, Su, and Sun}]{chen2024tree} Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu~Su, and Huan Sun. 2024.",When is tree search useful for llm planning? it depends on the discriminator.,When is tree search useful for llm planning? it depends on the discriminator.,,"[{Chen et~al.(2024)Chen, White, Mooney, Payani, Su, and Sun}]{chen2024tree} Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu~Su, and Huan Sun. 2024. 
 When is tree search useful for llm planning? it depends on the discriminator. 
 \emph{arXiv preprint arXiv:2402.10890}."
2407.09985,chrestien2021heuristic,"[{Chrestien et~al.(2021)Chrestien, Pevny, Komenda, and Edelkamp}]{chrestien2021heuristic} Leah Chrestien, Tomas Pevny, Antonin Komenda, and Stefan Edelkamp. 2021.","Heuristic search planning with deep neural networks using imitation, attention and curriculum learning.","Heuristic search planning with deep neural networks using imitation, attention and curriculum learning.",,"[{Chrestien et~al.(2021)Chrestien, Pevny, Komenda, and Edelkamp}]{chrestien2021heuristic} Leah Chrestien, Tomas Pevny, Antonin Komenda, and Stefan Edelkamp. 2021. 
 Heuristic search planning with deep neural networks using imitation, attention and curriculum learning. 
 \emph{arXiv preprint arXiv:2112.01918}."
2407.09985,dagan2023dynamic,"[{Dagan et~al.(2023)Dagan, Keller, and Lascarides}]{dagan2023dynamic} Gautier Dagan, Frank Keller, and Alex Lascarides. 2023.",Dynamic planning with a llm.,Dynamic planning with a llm.,,"[{Dagan et~al.(2023)Dagan, Keller, and Lascarides}]{dagan2023dynamic} Gautier Dagan, Frank Keller, and Alex Lascarides. 2023. 
 Dynamic planning with a llm. 
 \emph{arXiv preprint arXiv:2308.06391}."
2407.09985,gandhi2024stream,"[{Gandhi et~al.(2024)Gandhi, Lee, Grand, Liu, Cheng, Sharma, and Goodman}]{gandhi2024stream} Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah~D Goodman. 2024.",Stream of search (sos): Learning to search in language.,Stream of search (sos): Learning to search in language.,,"[{Gandhi et~al.(2024)Gandhi, Lee, Grand, Liu, Cheng, Sharma, and Goodman}]{gandhi2024stream} Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah~D Goodman. 2024. 
 Stream of search (sos): Learning to search in language. 
 \emph{arXiv preprint arXiv:2404.03683}."
2407.09985,hao2023reasoning,"[{Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu}]{hao2023reasoning} Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and Zhiting Hu. 2023.",Reasoning with language model is planning with world model.,Reasoning with language model is planning with world model.,,"[{Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu}]{hao2023reasoning} Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and Zhiting Hu. 2023. 
 Reasoning with language model is planning with world model. 
 \emph{arXiv preprint arXiv:2305.14992}."
2407.09985,lehnert2024beyond,"[{Lehnert et~al.(2024)Lehnert, Sukhbaatar, Mcvay, Rabbat, and Tian}]{lehnert2024beyond} Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. 2024.",Beyond a*: Better planning with transformers via search dynamics bootstrapping.,Beyond a*: Better planning with transformers via search dynamics bootstrapping.,,"[{Lehnert et~al.(2024)Lehnert, Sukhbaatar, Mcvay, Rabbat, and Tian}]{lehnert2024beyond} Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. 2024. 
 Beyond a*: Better planning with transformers via search dynamics bootstrapping. 
 \emph{arXiv preprint arXiv:2402.14083}."
2407.09985,liu2023llm+,"[{Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and Stone}]{liu2023llm+} Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023.",Llm+ p: Empowering large language models with optimal planning proficiency.,Llm+ p: Empowering large language models with optimal planning proficiency.,,"[{Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and Stone}]{liu2023llm+} Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. 
 Llm+ p: Empowering large language models with optimal planning proficiency. 
 \emph{arXiv preprint arXiv:2304.11477}."
2407.09985,marion2023less,"[{Marion et~al.(2023)Marion, {\""U}st{\""u}n, Pozzobon, Wang, Fadaee, and Hooker}]{marion2023less} Max Marion, Ahmet {\""U}st{\""u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023.",When less is more: Investigating data pruning for pretraining llms at scale.,When less is more: Investigating data pruning for pretraining llms at scale.,,"[{Marion et~al.(2023)Marion, {\""U}st{\""u}n, Pozzobon, Wang, Fadaee, and Hooker}]{marion2023less} Max Marion, Ahmet {\""U}st{\""u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. 
 When less is more: Investigating data pruning for pretraining llms at scale. 
 \emph{arXiv preprint arXiv:2309.04564}."
2407.09985,orseau2023levin,"[{Orseau et~al.(2023)Orseau, Hutter, and Lelis}]{orseau2023levin} Laurent Orseau, Marcus Hutter, and Levi~HS Lelis. 2023.",Levin tree search with context models.,Levin tree search with context models.,,"[{Orseau et~al.(2023)Orseau, Hutter, and Lelis}]{orseau2023levin} Laurent Orseau, Marcus Hutter, and Levi~HS Lelis. 2023. 
 Levin tree search with context models. 
 \emph{arXiv preprint arXiv:2305.16945}."
2407.09985,vlastelica2019differentiation,"[{Vlastelica et~al.(2019)Vlastelica, Paulus, Musil, Martius, and Rol{\'\i}nek}]{vlastelica2019differentiation} Marin Vlastelica, Anselm Paulus, V{\'\i}t Musil, Georg Martius, and Michal Rol{\'\i}nek. 2019.",Differentiation of blackbox combinatorial solvers.,Differentiation of blackbox combinatorial solvers.,,"[{Vlastelica et~al.(2019)Vlastelica, Paulus, Musil, Martius, and Rol{\'\i}nek}]{vlastelica2019differentiation} Marin Vlastelica, Anselm Paulus, V{\'\i}t Musil, Georg Martius, and Michal Rol{\'\i}nek. 2019. 
 Differentiation of blackbox combinatorial solvers. 
 \emph{arXiv preprint arXiv:1912.02175}."
2407.09985,wang2022self,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}."
2407.09985,yang2023coupling,"[{Yang et~al.(2023)Yang, Ishay, and Lee}]{yang2023coupling} Zhun Yang, Adam Ishay, and Joohyung Lee. 2023.",Coupling large language models with logic programming for robust and general reasoning from text.,Coupling large language models with logic programming for robust and general reasoning from text.,,"[{Yang et~al.(2023)Yang, Ishay, and Lee}]{yang2023coupling} Zhun Yang, Adam Ishay, and Joohyung Lee. 2023. 
 Coupling large language models with logic programming for robust and general reasoning from text. 
 \emph{arXiv preprint arXiv:2307.07696}."
2407.09985,zhou2023language,"[{Zhou et~al.(2023{\natexlab{a}})Zhou, Yan, Shlapentokh-Rothman, Wang, and Wang}]{zhou2023language} Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023{\natexlab{a}}.",Language agent tree search unifies reasoning acting and planning in language models.,Language agent tree search unifies reasoning acting and planning in language models.,,"[{Zhou et~al.(2023{\natexlab{a}})Zhou, Yan, Shlapentokh-Rothman, Wang, and Wang}]{zhou2023language} Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023{\natexlab{a}}. 
 Language agent tree search unifies reasoning acting and planning in language models. 
 \emph{arXiv preprint arXiv:2310.04406}."
2407.09985,zhou2023algorithms,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran}]{zhou2023algorithms} Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. 2023{\natexlab{b}}.",What algorithms can transformers learn? a study in length generalization.,What algorithms can transformers learn? a study in length generalization.,,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran}]{zhou2023algorithms} Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. 2023{\natexlab{b}}. 
 What algorithms can transformers learn? a study in length generalization. 
 \emph{arXiv preprint arXiv:2310.16028}."
2407.1038,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.1038,ali2024prompt,"[{Ali et~al.(2024)Ali, Li, Yang, Cheng, Cao, Huang, Hu, Yu, and Wang}]{ali2024prompt} Muhammad~Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu~Yu, and Di~Wang. 2024.",Prompt-saw: Leveraging relation-aware graphs for textual prompt compression.,Prompt-saw: Leveraging relation-aware graphs for textual prompt compression.,,"[{Ali et~al.(2024)Ali, Li, Yang, Cheng, Cao, Huang, Hu, Yu, and Wang}]{ali2024prompt} Muhammad~Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu~Yu, and Di~Wang. 2024. 
 Prompt-saw: Leveraging relation-aware graphs for textual prompt compression. 
 \emph{arXiv preprint arXiv:2404.00489}."
2407.1038,arora2023have,"[{Arora et~al.(2023)Arora, Singh et~al.}]{arora2023have} Daman Arora, Himanshu~Gaurav Singh, et~al. 2023.",Have llms advanced enough? a challenging problem solving benchmark for large language models.,Have llms advanced enough? a challenging problem solving benchmark for large language models.,,"[{Arora et~al.(2023)Arora, Singh et~al.}]{arora2023have} Daman Arora, Himanshu~Gaurav Singh, et~al. 2023. 
 Have llms advanced enough? a challenging problem solving benchmark for large language models. 
 \emph{arXiv preprint arXiv:2305.15074}."
2407.1038,bai2023qwen,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.",Qwen-vl: A frontier large vision-language model with versatile abilities.,Qwen-vl: A frontier large vision-language model with versatile abilities.,,"[{Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{bai2023qwen} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. 
 Qwen-vl: A frontier large vision-language model with versatile abilities. 
 \emph{arXiv preprint arXiv:2308.12966}."
2407.1038,bubeck2023sparks,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023.",Sparks of artificial general intelligence: Early experiments with gpt-4.,Sparks of artificial general intelligence: Early experiments with gpt-4.,,"[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks} S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al. 2023. 
 Sparks of artificial general intelligence: Early experiments with gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}."
2407.1038,chen2023unleashing,"[{Chen et~al.(2023)Chen, Zhang, Langren{\'e}, and Zhu}]{chen2023unleashing} Banghao Chen, Zhaofeng Zhang, Nicolas Langren{\'e}, and Shengxin Zhu. 2023.",Unleashing the potential of prompt engineering in large language models: a comprehensive review.,Unleashing the potential of prompt engineering in large language models: a comprehensive review.,,"[{Chen et~al.(2023)Chen, Zhang, Langren{\'e}, and Zhu}]{chen2023unleashing} Banghao Chen, Zhaofeng Zhang, Nicolas Langren{\'e}, and Shengxin Zhu. 2023. 
 Unleashing the potential of prompt engineering in large language models: a comprehensive review. 
 \emph{arXiv preprint arXiv:2310.14735}."
2407.1038,dong2024internlm,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang, Duan, Cao et~al.}]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et~al. 2024.",Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.,Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.,,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang, Duan, Cao et~al.}]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et~al. 2024. 
 Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. 
 \emph{arXiv preprint arXiv:2401.16420}."
2407.1038,gupta2023multi,"[{Gupta et~al.(2023)Gupta, Pandya, Kataria, Gupta, and Roth}]{gupta2023multi} Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, and Dan Roth. 2023.",Multi-set inoculation: Assessing model robustness across multiple challenge sets.,Multi-set inoculation: Assessing model robustness across multiple challenge sets.,,"[{Gupta et~al.(2023)Gupta, Pandya, Kataria, Gupta, and Roth}]{gupta2023multi} Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, and Dan Roth. 2023. 
 Multi-set inoculation: Assessing model robustness across multiple challenge sets. 
 \emph{arXiv preprint arXiv:2311.08662}."
2407.1038,he2024olympiadbench,"[{He et~al.(2024)He, Luo, Bai, Hu, Thai, Shen, Hu, Han, Huang, Zhang et~al.}]{he2024olympiadbench} Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen~Leng Thai, Junhao Shen, Jinyi Hu, Xu~Han, Yujie Huang, Yuxiang Zhang, et~al. 2024.",Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.,Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.,,"[{He et~al.(2024)He, Luo, Bai, Hu, Thai, Shen, Hu, Han, Huang, Zhang et~al.}]{he2024olympiadbench} Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen~Leng Thai, Junhao Shen, Jinyi Hu, Xu~Han, Yujie Huang, Yuxiang Zhang, et~al. 2024. 
 Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. 
 \emph{arXiv preprint arXiv:2402.14008}."
2407.1038,he2020pathvqa,"[{He et~al.(2020)He, Zhang, Mou, Xing, and Xie}]{he2020pathvqa} Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020.",Pathvqa: 30000+ questions for medical visual question answering.,Pathvqa: 30000+ questions for medical visual question answering.,,"[{He et~al.(2020)He, Zhang, Mou, Xing, and Xie}]{he2020pathvqa} Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020. 
 Pathvqa: 30000+ questions for medical visual question answering. 
 \emph{arXiv preprint arXiv:2003.10286}."
2407.1038,jiang2024mixtral,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2407.1038,khot2022decomposed,"[{Khot et~al.(2022)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal}]{khot2022decomposed} Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022.",Decomposed prompting: A modular approach for solving complex tasks.,Decomposed prompting: A modular approach for solving complex tasks.,,"[{Khot et~al.(2022)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal}]{khot2022decomposed} Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. 
 Decomposed prompting: A modular approach for solving complex tasks. 
 \emph{arXiv preprint arXiv:2210.02406}."
2407.1038,li2023seed,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}.",Seed-bench: Benchmarking multimodal llms with generative comprehension.,Seed-bench: Benchmarking multimodal llms with generative comprehension.,,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}. 
 Seed-bench: Benchmarking multimodal llms with generative comprehension. 
 \emph{arXiv preprint arXiv:2307.16125}."
2407.1038,liu2023mmbench,"[{Liu et~al.(2023)Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023.",Mmbench: Is your multi-modal model an all-around player?,Mmbench: Is your multi-modal model an all-around player?,,"[{Liu et~al.(2023)Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023. 
 Mmbench: Is your multi-modal model an all-around player? 
 \emph{arXiv preprint arXiv:2307.06281}."
2407.1038,lu2023mathvista,"[{Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023.",Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,,"[{Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. 
 Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 
 \emph{arXiv preprint arXiv:2310.02255}."
2407.1038,reid2024gemini,"[{Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser et~al.}]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 2024.",Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,,"[{Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser et~al.}]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 2024. 
 Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 
 \emph{arXiv preprint arXiv:2403.05530}."
2407.1038,sahoo2024systematic,"[{Sahoo et~al.(2024)Sahoo, Singh, Saha, Jain, Mondal, and Chadha}]{sahoo2024systematic} Pranab Sahoo, Ayush~Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024.",A systematic survey of prompt engineering in large language models: Techniques and applications.,A systematic survey of prompt engineering in large language models: Techniques and applications.,,"[{Sahoo et~al.(2024)Sahoo, Singh, Saha, Jain, Mondal, and Chadha}]{sahoo2024systematic} Pranab Sahoo, Ayush~Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. 
 A systematic survey of prompt engineering in large language models: Techniques and applications. 
 \emph{arXiv preprint arXiv:2402.07927}."
2407.1038,shao2024visual,"[{Shao et~al.(2024)Shao, Qian, Xiao, Song, Zong, Wang, Liu, and Li}]{shao2024visual} Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu~Liu, and Hongsheng Li. 2024.",Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models.,Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models.,,"[{Shao et~al.(2024)Shao, Qian, Xiao, Song, Zong, Wang, Liu, and Li}]{shao2024visual} Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu~Liu, and Hongsheng Li. 2024. 
 Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. 
 \emph{arXiv preprint arXiv:2403.16999}."
2407.1038,shi2023exploring,"[{Shi et~al.(2023)Shi, Peng, Liao, Lin, Chen, Liu, Zhang, and Jin}]{shi2023exploring} Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, and Lianwen Jin. 2023.",Exploring ocr capabilities of gpt-4v (ision): A quantitative and in-depth evaluation.,Exploring ocr capabilities of gpt-4v (ision): A quantitative and in-depth evaluation.,,"[{Shi et~al.(2023)Shi, Peng, Liao, Lin, Chen, Liu, Zhang, and Jin}]{shi2023exploring} Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, and Lianwen Jin. 2023. 
 Exploring ocr capabilities of gpt-4v (ision): A quantitative and in-depth evaluation. 
 \emph{arXiv preprint arXiv:2310.16809}."
2407.1038,srivastava2022beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2407.1038,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2407.1038,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2407.1038,wang2023scibench,"[{Wang et~al.(2023{\natexlab{b}})Wang, Hu, Lu, Zhu, Zhang, Subramaniam, Loomba, Zhang, Sun, and Wang}]{wang2023scibench} Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun~R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023{\natexlab{b}}.",Scibench: Evaluating college-level scientific problem-solving abilities of large language models.,Scibench: Evaluating college-level scientific problem-solving abilities of large language models.,,"[{Wang et~al.(2023{\natexlab{b}})Wang, Hu, Lu, Zhu, Zhang, Subramaniam, Loomba, Zhang, Sun, and Wang}]{wang2023scibench} Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun~R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023{\natexlab{b}}. 
 Scibench: Evaluating college-level scientific problem-solving abilities of large language models. 
 \emph{arXiv preprint arXiv:2307.10635}."
2407.1038,wang2024exploring,"[{Wang et~al.(2024)Wang, Chen, Han, Lin, Zhao, Liu, Zhai, Yuan, You, and Yang}]{wang2024exploring} Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. 2024.",Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning.,Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning.,,"[{Wang et~al.(2024)Wang, Chen, Han, Lin, Zhao, Liu, Zhai, Yuan, You, and Yang}]{wang2024exploring} Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. 2024. 
 Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning. 
 \emph{arXiv preprint arXiv:2401.06805}."
2407.1038,xu2024collage,"[{Xu et~al.(2024)Xu, Wang, Liu, and Xu}]{xu2024collage} Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. 2024.",Collage prompting: Budget-friendly visual recognition with gpt-4v.,Collage prompting: Budget-friendly visual recognition with gpt-4v.,,"[{Xu et~al.(2024)Xu, Wang, Liu, and Xu}]{xu2024collage} Siyu Xu, Yunke Wang, Daochang Liu, and Chang Xu. 2024. 
 Collage prompting: Budget-friendly visual recognition with gpt-4v. 
 \emph{arXiv preprint arXiv:2403.11468}."
2407.1038,zhang2024mathverse,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Jiang, Zhang, Lin, Guo, Qiu, Zhou, Lu, Chang, Gao et~al.}]{zhang2024mathverse} Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et~al. 2024{\natexlab{a}}.",Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?,Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?,,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Jiang, Zhang, Lin, Guo, Qiu, Zhou, Lu, Chang, Gao et~al.}]{zhang2024mathverse} Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et~al. 2024{\natexlab{a}}. 
 Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? 
 \emph{arXiv preprint arXiv:2403.14624}."
2407.1038,zhang2023multimodal,"[{Zhang et~al.(2023)Zhang, Zhang, Li, Zhao, Karypis, and Smola}]{zhang2023multimodal} Zhuosheng Zhang, Aston Zhang, Mu~Li, Hai Zhao, George Karypis, and Alex Smola. 2023.",Multimodal chain-of-thought reasoning in language models.,Multimodal chain-of-thought reasoning in language models.,,"[{Zhang et~al.(2023)Zhang, Zhang, Li, Zhao, Karypis, and Smola}]{zhang2023multimodal} Zhuosheng Zhang, Aston Zhang, Mu~Li, Hai Zhao, George Karypis, and Alex Smola. 2023. 
 Multimodal chain-of-thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2302.00923}."
2407.1038,zhao2023clip4str,"[{Zhao et~al.(2023)Zhao, Quan, Zhu, and Yang}]{zhao2023clip4str} Shuai Zhao, Ruijie Quan, Linchao Zhu, and Yi~Yang. 2023.",Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model.,Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model.,,"[{Zhao et~al.(2023)Zhao, Quan, Zhu, and Yang}]{zhao2023clip4str} Shuai Zhao, Ruijie Quan, Linchao Zhu, and Yi~Yang. 2023. 
 Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model. 
 \emph{arXiv preprint arXiv:2305.14014}."
2407.10817,achiam-etal-2023-gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam-etal-2023-gpt} J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,   J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,https://arxiv.org/abs/2303.08774,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam-etal-2023-gpt} J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,   J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023. 
 URL \url{https://arxiv.org/abs/2303.08774}."
2407.10817,anil-etal-2023-palm,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil-etal-2023-palm} R.~Anil, A.~M. Dai, O.~Firat, M.~Johnson, D.~Lepikhin, A.~Passos, S.~Shakeri,   E.~Taropa, P.~Bailey, Z.~Chen, et~al.",Palm 2 technical report.,Palm 2 technical report.,https://arxiv.org/abs/2305.10403,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil-etal-2023-palm} R.~Anil, A.~M. Dai, O.~Firat, M.~Johnson, D.~Lepikhin, A.~Passos, S.~Shakeri,   E.~Taropa, P.~Bailey, Z.~Chen, et~al. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}, 2023. 
 URL \url{https://arxiv.org/abs/2305.10403}."
2407.10817,askell-etal-2021-general,"[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,   Joseph, Mann, DasSarma, et~al.]{askell-etal-2021-general} A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,   N.~Joseph, B.~Mann, N.~DasSarma, et~al.",A general language assistant as a laboratory for alignment.,A general language assistant as a laboratory for alignment.,https://arxiv.org/abs/2112.00861,"[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,   Joseph, Mann, DasSarma, et~al.]{askell-etal-2021-general} A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,   N.~Joseph, B.~Mann, N.~DasSarma, et~al. 
 A general language assistant as a laboratory for alignment. 
 \emph{arXiv preprint arXiv:2112.00861}, 2021. 
 URL \url{https://arxiv.org/abs/2112.00861}."
2407.10817,bai-etal-2022-training,"[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,   DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai-etal-2022-training} Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain,   S.~Fort, D.~Ganguli, T.~Henighan, et~al.",Training a helpful and harmless assistant with reinforcement learning   from human feedback.,Training a helpful and harmless assistant with reinforcement learning   from human feedback.,https://arxiv.org/abs/2204.05862,"[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,   DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai-etal-2022-training} Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain,   S.~Fort, D.~Ganguli, T.~Henighan, et~al. 
 Training a helpful and harmless assistant with reinforcement learning   from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}. 
 URL \url{https://arxiv.org/abs/2204.05862}."
2407.10817,bai-etal-2022-constitutional,"[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,   Jones, Chen, Goldie, Mirhoseini, McKinnon,   et~al.]{bai-etal-2022-constitutional} Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen,   A.~Goldie, A.~Mirhoseini, C.~McKinnon, et~al.",Constitutional ai: Harmlessness from ai feedback.,Constitutional ai: Harmlessness from ai feedback.,https://arxiv.org/abs/2212.08073,"[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,   Jones, Chen, Goldie, Mirhoseini, McKinnon,   et~al.]{bai-etal-2022-constitutional} Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen,   A.~Goldie, A.~Mirhoseini, C.~McKinnon, et~al. 
 Constitutional ai: Harmlessness from ai feedback. 
 \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}. 
 URL \url{https://arxiv.org/abs/2212.08073}."
2407.10817,bubeck-etal-2023-sparks,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,   Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck-etal-2023-sparks} S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar,   P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, et~al.",Sparks of artificial general intelligence: Early experiments with   gpt-4.,Sparks of artificial general intelligence: Early experiments with   gpt-4.,arXiv preprint arXiv:2303.12712,"[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,   Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck-etal-2023-sparks} S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar,   P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, et~al. 
 Sparks of artificial general intelligence: Early experiments with   gpt-4. 
 \emph{arXiv preprint arXiv:2303.12712}, 2023. 
 URL \url{arXiv preprint arXiv:2303.12712}."
2407.10817,chen-etal-2021-evaluating,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen-etal-2021-evaluating} M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. D.~O. Pinto, J.~Kaplan, H.~Edwards,   Y.~Burda, N.~Joseph, G.~Brockman, et~al.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,https://arxiv.org/abs/2107.03374,"[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,   Burda, Joseph, Brockman, et~al.]{chen-etal-2021-evaluating} M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. D.~O. Pinto, J.~Kaplan, H.~Edwards,   Y.~Burda, N.~Joseph, G.~Brockman, et~al. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}, 2021. 
 URL \url{https://arxiv.org/abs/2107.03374}."
2407.10817,chiang-etal-2024-chatbot,"[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang,   Zhu, Jordan, Gonzalez, et~al.]{chiang-etal-2024-chatbot} W.-L. Chiang, L.~Zheng, Y.~Sheng, A.~N. Angelopoulos, T.~Li, D.~Li, H.~Zhang,   B.~Zhu, M.~Jordan, J.~E. Gonzalez, et~al.",Chatbot arena: An open platform for evaluating llms by human   preference.,Chatbot arena: An open platform for evaluating llms by human   preference.,https://arxiv.org/abs/2403.04132,"[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang,   Zhu, Jordan, Gonzalez, et~al.]{chiang-etal-2024-chatbot} W.-L. Chiang, L.~Zheng, Y.~Sheng, A.~N. Angelopoulos, T.~Li, D.~Li, H.~Zhang,   B.~Zhu, M.~Jordan, J.~E. Gonzalez, et~al. 
 Chatbot arena: An open platform for evaluating llms by human   preference. 
 \emph{arXiv preprint arXiv:2403.04132}, 2024. 
 URL \url{https://arxiv.org/abs/2403.04132}."
2407.10817,chowdhery-etal-2022-scaling,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,   Roberts, Barham, Chung, Sutton, Gehrmann,   et~al.]{chowdhery-etal-2022-scaling} A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,   H.~W. Chung, C.~Sutton, S.~Gehrmann, et~al.",Pa{LM}: Scaling language modeling with pathways.,Pa{LM}: Scaling language modeling with pathways.,https://arxiv.org/abs/2204.02311,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,   Roberts, Barham, Chung, Sutton, Gehrmann,   et~al.]{chowdhery-etal-2022-scaling} A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,   H.~W. Chung, C.~Sutton, S.~Gehrmann, et~al. 
 Pa{LM}: Scaling language modeling with pathways. 
 \emph{arXiv preprint arXiv:2204.02311}, 2022. 
 URL \url{https://arxiv.org/abs/2204.02311}."
2407.10817,dubois-etal-2024-length,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and   Hashimoto]{dubois-etal-2024-length} Y.~Dubois, B.~Galambosi, P.~Liang, and T.~B. Hashimoto.",Length-controlled alpacaeval: A simple way to debias automatic   evaluators.,Length-controlled alpacaeval: A simple way to debias automatic   evaluators.,https://arxiv.org/abs/2404.04475,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and   Hashimoto]{dubois-etal-2024-length} Y.~Dubois, B.~Galambosi, P.~Liang, and T.~B. Hashimoto. 
 Length-controlled alpacaeval: A simple way to debias automatic   evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}, 2024. 
 URL \url{https://arxiv.org/abs/2404.04475}."
2407.10817,ganguli-etal-2022-red,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath,   Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli-etal-2022-red} D.~Ganguli, L.~Lovitt, J.~Kernion, A.~Askell, Y.~Bai, S.~Kadavath, B.~Mann,   E.~Perez, N.~Schiefer, K.~Ndousse, et~al.","Red teaming language models to reduce harms: Methods, scaling   behaviors, and lessons learned.","Red teaming language models to reduce harms: Methods, scaling   behaviors, and lessons learned.",https://arxiv.org/abs/2209.07858,"[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath,   Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli-etal-2022-red} D.~Ganguli, L.~Lovitt, J.~Kernion, A.~Askell, Y.~Bai, S.~Kadavath, B.~Mann,   E.~Perez, N.~Schiefer, K.~Ndousse, et~al. 
 Red teaming language models to reduce harms: Methods, scaling   behaviors, and lessons learned. 
 \emph{arXiv preprint arXiv:2209.07858}, 2022. 
 URL \url{https://arxiv.org/abs/2209.07858}."
2407.10817,gemini-team-2023-gemini,"[Gemini et~al.(2023)Gemini, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{gemini-team-2023-gemini} T.~Gemini, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut,   J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,https://arxiv.org/abs/2312.11805,"[Gemini et~al.(2023)Gemini, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{gemini-team-2023-gemini} T.~Gemini, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut,   J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023. 
 URL \url{https://arxiv.org/abs/2312.11805}."
2407.10817,goyal-etal-2022-news,"[Goyal et~al.(2022)Goyal, Li, and Durrett]{goyal-etal-2022-news} T.~Goyal, J.~J. Li, and G.~Durrett.",News summarization and evaluation in the era of gpt-3.,News summarization and evaluation in the era of gpt-3.,https://arxiv.org/abs/2209.12356,"[Goyal et~al.(2022)Goyal, Li, and Durrett]{goyal-etal-2022-news} T.~Goyal, J.~J. Li, and G.~Durrett. 
 News summarization and evaluation in the era of gpt-3. 
 \emph{arXiv preprint arXiv:2209.12356}, 2022. 
 URL \url{https://arxiv.org/abs/2209.12356}."
2407.10817,gudibande-etal-2023-the,"[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel,   Levine, and Song]{gudibande-etal-2023-the} A.~Gudibande, E.~Wallace, C.~Snell, X.~Geng, H.~Liu, P.~Abbeel, S.~Levine, and   D.~Song.",The false promise of imitating proprietary llms.,The false promise of imitating proprietary llms.,https://arxiv.org/abs/2305.15717,"[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel,   Levine, and Song]{gudibande-etal-2023-the} A.~Gudibande, E.~Wallace, C.~Snell, X.~Geng, H.~Liu, P.~Abbeel, S.~Levine, and   D.~Song. 
 The false promise of imitating proprietary llms. 
 \emph{arXiv preprint arXiv:2305.15717}, 2023. 
 URL \url{https://arxiv.org/abs/2305.15717}."
2407.10817,ivison-etal-2023-camels,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi,   Jang, Wadden, Smith, Beltagy, et~al.]{ivison-etal-2023-camels} H.~Ivison, Y.~Wang, V.~Pyatkin, N.~Lambert, M.~Peters, P.~Dasigi, J.~Jang,   D.~Wadden, N.~A. Smith, I.~Beltagy, et~al.",Camels in a changing climate: Enhancing lm adaptation with tulu 2.,Camels in a changing climate: Enhancing lm adaptation with tulu 2.,https://arxiv.org/abs/2311.10702,"[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi,   Jang, Wadden, Smith, Beltagy, et~al.]{ivison-etal-2023-camels} H.~Ivison, Y.~Wang, V.~Pyatkin, N.~Lambert, M.~Peters, P.~Dasigi, J.~Jang,   D.~Wadden, N.~A. Smith, I.~Beltagy, et~al. 
 Camels in a changing climate: Enhancing lm adaptation with tulu 2. 
 \emph{arXiv preprint arXiv:2311.10702}, 2023. 
 URL \url{https://arxiv.org/abs/2311.10702}."
2407.10817,jiang-etal-2024-mixtral,"[Jiang et~al.(2024{\natexlab{a}})Jiang, Sablayrolles, Roux, Mensch,   Savary, Bamford, Chaplot, Casas, Hanna, Bressand,   et~al.]{jiang-etal-2024-mixtral} A.~Q. Jiang, A.~Sablayrolles, A.~Roux, A.~Mensch, B.~Savary, C.~Bamford, D.~S.   Chaplot, D.~d.~l. Casas, E.~B. Hanna, F.~Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,https://arxiv.org/abs/2401.04088,"[Jiang et~al.(2024{\natexlab{a}})Jiang, Sablayrolles, Roux, Mensch,   Savary, Bamford, Chaplot, Casas, Hanna, Bressand,   et~al.]{jiang-etal-2024-mixtral} A.~Q. Jiang, A.~Sablayrolles, A.~Roux, A.~Mensch, B.~Savary, C.~Bamford, D.~S.   Chaplot, D.~d.~l. Casas, E.~B. Hanna, F.~Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024{\natexlab{a}}. 
 URL \url{https://arxiv.org/abs/2401.04088}."
2407.10817,karpinska-etal-2024-one,"[Karpinska et~al.(2024)Karpinska, Thai, Lo, Goyal, and   Iyyer]{karpinska-etal-2024-one} M.~Karpinska, K.~Thai, K.~Lo, T.~Goyal, and M.~Iyyer.","One thousand and one pairs: A"" novel"" challenge for long-context   language models.","One thousand and one pairs: A"" novel"" challenge for long-context   language models.",https://arxiv.org/abs/2406.16264,"[Karpinska et~al.(2024)Karpinska, Thai, Lo, Goyal, and   Iyyer]{karpinska-etal-2024-one} M.~Karpinska, K.~Thai, K.~Lo, T.~Goyal, and M.~Iyyer. 
 One thousand and one pairs: A"" novel"" challenge for long-context   language models. 
 \emph{arXiv preprint arXiv:2406.16264}, 2024. 
 URL \url{https://arxiv.org/abs/2406.16264}."
2407.10817,kim-etal-2024-prometheus-2,"[Kim et~al.(2024{\natexlab{b}})Kim, Suk, Longpre, Lin, Shin, Welleck,   Neubig, Lee, Lee, and Seo]{kim-etal-2024-prometheus-2} S.~Kim, J.~Suk, S.~Longpre, B.~Y. Lin, J.~Shin, S.~Welleck, G.~Neubig, M.~Lee,   K.~Lee, and M.~Seo.",Prometheus 2: An open source language model specialized in evaluating   other language models.,Prometheus 2: An open source language model specialized in evaluating   other language models.,https://arxiv.org/abs/2405.01535,"[Kim et~al.(2024{\natexlab{b}})Kim, Suk, Longpre, Lin, Shin, Welleck,   Neubig, Lee, Lee, and Seo]{kim-etal-2024-prometheus-2} S.~Kim, J.~Suk, S.~Longpre, B.~Y. Lin, J.~Shin, S.~Welleck, G.~Neubig, M.~Lee,   K.~Lee, and M.~Seo. 
 Prometheus 2: An open source language model specialized in evaluating   other language models. 
 \emph{arXiv preprint arXiv:2405.01535}, 2024{\natexlab{b}}. 
 URL \url{https://arxiv.org/abs/2405.01535}."
2407.10817,kim-etal-2024-fables,"[Kim et~al.(2024{\natexlab{c}})Kim, Chang, Karpinska, Garimella,   Manjunatha, Lo, Goyal, and Iyyer]{kim-etal-2024-fables} Y.~Kim, Y.~Chang, M.~Karpinska, A.~Garimella, V.~Manjunatha, K.~Lo, T.~Goyal,   and M.~Iyyer.",Fables: Evaluating faithfulness and content selection in book-length   summarization.,Fables: Evaluating faithfulness and content selection in book-length   summarization.,https://arxiv.org/abs/2404.01261,"[Kim et~al.(2024{\natexlab{c}})Kim, Chang, Karpinska, Garimella,   Manjunatha, Lo, Goyal, and Iyyer]{kim-etal-2024-fables} Y.~Kim, Y.~Chang, M.~Karpinska, A.~Garimella, V.~Manjunatha, K.~Lo, T.~Goyal,   and M.~Iyyer. 
 Fables: Evaluating faithfulness and content selection in book-length   summarization. 
 \emph{arXiv preprint arXiv:2404.01261}, 2024{\natexlab{c}}. 
 URL \url{https://arxiv.org/abs/2404.01261}."
2407.10817,koo-etal-2023-benchmarking,"[Koo et~al.(2023)Koo, Lee, Raheja, Park, Kim, and   Kang]{koo-etal-2023-benchmarking} R.~Koo, M.~Lee, V.~Raheja, J.~I. Park, Z.~M. Kim, and D.~Kang.",Benchmarking cognitive biases in large language models as evaluators.,Benchmarking cognitive biases in large language models as evaluators.,https://arxiv.org/abs/2309.17012,"[Koo et~al.(2023)Koo, Lee, Raheja, Park, Kim, and   Kang]{koo-etal-2023-benchmarking} R.~Koo, M.~Lee, V.~Raheja, J.~I. Park, Z.~M. Kim, and D.~Kang. 
 Benchmarking cognitive biases in large language models as evaluators. 
 \emph{arXiv preprint arXiv:2309.17012}, 2023. 
 URL \url{https://arxiv.org/abs/2309.17012}."
2407.10817,lambert-etal-2024-rewardbench,"[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu,   Dziri, Kumar, Zick, Choi, et~al.]{lambert-etal-2024-rewardbench} N.~Lambert, V.~Pyatkin, J.~Morrison, L.~Miranda, B.~Y. Lin, K.~Chandu,   N.~Dziri, S.~Kumar, T.~Zick, Y.~Choi, et~al.",Rewardbench: Evaluating reward models for language modeling.,Rewardbench: Evaluating reward models for language modeling.,https://arxiv.org/abs/2403.13787,"[Lambert et~al.(2024)Lambert, Pyatkin, Morrison, Miranda, Lin, Chandu,   Dziri, Kumar, Zick, Choi, et~al.]{lambert-etal-2024-rewardbench} N.~Lambert, V.~Pyatkin, J.~Morrison, L.~Miranda, B.~Y. Lin, K.~Chandu,   N.~Dziri, S.~Kumar, T.~Zick, Y.~Choi, et~al. 
 Rewardbench: Evaluating reward models for language modeling. 
 \emph{arXiv preprint arXiv:2403.13787}, 2024. 
 URL \url{https://arxiv.org/abs/2403.13787}."
2407.10817,lin-etal-2024-wildbench,"[Lin et~al.(2024)Lin, Deng, Chandu, Brahman, Ravichander, Pyatkin,   Dziri, Bras, and Choi]{lin-etal-2024-wildbench} B.~Y. Lin, Y.~Deng, K.~Chandu, F.~Brahman, A.~Ravichander, V.~Pyatkin,   N.~Dziri, R.~L. Bras, and Y.~Choi.",Wildbench: Benchmarking llms with challenging tasks from real users   in the wild.,Wildbench: Benchmarking llms with challenging tasks from real users   in the wild.,https://arxiv.org/abs/2406.04770,"[Lin et~al.(2024)Lin, Deng, Chandu, Brahman, Ravichander, Pyatkin,   Dziri, Bras, and Choi]{lin-etal-2024-wildbench} B.~Y. Lin, Y.~Deng, K.~Chandu, F.~Brahman, A.~Ravichander, V.~Pyatkin,   N.~Dziri, R.~L. Bras, and Y.~Choi. 
 Wildbench: Benchmarking llms with challenging tasks from real users   in the wild. 
 \emph{arXiv preprint arXiv:2406.04770}, 2024. 
 URL \url{https://arxiv.org/abs/2406.04770}."
2407.10817,liu-etal-2023-llms,"[Liu et~al.(2023{\natexlab{b}})Liu, Moosavi, and   Lin]{liu-etal-2023-llms} Y.~Liu, N.~S. Moosavi, and C.~Lin.",Llms as narcissistic evaluators: When ego inflates evaluation scores.,Llms as narcissistic evaluators: When ego inflates evaluation scores.,https://arxiv.org/abs/2311.09766,"[Liu et~al.(2023{\natexlab{b}})Liu, Moosavi, and   Lin]{liu-etal-2023-llms} Y.~Liu, N.~S. Moosavi, and C.~Lin. 
 Llms as narcissistic evaluators: When ego inflates evaluation scores. 
 \emph{arXiv preprint arXiv:2311.09766}, 2023{\natexlab{b}}. 
 URL \url{https://arxiv.org/abs/2311.09766}."
2407.10817,moon-etal-2023-coffee,"[Moon et~al.(2023)Moon, Song, Chae, Kang, Kwon, Ong, Hwang, and   Yeo]{moon-etal-2023-coffee} S.~Moon, Y.~Song, H.~Chae, D.~Kang, T.~Kwon, K.~T.-i. Ong, S.-w. Hwang, and   J.~Yeo.",Coffee: Boost your code llms by fixing bugs with feedback.,Coffee: Boost your code llms by fixing bugs with feedback.,https://arxiv.org/abs/2311.07215,"[Moon et~al.(2023)Moon, Song, Chae, Kang, Kwon, Ong, Hwang, and   Yeo]{moon-etal-2023-coffee} S.~Moon, Y.~Song, H.~Chae, D.~Kang, T.~Kwon, K.~T.-i. Ong, S.-w. Hwang, and   J.~Yeo. 
 Coffee: Boost your code llms by fixing bugs with feedback. 
 \emph{arXiv preprint arXiv:2311.07215}, 2023. 
 URL \url{https://arxiv.org/abs/2311.07215}."
2407.10817,muennighoff-etal-2023-octopack,"[Muennighoff et~al.(2023)Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo,   Singh, Tang, Von~Werra, and Longpre]{muennighoff-etal-2023-octopack} N.~Muennighoff, Q.~Liu, A.~Zebaze, Q.~Zheng, B.~Hui, T.~Y. Zhuo, S.~Singh,   X.~Tang, L.~Von~Werra, and S.~Longpre.",Octopack: Instruction tuning code large language models.,Octopack: Instruction tuning code large language models.,https://arxiv.org/abs/2308.07124,"[Muennighoff et~al.(2023)Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo,   Singh, Tang, Von~Werra, and Longpre]{muennighoff-etal-2023-octopack} N.~Muennighoff, Q.~Liu, A.~Zebaze, Q.~Zheng, B.~Hui, T.~Y. Zhuo, S.~Singh,   X.~Tang, L.~Von~Werra, and S.~Longpre. 
 Octopack: Instruction tuning code large language models. 
 \emph{arXiv preprint arXiv:2308.07124}, 2023. 
 URL \url{https://arxiv.org/abs/2308.07124}."
2407.10817,nakano-etal-2021-webgpt,"[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,   Jain, Kosaraju, Saunders, et~al.]{nakano-etal-2021-webgpt} R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,   V.~Kosaraju, W.~Saunders, et~al.",Webgpt: Browser-assisted question-answering with human feedback.,Webgpt: Browser-assisted question-answering with human feedback.,https://arxiv.org/abs/2112.09332,"[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,   Jain, Kosaraju, Saunders, et~al.]{nakano-etal-2021-webgpt} R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,   V.~Kosaraju, W.~Saunders, et~al. 
 Webgpt: Browser-assisted question-answering with human feedback. 
 \emph{arXiv preprint arXiv:2112.09332}, 2021. 
 URL \url{https://arxiv.org/abs/2112.09332}."
2407.10817,panickssery-etal-2024-llm,"[Panickssery et~al.(2024)Panickssery, Bowman, and   Feng]{panickssery-etal-2024-llm} A.~Panickssery, S.~R. Bowman, and S.~Feng.",Llm evaluators recognize and favor their own generations.,Llm evaluators recognize and favor their own generations.,https://arxiv.org/abs/2404.13076,"[Panickssery et~al.(2024)Panickssery, Bowman, and   Feng]{panickssery-etal-2024-llm} A.~Panickssery, S.~R. Bowman, and S.~Feng. 
 Llm evaluators recognize and favor their own generations. 
 \emph{arXiv preprint arXiv:2404.13076}, 2024. 
 URL \url{https://arxiv.org/abs/2404.13076}."
2407.10817,reid-etal-2024-gemini,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap,   Alayrac, Soricut, Lazaridou, Firat, Schrittwieser,   et~al.]{reid-etal-2024-gemini} M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac,   R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al.",Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context.,Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context.,https://arxiv.org/abs/2403.05530,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap,   Alayrac, Soricut, Lazaridou, Firat, Schrittwieser,   et~al.]{reid-etal-2024-gemini} M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac,   R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al. 
 Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context. 
 \emph{arXiv preprint arXiv:2403.05530}, 2024. 
 URL \url{https://arxiv.org/abs/2403.05530}."
2407.10817,su-etal-2022-empirical,[Su and Xu(2022)]{su-etal-2022-empirical} Y.~Su and J.~Xu.,An empirical study on contrastive search and contrastive decoding for   open-ended text generation.,An empirical study on contrastive search and contrastive decoding for   open-ended text generation.,https://arxiv.org/abs/2211.10797,"[Su and Xu(2022)]{su-etal-2022-empirical} Y.~Su and J.~Xu. 
 An empirical study on contrastive search and contrastive decoding for   open-ended text generation. 
 \emph{arXiv preprint arXiv:2211.10797}, 2022. 
 URL \url{https://arxiv.org/abs/2211.10797}."
2407.10817,tang-etal-2024-minicheck,"[Tang et~al.(2024)Tang, Laban, and Durrett]{tang-etal-2024-minicheck} L.~Tang, P.~Laban, and G.~Durrett.",Minicheck: Efficient fact-checking of llms on grounding documents.,Minicheck: Efficient fact-checking of llms on grounding documents.,https://arxiv.org/abs/2404.10774,"[Tang et~al.(2024)Tang, Laban, and Durrett]{tang-etal-2024-minicheck} L.~Tang, P.~Laban, and G.~Durrett. 
 Minicheck: Efficient fact-checking of llms on grounding documents. 
 \emph{arXiv preprint arXiv:2404.10774}, 2024. 
 URL \url{https://arxiv.org/abs/2404.10774}."
2407.10817,vu-etal-2023-fresh,"[Vu et~al.(2023)Vu, Iyyer, Wang, Constant, Wei, Wei, Tar, Sung, Zhou,   Le, et~al.]{vu-etal-2023-fresh} T.~Vu, M.~Iyyer, X.~Wang, N.~Constant, J.~Wei, J.~Wei, C.~Tar, Y.-H. Sung,   D.~Zhou, Q.~Le, et~al.",Freshllms: Refreshing large language models with search engine   augmentation.,Freshllms: Refreshing large language models with search engine   augmentation.,https://arxiv.org/abs/2310.03214,"[Vu et~al.(2023)Vu, Iyyer, Wang, Constant, Wei, Wei, Tar, Sung, Zhou,   Le, et~al.]{vu-etal-2023-fresh} T.~Vu, M.~Iyyer, X.~Wang, N.~Constant, J.~Wei, J.~Wei, C.~Tar, Y.-H. Sung,   D.~Zhou, Q.~Le, et~al. 
 Freshllms: Refreshing large language models with search engine   augmentation. 
 \emph{arXiv preprint arXiv:2310.03214}, 2023. 
 URL \url{https://arxiv.org/abs/2310.03214}."
2407.10817,wang-etal-2023-helpsteer,"[Wang et~al.(2023{\natexlab{b}})Wang, Dong, Zeng, Adams, Sreedhar,   Egert, Delalleau, Scowcroft, Kant, Swope, et~al.]{wang-etal-2023-helpsteer} Z.~Wang, Y.~Dong, J.~Zeng, V.~Adams, M.~N. Sreedhar, D.~Egert, O.~Delalleau,   J.~P. Scowcroft, N.~Kant, A.~Swope, et~al.",Helpsteer: Multi-attribute helpfulness dataset for steerlm.,Helpsteer: Multi-attribute helpfulness dataset for steerlm.,https://arxiv.org/abs/2311.09528,"[Wang et~al.(2023{\natexlab{b}})Wang, Dong, Zeng, Adams, Sreedhar,   Egert, Delalleau, Scowcroft, Kant, Swope, et~al.]{wang-etal-2023-helpsteer} Z.~Wang, Y.~Dong, J.~Zeng, V.~Adams, M.~N. Sreedhar, D.~Egert, O.~Delalleau,   J.~P. Scowcroft, N.~Kant, A.~Swope, et~al. 
 Helpsteer: Multi-attribute helpfulness dataset for steerlm. 
 \emph{arXiv preprint arXiv:2311.09528}, 2023{\natexlab{b}}. 
 URL \url{https://arxiv.org/abs/2311.09528}."
2407.10817,wang-etal-2024-helpsteer2,"[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang,   Sreedhar, and Kuchaiev]{wang-etal-2024-helpsteer2} Z.~Wang, Y.~Dong, O.~Delalleau, J.~Zeng, G.~Shen, D.~Egert, J.~J. Zhang, M.~N.   Sreedhar, and O.~Kuchaiev.",Helpsteer2: Open-source dataset for training top-performing reward   models.,Helpsteer2: Open-source dataset for training top-performing reward   models.,https://arxiv.org/abs/2406.08673,"[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang,   Sreedhar, and Kuchaiev]{wang-etal-2024-helpsteer2} Z.~Wang, Y.~Dong, O.~Delalleau, J.~Zeng, G.~Shen, D.~Egert, J.~J. Zhang, M.~N.   Sreedhar, and O.~Kuchaiev. 
 Helpsteer2: Open-source dataset for training top-performing reward   models. 
 \emph{arXiv preprint arXiv:2406.08673}, 2024. 
 URL \url{https://arxiv.org/abs/2406.08673}."
2407.10817,wei-etal-2024-long,"[Wei et~al.(2024)Wei, Yang, Song, Lu, Hu, Tran, Peng, Liu, Huang, Du,   et~al.]{wei-etal-2024-long} J.~Wei, C.~Yang, X.~Song, Y.~Lu, N.~Hu, D.~Tran, D.~Peng, R.~Liu, D.~Huang,   C.~Du, et~al.",Long-form factuality in large language models.,Long-form factuality in large language models.,https://arxiv.org/abs/2403.18802,"[Wei et~al.(2024)Wei, Yang, Song, Lu, Hu, Tran, Peng, Liu, Huang, Du,   et~al.]{wei-etal-2024-long} J.~Wei, C.~Yang, X.~Song, Y.~Lu, N.~Hu, D.~Tran, D.~Peng, R.~Liu, D.~Huang,   C.~Du, et~al. 
 Long-form factuality in large language models. 
 \emph{arXiv preprint arXiv:2403.18802}, 2024. 
 URL \url{https://arxiv.org/abs/2403.18802}."
2407.10817,wu-etal-2023-ragtruth,"[Wu et~al.(2023{\natexlab{a}})Wu, Zhu, Xu, Shum, Niu, Zhong, Song, and   Zhang]{wu-etal-2023-ragtruth} Y.~Wu, J.~Zhu, S.~Xu, K.~Shum, C.~Niu, R.~Zhong, J.~Song, and T.~Zhang.",Ragtruth: A hallucination corpus for developing trustworthy   retrieval-augmented language models.,Ragtruth: A hallucination corpus for developing trustworthy   retrieval-augmented language models.,https://arxiv.org/abs/2401.00396,"[Wu et~al.(2023{\natexlab{a}})Wu, Zhu, Xu, Shum, Niu, Zhong, Song, and   Zhang]{wu-etal-2023-ragtruth} Y.~Wu, J.~Zhu, S.~Xu, K.~Shum, C.~Niu, R.~Zhong, J.~Song, and T.~Zhang. 
 Ragtruth: A hallucination corpus for developing trustworthy   retrieval-augmented language models. 
 \emph{arXiv preprint arXiv:2401.00396}, 2023{\natexlab{a}}. 
 URL \url{https://arxiv.org/abs/2401.00396}."
2407.10817,zhao-etal-2023-slic,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and   Liu]{zhao-etal-2023-slic} Y.~Zhao, R.~Joshi, T.~Liu, M.~Khalman, M.~Saleh, and P.~J. Liu.",Slic-hf: Sequence likelihood calibration with human feedback.,Slic-hf: Sequence likelihood calibration with human feedback.,https://arxiv.org/abs/2305.10425,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and   Liu]{zhao-etal-2023-slic} Y.~Zhao, R.~Joshi, T.~Liu, M.~Khalman, M.~Saleh, and P.~J. Liu. 
 Slic-hf: Sequence likelihood calibration with human feedback. 
 \emph{arXiv preprint arXiv:2305.10425}, 2023. 
 URL \url{https://arxiv.org/abs/2305.10425}."
2407.10969,minillm,"[GDWH23]{minillm} Yuxian Gu, Li~Dong, Furu Wei, and Minlie Huang.",Knowledge distillation of large language models.,Knowledge distillation of large language models.,,"[GDWH23]{minillm} Yuxian Gu, Li~Dong, Furu Wei, and Minlie Huang. 
 Knowledge distillation of large language models. 
 {\em arXiv preprint arXiv:2306.08543}, 2023."
2407.10969,turbosparse,"[SXZ{\etalchar{+}}24]{turbosparse} Yixin Song, Haotong Xie, Zhengyan Zhang, Bo~Wen, Li~Ma, Zeyu Mi, and Haibo Chen.",Turbo sparse: Achieving llm sota performance with minimal activated parameters.,Turbo sparse: Achieving llm sota performance with minimal activated parameters.,,"[SXZ{\etalchar{+}}24]{turbosparse} Yixin Song, Haotong Xie, Zhengyan Zhang, Bo~Wen, Li~Ma, Zeyu Mi, and Haibo Chen. 
 Turbo sparse: Achieving llm sota performance with minimal activated parameters. 
 {\em arXiv preprint arXiv:2406.05955}, 2024."
2407.11681,chavan2023oneforall,"[\protect\citeauthoryear{Chavan \bgroup \em et al.\egroup }{2023}]{chavan2023oneforall} Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen.",{One-for-All}: Generalized lora for parameter-efficient fine-tuning.,{One-for-All}: Generalized lora for parameter-efficient fine-tuning.,,"[\protect\citeauthoryear{Chavan \bgroup \em et al.\egroup }{2023}]{chavan2023oneforall} Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. 
 {One-for-All}: Generalized lora for parameter-efficient fine-tuning. 
 {\em arXiv preprint arXiv:2306.07967}, 2023."
2407.11681,cheng2023survey,"[\protect\citeauthoryear{Cheng \bgroup \em et al.\egroup }{2023}]{cheng2023survey} Hongrong Cheng, Miao Zhang, and Javen~Qinfeng Shi.","A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations.","A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations.",,"[\protect\citeauthoryear{Cheng \bgroup \em et al.\egroup }{2023}]{cheng2023survey} Hongrong Cheng, Miao Zhang, and Javen~Qinfeng Shi. 
 A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations. 
 {\em arXiv preprint arXiv:2308.06767}, 2023."
2407.11681,dettmers2023qlora,"[\protect\citeauthoryear{Dettmers \bgroup \em et al.\egroup }{2023}]{dettmers2023qlora} Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.",{QLoRA}: Efficient finetuning of quantized {LLMs}.,{QLoRA}: Efficient finetuning of quantized {LLMs}.,,"[\protect\citeauthoryear{Dettmers \bgroup \em et al.\egroup }{2023}]{dettmers2023qlora} Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 
 {QLoRA}: Efficient finetuning of quantized {LLMs}. 
 {\em arXiv preprint arXiv:2305.14314}, 2023."
2407.11681,frantar2023sparsegpt,[\protect\citeauthoryear{Frantar and Alistarh}{2023}]{frantar2023sparsegpt} Elias Frantar and Dan Alistarh.,{SparseGPT}: Massive language models can be accurately pruned in one-shot.,{SparseGPT}: Massive language models can be accurately pruned in one-shot.,,"[\protect\citeauthoryear{Frantar and Alistarh}{2023}]{frantar2023sparsegpt} Elias Frantar and Dan Alistarh. 
 {SparseGPT}: Massive language models can be accurately pruned in one-shot. 
 {\em arXiv preprint arXiv:2301.00774}, 2023."
2407.11681,gasnikov2022randomized,"[\protect\citeauthoryear{Gasnikov \bgroup \em et al.\egroup }{2022}]{gasnikov2022randomized} Alexander Gasnikov, Darina Dvinskikh, Pavel Dvurechensky, Eduard Gorbunov, Aleksander Beznosikov, and Alexander Lobanovu.",Randomized gradient-free methods in convex optimization.,Randomized gradient-free methods in convex optimization.,,"[\protect\citeauthoryear{Gasnikov \bgroup \em et al.\egroup }{2022}]{gasnikov2022randomized} Alexander Gasnikov, Darina Dvinskikh, Pavel Dvurechensky, Eduard Gorbunov, Aleksander Beznosikov, and Alexander Lobanovu. 
 Randomized gradient-free methods in convex optimization. 
 {\em arXiv preprint arXiv:2211.13566}, 2022."
2407.11681,li2022simultaneous,"[\protect\citeauthoryear{Li \bgroup \em et al.\egroup }{2022a}]{li2022simultaneous} Shiru Li, Yong Xia, and Zi~Xu.",Simultaneous perturbation stochastic approximation: towards one-measurement per iteration.,Simultaneous perturbation stochastic approximation: towards one-measurement per iteration.,,"[\protect\citeauthoryear{Li \bgroup \em et al.\egroup }{2022a}]{li2022simultaneous} Shiru Li, Yong Xia, and Zi~Xu. 
 Simultaneous perturbation stochastic approximation: towards one-measurement per iteration. 
 {\em arXiv preprint arXiv:2203.03075}, 2022."
2407.11681,merity2016pointer,"[\protect\citeauthoryear{Merity \bgroup \em et al.\egroup }{2016}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.",Pointer sentinel mixture models.,Pointer sentinel mixture models.,,"[\protect\citeauthoryear{Merity \bgroup \em et al.\egroup }{2016}]{merity2016pointer} Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 
 Pointer sentinel mixture models. 
 {\em arXiv preprint arXiv:1609.07843}, 2016."
2407.11681,open2023gpt4,[\protect\citeauthoryear{OpenAI}{2023}]{open2023gpt4} OpenAI.,{GPT-4} technical report.,{GPT-4} technical report.,,"[\protect\citeauthoryear{OpenAI}{2023}]{open2023gpt4} OpenAI. 
 {GPT-4} technical report. 
 {\em arXiv preprint arXiv:2303.08774}, 2023."
2407.11681,touvron2023llama,"[\protect\citeauthoryear{Touvron \bgroup \em et al.\egroup }{2023}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, et~al.",{LLaMA}: Open and efficient foundation language models.,{LLaMA}: Open and efficient foundation language models.,,"[\protect\citeauthoryear{Touvron \bgroup \em et al.\egroup }{2023}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, et~al. 
 {LLaMA}: Open and efficient foundation language models. 
 {\em arXiv preprint arXiv:2302.13971}, 2023."
2407.11681,workshop2023bloom,[\protect\citeauthoryear{Workshop}{2023}]{workshop2023bloom} BigScience Workshop.,{BLOOM}: A 176b-parameter open-access multilingual language model.,{BLOOM}: A 176b-parameter open-access multilingual language model.,,"[\protect\citeauthoryear{Workshop}{2023}]{workshop2023bloom} BigScience Workshop. 
 {BLOOM}: A 176b-parameter open-access multilingual language model. 
 {\em arXiv preprint arXiv:2211.05100}, 2023."
2407.11681,wu2023survey,"[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup }{2023}]{wu2023survey} Likang Wu, Zhi Zheng, Zhaopeng Qiu, et~al.",A survey on large language models for recommendation.,A survey on large language models for recommendation.,,"[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup }{2023}]{wu2023survey} Likang Wu, Zhi Zheng, Zhaopeng Qiu, et~al. 
 A survey on large language models for recommendation. 
 {\em arXiv preprint arXiv:2305.19860}, 2023."
2407.11681,zhang2022opt,"[\protect\citeauthoryear{Zhang \bgroup \em et al.\egroup }{2022}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, et~al.",{OPT}: Open pre-trained transformer language models.,{OPT}: Open pre-trained transformer language models.,,"[\protect\citeauthoryear{Zhang \bgroup \em et al.\egroup }{2022}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, et~al. 
 {OPT}: Open pre-trained transformer language models. 
 {\em arXiv preprint arXiv:2205.01068}, 2022."
2407.12043,ahdritz2024distinguishing,"[Ahdritz et~al.(2024)Ahdritz, Qin, Vyas, Barak, and Edelman]{ahdritz2024distinguishing} Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin~L Edelman.",Distinguishing the knowable from the unknowable with language models.,Distinguishing the knowable from the unknowable with language models.,,"[Ahdritz et~al.(2024)Ahdritz, Qin, Vyas, Barak, and Edelman]{ahdritz2024distinguishing} Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin~L Edelman. 
 Distinguishing the knowable from the unknowable with language models. 
 \emph{arXiv preprint arXiv:2402.03563}, 2024."
2407.12043,amayuelas2023knowledge,"[Amayuelas et~al.(2023)Amayuelas, Pan, Chen, and Wang]{amayuelas2023knowledge} Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang.",Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models.,Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models.,https://arxiv.org/abs/2305.13712,"[Amayuelas et~al.(2023)Amayuelas, Pan, Chen, and Wang]{amayuelas2023knowledge} Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 
 Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. 
 arXiv preprint arXiv:2305.13712, 2023. 
 URL \url{https://arxiv.org/abs/2305.13712}."
2407.12043,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2407.12043,derczynski2023assessing,"[Derczynski et~al.(2023)Derczynski, Kirk, Balachandran, Kumar, Tsvetkov, Leiser, and Mohammad]{derczynski2023assessing} Leon Derczynski, Hannah~Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, MR~Leiser, and Saif Mohammad.",Assessing language model deployment with risk cards.,Assessing language model deployment with risk cards.,,"[Derczynski et~al.(2023)Derczynski, Kirk, Balachandran, Kumar, Tsvetkov, Leiser, and Mohammad]{derczynski2023assessing} Leon Derczynski, Hannah~Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, MR~Leiser, and Saif Mohammad. 
 Assessing language model deployment with risk cards. 
 \emph{arXiv preprint arXiv:2303.18190}, 2023."
2407.12043,derner2023beyond,[Derner \& Batisti{\v{c}}(2023)Derner and Batisti{\v{c}}]{derner2023beyond} Erik Derner and Kristina Batisti{\v{c}}.,Beyond the safeguards: Exploring the security risks of chatgpt.,Beyond the safeguards: Exploring the security risks of chatgpt.,,"[Derner \& Batisti{\v{c}}(2023)Derner and Batisti{\v{c}}]{derner2023beyond} Erik Derner and Kristina Batisti{\v{c}}. 
 Beyond the safeguards: Exploring the security risks of chatgpt. 
 \emph{arXiv preprint arXiv:2305.08005}, 2023."
2407.12043,desai2020calibration,[Desai \& Durrett(2020)Desai and Durrett]{desai2020calibration} Shrey Desai and Greg Durrett.,Calibration of pre-trained transformers.,Calibration of pre-trained transformers.,,"[Desai \& Durrett(2020)Desai and Durrett]{desai2020calibration} Shrey Desai and Greg Durrett. 
 Calibration of pre-trained transformers. 
 \emph{arXiv preprint arXiv:2003.07892}, 2020."
2407.12043,feng2024don,"[Feng et~al.(2024)Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov.","Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration.","Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration.",,"[Feng et~al.(2024)Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 
 Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. 
 \emph{arXiv preprint arXiv:2402.00367}, 2024."
2407.12043,hestness2017deep,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}, 2017."
2407.12043,hoffmann2022training,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2407.12043,huang2023lorahub,"[Huang et~al.(2023)Huang, Liu, Lin, Pang, Du, and Lin]{huang2023lorahub} Chengsong Huang, Qian Liu, Bill~Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin.",Lorahub: Efficient cross-task generalization via dynamic lora composition.,Lorahub: Efficient cross-task generalization via dynamic lora composition.,,"[Huang et~al.(2023)Huang, Liu, Lin, Pang, Du, and Lin]{huang2023lorahub} Chengsong Huang, Qian Liu, Bill~Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. 
 Lorahub: Efficient cross-task generalization via dynamic lora composition. 
 \emph{arXiv preprint arXiv:2307.13269}, 2023."
2407.12043,kadavath2022language,"[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al.",Language models (mostly) know what they know.,Language models (mostly) know what they know.,,"[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 
 Language models (mostly) know what they know. 
 \emph{arXiv preprint arXiv:2207.05221}, 2022."
2407.12043,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2407.12043,kim2024epistemology,[Kim \& Thorne(2024)Kim and Thorne]{kim2024epistemology} Minsu Kim and James Thorne.,Epistemology of language models: Do language models have holistic knowledge?,Epistemology of language models: Do language models have holistic knowledge?,,"[Kim \& Thorne(2024)Kim and Thorne]{kim2024epistemology} Minsu Kim and James Thorne. 
 Epistemology of language models: Do language models have holistic knowledge? 
 \emph{arXiv preprint arXiv:2403.12862}, 2024."
2407.12043,kim20222,"[Kim et~al.(2022)Kim, Htut, Bowman, and Petty]{kim20222} Najoung Kim, Phu~Mon Htut, Samuel~R Bowman, and Jackson Petty.",Qa$^2$: Question answering with questionable assumptions.,Qa$^2$: Question answering with questionable assumptions.,,"[Kim et~al.(2022)Kim, Htut, Bowman, and Petty]{kim20222} Najoung Kim, Phu~Mon Htut, Samuel~R Bowman, and Jackson Petty. 
 Qa$^2$: Question answering with questionable assumptions. 
 \emph{arXiv preprint arXiv:2212.10003}, 2022."
2407.12043,kirk2023personalisation,"[Kirk et~al.(2023)Kirk, Vidgen, R{\""o}ttger, and Hale]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul R{\""o}ttger, and Scott~A Hale.",Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,,"[Kirk et~al.(2023)Kirk, Vidgen, R{\""o}ttger, and Hale]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul R{\""o}ttger, and Scott~A Hale. 
 Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. 
 \emph{arXiv preprint arXiv:2303.05453}, 2023."
2407.12043,lee2024learning,"[Lee et~al.(2024)Lee, Kim, Cherif, Dobre, Lee, Hwang, Kawaguchi, Gidel, Bengio, Malkin, et~al.]{lee2024learning} Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung~Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et~al.",Learning diverse attacks on large language models for robust red-teaming and safety tuning.,Learning diverse attacks on large language models for robust red-teaming and safety tuning.,,"[Lee et~al.(2024)Lee, Kim, Cherif, Dobre, Lee, Hwang, Kawaguchi, Gidel, Bengio, Malkin, et~al.]{lee2024learning} Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung~Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et~al. 
 Learning diverse attacks on large language models for robust red-teaming and safety tuning. 
 \emph{arXiv preprint arXiv:2405.18540}, 2024."
2407.12043,li2023multi,"[Li et~al.(2023{\natexlab{a}})Li, Guo, Fan, Xu, and Song]{li2023multi} Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.",Multi-step jailbreaking privacy attacks on chatgpt.,Multi-step jailbreaking privacy attacks on chatgpt.,,"[Li et~al.(2023{\natexlab{a}})Li, Guo, Fan, Xu, and Song]{li2023multi} Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 
 Multi-step jailbreaking privacy attacks on chatgpt. 
 \emph{arXiv preprint arXiv:2304.05197}, 2023{\natexlab{a}}."
2407.12043,liu2023trustworthy,"[Liu et~al.(2023{\natexlab{b}})Liu, Yao, Ton, Zhang, Cheng, Klochkov, Taufiq, and Li]{liu2023trustworthy} Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo~Hao Cheng, Yegor Klochkov, Muhammad~Faaiz Taufiq, and Hang Li.",Trustworthy llms: a survey and guideline for evaluating large language models' alignment.,Trustworthy llms: a survey and guideline for evaluating large language models' alignment.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Yao, Ton, Zhang, Cheng, Klochkov, Taufiq, and Li]{liu2023trustworthy} Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo~Hao Cheng, Yegor Klochkov, Muhammad~Faaiz Taufiq, and Hang Li. 
 Trustworthy llms: a survey and guideline for evaluating large language models' alignment. 
 \emph{arXiv preprint arXiv:2308.05374}, 2023{\natexlab{b}}."
2407.12043,lukas2023analyzing,"[Lukas et~al.(2023)Lukas, Salem, Sim, Tople, Wutschitz, and Zanella-B{\'e}guelin]{lukas2023analyzing} Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B{\'e}guelin.",Analyzing leakage of personally identifiable information in language models.,Analyzing leakage of personally identifiable information in language models.,,"[Lukas et~al.(2023)Lukas, Salem, Sim, Tople, Wutschitz, and Zanella-B{\'e}guelin]{lukas2023analyzing} Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B{\'e}guelin. 
 Analyzing leakage of personally identifiable information in language models. 
 \emph{arXiv preprint arXiv:2302.00539}, 2023."
2407.12043,rottger2024safetyprompts,"[R{\""o}ttger et~al.(2024)R{\""o}ttger, Pernisi, Vidgen, and Hovy]{rottger2024safetyprompts} Paul R{\""o}ttger, Fabio Pernisi, Bertie Vidgen, and Dirk Hovy.",Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety.,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety.,,"[R{\""o}ttger et~al.(2024)R{\""o}ttger, Pernisi, Vidgen, and Hovy]{rottger2024safetyprompts} Paul R{\""o}ttger, Fabio Pernisi, Bertie Vidgen, and Dirk Hovy. 
 Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety. 
 \emph{arXiv preprint arXiv:2404.05399}, 2024."
2407.12043,srivastava2022beyond,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,https://arxiv.org/abs/2206.04615,"[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 arXiv preprint arXiv:2206.04615, 2022. 
 URL \url{https://arxiv.org/abs/2206.04615}."
2407.12043,touvron2023llama,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{b}}."
2407.12043,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{c}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{c}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas   Scialom. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv: 2307.09288}, 2023{\natexlab{c}}."
2407.12043,varshney2022investigating,"[Varshney et~al.(2022)Varshney, Mishra, and Baral]{varshney2022investigating} Neeraj Varshney, Swaroop Mishra, and Chitta Baral.","Investigating selective prediction approaches across several tasks in iid, ood, and adversarial settings.","Investigating selective prediction approaches across several tasks in iid, ood, and adversarial settings.",,"[Varshney et~al.(2022)Varshney, Mishra, and Baral]{varshney2022investigating} Neeraj Varshney, Swaroop Mishra, and Chitta Baral. 
 Investigating selective prediction approaches across several tasks in iid, ood, and adversarial settings. 
 \emph{arXiv preprint arXiv:2203.00211}, 2022."
2407.12043,wang2023not,"[Wang et~al.(2023{\natexlab{d}})Wang, Li, Han, Nakov, and Baldwin]{wang2023not} Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.",Do-not-answer: A dataset for evaluating safeguards in llms.,Do-not-answer: A dataset for evaluating safeguards in llms.,,"[Wang et~al.(2023{\natexlab{d}})Wang, Li, Han, Nakov, and Baldwin]{wang2023not} Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 
 Do-not-answer: A dataset for evaluating safeguards in llms. 
 \emph{arXiv preprint arXiv:2308.13387}, 2023{\natexlab{d}}."
2407.12043,zhang2023clarify,[Zhang \& Choi(2023)Zhang and Choi]{zhang2023clarify} Michael~JQ Zhang and Eunsol Choi.,Clarify when necessary: Resolving ambiguity through interaction with lms.,Clarify when necessary: Resolving ambiguity through interaction with lms.,,"[Zhang \& Choi(2023)Zhang and Choi]{zhang2023clarify} Michael~JQ Zhang and Eunsol Choi. 
 Clarify when necessary: Resolving ambiguity through interaction with lms. 
 \emph{arXiv preprint arXiv:2311.09469}, 2023."
2407.12043,zhang2023safetybench,"[Zhang et~al.(2023)Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench} Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.",Safetybench: Evaluating the safety of large language models with multiple choice questions.,Safetybench: Evaluating the safety of large language models with multiple choice questions.,,"[Zhang et~al.(2023)Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench} Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 
 Safetybench: Evaluating the safety of large language models with multiple choice questions. 
 \emph{arXiv preprint arXiv:2309.07045}, 2023."
2407.12043,zhong2024rose,"[Zhong et~al.(2024)Zhong, Ding, Liu, Du, and Tao]{zhong2024rose} Qihuang Zhong, Liang Ding, Juhua Liu, Bo~Du, and Dacheng Tao.",Rose doesn't do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding.,Rose doesn't do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding.,,"[Zhong et~al.(2024)Zhong, Ding, Liu, Du, and Tao]{zhong2024rose} Qihuang Zhong, Liang Ding, Juhua Liu, Bo~Du, and Dacheng Tao. 
 Rose doesn't do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. 
 \emph{arXiv preprint arXiv:2402.11889}, 2024."
2407.1258,agnolucci2024isearle,"[ABBDB24]{agnolucci2024isearle} Lorenzo Agnolucci, Alberto Baldrati, Marco Bertini, and Alberto Del~Bimbo.",isearle: Improving textual inversion for zero-shot composed image retrieval.,isearle: Improving textual inversion for zero-shot composed image retrieval.,,"[ABBDB24]{agnolucci2024isearle} Lorenzo Agnolucci, Alberto Baldrati, Marco Bertini, and Alberto Del~Bimbo. 
 isearle: Improving textual inversion for zero-shot composed image retrieval. 
 {\em arXiv preprint arXiv:2405.02951}, 2024."
2407.1258,gao2023llama,"[GHZ{\etalchar{+}}23]{gao2023llama} Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al.",Llama-adapter v2: Parameter-efficient visual instruction model.,Llama-adapter v2: Parameter-efficient visual instruction model.,,"[GHZ{\etalchar{+}}23]{gao2023llama} Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al. 
 Llama-adapter v2: Parameter-efficient visual instruction model. 
 {\em arXiv preprint arXiv:2304.15010}, 2023."
2407.1258,gao2021simcse,"[GYC21]{gao2021simcse} Tianyu Gao, Xingcheng Yao, and Danqi Chen.",Simcse: Simple contrastive learning of sentence embeddings.,Simcse: Simple contrastive learning of sentence embeddings.,,"[GYC21]{gao2021simcse} Tianyu Gao, Xingcheng Yao, and Danqi Chen. 
 Simcse: Simple contrastive learning of sentence embeddings. 
 {\em arXiv preprint arXiv:2104.08821}, 2021."
2407.1258,jiang2023scaling,"[JHL{\etalchar{+}}23]{jiang2023scaling} Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang.",Scaling sentence embeddings with large language models.,Scaling sentence embeddings with large language models.,,"[JHL{\etalchar{+}}23]{jiang2023scaling} Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. 
 Scaling sentence embeddings with large language models. 
 {\em arXiv preprint arXiv:2307.16645}, 2023."
2407.1258,jiang2022promptbert,"[JJH{\etalchar{+}}22]{jiang2022promptbert} Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi~Zhang.",Promptbert: Improving bert sentence embeddings with prompts.,Promptbert: Improving bert sentence embeddings with prompts.,,"[JJH{\etalchar{+}}22]{jiang2022promptbert} Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi~Zhang. 
 Promptbert: Improving bert sentence embeddings with prompts. 
 {\em arXiv preprint arXiv:2201.04337}, 2022."
2407.1258,karthik2023vision,"[KRMA23]{karthik2023vision} Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata.",Vision-by-language for training-free compositional image retrieval.,Vision-by-language for training-free compositional image retrieval.,,"[KRMA23]{karthik2023vision} Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. 
 Vision-by-language for training-free compositional image retrieval. 
 {\em arXiv preprint arXiv:2310.09291}, 2023."
2407.1258,liu2022universal,"[LXL{\etalchar{+}}22]{liu2022universal} Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge~Yu.",Universal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval.,Universal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval.,,"[LXL{\etalchar{+}}22]{liu2022universal} Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge~Yu. 
 Universal vision-language dense retrieval: Learning a unified representation space for multi-modal retrieval. 
 {\em arXiv preprint arXiv:2209.00179}, 2022."
2407.1258,muennighoff2022sgpt,[Mue22]{muennighoff2022sgpt} Niklas Muennighoff.,Sgpt: Gpt sentence embeddings for semantic search.,Sgpt: Gpt sentence embeddings for semantic search.,,"[Mue22]{muennighoff2022sgpt} Niklas Muennighoff. 
 Sgpt: Gpt sentence embeddings for semantic search. 
 {\em arXiv preprint arXiv:2202.08904}, 2022."
2407.1258,sentencet5,"[N{\'A}C{\etalchar{+}}21]{sentencet5} Jianmo Ni, Gustavo~Hern{\'a}ndez {\'A}brego, Noah Constant, Ji~Ma, Keith~B Hall, Daniel Cer, and Yinfei Yang.",Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.,Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.,,"[N{\'A}C{\etalchar{+}}21]{sentencet5} Jianmo Ni, Gustavo~Hern{\'a}ndez {\'A}brego, Noah Constant, Ji~Ma, Keith~B Hall, Daniel Cer, and Yinfei Yang. 
 Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. 
 {\em arXiv preprint arXiv:2108.08877}, 2021."
2407.1258,sun2023eva,"[SFW{\etalchar{+}}23]{sun2023eva} Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.",Eva-clip: Improved training techniques for clip at scale.,Eva-clip: Improved training techniques for clip at scale.,,"[SFW{\etalchar{+}}23]{sun2023eva} Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 
 Eva-clip: Improved training techniques for clip at scale. 
 {\em arXiv preprint arXiv:2303.15389}, 2023."
2407.1258,wei2023uniir,"[WCC{\etalchar{+}}23]{wei2023uniir} Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge~Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.",Uniir: Training and benchmarking universal multimodal information retrievers.,Uniir: Training and benchmarking universal multimodal information retrievers.,,"[WCC{\etalchar{+}}23]{wei2023uniir} Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge~Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 
 Uniir: Training and benchmarking universal multimodal information retrievers. 
 {\em arXiv preprint arXiv:2311.17136}, 2023."
2407.1258,wang2023improving,"[WYH{\etalchar{+}}23]{wang2023improving} Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.",Improving text embeddings with large language models.,Improving text embeddings with large language models.,,"[WYH{\etalchar{+}}23]{wang2023improving} Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 
 Improving text embeddings with large language models. 
 {\em arXiv preprint arXiv:2401.00368}, 2023."
2407.1258,yin2023survey,"[YFZ{\etalchar{+}}23]{yin2023survey} Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke~Li, Xing Sun, Tong Xu, and Enhong Chen.",A survey on multimodal large language models.,A survey on multimodal large language models.,,"[YFZ{\etalchar{+}}23]{yin2023survey} Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke~Li, Xing Sun, Tong Xu, and Enhong Chen. 
 A survey on multimodal large language models. 
 {\em arXiv preprint arXiv:2306.13549}, 2023."
2407.1258,Zhou2024vista,"[ZLX{\etalchar{+}}24]{Zhou2024vista} Junjie Zhou, Zheng Liu, Shitao Xiao, Bo~Zhao, and Yongping Xiong.",{VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval}.,{VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval}.,,"[ZLX{\etalchar{+}}24]{Zhou2024vista} Junjie Zhou, Zheng Liu, Shitao Xiao, Bo~Zhao, and Yongping Xiong. 
 {VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval}. 
 {\em arXiv preprint arXiv:2406.04292}, 2024."
2407.1258,zhang2024long,"[ZZD{\etalchar{+}}24]{zhang2024long} Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang.",Long-clip: Unlocking the long-text capability of clip.,Long-clip: Unlocking the long-text capability of clip.,,"[ZZD{\etalchar{+}}24]{zhang2024long} Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 
 Long-clip: Unlocking the long-text capability of clip. 
 {\em arXiv preprint arXiv:2403.15378}, 2024."
2407.12665,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.12665,bai2023qwen,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,   et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang,   et~al.]{bai2023qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,   Wenbin Ge, Yu~Han, Fei Huang, et~al. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2407.12665,cai2024medusa,"[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and   Dao]{cai2024medusa} Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D Lee, Deming Chen,   and Tri Dao.",Medusa: Simple llm inference acceleration framework with multiple   decoding heads.,Medusa: Simple llm inference acceleration framework with multiple   decoding heads.,,"[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and   Dao]{cai2024medusa} Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D Lee, Deming Chen,   and Tri Dao. 
 Medusa: Simple llm inference acceleration framework with multiple   decoding heads. 
 \emph{arXiv preprint arXiv:2401.10774}, 2024."
2407.12665,chen2023accelerating,"[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and   Jumper]{chen2023accelerating} Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,   Laurent Sifre, and John Jumper.",Accelerating large language model decoding with speculative sampling.,Accelerating large language model decoding with speculative sampling.,,"[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and   Jumper]{chen2023accelerating} Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,   Laurent Sifre, and John Jumper. 
 Accelerating large language model decoding with speculative sampling. 
 \emph{arXiv preprint arXiv:2302.01318}, 2023."
2407.12665,chen2015net2net,"[Chen et~al.(2015)Chen, Goodfellow, and Shlens]{chen2015net2net} Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.",Net2net: Accelerating learning via knowledge transfer.,Net2net: Accelerating learning via knowledge transfer.,,"[Chen et~al.(2015)Chen, Goodfellow, and Shlens]{chen2015net2net} Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. 
 Net2net: Accelerating learning via knowledge transfer. 
 \emph{arXiv preprint arXiv:1511.05641}, 2015."
2407.12665,clark2018think,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2407.12665,duan2024vision,"[Duan et~al.(2024)Duan, Wang, Chen, Zhu, Lu, Lu, Qiao, Li, Dai, and   Wang]{duan2024vision} Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu~Qiao,   Hongsheng Li, Jifeng Dai, and Wenhai Wang.",Vision-rwkv: Efficient and scalable visual perception with rwkv-like   architectures.,Vision-rwkv: Efficient and scalable visual perception with rwkv-like   architectures.,,"[Duan et~al.(2024)Duan, Wang, Chen, Zhu, Lu, Lu, Qiao, Li, Dai, and   Wang]{duan2024vision} Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu~Qiao,   Hongsheng Li, Jifeng Dai, and Wenhai Wang. 
 Vision-rwkv: Efficient and scalable visual perception with rwkv-like   architectures. 
 \emph{arXiv preprint arXiv:2403.02308}, 2024."
2407.12665,gao2020pile,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima, et~al.]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima, et~al.]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}, 2020."
2407.12665,ho2024block,"[Ho et~al.(2024)Ho, Bae, Kim, Jo, Kim, Schuster, Fisch, Thorne, and   Yun]{ho2024block} Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster,   Adam Fisch, James Thorne, and Se-Young Yun.",Block transformer: Global-to-local language modeling for fast   inference.,Block transformer: Global-to-local language modeling for fast   inference.,,"[Ho et~al.(2024)Ho, Bae, Kim, Jo, Kim, Schuster, Fisch, Thorne, and   Yun]{ho2024block} Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster,   Adam Fisch, James Thorne, and Se-Young Yun. 
 Block transformer: Global-to-local language modeling for fast   inference. 
 \emph{arXiv preprint arXiv:2406.02657}, 2024."
2407.12665,hoffmann2022training,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2407.12665,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2407.12665,li2024amphista,"[Li et~al.(2024)Li, Yang, Gao, Liu, Liu, Li, Peng, Tian, and   Barsoum]{li2024amphista} Zeping Li, Xinlong Yang, Ziheng Gao, Ji~Liu, Zhuang Liu, Dong Li, Jinzhang   Peng, Lu~Tian, and Emad Barsoum.",Amphista: Accelerate llm inference with bi-directional multiple   drafting heads in a non-autoregressive style.,Amphista: Accelerate llm inference with bi-directional multiple   drafting heads in a non-autoregressive style.,,"[Li et~al.(2024)Li, Yang, Gao, Liu, Liu, Li, Peng, Tian, and   Barsoum]{li2024amphista} Zeping Li, Xinlong Yang, Ziheng Gao, Ji~Liu, Zhuang Liu, Dong Li, Jinzhang   Peng, Lu~Tian, and Emad Barsoum. 
 Amphista: Accelerate llm inference with bi-directional multiple   drafting heads in a non-autoregressive style. 
 \emph{arXiv preprint arXiv:2406.13170}, 2024."
2407.12665,lin2024bita,"[Lin et~al.(2024)Lin, Yi, Li, Yang, Yu, Lu, and Xiao]{lin2024bita} Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, and   Rong Xiao.",Bita: Bi-directional tuning for lossless acceleration in large   language models.,Bita: Bi-directional tuning for lossless acceleration in large   language models.,,"[Lin et~al.(2024)Lin, Yi, Li, Yang, Yu, Lu, and Xiao]{lin2024bita} Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, and   Rong Xiao. 
 Bita: Bi-directional tuning for lossless acceleration in large   language models. 
 \emph{arXiv preprint arXiv:2401.12522}, 2024."
2407.12665,mujika2023hierarchical,[Mujika(2023)]{mujika2023hierarchical} Asier Mujika.,Hierarchical attention encoder decoder.,Hierarchical attention encoder decoder.,,"[Mujika(2023)]{mujika2023hierarchical} Asier Mujika. 
 Hierarchical attention encoder decoder. 
 \emph{arXiv preprint arXiv:2306.01070}, 2023."
2407.12665,shazeer2020glu,[Shazeer(2020)]{shazeer2020glu} Noam Shazeer.,Glu variants improve transformer.,Glu variants improve transformer.,,"[Shazeer(2020)]{shazeer2020glu} Noam Shazeer. 
 Glu variants improve transformer. 
 \emph{arXiv preprint arXiv:2002.05202}, 2020."
2407.12665,su2021roformer,"[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2021roformer} Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.",Roformer: Enhanced transformer with rotary position embedding.,Roformer: Enhanced transformer with rotary position embedding.,,"[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2021roformer} Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu. 
 Roformer: Enhanced transformer with rotary position embedding. 
 \emph{arXiv preprint arXiv:2104.09864}, 2021."
2407.12665,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2407.12665,touvron2023llama,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,   Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,   et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne   Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric   Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}."
2407.12665,touvron2023llama2,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,   Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,   et~al.]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}."
2407.12665,wan2023efficient,"[Wan et~al.(2023)Wan, Wang, Liu, Alam, Zheng, et~al.]{wan2023efficient} Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu~Zheng, et~al.",Efficient large language models: A survey.,Efficient large language models: A survey.,,"[Wan et~al.(2023)Wan, Wang, Liu, Alam, Zheng, et~al.]{wan2023efficient} Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu~Zheng, et~al. 
 Efficient large language models: A survey. 
 \emph{arXiv preprint arXiv:2312.03863}, 1, 2023."
2407.12665,yang2020progressively,"[Yang et~al.(2020)Yang, Wang, Yang, Li, He, and   Zhang]{yang2020progressively} Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru~He, and Jingqiao Zhang.",Progressively stacking 2.0: A multi-stage layerwise training method   for bert training speedup.,Progressively stacking 2.0: A multi-stage layerwise training method   for bert training speedup.,,"[Yang et~al.(2020)Yang, Wang, Yang, Li, He, and   Zhang]{yang2020progressively} Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru~He, and Jingqiao Zhang. 
 Progressively stacking 2.0: A multi-stage layerwise training method   for bert training speedup. 
 \emph{arXiv preprint arXiv:2011.13635}, 2020."
2407.12665,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2407.12665,zhu2024vision,"[Zhu et~al.(2024)Zhu, Liao, Zhang, Wang, Liu, and Wang]{zhu2024vision} Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang   Wang.",Vision mamba: Efficient visual representation learning with   bidirectional state space model.,Vision mamba: Efficient visual representation learning with   bidirectional state space model.,,"[Zhu et~al.(2024)Zhu, Liao, Zhang, Wang, Liu, and Wang]{zhu2024vision} Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang   Wang. 
 Vision mamba: Efficient visual representation learning with   bidirectional state space model. 
 \emph{arXiv preprint arXiv:2401.09417}, 2024."
2407.12735,abdin2024phi,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.",Phi-3 technical report: A highly capable language model locally on your phone.,Phi-3 technical report: A highly capable language model locally on your phone.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024. 
 Phi-3 technical report: A highly capable language model locally on your phone. 
 \emph{arXiv preprint arXiv:2404.14219}."
2407.12735,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.12735,chen2023can,"[{Chen et~al.(2023)Chen, Hu, Luan, Sun, Changpinyo, Ritter, and Chang}]{chen2023can} Yang Chen, Hexiang Hu, Yi~Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023.",Can pre-trained vision and language models answer visual information-seeking questions?,Can pre-trained vision and language models answer visual information-seeking questions?,,"[{Chen et~al.(2023)Chen, Hu, Luan, Sun, Changpinyo, Ritter, and Chang}]{chen2023can} Yang Chen, Hexiang Hu, Yi~Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. 
 Can pre-trained vision and language models answer visual information-seeking questions? 
 \emph{arXiv preprint arXiv:2302.11713}."
2407.12735,douze2024faiss,"[{Douze et~al.(2024)Douze, Guzhva, Deng, Johnson, Szilvasy, Mazar{\'e}, Lomeli, Hosseini, and J{\'e}gou}]{douze2024faiss} Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar{\'e}, Maria Lomeli, Lucas Hosseini, and Herv{\'e} J{\'e}gou. 2024.",The faiss library.,The faiss library.,,"[{Douze et~al.(2024)Douze, Guzhva, Deng, Johnson, Szilvasy, Mazar{\'e}, Lomeli, Hosseini, and J{\'e}gou}]{douze2024faiss} Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar{\'e}, Maria Lomeli, Lucas Hosseini, and Herv{\'e} J{\'e}gou. 2024. 
 The faiss library. 
 \emph{arXiv preprint arXiv:2401.08281}."
2407.12735,team2023gemini,"[{{Gemini Team} et~al.(2023){Gemini Team}, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{{Gemini Team} et~al.(2023){Gemini Team}, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} {Gemini Team}, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2407.12735,gui2021kat,"[{Gui et~al.(2021)Gui, Wang, Huang, Hauptmann, Bisk, and Gao}]{gui2021kat} Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021.",Kat: A knowledge augmented transformer for vision-and-language.,Kat: A knowledge augmented transformer for vision-and-language.,,"[{Gui et~al.(2021)Gui, Wang, Huang, Hauptmann, Bisk, and Gao}]{gui2021kat} Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021. 
 Kat: A knowledge augmented transformer for vision-and-language. 
 \emph{arXiv preprint arXiv:2112.08614}."
2407.12735,hu2022promptcap,"[{Hu et~al.(2022)Hu, Hua, Yang, Shi, Smith, and Luo}]{hu2022promptcap} Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah~A Smith, and Jiebo Luo. 2022.",Promptcap: Prompt-guided task-aware image captioning.,Promptcap: Prompt-guided task-aware image captioning.,,"[{Hu et~al.(2022)Hu, Hua, Yang, Shi, Smith, and Luo}]{hu2022promptcap} Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah~A Smith, and Jiebo Luo. 2022. 
 Promptcap: Prompt-guided task-aware image captioning. 
 \emph{arXiv preprint arXiv:2211.09699}."
2407.12735,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2407.12735,li2023evaluating,"[{Li et~al.(2023{\natexlab{c}})Li, Du, Zhou, Wang, Zhao, and Wen}]{li2023evaluating} Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen. 2023{\natexlab{c}}.",Evaluating object hallucination in large vision-language models.,Evaluating object hallucination in large vision-language models.,,"[{Li et~al.(2023{\natexlab{c}})Li, Du, Zhou, Wang, Zhao, and Wen}]{li2023evaluating} Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen. 2023{\natexlab{c}}. 
 Evaluating object hallucination in large vision-language models. 
 \emph{arXiv preprint arXiv:2305.10355}."
2407.12735,li2023comprehensive,"[{Li et~al.(2023{\natexlab{d}})Li, Wang, Hu, Chen, Zhong, Lyu, and Zhang}]{li2023comprehensive} Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, and Min Zhang. 2023{\natexlab{d}}.",A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering.,A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering.,,"[{Li et~al.(2023{\natexlab{d}})Li, Wang, Hu, Chen, Zhong, Lyu, and Zhang}]{li2023comprehensive} Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, and Min Zhang. 2023{\natexlab{d}}. 
 A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. 
 \emph{arXiv preprint arXiv:2311.07536}."
2407.12735,lin2024preflmr,"[{Lin et~al.(2024)Lin, Mei, Chen, and Byrne}]{lin2024preflmr} Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. 2024.",Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers.,Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers.,,"[{Lin et~al.(2024)Lin, Mei, Chen, and Byrne}]{lin2024preflmr} Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. 2024. 
 Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers. 
 \emph{arXiv preprint arXiv:2402.08327}."
2407.12735,sun2024eva,"[{Sun et~al.(2024)Sun, Wang, Yu, Cui, Zhang, Zhang, and Wang}]{sun2024eva} Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. 2024.",Eva-clip-18b: Scaling clip to 18 billion parameters.,Eva-clip-18b: Scaling clip to 18 billion parameters.,,"[{Sun et~al.(2024)Sun, Wang, Yu, Cui, Zhang, Zhang, and Wang}]{sun2024eva} Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. 2024. 
 Eva-clip-18b: Scaling clip to 18 billion parameters. 
 \emph{arXiv preprint arXiv:2402.04252}."
2407.12865,GSM8K,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{GSM8K} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{GSM8K} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.12865,ucb,[{Kuleshov and Precup(2014)}]{ucb} Volodymyr Kuleshov and Doina Precup. 2014.,Algorithms for multi-armed bandit problems.,Algorithms for multi-armed bandit problems.,,"[{Kuleshov and Precup(2014)}]{ucb} Volodymyr Kuleshov and Doina Precup. 2014. 
 Algorithms for multi-armed bandit problems. 
 \emph{arXiv preprint arXiv:1402.6028}."
2407.12865,are_llms_good_optimizers,"[{Ma et~al.(2024)Ma, Wang, Zhou, Li, Du, Gui, Zhang, and Huang}]{are_llms_good_optimizers} Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2024.",Are large language models good prompt optimizers?,Are large language models good prompt optimizers?,,"[{Ma et~al.(2024)Ma, Wang, Zhou, Li, Du, Gui, Zhang, and Huang}]{are_llms_good_optimizers} Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2024. 
 Are large language models good prompt optimizers? 
 \emph{arXiv preprint arXiv:2402.02101}."
2407.12865,orca_math,"[{Mitra et~al.(2024)Mitra, Khanpour, Rosset, and Awadallah}]{orca_math} Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024.",Orca-math: Unlocking the potential of slms in grade school math.,Orca-math: Unlocking the potential of slms in grade school math.,,"[{Mitra et~al.(2024)Mitra, Khanpour, Rosset, and Awadallah}]{orca_math} Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024. 
 Orca-math: Unlocking the potential of slms in grade school math. 
 \emph{arXiv preprint arXiv:2402.14830}."
2407.12865,llmAsOptimizers,"[{Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{llmAsOptimizers} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2023.",Large language models as optimizers.,Large language models as optimizers.,,"[{Yang et~al.(2023)Yang, Wang, Lu, Liu, Le, Zhou, and Chen}]{llmAsOptimizers} Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V Le, Denny Zhou, and Xinyun Chen. 2023. 
 Large language models as optimizers. 
 \emph{arXiv preprint arXiv:2309.03409}."
2407.14562,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.14562,casper2023open,"[{Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire et~al.}]{casper2023open} Stephen Casper, Xander Davies, Claudia Shi, Thomas~Krendl Gilbert, J{\'e}r{\'e}my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et~al. 2023.",Open problems and fundamental limitations of reinforcement learning from human feedback.,Open problems and fundamental limitations of reinforcement learning from human feedback.,,"[{Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire et~al.}]{casper2023open} Stephen Casper, Xander Davies, Claudia Shi, Thomas~Krendl Gilbert, J{\'e}r{\'e}my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et~al. 2023. 
 Open problems and fundamental limitations of reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2307.15217}."
2407.14562,d_humaneval,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{d_humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.",Evaluating large language models trained on code.,Evaluating large language models trained on code.,,"[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{d_humaneval} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021. 
 Evaluating large language models trained on code. 
 \emph{arXiv preprint arXiv:2107.03374}."
2407.14562,chen2023challenges,"[{Chen et~al.(2023)Chen, Li, Chang, Huang, Zhao, Zhang, and Li}]{chen2023challenges} Xiaoliang Chen, Liangbin Li, Le~Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, and Dinuo Li. 2023.",Challenges and contributing factors in the utilization of large language models (llms).,Challenges and contributing factors in the utilization of large language models (llms).,,"[{Chen et~al.(2023)Chen, Li, Chang, Huang, Zhao, Zhang, and Li}]{chen2023challenges} Xiaoliang Chen, Liangbin Li, Le~Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, and Dinuo Li. 2023. 
 Challenges and contributing factors in the utilization of large language models (llms). 
 \emph{arXiv preprint arXiv:2310.13343}."
2407.14562,d_gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{d_gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{d_gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.14562,dong2023abilities,"[{Dong et~al.(2023)Dong, Yuan, Lu, Li, Xue, Liu, Wang, Yuan, Zhou, and Zhou}]{dong2023abilities} Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023.",How abilities in large language models are affected by supervised fine-tuning data composition.,How abilities in large language models are affected by supervised fine-tuning data composition.,,"[{Dong et~al.(2023)Dong, Yuan, Lu, Li, Xue, Liu, Wang, Yuan, Zhou, and Zhou}]{dong2023abilities} Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. 
 How abilities in large language models are affected by supervised fine-tuning data composition. 
 \emph{arXiv preprint arXiv:2310.05492}."
2407.14562,dong2022survey,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022.",A survey on in-context learning.,A survey on in-context learning.,,"[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui}]{dong2022survey} Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui. 2022. 
 A survey on in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}."
2407.14562,gunel2020supervised,"[{Gunel et~al.(2020)Gunel, Du, Conneau, and Stoyanov}]{gunel2020supervised} Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. 2020.",Supervised contrastive learning for pre-trained language model fine-tuning.,Supervised contrastive learning for pre-trained language model fine-tuning.,,"[{Gunel et~al.(2020)Gunel, Du, Conneau, and Stoyanov}]{gunel2020supervised} Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. 2020. 
 Supervised contrastive learning for pre-trained language model fine-tuning. 
 \emph{arXiv preprint arXiv:2011.01403}."
2407.14562,d_mmlu,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{d_mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{d_mmlu} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2407.14562,d_math,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{d_math} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.",Measuring mathematical problem solving with the math dataset.,Measuring mathematical problem solving with the math dataset.,,"[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{d_math} Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 
 Measuring mathematical problem solving with the math dataset. 
 \emph{arXiv preprint arXiv:2103.03874}."
2407.14562,lin2023speciality,"[{Lin et~al.(2023)Lin, Tan, Lin, Zheng, Pi, Zhang, Diao, Wang, Zhao, Yao et~al.}]{lin2023speciality} Yong Lin, Lu~Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, et~al. 2023.",Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models.,Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models.,,"[{Lin et~al.(2023)Lin, Tan, Lin, Zheng, Pi, Zhang, Diao, Wang, Zhao, Yao et~al.}]{lin2023speciality} Yong Lin, Lu~Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, et~al. 2023. 
 Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. 
 \emph{arXiv preprint arXiv:2309.06256}."
2407.14562,long2023large,[{Long(2023)}]{long2023large} Jieyi Long. 2023.,Large language model guided tree-of-thought.,Large language model guided tree-of-thought.,,"[{Long(2023)}]{long2023large} Jieyi Long. 2023. 
 Large language model guided tree-of-thought. 
 \emph{arXiv preprint arXiv:2305.08291}."
2407.14562,pan2023logic,"[{Pan et~al.(2023{\natexlab{b}})Pan, Albalak, Wang, and Wang}]{pan2023logic} Liangming Pan, Alon Albalak, Xinyi Wang, and William~Yang Wang. 2023{\natexlab{b}}.",Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.,Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.,,"[{Pan et~al.(2023{\natexlab{b}})Pan, Albalak, Wang, and Wang}]{pan2023logic} Liangming Pan, Alon Albalak, Xinyi Wang, and William~Yang Wang. 2023{\natexlab{b}}. 
 Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. 
 \emph{arXiv preprint arXiv:2305.12295}."
2407.14562,d_gpqa,"[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{d_gpqa} David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023.",Gpqa: A graduate-level google-proof q\&a benchmark.,Gpqa: A graduate-level google-proof q\&a benchmark.,,"[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{d_gpqa} David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023. 
 Gpqa: A graduate-level google-proof q\&a benchmark. 
 \emph{arXiv preprint arXiv:2311.12022}."
2407.14562,d_prontoqa,[{Saparov and He(2022)}]{d_prontoqa} Abulhair Saparov and He~He. 2022.,Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.,Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.,,"[{Saparov and He(2022)}]{d_prontoqa} Abulhair Saparov and He~He. 2022. 
 Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. 
 \emph{arXiv preprint arXiv:2210.01240}."
2407.14562,shi2024continual,"[{Shi et~al.(2024)Shi, Xu, Wang, Qin, Wang, Wang, and Wang}]{shi2024continual} Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. 2024.",Continual learning of large language models: A comprehensive survey.,Continual learning of large language models: A comprehensive survey.,,"[{Shi et~al.(2024)Shi, Xu, Wang, Qin, Wang, Wang, and Wang}]{shi2024continual} Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, and Hao Wang. 2024. 
 Continual learning of large language models: A comprehensive survey. 
 \emph{arXiv preprint arXiv:2404.16789}."
2407.14562,d_proofwriter,"[{Tafjord et~al.(2020)Tafjord, Mishra, and Clark}]{d_proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark. 2020.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.",,"[{Tafjord et~al.(2020)Tafjord, Mishra, and Clark}]{d_proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark. 2020. 
 Proofwriter: Generating implications, proofs, and abductive statements over natural language. 
 \emph{arXiv preprint arXiv:2012.13048}."
2407.14562,wu2023comparative,"[{Wu et~al.(2023)Wu, Koo, Blum, Black, Kao, Scalzo, and Kurtz}]{wu2023comparative} Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien Scalzo, and Ira Kurtz. 2023.","A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology.","A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology.",,"[{Wu et~al.(2023)Wu, Koo, Blum, Black, Kao, Scalzo, and Kurtz}]{wu2023comparative} Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien Scalzo, and Ira Kurtz. 2023. 
 A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. 
 \emph{arXiv preprint arXiv:2308.04709}."
2407.14562,yang2023neuro,"[{Yang et~al.(2023)Yang, Li, Cui, Bing, and Lam}]{yang2023neuro} Sen Yang, Xin Li, Leyang Cui, Lidong Bing, and Wai Lam. 2023.",Neuro-symbolic integration brings causal and reliable reasoning proofs.,Neuro-symbolic integration brings causal and reliable reasoning proofs.,,"[{Yang et~al.(2023)Yang, Li, Cui, Bing, and Lam}]{yang2023neuro} Sen Yang, Xin Li, Leyang Cui, Lidong Bing, and Wai Lam. 2023. 
 Neuro-symbolic integration brings causal and reliable reasoning proofs. 
 \emph{arXiv preprint arXiv:2311.09802}."
2407.14562,zhai2023investigating,"[{Zhai et~al.(2023)Zhai, Tong, Li, Cai, Qu, Lee, and Ma}]{zhai2023investigating} Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu~Cai, Qing Qu, Yong~Jae Lee, and Yi~Ma. 2023.",Investigating the catastrophic forgetting in multimodal large language models.,Investigating the catastrophic forgetting in multimodal large language models.,,"[{Zhai et~al.(2023)Zhai, Tong, Li, Cai, Qu, Lee, and Ma}]{zhai2023investigating} Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu~Cai, Qing Qu, Yong~Jae Lee, and Yi~Ma. 2023. 
 Investigating the catastrophic forgetting in multimodal large language models. 
 \emph{arXiv preprint arXiv:2309.10313}."
2407.14562,zhang2022automatic,"[{Zhang et~al.(2022)Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022.",Automatic chain of thought prompting in large language models.,Automatic chain of thought prompting in large language models.,,"[{Zhang et~al.(2022)Zhang, Zhang, Li, and Smola}]{zhang2022automatic} Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola. 2022. 
 Automatic chain of thought prompting in large language models. 
 \emph{arXiv preprint arXiv:2210.03493}."
2407.14562,zhao2023survey,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023.",A survey of large language models.,A survey of large language models.,,"[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 2023. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}."
2407.14562,zhou2024survey,"[{Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li et~al.}]{zhou2024survey} Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al. 2024.",A survey on efficient inference for large language models.,A survey on efficient inference for large language models.,,"[{Zhou et~al.(2024)Zhou, Ning, Hong, Fu, Xu, Li, Lou, Wang, Yuan, Li et~al.}]{zhou2024survey} Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al. 2024. 
 A survey on efficient inference for large language models. 
 \emph{arXiv preprint arXiv:2404.14294}."
2407.14614,tamkin2023evaluating,"[Tamkin et~al.(2023)Tamkin, Askell, Lovitt, Durmus, Joseph, Kravec, Nguyen, Kaplan, and Ganguli]{tamkin2023evaluating} Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli.",Evaluating and mitigating discrimination in language model decisions.,Evaluating and mitigating discrimination in language model decisions.,,"[Tamkin et~al.(2023)Tamkin, Askell, Lovitt, Durmus, Joseph, Kravec, Nguyen, Kaplan, and Ganguli]{tamkin2023evaluating} Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli. 
 Evaluating and mitigating discrimination in language model decisions. 
 \emph{arXiv preprint arXiv:2312.03689}, 2023."
2407.14614,gaebler2024auditing,"[Gaebler et~al.(2024)Gaebler, Goel, Huq, and Tambe]{gaebler2024auditing} Johann~D Gaebler, Sharad Goel, Aziz Huq, and Prasanna Tambe.",Auditing the use of language models to guide hiring decisions.,Auditing the use of language models to guide hiring decisions.,,"[Gaebler et~al.(2024)Gaebler, Goel, Huq, and Tambe]{gaebler2024auditing} Johann~D Gaebler, Sharad Goel, Aziz Huq, and Prasanna Tambe. 
 Auditing the use of language models to guide hiring decisions. 
 \emph{arXiv preprint arXiv:2404.03086}, 2024."
2407.14614,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.14614,perdomo2023difficult,"[Perdomo et~al.(2023)Perdomo, Britton, Hardt, and Abebe]{perdomo2023difficult} Juan~Carlos Perdomo, Tolani Britton, Moritz Hardt, and Rediet Abebe.",Difficult lessons on social prediction from wisconsin public schools.,Difficult lessons on social prediction from wisconsin public schools.,,"[Perdomo et~al.(2023)Perdomo, Britton, Hardt, and Abebe]{perdomo2023difficult} Juan~Carlos Perdomo, Tolani Britton, Moritz Hardt, and Rediet Abebe. 
 Difficult lessons on social prediction from wisconsin public schools. 
 \emph{arXiv preprint arXiv:2304.06205}, 2023."
2407.14614,kadavath2022language,"[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al.",Language models (mostly) know what they know.,Language models (mostly) know what they know.,,"[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 
 Language models (mostly) know what they know. 
 \emph{arXiv preprint arXiv:2207.05221}, 2022."
2407.14614,clark2018think,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2407.14614,cobbe2021training,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021."
2407.14614,durmus2023towards,"[Durmus et~al.(2023)Durmus, Nyugen, Liao, Schiefer, Askell, Bakhtin, Chen, Hatfield-Dodds, Hernandez, Joseph, et~al.]{durmus2023towards} Esin Durmus, Karina Nyugen, Thomas~I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et~al.",Towards measuring the representation of subjective global opinions in language models.,Towards measuring the representation of subjective global opinions in language models.,,"[Durmus et~al.(2023)Durmus, Nyugen, Liao, Schiefer, Askell, Bakhtin, Chen, Hatfield-Dodds, Hernandez, Joseph, et~al.]{durmus2023towards} Esin Durmus, Karina Nyugen, Thomas~I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et~al. 
 Towards measuring the representation of subjective global opinions in language models. 
 \emph{arXiv preprint arXiv:2306.16388}, 2023."
2407.14614,dominguez2023questioning,"[Dominguez-Olmedo et~al.(2023)Dominguez-Olmedo, Hardt, and Mendler-D{\""u}nner]{dominguez2023questioning} Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-D{\""u}nner.",Questioning the survey responses of large language models.,Questioning the survey responses of large language models.,,"[Dominguez-Olmedo et~al.(2023)Dominguez-Olmedo, Hardt, and Mendler-D{\""u}nner]{dominguez2023questioning} Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-D{\""u}nner. 
 Questioning the survey responses of large language models. 
 \emph{ArXiv preprint arXiv:2306.07951}, 2023."
2407.14614,band2024linguistic,"[Band et~al.(2024)Band, Li, Ma, and Hashimoto]{band2024linguistic} Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto.",Linguistic calibration of language models.,Linguistic calibration of language models.,,"[Band et~al.(2024)Band, Li, Ma, and Hashimoto]{band2024linguistic} Neil Band, Xuechen Li, Tengyu Ma, and Tatsunori Hashimoto. 
 Linguistic calibration of language models. 
 \emph{arXiv preprint arXiv:2404.00474}, 2024."
2407.14614,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2407.14614,young2024yi,"[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang, et~al.]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu, Chen, Chang, et~al.]{young2024yi} Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}, 2024."
2407.14985,chen2024parallel,"[Chen et~al.(2024)Chen, Zhao, Yu, McKeown, and He]{chen2024parallel} Y.~Chen, C.~Zhao, Z.~Yu, K.~McKeown, and H.~He.",Parallel structures in pre-training data yield in-context learning.,Parallel structures in pre-training data yield in-context learning.,,"[Chen et~al.(2024)Chen, Zhao, Yu, McKeown, and He]{chen2024parallel} Y.~Chen, C.~Zhao, Z.~Yu, K.~McKeown, and H.~He. 
 Parallel structures in pre-training data yield in-context learning. 
 \emph{arXiv preprint arXiv:2402.12530}, 2024."
2407.14985,gao2020pile,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile} L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile} L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang, H.~He, A.~Thite, N.~Nabeshima, et~al. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}, 2020."
2407.14985,hendrycks2020measuring,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring} D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and J.~Steinhardt.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring} D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and J.~Steinhardt. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}, 2020."
2407.14985,jiang2024investigating,"[Jiang et~al.(2024)Jiang, Liu, Zhong, Schaeffer, Ouyang, Han, and Koyejo]{jiang2024investigating} M.~Jiang, K.~Z. Liu, M.~Zhong, R.~Schaeffer, S.~Ouyang, J.~Han, and S.~Koyejo.",Investigating data contamination for pre-training language models.,Investigating data contamination for pre-training language models.,,"[Jiang et~al.(2024)Jiang, Liu, Zhong, Schaeffer, Ouyang, Han, and Koyejo]{jiang2024investigating} M.~Jiang, K.~Z. Liu, M.~Zhong, R.~Schaeffer, S.~Ouyang, J.~Han, and S.~Koyejo. 
 Investigating data contamination for pre-training language models. 
 \emph{arXiv preprint arXiv:2401.06059}, 2024."
2407.14985,kirchenbauer2024lmd3,"[Kirchenbauer et~al.(2024)Kirchenbauer, Honke, Somepalli, Geiping, Ippolito, Lee, Goldstein, and Andre]{kirchenbauer2024lmd3} J.~Kirchenbauer, G.~Honke, G.~Somepalli, J.~Geiping, D.~Ippolito, K.~Lee, T.~Goldstein, and D.~Andre.",Lmd3: Language model data density dependence.,Lmd3: Language model data density dependence.,,"[Kirchenbauer et~al.(2024)Kirchenbauer, Honke, Somepalli, Geiping, Ippolito, Lee, Goldstein, and Andre]{kirchenbauer2024lmd3} J.~Kirchenbauer, G.~Honke, G.~Somepalli, J.~Geiping, D.~Ippolito, K.~Lee, T.~Goldstein, and D.~Andre. 
 Lmd3: Language model data density dependence. 
 \emph{arXiv preprint arXiv:2405.06331}, 2024."
2407.14985,srivastava2024functional,"[Srivastava et~al.(2024)Srivastava, PV, Menon, Sukumar, Philipose, Prince, Thomas, et~al.]{srivastava2024functional} S.~Srivastava, A.~PV, S.~Menon, A.~Sukumar, A.~Philipose, S.~Prince, S.~Thomas, et~al.","Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap.","Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap.",,"[Srivastava et~al.(2024)Srivastava, PV, Menon, Sukumar, Philipose, Prince, Thomas, et~al.]{srivastava2024functional} S.~Srivastava, A.~PV, S.~Menon, A.~Sukumar, A.~Philipose, S.~Prince, S.~Thomas, et~al. 
 Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. 
 \emph{arXiv preprint arXiv:2402.19450}, 2024."
2407.14985,wang2024understanding,"[Wang et~al.(2024)Wang, Amayuelas, Zhang, Pan, Chen, and Wang]{wang2024understanding} X.~Wang, A.~Amayuelas, K.~Zhang, L.~Pan, W.~Chen, and W.~Y. Wang.",Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation.,Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation.,,"[Wang et~al.(2024)Wang, Amayuelas, Zhang, Pan, Chen, and Wang]{wang2024understanding} X.~Wang, A.~Amayuelas, K.~Zhang, L.~Pan, W.~Chen, and W.~Y. Wang. 
 Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation. 
 \emph{arXiv preprint arXiv:2402.03268}, 2024."
2407.15042,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.15042,chen2021transunet,"[Chen et~al.(2021)Chen, Lu, Yu, Luo, Adeli, Wang, Lu, Yuille, and Zhou]{chen2021transunet} Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan~L Yuille, and Yuyin Zhou.",Transunet: Transformers make strong encoders for medical image segmentation.,Transunet: Transformers make strong encoders for medical image segmentation.,,"[Chen et~al.(2021)Chen, Lu, Yu, Luo, Adeli, Wang, Lu, Yuille, and Zhou]{chen2021transunet} Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan~L Yuille, and Yuyin Zhou. 
 Transunet: Transformers make strong encoders for medical image segmentation. 
 \emph{arXiv preprint arXiv:2102.04306}, 2021."
2407.15042,hu2021lora,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}, 2021."
2407.15042,xu2023parameter,"[Xu et~al.(2023)Xu, Xie, Qin, Tao, and Wang]{xu2023parameter} Lingling Xu, Haoran Xie, Si-Zhao~Joe Qin, Xiaohui Tao, and Fu~Lee Wang.",Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.,Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.,,"[Xu et~al.(2023)Xu, Xie, Qin, Tao, and Wang]{xu2023parameter} Lingling Xu, Haoran Xie, Si-Zhao~Joe Qin, Xiaohui Tao, and Fu~Lee Wang. 
 Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment. 
 \emph{arXiv preprint arXiv:2312.12148}, 2023."
2407.15042,zhang2023customized,[Zhang and Liu(2023)]{zhang2023customized} Kaidong Zhang and Dong Liu.,Customized segment anything model for medical image segmentation.,Customized segment anything model for medical image segmentation.,,"[Zhang and Liu(2023)]{zhang2023customized} Kaidong Zhang and Dong Liu. 
 Customized segment anything model for medical image segmentation. 
 \emph{arXiv preprint arXiv:2304.13785}, 2023."
2407.15042,zhao2024galore,"[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore} Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.",Galore: Memory-efficient llm training by gradient low-rank projection.,Galore: Memory-efficient llm training by gradient low-rank projection.,,"[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore} Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 
 Galore: Memory-efficient llm training by gradient low-rank projection. 
 \emph{arXiv preprint arXiv:2403.03507}, 2024."
2407.15399,touvron2023llama,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023."
2407.17022,chan2023chateval,"[Chan et~al.(2023)Chan, Chen, Su, Yu, Xue, Zhang, Fu, and Liu]{chan2023chateval} Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.",Chateval: Towards better llm-based evaluators through multi-agent debate.,Chateval: Towards better llm-based evaluators through multi-agent debate.,,"[Chan et~al.(2023)Chan, Chen, Su, Yu, Xue, Zhang, Fu, and Liu]{chan2023chateval} Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 
 Chateval: Towards better llm-based evaluators through multi-agent debate. 
 \emph{arXiv preprint arXiv:2308.07201}, 2023."
2407.17022,dubois2024length,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto.",Length-controlled alpacaeval: A simple way to debias automatic evaluators.,Length-controlled alpacaeval: A simple way to debias automatic evaluators.,,"[Dubois et~al.(2024)Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 
 Length-controlled alpacaeval: A simple way to debias automatic evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}, 2024."
2407.17022,gao2024llm,"[Gao et~al.(2024)Gao, Chen, Zhang, Huang, Wan, and Sun]{gao2024llm} Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, and Lichao Sun.",Llm-as-a-coauthor: The challenges of detecting llm-human mixcase.,Llm-as-a-coauthor: The challenges of detecting llm-human mixcase.,,"[Gao et~al.(2024)Gao, Chen, Zhang, Huang, Wan, and Sun]{gao2024llm} Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, and Lichao Sun. 
 Llm-as-a-coauthor: The challenges of detecting llm-human mixcase. 
 \emph{arXiv preprint arXiv:2401.05952}, 2024."
2407.17022,holtzman2019curious,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.",The curious case of neural text degeneration.,The curious case of neural text degeneration.,,"[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 
 The curious case of neural text degeneration. 
 \emph{arXiv preprint arXiv:1904.09751}, 2019."
2407.17022,joseph2024factpico,"[Joseph et~al.(2024)Joseph, Chen, Trienes, G{\""o}ke, Coers, Xu, Wallace, and Li]{joseph2024factpico} Sebastian~Antony Joseph, Lily Chen, Jan Trienes, Hannah~Louisa G{\""o}ke, Monika Coers, Wei Xu, Byron~C Wallace, and Junyi~Jessy Li.",Factpico: Factuality evaluation for plain language summarization of medical evidence.,Factpico: Factuality evaluation for plain language summarization of medical evidence.,,"[Joseph et~al.(2024)Joseph, Chen, Trienes, G{\""o}ke, Coers, Xu, Wallace, and Li]{joseph2024factpico} Sebastian~Antony Joseph, Lily Chen, Jan Trienes, Hannah~Louisa G{\""o}ke, Monika Coers, Wei Xu, Byron~C Wallace, and Junyi~Jessy Li. 
 Factpico: Factuality evaluation for plain language summarization of medical evidence. 
 \emph{arXiv preprint arXiv:2402.11456}, 2024."
2407.17022,kim2023lifetox,"[Kim et~al.(2023{\natexlab{a}})Kim, Koo, Lee, Park, Lee, and Jung]{kim2023lifetox} Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, and Kyomin Jung.",Lifetox: Unveiling implicit toxicity in life advice.,Lifetox: Unveiling implicit toxicity in life advice.,,"[Kim et~al.(2023{\natexlab{a}})Kim, Koo, Lee, Park, Lee, and Jung]{kim2023lifetox} Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, and Kyomin Jung. 
 Lifetox: Unveiling implicit toxicity in life advice. 
 \emph{arXiv preprint arXiv:2311.09585}, 2023{\natexlab{a}}."
2407.17022,kim2023prometheus,"[Kim et~al.(2023{\natexlab{b}})Kim, Shin, Cho, Jang, Longpre, Lee, Yun, Shin, Kim, Thorne, et~al.]{kim2023prometheus} Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et~al.",Prometheus: Inducing fine-grained evaluation capability in language models.,Prometheus: Inducing fine-grained evaluation capability in language models.,,"[Kim et~al.(2023{\natexlab{b}})Kim, Shin, Cho, Jang, Longpre, Lee, Yun, Shin, Kim, Thorne, et~al.]{kim2023prometheus} Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et~al. 
 Prometheus: Inducing fine-grained evaluation capability in language models. 
 \emph{arXiv preprint arXiv:2310.08491}, 2023{\natexlab{b}}."
2407.17022,kim2024biggen,"[Kim et~al.(2024{\natexlab{a}})Kim, Suk, Cho, Longpre, Kim, Yoon, Son, Cho, Shafayat, Baek, et~al.]{kim2024biggen} Seungone Kim, Juyoung Suk, Ji~Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et~al.",The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models.,The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models.,,"[Kim et~al.(2024{\natexlab{a}})Kim, Suk, Cho, Longpre, Kim, Yoon, Son, Cho, Shafayat, Baek, et~al.]{kim2024biggen} Seungone Kim, Juyoung Suk, Ji~Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et~al. 
 The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models. 
 \emph{arXiv preprint arXiv:2406.05761}, 2024{\natexlab{a}}."
2407.17022,kim2024prometheus,"[Kim et~al.(2024{\natexlab{b}})Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo]{kim2024prometheus} Seungone Kim, Juyoung Suk, Shayne Longpre, Bill~Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo.",Prometheus 2: An open source language model specialized in evaluating other language models.,Prometheus 2: An open source language model specialized in evaluating other language models.,,"[Kim et~al.(2024{\natexlab{b}})Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo]{kim2024prometheus} Seungone Kim, Juyoung Suk, Shayne Longpre, Bill~Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 
 Prometheus 2: An open source language model specialized in evaluating other language models. 
 \emph{arXiv preprint arXiv:2405.01535}, 2024{\natexlab{b}}."
2407.17022,kim2023evallm,"[Kim et~al.(2023{\natexlab{c}})Kim, Lee, Shin, Kim, and Kim]{kim2023evallm} Tae~Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim.",Evallm: Interactive evaluation of large language model prompts on user-defined criteria.,Evallm: Interactive evaluation of large language model prompts on user-defined criteria.,,"[Kim et~al.(2023{\natexlab{c}})Kim, Lee, Shin, Kim, and Kim]{kim2023evallm} Tae~Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 
 Evallm: Interactive evaluation of large language model prompts on user-defined criteria. 
 \emph{arXiv preprint arXiv:2309.13633}, 2023{\natexlab{c}}."
2407.17022,lee2024prometheus,"[Lee et~al.(2024)Lee, Kim, Park, Kim, and Seo]{lee2024prometheus} Seongyun Lee, Seungone Kim, Sue~Hyun Park, Geewook Kim, and Minjoon Seo.",Prometheus-vision: Vision-language model as a judge for fine-grained evaluation.,Prometheus-vision: Vision-language model as a judge for fine-grained evaluation.,,"[Lee et~al.(2024)Lee, Kim, Park, Kim, and Seo]{lee2024prometheus} Seongyun Lee, Seungone Kim, Sue~Hyun Park, Geewook Kim, and Minjoon Seo. 
 Prometheus-vision: Vision-language model as a judge for fine-grained evaluation. 
 \emph{arXiv preprint arXiv:2401.06591}, 2024."
2407.17022,liu2023gpteval,"[Liu et~al.(2023)Liu, Iter, Xu, Wang, Xu, and Zhu]{liu2023gpteval} Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.",Gpteval: Nlg evaluation using gpt-4 with better human alignment.,Gpteval: Nlg evaluation using gpt-4 with better human alignment.,,"[Liu et~al.(2023)Liu, Iter, Xu, Wang, Xu, and Zhu]{liu2023gpteval} Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 
 Gpteval: Nlg evaluation using gpt-4 with better human alignment. 
 \emph{arXiv preprint arXiv:2303.16634}, 2023."
2407.17022,orenstrakh2023detecting,"[Orenstrakh et~al.(2023)Orenstrakh, Karnalim, Suarez, and Liut]{orenstrakh2023detecting} Michael~Sheinman Orenstrakh, Oscar Karnalim, Carlos~Anibal Suarez, and Michael Liut.",Detecting llm-generated text in computing education: A comparative study for chatgpt cases.,Detecting llm-generated text in computing education: A comparative study for chatgpt cases.,,"[Orenstrakh et~al.(2023)Orenstrakh, Karnalim, Suarez, and Liut]{orenstrakh2023detecting} Michael~Sheinman Orenstrakh, Oscar Karnalim, Carlos~Anibal Suarez, and Michael Liut. 
 Detecting llm-generated text in computing education: A comparative study for chatgpt cases. 
 \emph{arXiv preprint arXiv:2307.07411}, 2023."
2407.17022,verga2024replacing,"[Verga et~al.(2024)Verga, Hofstatter, Althammer, Su, Piktus, Arkhangorodsky, Xu, White, and Lewis]{verga2024replacing} Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis.",Replacing judges with juries: Evaluating llm generations with a panel of diverse models.,Replacing judges with juries: Evaluating llm generations with a panel of diverse models.,,"[Verga et~al.(2024)Verga, Hofstatter, Althammer, Su, Piktus, Arkhangorodsky, Xu, White, and Lewis]{verga2024replacing} Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 
 Replacing judges with juries: Evaluating llm generations with a panel of diverse models. 
 \emph{arXiv preprint arXiv:2404.18796}, 2024."
2407.17022,ye2023flask,"[Ye et~al.(2023)Ye, Kim, Kim, Hwang, Kim, Jo, Thorne, Kim, and Seo]{ye2023flask} Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo.",Flask: Fine-grained language model evaluation based on alignment skill sets.,Flask: Fine-grained language model evaluation based on alignment skill sets.,,"[Ye et~al.(2023)Ye, Kim, Kim, Hwang, Kim, Jo, Thorne, Kim, and Seo]{ye2023flask} Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 
 Flask: Fine-grained language model evaluation based on alignment skill sets. 
 \emph{arXiv preprint arXiv:2307.10928}, 2023."
2407.17022,zhang2019bertscore,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}, 2019."
2407.17022,zheng2023judging,"[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.",Judging llm-as-a-judge with mt-bench and chatbot arena.,Judging llm-as-a-judge with mt-bench and chatbot arena.,,"[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 
 Judging llm-as-a-judge with mt-bench and chatbot arena. 
 \emph{arXiv preprint arXiv:2306.05685}, 2023."
2407.17417,bentley2020quantifying,"[Bentley et~al.(2020)Bentley, Gibney, Hoppenworth, and Jha]{bentley2020quantifying} J.~W. Bentley, D.~Gibney, G.~Hoppenworth, and S.~K. Jha.",Quantifying membership inference vulnerability via generalization gap and other model metrics.,Quantifying membership inference vulnerability via generalization gap and other model metrics.,,"[Bentley et~al.(2020)Bentley, Gibney, Hoppenworth, and Jha]{bentley2020quantifying} J.~W. Bentley, D.~Gibney, G.~Hoppenworth, and S.~K. Jha. 
 Quantifying membership inference vulnerability via generalization gap and other model metrics. 
 \emph{arXiv preprint arXiv:2009.05669}, 2020."
2407.17417,black2022gpt,"[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, et~al.]{black2022gpt} S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He, C.~Leahy, K.~McDonell, J.~Phang, et~al.",Gpt-neox-20b: An open-source autoregressive language model.,Gpt-neox-20b: An open-source autoregressive language model.,,"[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, et~al.]{black2022gpt} S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He, C.~Leahy, K.~McDonell, J.~Phang, et~al. 
 Gpt-neox-20b: An open-source autoregressive language model. 
 \emph{arXiv preprint arXiv:2204.06745}, 2022."
2407.17417,carlini2022quantifying,"[Carlini et~al.(2022{\natexlab{b}})Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang]{carlini2022quantifying} N.~Carlini, D.~Ippolito, M.~Jagielski, K.~Lee, F.~Tramer, and C.~Zhang.",Quantifying memorization across neural language models.,Quantifying memorization across neural language models.,,"[Carlini et~al.(2022{\natexlab{b}})Carlini, Ippolito, Jagielski, Lee, Tramer, and Zhang]{carlini2022quantifying} N.~Carlini, D.~Ippolito, M.~Jagielski, K.~Lee, F.~Tramer, and C.~Zhang. 
 Quantifying memorization across neural language models. 
 \emph{arXiv preprint arXiv:2202.07646}, 2022{\natexlab{b}}."
2407.17417,christ2023undetectable,"[Christ et~al.(2023)Christ, Gunn, and Zamir]{christ2023undetectable} M.~Christ, S.~Gunn, and O.~Zamir.",Undetectable watermarks for language models.,Undetectable watermarks for language models.,,"[Christ et~al.(2023)Christ, Gunn, and Zamir]{christ2023undetectable} M.~Christ, S.~Gunn, and O.~Zamir. 
 Undetectable watermarks for language models. 
 \emph{arXiv preprint arXiv:2306.09194}, 2023."
2407.17417,das2024blind,"[Das et~al.(2024)Das, Zhang, and Tram{\`e}r]{das2024blind} D.~Das, J.~Zhang, and F.~Tram{\`e}r.",Blind baselines beat membership inference attacks for foundation models.,Blind baselines beat membership inference attacks for foundation models.,,"[Das et~al.(2024)Das, Zhang, and Tram{\`e}r]{das2024blind} D.~Das, J.~Zhang, and F.~Tram{\`e}r. 
 Blind baselines beat membership inference attacks for foundation models. 
 \emph{arXiv preprint arXiv:2406.16201}, 2024."
2407.17417,duarte2024cop,"[Duarte et~al.(2024)Duarte, Zhao, Oliveira, and Li]{duarte2024cop} A.~V. Duarte, X.~Zhao, A.~L. Oliveira, and L.~Li.",De-cop: Detecting copyrighted content in language models training data.,De-cop: Detecting copyrighted content in language models training data.,,"[Duarte et~al.(2024)Duarte, Zhao, Oliveira, and Li]{duarte2024cop} A.~V. Duarte, X.~Zhao, A.~L. Oliveira, and L.~Li. 
 De-cop: Detecting copyrighted content in language models training data. 
 \emph{arXiv preprint arXiv:2402.09910}, 2024."
2407.17417,elkin2023can,"[Elkin-Koren et~al.(2023)Elkin-Koren, Hacohen, Livni, and Moran]{elkin2023can} N.~Elkin-Koren, U.~Hacohen, R.~Livni, and S.~Moran.",Can copyright be reduced to privacy?,Can copyright be reduced to privacy?,,"[Elkin-Koren et~al.(2023)Elkin-Koren, Hacohen, Livni, and Moran]{elkin2023can} N.~Elkin-Koren, U.~Hacohen, R.~Livni, and S.~Moran. 
 Can copyright be reduced to privacy? 
 \emph{arXiv preprint arXiv:2305.14822}, 2023."
2407.17417,hacohen2024not,"[Hacohen et~al.(2024)Hacohen, Haviv, Sarfaty, Friedman, Elkin-Koren, Livni, and Bermano]{hacohen2024not} U.~Hacohen, A.~Haviv, S.~Sarfaty, B.~Friedman, N.~Elkin-Koren, R.~Livni, and A.~H. Bermano.",Not all similarities are created equal: Leveraging data-driven biases to inform genai copyright disputes.,Not all similarities are created equal: Leveraging data-driven biases to inform genai copyright disputes.,,"[Hacohen et~al.(2024)Hacohen, Haviv, Sarfaty, Friedman, Elkin-Koren, Livni, and Bermano]{hacohen2024not} U.~Hacohen, A.~Haviv, S.~Sarfaty, B.~Friedman, N.~Elkin-Koren, R.~Livni, and A.~H. Bermano. 
 Not all similarities are created equal: Leveraging data-driven biases to inform genai copyright disputes. 
 \emph{arXiv preprint arXiv:2403.17691}, 2024."
2407.17417,hans2024like,"[Hans et~al.(2024)Hans, Wen, Jain, Kirchenbauer, Kazemi, Singhania, Singh, Somepalli, Geiping, Bhatele, et~al.]{hans2024like} A.~Hans, Y.~Wen, N.~Jain, J.~Kirchenbauer, H.~Kazemi, P.~Singhania, S.~Singh, G.~Somepalli, J.~Geiping, A.~Bhatele, et~al.","Be like a goldfish, don't memorize! mitigating memorization in generative llms.","Be like a goldfish, don't memorize! mitigating memorization in generative llms.",,"[Hans et~al.(2024)Hans, Wen, Jain, Kirchenbauer, Kazemi, Singhania, Singh, Somepalli, Geiping, Bhatele, et~al.]{hans2024like} A.~Hans, Y.~Wen, N.~Jain, J.~Kirchenbauer, H.~Kazemi, P.~Singhania, S.~Singh, G.~Somepalli, J.~Geiping, A.~Bhatele, et~al. 
 Be like a goldfish, don't memorize! mitigating memorization in generative llms. 
 \emph{arXiv preprint arXiv:2406.10209}, 2024."
2407.17417,karamolegkou2023copyright,"[Karamolegkou et~al.(2023)Karamolegkou, Li, Zhou, and S{\o}gaard]{karamolegkou2023copyright} A.~Karamolegkou, J.~Li, L.~Zhou, and A.~S{\o}gaard.",Copyright violations and large language models.,Copyright violations and large language models.,,"[Karamolegkou et~al.(2023)Karamolegkou, Li, Zhou, and S{\o}gaard]{karamolegkou2023copyright} A.~Karamolegkou, J.~Li, L.~Zhou, and A.~S{\o}gaard. 
 Copyright violations and large language models. 
 \emph{arXiv preprint arXiv:2310.13771}, 2023."
2407.17417,kuditipudi2023robust,"[Kuditipudi et~al.(2023)Kuditipudi, Thickstun, Hashimoto, and Liang]{kuditipudi2023robust} R.~Kuditipudi, J.~Thickstun, T.~Hashimoto, and P.~Liang.",Robust distortion-free watermarks for language models.,Robust distortion-free watermarks for language models.,,"[Kuditipudi et~al.(2023)Kuditipudi, Thickstun, Hashimoto, and Liang]{kuditipudi2023robust} R.~Kuditipudi, J.~Thickstun, T.~Hashimoto, and P.~Liang. 
 Robust distortion-free watermarks for language models. 
 \emph{arXiv preprint arXiv:2307.15593}, 2023."
2407.17417,lee2021deduplicating,"[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{lee2021deduplicating} K.~Lee, D.~Ippolito, A.~Nystrom, C.~Zhang, D.~Eck, C.~Callison-Burch, and N.~Carlini.",Deduplicating training data makes language models better.,Deduplicating training data makes language models better.,,"[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch, and Carlini]{lee2021deduplicating} K.~Lee, D.~Ippolito, A.~Nystrom, C.~Zhang, D.~Eck, C.~Callison-Burch, and N.~Carlini. 
 Deduplicating training data makes language models better. 
 \emph{arXiv preprint arXiv:2107.06499}, 2021."
2407.17417,mireshghallah2022quantifying,"[Mireshghallah et~al.(2022)Mireshghallah, Goyal, Uniyal, Berg-Kirkpatrick, and Shokri]{mireshghallah2022quantifying} F.~Mireshghallah, K.~Goyal, A.~Uniyal, T.~Berg-Kirkpatrick, and R.~Shokri.",Quantifying privacy risks of masked language models using membership inference attacks.,Quantifying privacy risks of masked language models using membership inference attacks.,,"[Mireshghallah et~al.(2022)Mireshghallah, Goyal, Uniyal, Berg-Kirkpatrick, and Shokri]{mireshghallah2022quantifying} F.~Mireshghallah, K.~Goyal, A.~Uniyal, T.~Berg-Kirkpatrick, and R.~Shokri. 
 Quantifying privacy risks of masked language models using membership inference attacks. 
 \emph{arXiv preprint arXiv:2203.03929}, 2022."
2407.17417,oren2023proving,"[Oren et~al.(2023)Oren, Meister, Chatterji, Ladhak, and Hashimoto]{oren2023proving} Y.~Oren, N.~Meister, N.~Chatterji, F.~Ladhak, and T.~B. Hashimoto.",Proving test set contamination in black box language models.,Proving test set contamination in black box language models.,,"[Oren et~al.(2023)Oren, Meister, Chatterji, Ladhak, and Hashimoto]{oren2023proving} Y.~Oren, N.~Meister, N.~Chatterji, F.~Ladhak, and T.~B. Hashimoto. 
 Proving test set contamination in black box language models. 
 \emph{arXiv preprint arXiv:2310.17623}, 2023."
2407.17417,ren2024copyright,"[Ren et~al.(2024)Ren, Xu, He, Cui, Zeng, Zhang, Wen, Ding, Liu, Chang, et~al.]{ren2024copyright} J.~Ren, H.~Xu, P.~He, Y.~Cui, S.~Zeng, J.~Zhang, H.~Wen, J.~Ding, H.~Liu, Y.~Chang, et~al.",Copyright protection in generative ai: A technical perspective.,Copyright protection in generative ai: A technical perspective.,,"[Ren et~al.(2024)Ren, Xu, He, Cui, Zeng, Zhang, Wen, Ding, Liu, Chang, et~al.]{ren2024copyright} J.~Ren, H.~Xu, P.~He, Y.~Cui, S.~Zeng, J.~Zhang, H.~Wen, J.~Ding, H.~Liu, Y.~Chang, et~al. 
 Copyright protection in generative ai: A technical perspective. 
 \emph{arXiv preprint arXiv:2402.02333}, 2024."
2407.17417,shi2023detecting,"[Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2023detecting} W.~Shi, A.~Ajith, M.~Xia, Y.~Huang, D.~Liu, T.~Blevins, D.~Chen, and L.~Zettlemoyer.",Detecting pretraining data from large language models.,Detecting pretraining data from large language models.,,"[Shi et~al.(2023)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2023detecting} W.~Shi, A.~Ajith, M.~Xia, Y.~Huang, D.~Liu, T.~Blevins, D.~Chen, and L.~Zettlemoyer. 
 Detecting pretraining data from large language models. 
 \emph{arXiv preprint arXiv:2310.16789}, 2023."
2407.17417,touvron2023llama,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama} H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}, 2023."
2407.17417,zhang2024min,"[Zhang et~al.(2024)Zhang, Sun, Yeats, Ouyang, Kuo, Zhang, Yang, and Li]{zhang2024min} J.~Zhang, J.~Sun, E.~Yeats, Y.~Ouyang, M.~Kuo, J.~Zhang, H.~Yang, and H.~Li.",Min-k\%++: Improved baseline for detecting pre-training data from large language models.,Min-k\%++: Improved baseline for detecting pre-training data from large language models.,,"[Zhang et~al.(2024)Zhang, Sun, Yeats, Ouyang, Kuo, Zhang, Yang, and Li]{zhang2024min} J.~Zhang, J.~Sun, E.~Yeats, Y.~Ouyang, M.~Kuo, J.~Zhang, H.~Yang, and H.~Li. 
 Min-k\%++: Improved baseline for detecting pre-training data from large language models. 
 \emph{arXiv preprint arXiv:2404.02936}, 2024."
2407.17417,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, et~al.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt} S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, et~al. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2407.17417,zhao2023provable,"[Zhao et~al.(2023)Zhao, Ananth, Li, and Wang]{zhao2023provable} X.~Zhao, P.~Ananth, L.~Li, and Y.-X. Wang.",Provable robust watermarking for ai-generated text.,Provable robust watermarking for ai-generated text.,,"[Zhao et~al.(2023)Zhao, Ananth, Li, and Wang]{zhao2023provable} X.~Zhao, P.~Ananth, L.~Li, and Y.-X. Wang. 
 Provable robust watermarking for ai-generated text. 
 \emph{arXiv preprint arXiv:2306.17439}, 2023."
2407.17467,explain_scaling_law,"[{Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma}]{explain_scaling_law} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 2021.",Explaining neural scaling laws.,Explaining neural scaling laws.,,"[{Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma}]{explain_scaling_law} Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 2021. 
 Explaining neural scaling laws. 
 \emph{arXiv preprint arXiv:2102.06701}."
2407.17467,med_llm,"[{Chen et~al.(2023)Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{med_llm} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023.",Meditron-70b: Scaling medical pretraining for large language models.,Meditron-70b: Scaling medical pretraining for large language models.,,"[{Chen et~al.(2023)Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{med_llm} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023. 
 Meditron-70b: Scaling medical pretraining for large language models. 
 \emph{arXiv preprint arXiv:2311.16079}."
2407.17467,law_llm,"[{Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado et~al.}]{law_llm} Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre~FT Martins, Fabrizio Esposito, Vera~L{\'u}cia Raposo, Sofia Morgado, et~al. 2024.",Saullm-7b: A pioneering large language model for law.,Saullm-7b: A pioneering large language model for law.,,"[{Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado et~al.}]{law_llm} Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre~FT Martins, Fabrizio Esposito, Vera~L{\'u}cia Raposo, Sofia Morgado, et~al. 2024. 
 Saullm-7b: A pioneering large language model for law. 
 \emph{arXiv preprint arXiv:2403.03883}."
2407.17467,cpt_in_cv,"[{Cossu et~al.(2022)Cossu, Tuytelaars, Carta, Passaro, Lomonaco, and Bacciu}]{cpt_in_cv} Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. 2022.",Continual pre-training mitigates forgetting in language and vision.,Continual pre-training mitigates forgetting in language and vision.,,"[{Cossu et~al.(2022)Cossu, Tuytelaars, Carta, Passaro, Lomonaco, and Bacciu}]{cpt_in_cv} Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. 2022. 
 Continual pre-training mitigates forgetting in language and vision. 
 \emph{arXiv preprint arXiv:2205.09357}."
2407.17467,understanding_loss,"[{Du et~al.(2024)Du, Zeng, Dong, and Tang}]{understanding_loss} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. 2024.",Understanding emergent abilities of language models from the loss perspective.,Understanding emergent abilities of language models from the loss perspective.,,"[{Du et~al.(2024)Du, Zeng, Dong, and Tang}]{understanding_loss} Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. 2024. 
 Understanding emergent abilities of language models from the loss perspective. 
 \emph{arXiv preprint arXiv:2403.15796}."
2407.17467,multimodel_scaling_law,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{multimodel_scaling_law} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{multimodel_scaling_law} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}."
2407.17467,openai_sft_scaling_law,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{openai_sft_scaling_law} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021.",Scaling laws for transfer.,Scaling laws for transfer.,,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and McCandlish}]{openai_sft_scaling_law} Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. 
 Scaling laws for transfer. 
 \emph{arXiv preprint arXiv:2102.01293}."
2407.17467,deep_learning_scaling_law_baidu,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{deep_learning_scaling_law_baidu} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{deep_learning_scaling_law_baidu} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}."
2407.17467,chinchilla_scaling_law,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{chinchilla_scaling_law} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{chinchilla_scaling_law} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2407.17467,openai_scaling_law,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{openai_scaling_law} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{openai_scaling_law} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2407.17467,autocoder,"[{Lei et~al.(2024)Lei, Li, and Chen}]{autocoder} Bin Lei, Yuchen Li, and Qiuwu Chen. 2024.",Autocoder: Enhancing code large language model with$\backslash$textsc $\{$AIEV-Instruct$\}$.,Autocoder: Enhancing code large language model with$\backslash$textsc $\{$AIEV-Instruct$\}$.,,"[{Lei et~al.(2024)Lei, Li, and Chen}]{autocoder} Bin Lei, Yuchen Li, and Qiuwu Chen. 2024. 
 Autocoder: Enhancing code large language model with$\backslash$textsc $\{$AIEV-Instruct$\}$. 
 \emph{arXiv preprint arXiv:2405.14906}."
2407.17467,rectified_sft_scaling_law,"[{Lin et~al.(2024)Lin, Huang, Ye, Chen, Wang, Li, Ma, Wan, Zou, and Liang}]{rectified_sft_scaling_law} Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, and Yitao Liang. 2024.",Selecting large language model to fine-tune via rectified scaling law.,Selecting large language model to fine-tune via rectified scaling law.,,"[{Lin et~al.(2024)Lin, Huang, Ye, Chen, Wang, Li, Ma, Wan, Zou, and Liang}]{rectified_sft_scaling_law} Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, and Yitao Liang. 2024. 
 Selecting large language model to fine-tune via rectified scaling law. 
 \emph{arXiv preprint arXiv:2402.02314}."
2407.17467,empirical_cv_catastrophic_forgetting,"[{Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang}]{empirical_cv_catastrophic_forgetting} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023.",An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,An empirical study of catastrophic forgetting in large language models during continual fine-tuning.,,"[{Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang}]{empirical_cv_catastrophic_forgetting} Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. 
 An empirical study of catastrophic forgetting in large language models during continual fine-tuning. 
 \emph{arXiv preprint arXiv:2308.08747}."
2407.17467,data_scaling_law,[{Pandey(2024)}]{data_scaling_law} Rohan Pandey. 2024.,gzip predicts data-dependent scaling laws.,gzip predicts data-dependent scaling laws.,,"[{Pandey(2024)}]{data_scaling_law} Rohan Pandey. 2024. 
 gzip predicts data-dependent scaling laws. 
 \emph{arXiv preprint arXiv:2405.16684}."
2407.17467,ali_cpt_scaling_law,"[{Que et~al.(2024)Que, Liu, Zhang, Zhang, Qu, Ma, Duan, Bai, Wang, Zhang et~al.}]{ali_cpt_scaling_law} Haoran Que, Jiaheng Liu, Ge~Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et~al. 2024.",D-cpt law: Domain-specific continual pre-training scaling law for large language models.,D-cpt law: Domain-specific continual pre-training scaling law for large language models.,,"[{Que et~al.(2024)Que, Liu, Zhang, Zhang, Qu, Ma, Duan, Bai, Wang, Zhang et~al.}]{ali_cpt_scaling_law} Haoran Que, Jiaheng Liu, Ge~Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et~al. 2024. 
 D-cpt law: Domain-specific continual pre-training scaling law for large language models. 
 \emph{arXiv preprint arXiv:2406.01375}."
2407.17467,nanolm,[{Yao and Wang(2023)}]{nanolm} Yiqun Yao and Yequan Wang. 2023.,Research without re-search: Maximal update parametrization yields accurate loss prediction across scales.,Research without re-search: Maximal update parametrization yields accurate loss prediction across scales.,,"[{Yao and Wang(2023)}]{nanolm} Yiqun Yao and Yequan Wang. 2023. 
 Research without re-search: Maximal update parametrization yields accurate loss prediction across scales. 
 \emph{arXiv preprint arXiv:2304.06875}."
2407.17467,pujianglab_mixing_law,"[{Ye et~al.(2024)Ye, Liu, Sun, Zhou, Zhan, and Qiu}]{pujianglab_mixing_law} Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. 2024.",Data mixing laws: Optimizing data mixtures by predicting language modeling performance.,Data mixing laws: Optimizing data mixtures by predicting language modeling performance.,,"[{Ye et~al.(2024)Ye, Liu, Sun, Zhou, Zhan, and Qiu}]{pujianglab_mixing_law} Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. 2024. 
 Data mixing laws: Optimizing data mixtures by predicting language modeling performance. 
 \emph{arXiv preprint arXiv:2403.16952}."
2407.17467,cpt_llm_survey,"[{Y{\i}ld{\i}z et~al.(2024)Y{\i}ld{\i}z, Ravichandran, Punia, Bethge, and Ermis}]{cpt_llm_survey} {\c{C}}a{\u{g}}atay Y{\i}ld{\i}z, Nishaanth~Kanna Ravichandran, Prishruit Punia, Matthias Bethge, and Beyza Ermis. 2024.",Investigating continual pretraining in large language models: Insights and implications.,Investigating continual pretraining in large language models: Insights and implications.,,"[{Y{\i}ld{\i}z et~al.(2024)Y{\i}ld{\i}z, Ravichandran, Punia, Bethge, and Ermis}]{cpt_llm_survey} {\c{C}}a{\u{g}}atay Y{\i}ld{\i}z, Nishaanth~Kanna Ravichandran, Prishruit Punia, Matthias Bethge, and Beyza Ermis. 2024. 
 Investigating continual pretraining in large language models: Insights and implications. 
 \emph{arXiv preprint arXiv:2402.17400}."
2407.17467,scaling_relationship_loss_ali,"[{Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou}]{scaling_relationship_loss_ali} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.",Scaling relationship on learning mathematical reasoning with large language models.,Scaling relationship on learning mathematical reasoning with large language models.,,"[{Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou}]{scaling_relationship_loss_ali} Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. 
 Scaling relationship on learning mathematical reasoning with large language models. 
 \emph{arXiv preprint arXiv:2308.01825}."
2407.17467,zhanghengyuan_balancing,"[{Zhang et~al.(2024)Zhang, Wu, Li, Yang, Zhao, Jiang, and Tan}]{zhanghengyuan_balancing} Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui Zhao, Yong Jiang, and Fei Tan. 2024.",Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model.,Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model.,,"[{Zhang et~al.(2024)Zhang, Wu, Li, Yang, Zhao, Jiang, and Tan}]{zhanghengyuan_balancing} Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui Zhao, Yong Jiang, and Fei Tan. 2024. 
 Balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model. 
 \emph{arXiv preprint arXiv:2404.10306}."
2407.17636,zhang2019bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.   2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.   2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
2407.18418,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2407.18418,ahdritz2024distinguishing,"[{Ahdritz et~al.(2024)Ahdritz, Qin, Vyas, Barak, and Edelman}]{ahdritz2024distinguishing} Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin~L Edelman. 2024.",Distinguishing the knowable from the unknowable with language models.,Distinguishing the knowable from the unknowable with language models.,,"[{Ahdritz et~al.(2024)Ahdritz, Qin, Vyas, Barak, and Edelman}]{ahdritz2024distinguishing} Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin~L Edelman. 2024. 
 Distinguishing the knowable from the unknowable with language models. 
 \emph{arXiv preprint arXiv:2402.03563}."
2407.18418,amayuelas2023knowledge,"[{Amayuelas et~al.(2023)Amayuelas, Pan, Chen, and Wang}]{amayuelas2023knowledge} Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 2023.",Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models.,Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models.,,"[{Amayuelas et~al.(2023)Amayuelas, Pan, Chen, and Wang}]{amayuelas2023knowledge} Alfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 2023. 
 Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. 
 \emph{arXiv preprint arXiv:2305.13712}."
2407.18418,anwar2024foundational,"[{Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana, Jenner, Casper, Sourbut et~al.}]{anwar2024foundational} Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep~Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et~al. 2024.",Foundational challenges in assuring alignment and safety of large language models.,Foundational challenges in assuring alignment and safety of large language models.,,"[{Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana, Jenner, Casper, Sourbut et~al.}]{anwar2024foundational} Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep~Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et~al. 2024. 
 Foundational challenges in assuring alignment and safety of large language models. 
 \emph{arXiv preprint arXiv:2404.09932}."
2407.18418,brahman-kumar2024,"[{Brahman et~al.(2024)Brahman, Kumar, Balachandran, Dasigi, Pyatkin, Ravichander, Wiegreffe, Dziri, Chandu, Hessel et~al.}]{brahman-kumar2024} Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et~al. 2024.",The art of saying no: Contextual noncompliance in language models.,The art of saying no: Contextual noncompliance in language models.,,"[{Brahman et~al.(2024)Brahman, Kumar, Balachandran, Dasigi, Pyatkin, Ravichander, Wiegreffe, Dziri, Chandu, Hessel et~al.}]{brahman-kumar2024} Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et~al. 2024. 
 The art of saying no: Contextual noncompliance in language models. 
 \emph{arXiv preprint arXiv:2407.12043}."
2407.18418,cao2023defending,"[{Cao et~al.(2023)Cao, Cao, Lin, and Chen}]{cao2023defending} Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen. 2023.",Defending against alignment-breaking attacks via robustly aligned {LLM}.,Defending against alignment-breaking attacks via robustly aligned {LLM}.,,"[{Cao et~al.(2023)Cao, Cao, Lin, and Chen}]{cao2023defending} Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen. 2023. 
 Defending against alignment-breaking attacks via robustly aligned {LLM}. 
 \emph{arXiv preprint arXiv:2309.14348}."
2407.18418,chao2024jailbreakbench,"[{Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramer et~al.}]{chao2024jailbreakbench} Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J Pappas, Florian Tramer, et~al. 2024.",Jailbreakbench: An open robustness benchmark for jailbreaking large language models.,Jailbreakbench: An open robustness benchmark for jailbreaking large language models.,,"[{Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramer et~al.}]{chao2024jailbreakbench} Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J Pappas, Florian Tramer, et~al. 2024. 
 Jailbreakbench: An open robustness benchmark for jailbreaking large language models. 
 \emph{arXiv preprint arXiv:2404.01318}."
2407.18418,chao2023jailbreaking,"[{Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong}]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong. 2023.",Jailbreaking black box large language models in twenty queries.,Jailbreaking black box large language models in twenty queries.,,"[{Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong}]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong. 2023. 
 Jailbreaking black box large language models in twenty queries. 
 \emph{arXiv preprint arXiv:2310.08419}."
2407.18418,chowdhery2022palm,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022.",Palm: Scaling language modeling with pathways.,Palm: Scaling language modeling with pathways.,,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2022. 
 Palm: Scaling language modeling with pathways. 
 \emph{arXiv preprint arXiv:2204.02311}."
2407.18418,cole2023selectively,"[{Cole et~al.(2023)Cole, Zhang, Gillick, Eisenschlos, Dhingra, and Eisenstein}]{cole2023selectively} Jeremy~R Cole, Michael~JQ Zhang, Daniel Gillick, Julian~Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. 2023.",Selectively answering ambiguous questions.,Selectively answering ambiguous questions.,,"[{Cole et~al.(2023)Cole, Zhang, Gillick, Eisenschlos, Dhingra, and Eisenstein}]{cole2023selectively} Jeremy~R Cole, Michael~JQ Zhang, Daniel Gillick, Julian~Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. 2023. 
 Selectively answering ambiguous questions. 
 \emph{arXiv preprint arXiv:2305.14613}."
2407.18418,duan2023shifting,"[{Duan et~al.(2023)Duan, Cheng, Wang, Wang, Zavalny, Xu, Kailkhura, and Xu}]{duan2023shifting} Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2023.",Shifting attention to relevance: Towards the uncertainty estimation of large language models.,Shifting attention to relevance: Towards the uncertainty estimation of large language models.,,"[{Duan et~al.(2023)Duan, Cheng, Wang, Wang, Zavalny, Xu, Kailkhura, and Xu}]{duan2023shifting} Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2023. 
 Shifting attention to relevance: Towards the uncertainty estimation of large language models. 
 \emph{arXiv preprint arXiv:2307.01379}."
2407.18418,feng2024teaching,"[{Feng et~al.(2024{\natexlab{a}})Feng, Shi, Wang, Ding, Ahia, Li, Balachandran, Sitaram, and Tsvetkov}]{feng2024teaching} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Orevaoghene Ahia, Shuyue~Stella Li, Vidhisha Balachandran, Sunayana Sitaram, and Yulia Tsvetkov. 2024{\natexlab{a}}.",Teaching {LLMs} to abstain across languages via multilingual feedback.,Teaching {LLMs} to abstain across languages via multilingual feedback.,,"[{Feng et~al.(2024{\natexlab{a}})Feng, Shi, Wang, Ding, Ahia, Li, Balachandran, Sitaram, and Tsvetkov}]{feng2024teaching} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Orevaoghene Ahia, Shuyue~Stella Li, Vidhisha Balachandran, Sunayana Sitaram, and Yulia Tsvetkov. 2024{\natexlab{a}}. 
 Teaching {LLMs} to abstain across languages via multilingual feedback. 
 \emph{arXiv preprint arXiv:2406.15948}."
2407.18418,feng2024don,"[{Feng et~al.(2024{\natexlab{b}})Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov}]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024{\natexlab{b}}.","Don't hallucinate, abstain: Identifying {LLM} knowledge gaps via {Multi-LLM} collaboration.","Don't hallucinate, abstain: Identifying {LLM} knowledge gaps via {Multi-LLM} collaboration.",,"[{Feng et~al.(2024{\natexlab{b}})Feng, Shi, Wang, Ding, Balachandran, and Tsvetkov}]{feng2024don} Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024{\natexlab{b}}. 
 Don't hallucinate, abstain: Identifying {LLM} knowledge gaps via {Multi-LLM} collaboration. 
 \emph{arXiv preprint arXiv:2402.00367}."
2407.18418,guo2024controllable,"[{Guo et~al.(2024)Guo, Cui, Yuan, Ding, Wang, Chen, Sun, Xie, Zhou, Lin et~al.}]{guo2024controllable} Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et~al. 2024.",Controllable preference optimization: Toward controllable multi-objective alignment.,Controllable preference optimization: Toward controllable multi-objective alignment.,,"[{Guo et~al.(2024)Guo, Cui, Yuan, Ding, Wang, Chen, Sun, Xie, Zhou, Lin et~al.}]{guo2024controllable} Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et~al. 2024. 
 Controllable preference optimization: Toward controllable multi-objective alignment. 
 \emph{arXiv preprint arXiv:2402.19085}."
2407.18418,han2024wildguard,"[{Han et~al.(2024)Han, Rao, Ettinger, Jiang, Lin, Lambert, Choi, and Dziri}]{han2024wildguard} Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill~Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024.","Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms.","Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms.",,"[{Han et~al.(2024)Han, Rao, Ettinger, Jiang, Lin, Lambert, Choi, and Dziri}]{han2024wildguard} Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill~Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. 
 Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. 
 \emph{arXiv preprint arXiv:2406.18495}."
2407.18418,hestness2017deep,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}."
2407.18418,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2407.18418,hou2023decomposing,"[{Hou et~al.(2023)Hou, Liu, Qian, Andreas, Chang, and Zhang}]{hou2023decomposing} Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. 2023.",Decomposing uncertainty for large language models through input clarification ensembling.,Decomposing uncertainty for large language models through input clarification ensembling.,,"[{Hou et~al.(2023)Hou, Liu, Qian, Andreas, Chang, and Zhang}]{hou2023decomposing} Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. 2023. 
 Decomposing uncertainty for large language models through input clarification ensembling. 
 \emph{arXiv preprint arXiv:2311.08718}."
2407.18418,huang2024calibrating,"[{Huang et~al.(2024{\natexlab{b}})Huang, Liu, Thirukovalluru, Cohan, and Dhingra}]{huang2024calibrating} Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, and Bhuwan Dhingra. 2024{\natexlab{b}}.",Calibrating long-form generations from large language models.,Calibrating long-form generations from large language models.,,"[{Huang et~al.(2024{\natexlab{b}})Huang, Liu, Thirukovalluru, Cohan, and Dhingra}]{huang2024calibrating} Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, and Bhuwan Dhingra. 2024{\natexlab{b}}. 
 Calibrating long-form generations from large language models. 
 \emph{arXiv preprint arXiv:2402.06544}."
2407.18418,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2407.18418,kim2024epistemology,[{Kim and Thorne(2024)}]{kim2024epistemology} Minsu Kim and James Thorne. 2024.,Epistemology of language models: Do language models have holistic knowledge?,Epistemology of language models: Do language models have holistic knowledge?,,"[{Kim and Thorne(2024)}]{kim2024epistemology} Minsu Kim and James Thorne. 2024. 
 Epistemology of language models: Do language models have holistic knowledge? 
 \emph{arXiv preprint arXiv:2403.12862}."
2407.18418,kirk2023personalisation,"[{Kirk et~al.(2023{\natexlab{b}})Kirk, Vidgen, R{\""o}ttger, and Hale}]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul R{\""o}ttger, and Scott~A Hale. 2023{\natexlab{b}}.",Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback.,,"[{Kirk et~al.(2023{\natexlab{b}})Kirk, Vidgen, R{\""o}ttger, and Hale}]{kirk2023personalisation} Hannah~Rose Kirk, Bertie Vidgen, Paul R{\""o}ttger, and Scott~A Hale. 2023{\natexlab{b}}. 
 Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. 
 \emph{arXiv preprint arXiv:2303.05453}."
2407.18418,li2024salad,"[{Li et~al.(2024{\natexlab{a}})Li, Dong, Wang, Hu, Zuo, Lin, Qiao, and Shao}]{li2024salad} Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu~Qiao, and Jing Shao. 2024{\natexlab{a}}.",Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.,,"[{Li et~al.(2024{\natexlab{a}})Li, Dong, Wang, Hu, Zuo, Lin, Qiao, and Shao}]{li2024salad} Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu~Qiao, and Jing Shao. 2024{\natexlab{a}}. 
 Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. 
 \emph{arXiv preprint arXiv:2402.05044}."
2407.18418,li2024mediq,"[{Li et~al.(2024{\natexlab{b}})Li, Balachandran, Feng, Ilgen, Pierson, Koh, and Tsvetkov}]{li2024mediq} Shuyue~Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang~Wei Koh, and Yulia Tsvetkov. 2024{\natexlab{b}}.",Medi{Q}: Question-asking {LLMs} for adaptive and reliable medical reasoning.,Medi{Q}: Question-asking {LLMs} for adaptive and reliable medical reasoning.,,"[{Li et~al.(2024{\natexlab{b}})Li, Balachandran, Feng, Ilgen, Pierson, Koh, and Tsvetkov}]{li2024mediq} Shuyue~Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang~Wei Koh, and Yulia Tsvetkov. 2024{\natexlab{b}}. 
 Medi{Q}: Question-asking {LLMs} for adaptive and reliable medical reasoning. 
 \emph{arXiv preprint arXiv:2406.00922}."
2407.18418,lin2022teaching,"[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.",Teaching models to express their uncertainty in words.,Teaching models to express their uncertainty in words.,,"[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin2022teaching} Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. 
 Teaching models to express their uncertainty in words. 
 \emph{arXiv preprint arXiv:2205.14334}."
2407.18418,mazeika2024harmbench,"[{Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li et~al.}]{mazeika2024harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al. 2024.",Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,,"[{Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li et~al.}]{mazeika2024harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al. 2024. 
 Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. 
 \emph{arXiv preprint arXiv:2402.04249}."
2407.18418,pisano2023bergeron,"[{Pisano et~al.(2023)Pisano, Ly, Sanders, Yao, Wang, Strzalkowski, and Si}]{pisano2023bergeron} Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang, Tomek Strzalkowski, and Mei Si. 2023.",Bergeron: Combating adversarial attacks through a conscience-based alignment framework.,Bergeron: Combating adversarial attacks through a conscience-based alignment framework.,,"[{Pisano et~al.(2023)Pisano, Ly, Sanders, Yao, Wang, Strzalkowski, and Si}]{pisano2023bergeron} Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang, Tomek Strzalkowski, and Mei Si. 2023. 
 Bergeron: Combating adversarial attacks through a conscience-based alignment framework. 
 \emph{arXiv preprint arXiv:2312.00029}."
2407.18418,qiu2023latent,"[{Qiu et~al.(2023)Qiu, Zhang, Li, He, and Lan}]{qiu2023latent} Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. 2023.",Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models.,Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models.,,"[{Qiu et~al.(2023)Qiu, Zhang, Li, He, and Lan}]{qiu2023latent} Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. 2023. 
 Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models. 
 \emph{arXiv preprint arXiv:2307.08487}."
2407.18418,rein2023gpqa,"[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{rein2023gpqa} David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023.",Gpqa: A graduate-level google-proof q\&a benchmark.,Gpqa: A graduate-level google-proof q\&a benchmark.,,"[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{rein2023gpqa} David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023. 
 Gpqa: A graduate-level google-proof q\&a benchmark. 
 \emph{arXiv preprint arXiv:2311.12022}."
2407.18418,rottger2024safetyprompts,"[{R{\""o}ttger et~al.(2024{\natexlab{b}})R{\""o}ttger, Pernisi, Vidgen, and Hovy}]{rottger2024safetyprompts} Paul R{\""o}ttger, Fabio Pernisi, Bertie Vidgen, and Dirk Hovy. 2024{\natexlab{b}}.",Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety.,Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety.,,"[{R{\""o}ttger et~al.(2024{\natexlab{b}})R{\""o}ttger, Pernisi, Vidgen, and Hovy}]{rottger2024safetyprompts} Paul R{\""o}ttger, Fabio Pernisi, Bertie Vidgen, and Dirk Hovy. 2024{\natexlab{b}}. 
 Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety. 
 \emph{arXiv preprint arXiv:2404.05399}."
2407.18418,schulman2017proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}."
2407.18418,shen2023anything,"[{Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang}]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023.",""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",""" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",,"[{Shen et~al.(2023)Shen, Chen, Backes, Shen, and Zhang}]{shen2023anything} Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. 
 "" do anything now"": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. 
 \emph{arXiv preprint arXiv:2308.03825}."
2407.18418,shrivastava2023llamas,"[{Shrivastava et~al.(2023)Shrivastava, Liang, and Kumar}]{shrivastava2023llamas} Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. 2023.",{LLaMAs} know what gpts don't show: Surrogate models for confidence estimation.,{LLaMAs} know what gpts don't show: Surrogate models for confidence estimation.,,"[{Shrivastava et~al.(2023)Shrivastava, Liang, and Kumar}]{shrivastava2023llamas} Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. 2023. 
 {LLaMAs} know what gpts don't show: Surrogate models for confidence estimation. 
 \emph{arXiv preprint arXiv:2311.08877}."
2407.18418,souly2024strongreject,"[{Souly et~al.(2024)Souly, Lu, Bowen, Trinh, Hsieh, Pandey, Abbeel, Svegliato, Emmons, Watkins et~al.}]{souly2024strongreject} Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu~Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et~al. 2024.",A strongreject for empty jailbreaks.,A strongreject for empty jailbreaks.,,"[{Souly et~al.(2024)Souly, Lu, Bowen, Trinh, Hsieh, Pandey, Abbeel, Svegliato, Emmons, Watkins et~al.}]{souly2024strongreject} Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu~Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et~al. 2024. 
 A strongreject for empty jailbreaks. 
 \emph{arXiv preprint arXiv:2402.10260}."
2407.18418,stengel2024lacie,"[{Stengel-Eskin et~al.(2024)Stengel-Eskin, Hase, and Bansal}]{stengel2024lacie} Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. 2024.",Lacie: Listener-aware finetuning for confidence calibration in large language models.,Lacie: Listener-aware finetuning for confidence calibration in large language models.,,"[{Stengel-Eskin et~al.(2024)Stengel-Eskin, Hase, and Bansal}]{stengel2024lacie} Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. 2024. 
 Lacie: Listener-aware finetuning for confidence calibration in large language models. 
 \emph{arXiv preprint arXiv:2405.21028}."
2407.18418,tomani2024uncertainty,"[{Tomani et~al.(2024)Tomani, Chaudhuri, Evtimov, Cremers, and Ibrahim}]{tomani2024uncertainty} Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. 2024.",Uncertainty-based abstention in {LLMs} improves safety and reduces hallucinations.,Uncertainty-based abstention in {LLMs} improves safety and reduces hallucinations.,,"[{Tomani et~al.(2024)Tomani, Chaudhuri, Evtimov, Cremers, and Ibrahim}]{tomani2024uncertainty} Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. 2024. 
 Uncertainty-based abstention in {LLMs} improves safety and reduces hallucinations. 
 \emph{arXiv preprint arXiv:2404.10960}."
2407.18418,varshney2023art,"[{Varshney et~al.(2023)Varshney, Dolin, Seth, and Baral}]{varshney2023art} Neeraj Varshney, Pavel Dolin, Agastya Seth, and Chitta Baral. 2023.",The art of defending: A systematic evaluation and analysis of {LLM} defense strategies on safety and over-defensiveness.,The art of defending: A systematic evaluation and analysis of {LLM} defense strategies on safety and over-defensiveness.,,"[{Varshney et~al.(2023)Varshney, Dolin, Seth, and Baral}]{varshney2023art} Neeraj Varshney, Pavel Dolin, Agastya Seth, and Chitta Baral. 2023. 
 The art of defending: A systematic evaluation and analysis of {LLM} defense strategies on safety and over-defensiveness. 
 \emph{arXiv preprint arXiv:2401.00287}."
2407.18418,wallace2024instruction,"[{Wallace et~al.(2024)Wallace, Xiao, Leike, Weng, Heidecke, and Beutel}]{wallace2024instruction} Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024.",The instruction hierarchy: Training {LLMs} to prioritize privileged instructions.,The instruction hierarchy: Training {LLMs} to prioritize privileged instructions.,,"[{Wallace et~al.(2024)Wallace, Xiao, Leike, Weng, Heidecke, and Beutel}]{wallace2024instruction} Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024. 
 The instruction hierarchy: Training {LLMs} to prioritize privileged instructions. 
 \emph{arXiv preprint arXiv:2404.13208}."
2407.18418,wang2024inferaligner,"[{Wang et~al.(2024{\natexlab{a}})Wang, Zhang, Li, Tan, Wang, Ren, Jiang, and Qiu}]{wang2024inferaligner} Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke~Ren, Botian Jiang, and Xipeng Qiu. 2024{\natexlab{a}}.",Inferaligner: Inference-time alignment for harmlessness through cross-model guidance.,Inferaligner: Inference-time alignment for harmlessness through cross-model guidance.,,"[{Wang et~al.(2024{\natexlab{a}})Wang, Zhang, Li, Tan, Wang, Ren, Jiang, and Qiu}]{wang2024inferaligner} Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke~Ren, Botian Jiang, and Xipeng Qiu. 2024{\natexlab{a}}. 
 Inferaligner: Inference-time alignment for harmlessness through cross-model guidance. 
 \emph{arXiv preprint arXiv:2401.11206}."
2407.18418,wang2023all,"[{Wang et~al.(2023{\natexlab{a}})Wang, Tu, Chen, Yuan, Huang, Jiao, and Lyu}]{wang2023all} Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael~R Lyu. 2023{\natexlab{a}}.",All languages matter: On the multilingual safety of large language models.,All languages matter: On the multilingual safety of large language models.,,"[{Wang et~al.(2023{\natexlab{a}})Wang, Tu, Chen, Yuan, Huang, Jiao, and Lyu}]{wang2023all} Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael~R Lyu. 2023{\natexlab{a}}. 
 All languages matter: On the multilingual safety of large language models. 
 \emph{arXiv preprint arXiv:2310.00905}."
2407.18418,wang2024defending,"[{Wang et~al.(2024{\natexlab{b}})Wang, Shi, Bai, and Hsieh}]{wang2024defending} Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. 2024{\natexlab{b}}.",Defending {LLMs} against jailbreaking attacks via backtranslation.,Defending {LLMs} against jailbreaking attacks via backtranslation.,,"[{Wang et~al.(2024{\natexlab{b}})Wang, Shi, Bai, and Hsieh}]{wang2024defending} Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. 2024{\natexlab{b}}. 
 Defending {LLMs} against jailbreaking attacks via backtranslation. 
 \emph{arXiv preprint arXiv:2402.16459}."
2407.18418,wen2024characterizing,"[{Wen et~al.(2024)Wen, Howe, and Wang}]{wen2024characterizing} Bingbing Wen, Bill Howe, and Lucy~Lu Wang. 2024.",Characterizing {LLM} abstention behavior in science qa with context perturbations.,Characterizing {LLM} abstention behavior in science qa with context perturbations.,,"[{Wen et~al.(2024)Wen, Howe, and Wang}]{wen2024characterizing} Bingbing Wen, Bill Howe, and Lucy~Lu Wang. 2024. 
 Characterizing {LLM} abstention behavior in science qa with context perturbations. 
 \emph{arXiv preprint arXiv:2404.12452}."
2407.18418,xie2024sorry,"[{Xie et~al.(2024)Xie, Qi, Zeng, Huang, Sehwag, Huang, He, Wei, Li, Sheng et~al.}]{xie2024sorry} Tinghao Xie, Xiangyu Qi, Yi~Zeng, Yangsibo Huang, Udari~Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et~al. 2024.",Sorry-bench: Systematically evaluating large language model safety refusal behaviors.,Sorry-bench: Systematically evaluating large language model safety refusal behaviors.,,"[{Xie et~al.(2024)Xie, Qi, Zeng, Huang, Sehwag, Huang, He, Wei, Li, Sheng et~al.}]{xie2024sorry} Tinghao Xie, Xiangyu Qi, Yi~Zeng, Yangsibo Huang, Udari~Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et~al. 2024. 
 Sorry-bench: Systematically evaluating large language model safety refusal behaviors. 
 \emph{arXiv preprint arXiv:2406.14598}."
2407.18418,xu2023cvalues,"[{Xu et~al.(2023{\natexlab{a}})Xu, Liu, Yan, Xu, Si, Zhou, Yi, Gao, Sang, Zhang et~al.}]{xu2023cvalues} Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et~al. 2023{\natexlab{a}}.",Cvalues: Measuring the values of chinese large language models from safety to responsibility.,Cvalues: Measuring the values of chinese large language models from safety to responsibility.,,"[{Xu et~al.(2023{\natexlab{a}})Xu, Liu, Yan, Xu, Si, Zhou, Yi, Gao, Sang, Zhang et~al.}]{xu2023cvalues} Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et~al. 2023{\natexlab{a}}. 
 Cvalues: Measuring the values of chinese large language models from safety to responsibility. 
 \emph{arXiv preprint arXiv:2307.09705}."
2407.18418,xu2023earth,"[{Xu et~al.(2023{\natexlab{b}})Xu, Lin, Yang, Zhang, Shi, Zhang, Fang, Xu, and Qiu}]{xu2023earth} Rongwu Xu, Brian~S Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. 2023{\natexlab{b}}.",The earth is flat because...: Investigating {LLMs'} belief towards misinformation via persuasive conversation.,The earth is flat because...: Investigating {LLMs'} belief towards misinformation via persuasive conversation.,,"[{Xu et~al.(2023{\natexlab{b}})Xu, Lin, Yang, Zhang, Shi, Zhang, Fang, Xu, and Qiu}]{xu2023earth} Rongwu Xu, Brian~S Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. 2023{\natexlab{b}}. 
 The earth is flat because...: Investigating {LLMs'} belief towards misinformation via persuasive conversation. 
 \emph{arXiv preprint arXiv:2312.09085}."
2407.18418,xu2024knowledge,"[{Xu et~al.(2024)Xu, Qi, Wang, Wang, Zhang, and Xu}]{xu2024knowledge} Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024.",Knowledge conflicts for llms: A survey.,Knowledge conflicts for llms: A survey.,,"[{Xu et~al.(2024)Xu, Qi, Wang, Wang, Zhang, and Xu}]{xu2024knowledge} Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. 
 Knowledge conflicts for llms: A survey. 
 \emph{arXiv preprint arXiv:2403.08319}."
2407.18418,yang2023alignment,"[{Yang et~al.(2023)Yang, Chern, Qiu, Neubig, and Liu}]{yang2023alignment} Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023.",Alignment for honesty.,Alignment for honesty.,,"[{Yang et~al.(2023)Yang, Chern, Qiu, Neubig, and Liu}]{yang2023alignment} Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. 
 Alignment for honesty. 
 \emph{arXiv preprint arXiv:2312.07000}."
2407.18418,yi2024survey,"[{Yi et~al.(2024)Yi, Ouyang, Liu, Liao, Xu, and Shen}]{yi2024survey} Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. 2024.",A survey on recent advances in {LLM-Based} multi-turn dialogue systems.,A survey on recent advances in {LLM-Based} multi-turn dialogue systems.,,"[{Yi et~al.(2024)Yi, Ouyang, Liu, Liao, Xu, and Shen}]{yi2024survey} Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. 2024. 
 A survey on recent advances in {LLM-Based} multi-turn dialogue systems. 
 \emph{arXiv preprint arXiv:2402.18013}."
2407.18418,yong2023low,"[{Yong et~al.(2023)Yong, Menghini, and Bach}]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach. 2023.",Low-resource languages jailbreak gpt-4.,Low-resource languages jailbreak gpt-4.,,"[{Yong et~al.(2023)Yong, Menghini, and Bach}]{yong2023low} Zheng-Xin Yong, Cristina Menghini, and Stephen~H Bach. 2023. 
 Low-resource languages jailbreak gpt-4. 
 \emph{arXiv preprint arXiv:2310.02446}."
2407.18418,yu2023gptfuzzer,"[{Yu et~al.(2023)Yu, Lin, and Xing}]{yu2023gptfuzzer} Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023.",Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.,Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.,,"[{Yu et~al.(2023)Yu, Lin, and Xing}]{yu2023gptfuzzer} Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. 
 Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. 
 \emph{arXiv preprint arXiv:2309.10253}."
2407.18418,zeng2024autodefense,"[{Zeng et~al.(2024)Zeng, Wu, Zhang, Wang, and Wu}]{zeng2024autodefense} Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. 2024.",Autodefense: Multi-agent {LLM} defense against jailbreak attacks.,Autodefense: Multi-agent {LLM} defense against jailbreak attacks.,,"[{Zeng et~al.(2024)Zeng, Wu, Zhang, Wang, and Wu}]{zeng2024autodefense} Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. 2024. 
 Autodefense: Multi-agent {LLM} defense against jailbreak attacks. 
 \emph{arXiv preprint arXiv:2403.04783}."
2407.18418,zhang2023defending,"[{Zhang et~al.(2023{\natexlab{b}})Zhang, Yang, Ke, and Huang}]{zhang2023defending} Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023{\natexlab{b}}.",Defending large language models against jailbreaking attacks through goal prioritization.,Defending large language models against jailbreaking attacks through goal prioritization.,,"[{Zhang et~al.(2023{\natexlab{b}})Zhang, Yang, Ke, and Huang}]{zhang2023defending} Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023{\natexlab{b}}. 
 Defending large language models against jailbreaking attacks through goal prioritization. 
 \emph{arXiv preprint arXiv:2311.09096}."
2407.18418,zheng2024prompt,"[{Zheng et~al.(2024)Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng}]{zheng2024prompt} Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. 2024.",Prompt-driven llm safeguarding via directed representation optimization.,Prompt-driven llm safeguarding via directed representation optimization.,,"[{Zheng et~al.(2024)Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng}]{zheng2024prompt} Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. 2024. 
 Prompt-driven llm safeguarding via directed representation optimization. 
 \emph{arXiv preprint arXiv:2401.18018}."
2407.18418,zhou2024relying,"[{Zhou et~al.(2024{\natexlab{b}})Zhou, Hwang, Ren, and Sap}]{zhou2024relying} Kaitlyn Zhou, Jena~D Hwang, Xiang Ren, and Maarten Sap. 2024{\natexlab{b}}.",Relying on the unreliable: The impact of language models' reluctance to express uncertainty.,Relying on the unreliable: The impact of language models' reluctance to express uncertainty.,,"[{Zhou et~al.(2024{\natexlab{b}})Zhou, Hwang, Ren, and Sap}]{zhou2024relying} Kaitlyn Zhou, Jena~D Hwang, Xiang Ren, and Maarten Sap. 2024{\natexlab{b}}. 
 Relying on the unreliable: The impact of language models' reluctance to express uncertainty. 
 \emph{arXiv preprint arXiv:2401.06730}."
2407.18418,zhou2024defending,"[{Zhou et~al.(2024{\natexlab{c}})Zhou, Han, Zhuang, Guo, Guo, Liang, Bao, and Zhang}]{zhou2024defending} Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. 2024{\natexlab{c}}.",Defending jailbreak prompts via in-context adversarial game.,Defending jailbreak prompts via in-context adversarial game.,,"[{Zhou et~al.(2024{\natexlab{c}})Zhou, Han, Zhuang, Guo, Guo, Liang, Bao, and Zhang}]{zhou2024defending} Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. 2024{\natexlab{c}}. 
 Defending jailbreak prompts via in-context adversarial game. 
 \emph{arXiv preprint arXiv:2402.13148}."
2407.18418,zou2023universal,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023.",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023. 
 Universal and transferable adversarial attacks on aligned language models. 
 \emph{arXiv preprint arXiv:2307.15043}."
2407.19474,sap2019socialiqa,"[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa} Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.",Socialiqa: Commonsense reasoning about social interactions.,Socialiqa: Commonsense reasoning about social interactions.,,"[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa} Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 
 Socialiqa: Commonsense reasoning about social interactions. 
 \emph{arXiv preprint arXiv:1904.09728}, 2019."
2407.19474,hessel2022androids,"[Hessel et~al.(2022{\natexlab{a}})Hessel, Marasovi{\'c}, Hwang, Lee, Da, Zellers, Mankoff, and Choi]{hessel2022androids} Jack Hessel, Ana Marasovi{\'c}, Jena~D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi.","Do androids laugh at electric sheep? humor"" understanding"" benchmarks from the new yorker caption contest.","Do androids laugh at electric sheep? humor"" understanding"" benchmarks from the new yorker caption contest.",,"[Hessel et~al.(2022{\natexlab{a}})Hessel, Marasovi{\'c}, Hwang, Lee, Da, Zellers, Mankoff, and Choi]{hessel2022androids} Jack Hessel, Ana Marasovi{\'c}, Jena~D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. 
 Do androids laugh at electric sheep? humor"" understanding"" benchmarks from the new yorker caption contest. 
 \emph{arXiv preprint arXiv:2209.06293}, 2022{\natexlab{a}}."
2407.19474,reid2024gemini,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al.",Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,,"[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 
 Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 
 \emph{arXiv preprint arXiv:2403.05530}, 2024."
2407.19474,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2407.19474,podell2023sdxl,"[Podell et~al.(2023{\natexlab{a}})Podell, English, Lacey, Blattmann, Dockhorn, M{\""u}ller, Penna, and Rombach]{podell2023sdxl} Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\""u}ller, Joe Penna, and Robin Rombach.",Sdxl: Improving latent diffusion models for high-resolution image synthesis.,Sdxl: Improving latent diffusion models for high-resolution image synthesis.,,"[Podell et~al.(2023{\natexlab{a}})Podell, English, Lacey, Blattmann, Dockhorn, M{\""u}ller, Penna, and Rombach]{podell2023sdxl} Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\""u}ller, Joe Penna, and Robin Rombach. 
 Sdxl: Improving latent diffusion models for high-resolution image synthesis. 
 \emph{arXiv preprint arXiv:2307.01952}, 2023{\natexlab{a}}."
2407.19474,sdxl,"[Podell et~al.(2023{\natexlab{b}})Podell, English, Lacey, Blattmann, Dockhorn, M{\""u}ller, Penna, and Rombach]{sdxl} Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\""u}ller, Joe Penna, and Robin Rombach.",Sdxl: Improving latent diffusion models for high-resolution image synthesis.,Sdxl: Improving latent diffusion models for high-resolution image synthesis.,,"[Podell et~al.(2023{\natexlab{b}})Podell, English, Lacey, Blattmann, Dockhorn, M{\""u}ller, Penna, and Rombach]{sdxl} Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\""u}ller, Joe Penna, and Robin Rombach. 
 Sdxl: Improving latent diffusion models for high-resolution image synthesis. 
 \emph{arXiv preprint arXiv:2307.01952}, 2023{\natexlab{b}}."
2407.19474,sdxl_lcm,"[Luo et~al.(2023)Luo, Tan, Patil, Gu, von Platen, Passos, Huang, Li, and Zhao]{sdxl_lcm} Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin{\'a}rio Passos, Longbo Huang, Jian Li, and Hang Zhao.",Lcm-lora: A universal stable-diffusion acceleration module.,Lcm-lora: A universal stable-diffusion acceleration module.,,"[Luo et~al.(2023)Luo, Tan, Patil, Gu, von Platen, Passos, Huang, Li, and Zhao]{sdxl_lcm} Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin{\'a}rio Passos, Longbo Huang, Jian Li, and Hang Zhao. 
 Lcm-lora: A universal stable-diffusion acceleration module. 
 \emph{arXiv preprint arXiv:2311.05556}, 2023."
2407.19474,li2023blip,"[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip} Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.,,"[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip} Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 
 Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. 
 \emph{arXiv preprint arXiv:2301.12597}, 2023."
2407.19474,alayrac2022flamingo,"[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo} Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et~al.",Flamingo: a visual language model for few-shot learning.,Flamingo: a visual language model for few-shot learning.,,"[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo} Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et~al. 
 Flamingo: a visual language model for few-shot learning. 
 \emph{arXiv preprint arXiv:2204.14198}, 2022."
2407.19474,bitton2022winogavil,"[Bitton et~al.(2022{\natexlab{a}})Bitton, Guetta, Yosef, Elovici, Bansal, Stanovsky, and Schwartz]{bitton2022winogavil} Yonatan Bitton, Nitzan~Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz.",{WinoGAViL}: Gamified association benchmark to challenge vision-and-language models.,{WinoGAViL}: Gamified association benchmark to challenge vision-and-language models.,,"[Bitton et~al.(2022{\natexlab{a}})Bitton, Guetta, Yosef, Elovici, Bansal, Stanovsky, and Schwartz]{bitton2022winogavil} Yonatan Bitton, Nitzan~Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. 
 {WinoGAViL}: Gamified association benchmark to challenge vision-and-language models. 
 \emph{arXiv preprint arXiv:2207.12576}, 2022{\natexlab{a}}."
2407.19474,bitton2022vasr,"[Bitton et~al.(2022{\natexlab{b}})Bitton, Yosef, Strugo, Shahaf, Schwartz, and Stanovsky]{bitton2022vasr} Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky.",{VASR}: Visual analogies of situation recognition.,{VASR}: Visual analogies of situation recognition.,,"[Bitton et~al.(2022{\natexlab{b}})Bitton, Yosef, Strugo, Shahaf, Schwartz, and Stanovsky]{bitton2022vasr} Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. 
 {VASR}: Visual analogies of situation recognition. 
 \emph{arXiv preprint arXiv:2212.04542}, 2022{\natexlab{b}}."
2407.19474,zhang2019bertscore,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}, 2019."
2407.19474,rei2020comet,"[Rei et~al.(2020)Rei, Stewart, Farinha, and Lavie]{rei2020comet} Ricardo Rei, Craig Stewart, Ana~C Farinha, and Alon Lavie.",Comet: A neural framework for mt evaluation.,Comet: A neural framework for mt evaluation.,,"[Rei et~al.(2020)Rei, Stewart, Farinha, and Lavie]{rei2020comet} Ricardo Rei, Craig Stewart, Ana~C Farinha, and Alon Lavie. 
 Comet: A neural framework for mt evaluation. 
 \emph{arXiv preprint arXiv:2009.09025}, 2020."
2407.19474,honovich2022true,"[Honovich et~al.(2022)Honovich, Aharoni, Herzig, Taitelbaum, Kukliansy, Cohen, Scialom, Szpektor, Hassidim, and Matias]{honovich2022true} Or~Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias.",True: Re-evaluating factual consistency evaluation.,True: Re-evaluating factual consistency evaluation.,,"[Honovich et~al.(2022)Honovich, Aharoni, Herzig, Taitelbaum, Kukliansy, Cohen, Scialom, Szpektor, Hassidim, and Matias]{honovich2022true} Or~Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 
 True: Re-evaluating factual consistency evaluation. 
 \emph{arXiv preprint arXiv:2204.04991}, 2022."
2407.19474,slobodkin2024multi,"[Slobodkin et~al.(2024)Slobodkin, Shapira, Levy, and Dagan]{slobodkin2024multi} Aviv Slobodkin, Ori Shapira, Ran Levy, and Ido Dagan.",Multi-review fusion-in-context.,Multi-review fusion-in-context.,,"[Slobodkin et~al.(2024)Slobodkin, Shapira, Levy, and Dagan]{slobodkin2024multi} Aviv Slobodkin, Ori Shapira, Ran Levy, and Ido Dagan. 
 Multi-review fusion-in-context. 
 \emph{arXiv preprint arXiv:2403.15351}, 2024."
2407.19474,hessel2021clipscore,"[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi]{hessel2021clipscore} Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.",Clipscore: A reference-free evaluation metric for image captioning.,Clipscore: A reference-free evaluation metric for image captioning.,,"[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and Choi]{hessel2021clipscore} Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi. 
 Clipscore: A reference-free evaluation metric for image captioning. 
 \emph{arXiv preprint arXiv:2104.08718}, 2021."
2407.19474,zhang2024avibench,"[Zhang et~al.(2024)Zhang, Shao, Liu, Ma, Luo, Qiao, and Zhang]{zhang2024avibench} Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu~Qiao, and Kaipeng Zhang.",Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions.,Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions.,,"[Zhang et~al.(2024)Zhang, Shao, Liu, Ma, Luo, Qiao, and Zhang]{zhang2024avibench} Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu~Qiao, and Kaipeng Zhang. 
 Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions. 
 \emph{arXiv preprint arXiv:2403.09346}, 2024."
2407.19474,song2024milebench,"[Song et~al.(2024)Song, Chen, Chen, Yu, Wan, and Wang]{song2024milebench} Dingjie Song, Shunian Chen, Guiming~Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang.",Milebench: Benchmarking mllms in long context.,Milebench: Benchmarking mllms in long context.,,"[Song et~al.(2024)Song, Chen, Chen, Yu, Wan, and Wang]{song2024milebench} Dingjie Song, Shunian Chen, Guiming~Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. 
 Milebench: Benchmarking mllms in long context. 
 \emph{arXiv preprint arXiv:2404.18532}, 2024."
2407.19474,fu2024blink,"[Fu et~al.(2024)Fu, Hu, Li, Feng, Wang, Lin, Roth, Smith, Ma, and Krishna]{fu2024blink} Xingyu Fu, Yushi Hu, Bangzheng Li, Yu~Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah~A Smith, Wei-Chiu Ma, and Ranjay Krishna.",Blink: Multimodal large language models can see but not perceive.,Blink: Multimodal large language models can see but not perceive.,,"[Fu et~al.(2024)Fu, Hu, Li, Feng, Wang, Lin, Roth, Smith, Ma, and Krishna]{fu2024blink} Xingyu Fu, Yushi Hu, Bangzheng Li, Yu~Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah~A Smith, Wei-Chiu Ma, and Ranjay Krishna. 
 Blink: Multimodal large language models can see but not perceive. 
 \emph{arXiv preprint arXiv:2404.12390}, 2024."
2407.19474,padlewski2024vibe,"[Padlewski et~al.(2024)Padlewski, Bain, Henderson, Zhu, Relan, Pham, Ong, Aleksiev, Ormazabal, Phua, et~al.]{padlewski2024vibe} Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et~al.",Vibe-eval: A hard evaluation suite for measuring progress of multimodal language models.,Vibe-eval: A hard evaluation suite for measuring progress of multimodal language models.,,"[Padlewski et~al.(2024)Padlewski, Bain, Henderson, Zhu, Relan, Pham, Ong, Aleksiev, Ormazabal, Phua, et~al.]{padlewski2024vibe} Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, et~al. 
 Vibe-eval: A hard evaluation suite for measuring progress of multimodal language models. 
 \emph{arXiv preprint arXiv:2405.02287}, 2024."
2407.19594,bai2022training,"[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}."
2407.19594,bai2022constitutional,"[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.",Constitutional ai: Harmlessness from ai feedback.,Constitutional ai: Harmlessness from ai feedback.,,"[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al. 
 Constitutional ai: Harmlessness from ai feedback. 
 \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}."
2407.19594,bowman2022measuring,"[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner, Luko{\v{s}}i{\=u}t{\.e}, Askell, Jones, Chen, et~al.]{bowman2022measuring} Samuel~R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Amanda Askell, Andy Jones, Anna Chen, et~al.",Measuring progress on scalable oversight for large language models.,Measuring progress on scalable oversight for large language models.,,"[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner, Luko{\v{s}}i{\=u}t{\.e}, Askell, Jones, Chen, et~al.]{bowman2022measuring} Samuel~R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Amanda Askell, Andy Jones, Anna Chen, et~al. 
 Measuring progress on scalable oversight for large language models. 
 \emph{arXiv preprint arXiv:2211.03540}, 2022."
2407.19594,burns2023weak,"[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, et~al.]{burns2023weak} Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al.",Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.,Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.,,"[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, et~al.]{burns2023weak} Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al. 
 Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. 
 \emph{arXiv preprint arXiv:2312.09390}, 2023."
2407.19594,chen2023alpagasus,"[Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, et~al.]{chen2023alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al.",Alpagasus: Training a better alpaca with fewer data.,Alpagasus: Training a better alpaca with fewer data.,,"[Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, et~al.]{chen2023alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 
 Alpagasus: Training a better alpaca with fewer data. 
 \emph{arXiv preprint arXiv:2307.08701}, 2023."
2407.19594,dubois2024length,"[Dubois et~al.(2024{\natexlab{a}})Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto.",Length-controlled alpacaeval: A simple way to debias automatic evaluators.,Length-controlled alpacaeval: A simple way to debias automatic evaluators.,,"[Dubois et~al.(2024{\natexlab{a}})Dubois, Galambosi, Liang, and Hashimoto]{dubois2024length} Yann Dubois, Bal{\'a}zs Galambosi, Percy Liang, and Tatsunori~B Hashimoto. 
 Length-controlled alpacaeval: A simple way to debias automatic evaluators. 
 \emph{arXiv preprint arXiv:2404.04475}, 2024{\natexlab{a}}."
2407.19594,kim2024prometheus,"[Kim et~al.(2024)Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo]{kim2024prometheus} Seungone Kim, Juyoung Suk, Shayne Longpre, Bill~Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo.",Prometheus 2: An open source language model specialized in evaluating other language models.,Prometheus 2: An open source language model specialized in evaluating other language models.,,"[Kim et~al.(2024)Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo]{kim2024prometheus} Seungone Kim, Juyoung Suk, Shayne Longpre, Bill~Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 
 Prometheus 2: An open source language model specialized in evaluating other language models. 
 \emph{arXiv preprint arXiv:2405.01535}, 2024."
2407.19594,lee2023rlaif,"[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.",Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,,"[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 
 Rlaif: Scaling reinforcement learning from human feedback with ai feedback. 
 \emph{arXiv preprint arXiv:2309.00267}, 2023."
2407.19594,leike2018scalable,"[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and Legg]{leike2018scalable} Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg.",Scalable agent alignment via reward modeling: a research direction.,Scalable agent alignment via reward modeling: a research direction.,,"[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and Legg]{leike2018scalable} Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 
 Scalable agent alignment via reward modeling: a research direction. 
 \emph{arXiv preprint arXiv:1811.07871}, 2018."
2407.19594,li2024crowdsourced,"[Li et~al.(2024)Li, Chiang, Frick, Dunlap, Wu, Zhu, Gonzalez, and Stoica]{li2024crowdsourced} Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph~E Gonzalez, and Ion Stoica.",From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.,From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.,,"[Li et~al.(2024)Li, Chiang, Frick, Dunlap, Wu, Zhu, Gonzalez, and Stoica]{li2024crowdsourced} Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph~E Gonzalez, and Ion Stoica. 
 From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. 
 \emph{arXiv preprint arXiv:2406.11939}, 2024."
2407.19594,li2023self,"[Li et~al.(2023)Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis.",Self-alignment with instruction backtranslation.,Self-alignment with instruction backtranslation.,,"[Li et~al.(2023)Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis]{li2023self} Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 
 Self-alignment with instruction backtranslation. 
 \emph{arXiv preprint arXiv:2308.06259}, 2023."
2407.19594,lightman2023let,"[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.",Let's verify step by step.,Let's verify step by step.,,"[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 
 Let's verify step by step. 
 \emph{arXiv preprint arXiv:2305.20050}, 2023."
2407.19594,mcaleese2024llm,"[McAleese et~al.(2024)McAleese, Pokorny, Uribe, Nitishinskaya, Trebacz, and Leike]{mcaleese2024llm} Nat McAleese, Rai~Michael Pokorny, Juan Felipe~Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike.",Llm critics help catch llm bugs.,Llm critics help catch llm bugs.,,"[McAleese et~al.(2024)McAleese, Pokorny, Uribe, Nitishinskaya, Trebacz, and Leike]{mcaleese2024llm} Nat McAleese, Rai~Michael Pokorny, Juan Felipe~Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 
 Llm critics help catch llm bugs. 
 \emph{arXiv preprint arXiv:2407.00215}, 2024."
2407.19594,openai2023gpt4,[OpenAI(2023)]{openai2023gpt4} OpenAI.,Gpt-4 technical report.,Gpt-4 technical report.,,"[OpenAI(2023)]{openai2023gpt4} OpenAI. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.19594,park2024disentangling,"[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn.",Disentangling length from quality in direct preference optimization.,Disentangling length from quality in direct preference optimization.,,"[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling} Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. 
 Disentangling length from quality in direct preference optimization. 
 \emph{arXiv preprint arXiv:2403.19159}, 2024."
2407.19594,perez2022red,"[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red} Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.",Red teaming language models with language models.,Red teaming language models with language models.,,"[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red} Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 
 Red teaming language models with language models. 
 \emph{arXiv preprint arXiv:2202.03286}, 2022."
2407.19594,saha2023branch,"[Saha et~al.(2023)Saha, Levy, Celikyilmaz, Bansal, Weston, and Li]{saha2023branch} Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.",Branch-solve-merge improves large language model evaluation and generation.,Branch-solve-merge improves large language model evaluation and generation.,,"[Saha et~al.(2023)Saha, Levy, Celikyilmaz, Bansal, Weston, and Li]{saha2023branch} Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 
 Branch-solve-merge improves large language model evaluation and generation. 
 \emph{arXiv preprint arXiv:2310.15123}, 2023."
2407.19594,sanh2021multitask,"[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al.",Multitask prompted training enables zero-shot task generalization.,Multitask prompted training enables zero-shot task generalization.,,"[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask} Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al. 
 Multitask prompted training enables zero-shot task generalization. 
 \emph{arXiv preprint arXiv:2110.08207}, 2021."
2407.19594,saunders2022self,"[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022self} William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.",Self-critiquing models for assisting human evaluators.,Self-critiquing models for assisting human evaluators.,,"[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022self} William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 
 Self-critiquing models for assisting human evaluators. 
 \emph{arXiv preprint arXiv:2206.05802}, 2022."
2407.19594,schulman2017proximal,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}, 2017."
2407.19594,sharma2023towards,"[Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.]{sharma2023towards} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R Johnston, et~al.",Towards understanding sycophancy in language models.,Towards understanding sycophancy in language models.,,"[Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.]{sharma2023towards} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R Johnston, et~al. 
 Towards understanding sycophancy in language models. 
 \emph{arXiv preprint arXiv:2310.13548}, 2023."
2407.19594,singhal2023long,"[Singhal et~al.(2023)Singhal, Goyal, Xu, and Durrett]{singhal2023long} Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett.",A long way to go: Investigating length correlations in rlhf.,A long way to go: Investigating length correlations in rlhf.,,"[Singhal et~al.(2023)Singhal, Goyal, Xu, and Durrett]{singhal2023long} Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. 
 A long way to go: Investigating length correlations in rlhf. 
 \emph{arXiv preprint arXiv:2310.03716}, 2023."
2407.19594,wang2024helpsteer2,"[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev]{wang2024helpsteer2} Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev.",Helpsteer2: Open-source dataset for training top-performing reward models.,Helpsteer2: Open-source dataset for training top-performing reward models.,,"[Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev]{wang2024helpsteer2} Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev. 
 Helpsteer2: Open-source dataset for training top-performing reward models. 
 \emph{arXiv preprint arXiv:2406.08673}, 2024."
2407.19594,wei2021finetuned,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}, 2021."
2407.19594,wu2023pairwise,"[Wu et~al.(2023)Wu, Zhu, Zhang, Wen, Ramchandran, and Jiao]{wu2023pairwise} Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, and Jiantao Jiao.",Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.,Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.,,"[Wu et~al.(2023)Wu, Zhu, Zhang, Wen, Ramchandran, and Jiao]{wu2023pairwise} Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, and Jiantao Jiao. 
 Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment. 
 \emph{arXiv preprint arXiv:2310.00212}, 2023."
2407.19594,wu2024self,"[Wu et~al.(2024)Wu, Sun, Yuan, Ji, Yang, and Gu]{wu2024self} Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu.",Self-play preference optimization for language model alignment.,Self-play preference optimization for language model alignment.,,"[Wu et~al.(2024)Wu, Sun, Yuan, Ji, Yang, and Gu]{wu2024self} Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. 
 Self-play preference optimization for language model alignment. 
 \emph{arXiv preprint arXiv:2405.00675}, 2024."
2407.19594,xu2023some,"[Xu et~al.(2023)Xu, Lee, Sukhbaatar, and Weston]{xu2023some} Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston.",Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,Some things are more cringe than others: Preference optimization with the pairwise cringe loss.,,"[Xu et~al.(2023)Xu, Lee, Sukhbaatar, and Weston]{xu2023some} Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. 
 Some things are more cringe than others: Preference optimization with the pairwise cringe loss. 
 \emph{arXiv preprint arXiv:2312.16682}, 2023."
2407.19594,yuan2024following,"[Yuan et~al.(2024{\natexlab{b}})Yuan, Kulikov, Yu, Cho, Sukhbaatar, Weston, and Xu]{yuan2024following} Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu.",Following length constraints in instructions.,Following length constraints in instructions.,,"[Yuan et~al.(2024{\natexlab{b}})Yuan, Kulikov, Yu, Cho, Sukhbaatar, Weston, and Xu]{yuan2024following} Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and Jing Xu. 
 Following length constraints in instructions. 
 \emph{arXiv preprint arXiv:2406.17744}, 2024{\natexlab{b}}."
2407.19594,zhao2023slic,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu.",Slic-hf: Sequence likelihood calibration with human feedback.,Slic-hf: Sequence likelihood calibration with human feedback.,,"[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic} Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu. 
 Slic-hf: Sequence likelihood calibration with human feedback. 
 \emph{arXiv preprint arXiv:2305.10425}, 2023."
2407.19594,zheng2023click,"[Zheng et~al.(2023)Zheng, Ke, Zhang, and Huang]{zheng2023click} Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang.",Click: Controllable text generation with sequence likelihood contrastive learning.,Click: Controllable text generation with sequence likelihood contrastive learning.,,"[Zheng et~al.(2023)Zheng, Ke, Zhang, and Huang]{zheng2023click} Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. 
 Click: Controllable text generation with sequence likelihood contrastive learning. 
 \emph{arXiv preprint arXiv:2306.03350}, 2023."
2407.19594,ziegler2019fine,"[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv:1909.08593}, 2019."
2407.19842,ba2016layer,"[\protect\citeauthoryear{Ba \bgroup \em et al.\egroup }{2016}]{ba2016layer} Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.",Layer normalization.,Layer normalization.,,"[\protect\citeauthoryear{Ba \bgroup \em et al.\egroup }{2016}]{ba2016layer} Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. 
 Layer normalization. 
 {\em arXiv preprint arXiv:1607.06450}, 2016."
2407.19842,elhage2022toy,"[\protect\citeauthoryear{Elhage \bgroup \em et al.\egroup }{2022}]{elhage2022toy} Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et~al.",Toy models of superposition.,Toy models of superposition.,,"[\protect\citeauthoryear{Elhage \bgroup \em et al.\egroup }{2022}]{elhage2022toy} Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et~al. 
 Toy models of superposition. 
 {\em arXiv preprint arXiv:2209.10652}, 2022."
2407.19842,kaplan2020scaling,"[\protect\citeauthoryear{Kaplan \bgroup \em et al.\egroup }{2020}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[\protect\citeauthoryear{Kaplan \bgroup \em et al.\egroup }{2020}]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 {\em arXiv preprint arXiv:2001.08361}, 2020."
2407.19842,liu2020adversarial,"[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup }{2020}]{liu2020adversarial} Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu~Wang, Hoifung Poon, and Jianfeng Gao.",Adversarial training for large neural language models.,Adversarial training for large neural language models.,,"[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup }{2020}]{liu2020adversarial} Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu~Wang, Hoifung Poon, and Jianfeng Gao. 
 Adversarial training for large neural language models. 
 {\em arXiv preprint arXiv:2004.08994}, 2020."
2407.19842,mcgrath2023hydra,"[\protect\citeauthoryear{McGrath \bgroup \em et al.\egroup }{2023}]{mcgrath2023hydra} Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg.",The hydra effect: Emergent self-repair in language model computations.,The hydra effect: Emergent self-repair in language model computations.,,"[\protect\citeauthoryear{McGrath \bgroup \em et al.\egroup }{2023}]{mcgrath2023hydra} Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. 
 The hydra effect: Emergent self-repair in language model computations. 
 {\em arXiv preprint arXiv:2307.15771}, 2023."
2407.19842,shayegani2023survey,"[\protect\citeauthoryear{Shayegani \bgroup \em et al.\egroup }{2023}]{shayegani2023survey} Erfan Shayegani, Md~Abdullah~Al Mamun, Yu~Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh.",Survey of vulnerabilities in large language models revealed by adversarial attacks.,Survey of vulnerabilities in large language models revealed by adversarial attacks.,,"[\protect\citeauthoryear{Shayegani \bgroup \em et al.\egroup }{2023}]{shayegani2023survey} Erfan Shayegani, Md~Abdullah~Al Mamun, Yu~Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 
 Survey of vulnerabilities in large language models revealed by adversarial attacks. 
 {\em arXiv preprint arXiv:2310.10844}, 2023."
2407.20105,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.",{GPT}-4 technical report.,{GPT}-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al. 
 {GPT}-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.20105,austin2021program,"[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program} Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et~al.",Program synthesis with large language models.,Program synthesis with large language models.,,"[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program} Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et~al. 
 Program synthesis with large language models. 
 \emph{arXiv preprint arXiv:2108.07732}, 2021."
2407.20105,chen2024copybench,"[Chen et~al.(2024)Chen, Asai, Mireshghallah, Min, Grimmelmann, Choi, Hajishirzi, Zettlemoyer, and Koh]{chen2024copybench} Chen, T., Asai, A., Mireshghallah, N., Min, S., Grimmelmann, J., Choi, Y., Hajishirzi, H., Zettlemoyer, L., and Koh, P.~W.",{CopyBench}: Measuring literal and non-literal reproduction of copyright-protected text in language model generation.,{CopyBench}: Measuring literal and non-literal reproduction of copyright-protected text in language model generation.,,"[Chen et~al.(2024)Chen, Asai, Mireshghallah, Min, Grimmelmann, Choi, Hajishirzi, Zettlemoyer, and Koh]{chen2024copybench} Chen, T., Asai, A., Mireshghallah, N., Min, S., Grimmelmann, J., Choi, Y., Hajishirzi, H., Zettlemoyer, L., and Koh, P.~W. 
 {CopyBench}: Measuring literal and non-literal reproduction of copyright-protected text in language model generation. 
 \emph{arXiv preprint arXiv:2407.07087}, 2024."
2407.20105,eldan2023s,"[Eldan \& Russinovich(2023)Eldan and Russinovich]{eldan2023s} Eldan, R. and Russinovich, M.",Who's {Harry Potter? approximate unlearning in LLMs}.,Who's {Harry Potter? approximate unlearning in LLMs}.,,"[Eldan \& Russinovich(2023)Eldan and Russinovich]{eldan2023s} Eldan, R. and Russinovich, M. 
 Who's {Harry Potter? approximate unlearning in LLMs}. 
 \emph{arXiv preprint arXiv:2310.02238}, 2023."
2407.20105,elkin2023can,"[Elkin-Koren et~al.(2023)Elkin-Koren, Hacohen, Livni, and Moran]{elkin2023can} Elkin-Koren, N., Hacohen, U., Livni, R., and Moran, S.",Can copyright be reduced to privacy?,Can copyright be reduced to privacy?,,"[Elkin-Koren et~al.(2023)Elkin-Koren, Hacohen, Livni, and Moran]{elkin2023can} Elkin-Koren, N., Hacohen, U., Livni, R., and Moran, S. 
 Can copyright be reduced to privacy? 
 \emph{arXiv preprint arXiv:2305.14822}, 2023."
2407.20105,gururangan2023scaling,"[Gururangan et~al.(2023)Gururangan, Li, Lewis, Shi, Althoff, Smith, and Zettlemoyer]{gururangan2023scaling} Gururangan, S., Li, M., Lewis, M., Shi, W., Althoff, T., Smith, N.~A., and Zettlemoyer, L.",Scaling expert language models with unsupervised domain discovery.,Scaling expert language models with unsupervised domain discovery.,,"[Gururangan et~al.(2023)Gururangan, Li, Lewis, Shi, Althoff, Smith, and Zettlemoyer]{gururangan2023scaling} Gururangan, S., Li, M., Lewis, M., Shi, W., Althoff, T., Smith, N.~A., and Zettlemoyer, L. 
 Scaling expert language models with unsupervised domain discovery. 
 \emph{arXiv preprint arXiv:2303.14177}, 2023."
2407.20105,hans2024like,"[Hans et~al.(2024)Hans, Wen, Jain, Kirchenbauer, Kazemi, Singhania, Singh, Somepalli, Geiping, Bhatele, et~al.]{hans2024like} Hans, A., Wen, Y., Jain, N., Kirchenbauer, J., Kazemi, H., Singhania, P., Singh, S., Somepalli, G., Geiping, J., Bhatele, A., et~al.","Be like a goldfish, don't memorize! mitigating memorization in generative {LLMs}.","Be like a goldfish, don't memorize! mitigating memorization in generative {LLMs}.",,"[Hans et~al.(2024)Hans, Wen, Jain, Kirchenbauer, Kazemi, Singhania, Singh, Somepalli, Geiping, Bhatele, et~al.]{hans2024like} Hans, A., Wen, Y., Jain, N., Kirchenbauer, J., Kazemi, H., Singhania, P., Singh, S., Somepalli, G., Geiping, J., Bhatele, A., et~al. 
 Be like a goldfish, don't memorize! mitigating memorization in generative {LLMs}. 
 \emph{arXiv preprint arXiv:2406.10209}, 2024."
2407.20105,henderson2023foundation,"[Henderson et~al.(2023)Henderson, Li, Jurafsky, Hashimoto, Lemley, and Liang]{henderson2023foundation} Henderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley, M.~A., and Liang, P.",Foundation models and fair use.,Foundation models and fair use.,,"[Henderson et~al.(2023)Henderson, Li, Jurafsky, Hashimoto, Lemley, and Liang]{henderson2023foundation} Henderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley, M.~A., and Liang, P. 
 Foundation models and fair use. 
 \emph{arXiv preprint arXiv:2303.15715}, 2023."
2407.20105,hsu2024let,"[Hsu et~al.(2024)Hsu, Chen, Liao, Ho, Wang, Hsu, and Shiu]{hsu2024let} Hsu, C.-J., Chen, Y.-C., Liao, F.-T., Ho, P.-C., Wang, Y.-H., Hsu, P.-C., and Shiu, D.-s.",Let's fuse step by step: A generative fusion decoding algorithm with {LLMs} for multi-modal text recognition.,Let's fuse step by step: A generative fusion decoding algorithm with {LLMs} for multi-modal text recognition.,,"[Hsu et~al.(2024)Hsu, Chen, Liao, Ho, Wang, Hsu, and Shiu]{hsu2024let} Hsu, C.-J., Chen, Y.-C., Liao, F.-T., Ho, P.-C., Wang, Y.-H., Hsu, P.-C., and Shiu, D.-s. 
 Let's fuse step by step: A generative fusion decoding algorithm with {LLMs} for multi-modal text recognition. 
 \emph{arXiv preprint arXiv:2405.14259}, 2024."
2407.20105,kocetkov2022stack,"[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite, Mitchell, Hughes, Wolf, et~al.]{kocetkov2022stack} Kocetkov, D., Li, R., Allal, L.~B., Li, J., Mou, C., Ferrandis, C.~M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et~al.",The stack: 3 {TB} of permissively licensed source code.,The stack: 3 {TB} of permissively licensed source code.,,"[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite, Mitchell, Hughes, Wolf, et~al.]{kocetkov2022stack} Kocetkov, D., Li, R., Allal, L.~B., Li, J., Mou, C., Ferrandis, C.~M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et~al. 
 The stack: 3 {TB} of permissively licensed source code. 
 \emph{arXiv preprint arXiv:2211.15533}, 2022."
2407.20105,lee2023talkin,"[Lee et~al.(2023)Lee, Cooper, and Grimmelmann]{lee2023talkin} Lee, K., Cooper, A.~F., and Grimmelmann, J.",Talkin''bout {AI} generation: Copyright and the generative-{AI} supply chain.,Talkin''bout {AI} generation: Copyright and the generative-{AI} supply chain.,,"[Lee et~al.(2023)Lee, Cooper, and Grimmelmann]{lee2023talkin} Lee, K., Cooper, A.~F., and Grimmelmann, J. 
 Talkin''bout {AI} generation: Copyright and the generative-{AI} supply chain. 
 \emph{arXiv preprint arXiv:2309.08133}, 2023."
2407.20105,li2023starcoder,"[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al.",{StarCoder}: may the source be with you!,{StarCoder}: may the source be with you!,,"[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder} Li, R., Allal, L.~B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et~al. 
 {StarCoder}: may the source be with you! 
 \emph{arXiv preprint arXiv:2305.06161}, 2023."
2407.20105,li2024purifying,"[Li et~al.(2024)Li, Liu, Pang, Du, Guo, Liu, and Lin]{li2024purifying} Li, T., Liu, Q., Pang, T., Du, C., Guo, Q., Liu, Y., and Lin, M.",Purifying large language models by ensembling a small language model.,Purifying large language models by ensembling a small language model.,,"[Li et~al.(2024)Li, Liu, Pang, Du, Guo, Liu, and Lin]{li2024purifying} Li, T., Liu, Q., Pang, T., Du, C., Guo, Q., Liu, Y., and Lin, M. 
 Purifying large language models by ensembling a small language model. 
 \emph{arXiv preprint arXiv:2402.14845}, 2024."
2407.20105,liu2024rethinking,"[Liu et~al.(2024{\natexlab{a}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Xu, Yao, Li, Varshney, et~al.]{liu2024rethinking} Liu, S., Yao, Y., Jia, J., Casper, S., Baracaldo, N., Hase, P., Xu, X., Yao, Y., Li, H., Varshney, K.~R., et~al.",Rethinking machine unlearning for large language models.,Rethinking machine unlearning for large language models.,,"[Liu et~al.(2024{\natexlab{a}})Liu, Yao, Jia, Casper, Baracaldo, Hase, Xu, Yao, Li, Varshney, et~al.]{liu2024rethinking} Liu, S., Yao, Y., Jia, J., Casper, S., Baracaldo, N., Hase, P., Xu, X., Yao, Y., Li, H., Varshney, K.~R., et~al. 
 Rethinking machine unlearning for large language models. 
 \emph{arXiv preprint arXiv:2402.08787}, 2024{\natexlab{a}}."
2407.20105,liu2024shield,"[Liu et~al.(2024{\natexlab{b}})Liu, Sun, Xu, Wu, Wang, Wang, and Gao]{liu2024shield} Liu, X., Sun, T., Xu, T., Wu, F., Wang, C., Wang, X., and Gao, J.",{SHIELD}: Evaluation and defense strategies for copyright compliance in llm text generation.,{SHIELD}: Evaluation and defense strategies for copyright compliance in llm text generation.,,"[Liu et~al.(2024{\natexlab{b}})Liu, Sun, Xu, Wu, Wang, Wang, and Gao]{liu2024shield} Liu, X., Sun, T., Xu, T., Wu, F., Wang, C., Wang, X., and Gao, J. 
 {SHIELD}: Evaluation and defense strategies for copyright compliance in llm text generation. 
 \emph{arXiv preprint arXiv:2406.12975}, 2024{\natexlab{b}}."
2407.20105,mavromatis2024pack,"[Mavromatis et~al.(2024)Mavromatis, Karypis, and Karypis]{mavromatis2024pack} Mavromatis, C., Karypis, P., and Karypis, G.",Pack of {LLMs}: Model fusion at test-time via perplexity optimization.,Pack of {LLMs}: Model fusion at test-time via perplexity optimization.,,"[Mavromatis et~al.(2024)Mavromatis, Karypis, and Karypis]{mavromatis2024pack} Mavromatis, C., Karypis, P., and Karypis, G. 
 Pack of {LLMs}: Model fusion at test-time via perplexity optimization. 
 \emph{arXiv preprint arXiv:2404.11531}, 2024."
2407.20105,mueller2024llms,"[Mueller et~al.(2024)Mueller, G{\""o}rge, Bernzen, Pirk, and Poretschkin]{mueller2024llms} Mueller, F.~B., G{\""o}rge, R., Bernzen, A.~K., Pirk, J.~C., and Poretschkin, M.",{LLMs} and memorization: On quality and specificity of copyright compliance.,{LLMs} and memorization: On quality and specificity of copyright compliance.,,"[Mueller et~al.(2024)Mueller, G{\""o}rge, Bernzen, Pirk, and Poretschkin]{mueller2024llms} Mueller, F.~B., G{\""o}rge, R., Bernzen, A.~K., Pirk, J.~C., and Poretschkin, M. 
 {LLMs} and memorization: On quality and specificity of copyright compliance. 
 \emph{arXiv preprint arXiv:2405.18492}, 2024."
2407.20105,nasr2023scalable,"[Nasr et~al.(2023)Nasr, Carlini, Hayase, Jagielski, Cooper, Ippolito, Choquette-Choo, Wallace, Tram{\`e}r, and Lee]{nasr2023scalable} Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A.~F., Ippolito, D., Choquette-Choo, C.~A., Wallace, E., Tram{\`e}r, F., and Lee, K.",Scalable extraction of training data from (production) language models.,Scalable extraction of training data from (production) language models.,,"[Nasr et~al.(2023)Nasr, Carlini, Hayase, Jagielski, Cooper, Ippolito, Choquette-Choo, Wallace, Tram{\`e}r, and Lee]{nasr2023scalable} Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A.~F., Ippolito, D., Choquette-Choo, C.~A., Wallace, E., Tram{\`e}r, F., and Lee, K. 
 Scalable extraction of training data from (production) language models. 
 \emph{arXiv preprint arXiv:2311.17035}, 2023."
2407.20105,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2407.20105,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.",\text{LLaMa} 2: Open foundation and fine-tuned chat models.,\text{LLaMa} 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. 
 \text{LLaMa} 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2407.20105,wei2024evaluating,"[Wei et~al.(2024)Wei, Shi, Huang, Smith, Zhang, Zettlemoyer, Li, and Henderson]{wei2024evaluating} Wei, B., Shi, W., Huang, Y., Smith, N.~A., Zhang, C., Zettlemoyer, L., Li, K., and Henderson, P.",Evaluating copyright takedown methods for language models.,Evaluating copyright takedown methods for language models.,,"[Wei et~al.(2024)Wei, Shi, Huang, Smith, Zhang, Zettlemoyer, Li, and Henderson]{wei2024evaluating} Wei, B., Shi, W., Huang, Y., Smith, N.~A., Zhang, C., Zettlemoyer, L., Li, K., and Henderson, P. 
 Evaluating copyright takedown methods for language models. 
 \emph{arXiv preprint arXiv:2406.18664}, 2024."
2407.20105,zhang2024negative,"[Zhang et~al.(2024{\natexlab{a}})Zhang, Lin, Bai, and Mei]{zhang2024negative} Zhang, R., Lin, L., Bai, Y., and Mei, S.",Negative preference optimization: From catastrophic collapse to effective unlearning.,Negative preference optimization: From catastrophic collapse to effective unlearning.,,"[Zhang et~al.(2024{\natexlab{a}})Zhang, Lin, Bai, and Mei]{zhang2024negative} Zhang, R., Lin, L., Bai, Y., and Mei, S. 
 Negative preference optimization: From catastrophic collapse to effective unlearning. 
 \emph{arXiv preprint arXiv:2404.05868}, 2024{\natexlab{a}}."
2407.20271,achiam2023gpt,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al.",Gpt-4 technical report.,Gpt-4 technical report.,,"[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,   Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,   Shyamal Anadkat, et~al. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}, 2023."
2407.20271,amini2019mathqa,"[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and   Hajishirzi]{amini2019mathqa} Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and   Hannaneh Hajishirzi.",Mathqa: Towards interpretable math word problem solving with   operation-based formalisms.,Mathqa: Towards interpretable math word problem solving with   operation-based formalisms.,,"[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and   Hajishirzi]{amini2019mathqa} Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and   Hannaneh Hajishirzi. 
 Mathqa: Towards interpretable math word problem solving with   operation-based formalisms. 
 \emph{arXiv preprint arXiv:1905.13319}, 2019."
2407.20271,anil2023palm,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,   Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,   et~al.",Palm 2 technical report.,Palm 2 technical report.,,"[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,   Taropa, Bailey, Chen, et~al.]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,   Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,   et~al. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}, 2023."
2407.20271,carlini2022quantifying,"[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and   Zhang]{carlini2022quantifying} Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian   Tramer, and Chiyuan Zhang.",Quantifying memorization across neural language models.,Quantifying memorization across neural language models.,,"[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and   Zhang]{carlini2022quantifying} Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian   Tramer, and Chiyuan Zhang. 
 Quantifying memorization across neural language models. 
 \emph{arXiv preprint arXiv:2202.07646}, 2022."
2407.20271,clark2018arc,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.","Think you have solved question answering? try arc, the ai2 reasoning   challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,   and Tafjord]{clark2018arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa   Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning   challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2407.20271,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",Bert: Pre-training of deep bidirectional transformers for language   understanding.,Bert: Pre-training of deep bidirectional transformers for language   understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2407.20271,dinan2018wizardwikipedia,"[Dinan et~al.(2018)Dinan, Roller, Shuster, Fan, Auli, and   Weston]{dinan2018wizardwikipedia} Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason   Weston.",Wizard of wikipedia: Knowledge-powered conversational agents.,Wizard of wikipedia: Knowledge-powered conversational agents.,,"[Dinan et~al.(2018)Dinan, Roller, Shuster, Fan, Auli, and   Weston]{dinan2018wizardwikipedia} Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason   Weston. 
 Wizard of wikipedia: Knowledge-powered conversational agents. 
 \emph{arXiv preprint arXiv:1811.01241}, 2018."
2407.20271,gao2024ethos,"[Gao et~al.(2024)Gao, Niu, Tang, Avestimehr, and   Annavaram]{gao2024ethos} Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, and Murali Annavaram.",Ethos: Rectifying language models in orthogonal parameter space.,Ethos: Rectifying language models in orthogonal parameter space.,,"[Gao et~al.(2024)Gao, Niu, Tang, Avestimehr, and   Annavaram]{gao2024ethos} Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, and Murali Annavaram. 
 Ethos: Rectifying language models in orthogonal parameter space. 
 \emph{arXiv preprint arXiv:2403.08994}, 2024."
2407.20271,gao2020pile,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima, et~al.]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima, et~al.]{gao2020pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles   Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}, 2020."
2407.20271,gu2024second,"[Gu et~al.(2024)Gu, Rashid, Sultana, and Mehnaz]{gu2024second} Kang Gu, Md~Rafi~Ur Rashid, Najrin Sultana, and Shagufta Mehnaz.",Second-order information matters: Revisiting machine unlearning for   large language models.,Second-order information matters: Revisiting machine unlearning for   large language models.,,"[Gu et~al.(2024)Gu, Rashid, Sultana, and Mehnaz]{gu2024second} Kang Gu, Md~Rafi~Ur Rashid, Najrin Sultana, and Shagufta Mehnaz. 
 Second-order information matters: Revisiting machine unlearning for   large language models. 
 \emph{arXiv preprint arXiv:2403.10557}, 2024."
2407.20271,guo2019certified,"[Guo et~al.(2019)Guo, Goldstein, Hannun, and Van   Der~Maaten]{guo2019certified} Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der~Maaten.",Certified data removal from machine learning models.,Certified data removal from machine learning models.,,"[Guo et~al.(2019)Guo, Goldstein, Hannun, and Van   Der~Maaten]{guo2019certified} Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der~Maaten. 
 Certified data removal from machine learning models. 
 \emph{arXiv preprint arXiv:1911.03030}, 2019."
2407.20271,hoffmann2022training,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor   Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes   Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2407.20271,jin2019pubmedqa,"[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu.",Pubmedqa: A dataset for biomedical research question answering.,Pubmedqa: A dataset for biomedical research question answering.,,"[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa} Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W Cohen, and Xinghua Lu. 
 Pubmedqa: A dataset for biomedical research question answering. 
 \emph{arXiv preprint arXiv:1909.06146}, 2019."
2407.20271,kingma2014adam,[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma and Ba(2014)]{kingma2014adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2407.20271,komeili2021internet,"[Komeili et~al.(2021)Komeili, Shuster, and Weston]{komeili2021internet} Mojtaba Komeili, Kurt Shuster, and Jason Weston.",Internet-augmented dialogue generation.,Internet-augmented dialogue generation.,,"[Komeili et~al.(2021)Komeili, Shuster, and Weston]{komeili2021internet} Mojtaba Komeili, Kurt Shuster, and Jason Weston. 
 Internet-augmented dialogue generation. 
 \emph{arXiv preprint arXiv:2107.07566}, 2021."
2407.20271,nguyen2022survey,"[Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and   Nguyen]{nguyen2022survey} Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew,   Hongzhi Yin, and Quoc Viet~Hung Nguyen.",A survey of machine unlearning.,A survey of machine unlearning.,,"[Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and   Nguyen]{nguyen2022survey} Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew,   Hongzhi Yin, and Quoc Viet~Hung Nguyen. 
 A survey of machine unlearning. 
 \emph{arXiv preprint arXiv:2209.02299}, 2022."
2407.20271,paperno2016lambada,"[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,   Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,   Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel   Fern{\'a}ndez.",The lambada dataset: Word prediction requiring a broad discourse   context.,The lambada dataset: Word prediction requiring a broad discourse   context.,,"[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,   Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,   Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel   Fern{\'a}ndez. 
 The lambada dataset: Word prediction requiring a broad discourse   context. 
 \emph{arXiv preprint arXiv:1606.06031}, 2016."
2407.20271,petroni2019language,"[Petroni et~al.(2019)Petroni, Rockt{\""a}schel, Lewis, Bakhtin, Wu,   Miller, and Riedel]{petroni2019language} Fabio Petroni, Tim Rockt{\""a}schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,   Alexander~H Miller, and Sebastian Riedel.",Language models as knowledge bases?,Language models as knowledge bases?,,"[Petroni et~al.(2019)Petroni, Rockt{\""a}schel, Lewis, Bakhtin, Wu,   Miller, and Riedel]{petroni2019language} Fabio Petroni, Tim Rockt{\""a}schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,   Alexander~H Miller, and Sebastian Riedel. 
 Language models as knowledge bases? 
 \emph{arXiv preprint arXiv:1909.01066}, 2019."
2407.20271,qu2024frontier,"[Qu et~al.(2024)Qu, Ding, Sun, Thilakarathna, Zhu, and   Niyato]{qu2024frontier} Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, and Dusit   Niyato.",The frontier of data erasure: Machine unlearning for large language   models.,The frontier of data erasure: Machine unlearning for large language   models.,,"[Qu et~al.(2024)Qu, Ding, Sun, Thilakarathna, Zhu, and   Niyato]{qu2024frontier} Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, and Dusit   Niyato. 
 The frontier of data erasure: Machine unlearning for large language   models. 
 \emph{arXiv preprint arXiv:2403.15779}, 2024."
2407.20271,rashkin2018empathetic,"[Rashkin et~al.(2018)Rashkin, Smith, Li, and   Boureau]{rashkin2018empathetic} Hannah Rashkin, Eric~Michael Smith, Margaret Li, and Y-Lan Boureau.",Towards empathetic open-domain conversation models: A new benchmark   and dataset.,Towards empathetic open-domain conversation models: A new benchmark   and dataset.,,"[Rashkin et~al.(2018)Rashkin, Smith, Li, and   Boureau]{rashkin2018empathetic} Hannah Rashkin, Eric~Michael Smith, Margaret Li, and Y-Lan Boureau. 
 Towards empathetic open-domain conversation models: A new benchmark   and dataset. 
 \emph{arXiv preprint arXiv:1811.00207}, 2018."
2407.20271,smith2020can,"[Smith et~al.(2020)Smith, Williamson, Shuster, Weston, and   Boureau]{smith2020can} Eric~Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan   Boureau.",Can you put it all together: Evaluating conversational agents'   ability to blend skills.,Can you put it all together: Evaluating conversational agents'   ability to blend skills.,,"[Smith et~al.(2020)Smith, Williamson, Shuster, Weston, and   Boureau]{smith2020can} Eric~Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan   Boureau. 
 Can you put it all together: Evaluating conversational agents'   ability to blend skills. 
 \emph{arXiv preprint arXiv:2004.08449}, 2020."
2407.20271,team2023gemini,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,   Schalkwyk, Dai, Hauth, et~al.]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,   Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}, 2023."
2407.20271,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,   Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,   Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine   Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,   et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2407.20271,yao2023large,"[Yao et~al.(2023)Yao, Xu, and Liu]{yao2023large} Yuanshun Yao, Xiaojun Xu, and Yang Liu.",Large language model unlearning.,Large language model unlearning.,,"[Yao et~al.(2023)Yao, Xu, and Liu]{yao2023large} Yuanshun Yao, Xiaojun Xu, and Yang Liu. 
 Large language model unlearning. 
 \emph{arXiv preprint arXiv:2310.10683}, 2023."
2407.20271,zellers2019hellaswag,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and   Choi]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}, 2019."
2407.20271,zhang2022opt,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,   Diab, Li, Lin, et~al.]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui   Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}, 2022."
2407.20271,zhang2019bertscore,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}, 2019."
2407.20271,zhao2023survey,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,   Dong, et~al.]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,   Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.",A survey of large language models.,A survey of large language models.,,"[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,   Dong, et~al.]{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,   Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al. 
 A survey of large language models. 
 \emph{arXiv preprint arXiv:2303.18223}, 2023."
2407.21058,baldini2023keeping,"[{Baldini et~al.(2023)Baldini, Yadav, Das, and   Varshney}]{baldini2023keeping} Baldini, I.; Yadav, C.; Das, P.; and Varshney, K.~R. 2023.",Keeping Up with the Language Models: Robustness-Bias Interplay in NLI   Data and Models.,Keeping Up with the Language Models: Robustness-Bias Interplay in NLI   Data and Models.,,"[{Baldini et~al.(2023)Baldini, Yadav, Das, and   Varshney}]{baldini2023keeping} Baldini, I.; Yadav, C.; Das, P.; and Varshney, K.~R. 2023. 
 Keeping Up with the Language Models: Robustness-Bias Interplay in NLI   Data and Models. 
 \emph{arXiv preprint arXiv:2305.12620}."
2407.21058,birhane2023hate,"[{Birhane et~al.(2023)Birhane, Prabhu, Han, and   Boddeti}]{birhane2023hate} Birhane, A.; Prabhu, V.; Han, S.; and Boddeti, V.~N. 2023.",On hate scaling laws for data-swamps.,On hate scaling laws for data-swamps.,,"[{Birhane et~al.(2023)Birhane, Prabhu, Han, and   Boddeti}]{birhane2023hate} Birhane, A.; Prabhu, V.; Han, S.; and Boddeti, V.~N. 2023. 
 On hate scaling laws for data-swamps. 
 \emph{arXiv preprint arXiv:2306.13141}."
2407.21058,blodgett2016demographic,"[{Blodgett, Green, and O'Connor(2016)}]{blodgett2016demographic} Blodgett, S.~L.; Green, L.; and O'Connor, B. 2016.",Demographic dialectal variation in social media: A case study of   African-American English.,Demographic dialectal variation in social media: A case study of   African-American English.,,"[{Blodgett, Green, and O'Connor(2016)}]{blodgett2016demographic} Blodgett, S.~L.; Green, L.; and O'Connor, B. 2016. 
 Demographic dialectal variation in social media: A case study of   African-American English. 
 \emph{arXiv preprint arXiv:1608.08868}."
2407.21058,conneau2019unsupervised,"[{Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,   Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}]{conneau2019unsupervised} Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzm{\'a}n,   F.; Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2019.",Unsupervised cross-lingual representation learning at scale.,Unsupervised cross-lingual representation learning at scale.,,"[{Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,   Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}]{conneau2019unsupervised} Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzm{\'a}n,   F.; Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. 
 Unsupervised cross-lingual representation learning at scale. 
 \emph{arXiv preprint arXiv:1911.02116}."
2407.21058,datta2014automated,"[{Datta, Tschantz, and Datta(2014)}]{datta2014automated} Datta, A.; Tschantz, M.~C.; and Datta, A. 2014.","Automated experiments on ad privacy settings: A tale of opacity,   choice, and discrimination.","Automated experiments on ad privacy settings: A tale of opacity,   choice, and discrimination.",,"[{Datta, Tschantz, and Datta(2014)}]{datta2014automated} Datta, A.; Tschantz, M.~C.; and Datta, A. 2014. 
 Automated experiments on ad privacy settings: A tale of opacity,   choice, and discrimination. 
 \emph{arXiv preprint arXiv:1408.6491}."
2407.21058,devlin2018bert,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and   Toutanova}]{devlin2018bert} Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.",{BERT}: Pre-training of deep bidirectional transformers for language   understanding.,{BERT}: Pre-training of deep bidirectional transformers for language   understanding.,,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and   Toutanova}]{devlin2018bert} Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. 
 {BERT}: Pre-training of deep bidirectional transformers for language   understanding. 
 \emph{arXiv preprint arXiv:1810.04805}."
2407.21058,gao2020pile,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima et~al.}]{gao2020pile} Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang,   J.; He, H.; Thite, A.; Nabeshima, N.; et~al. 2020.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,   He, Thite, Nabeshima et~al.}]{gao2020pile} Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang,   J.; He, H.; Thite, A.; Nabeshima, N.; et~al. 2020. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2407.21058,gehman2020realtoxicityprompts,"[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and   Smith}]{gehman2020realtoxicityprompts} Gehman, S.; Gururangan, S.; Sap, M.; Choi, Y.; and Smith, N.~A. 2020.",Realtoxicityprompts: Evaluating neural toxic degeneration in language   models.,Realtoxicityprompts: Evaluating neural toxic degeneration in language   models.,,"[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and   Smith}]{gehman2020realtoxicityprompts} Gehman, S.; Gururangan, S.; Sap, M.; Choi, Y.; and Smith, N.~A. 2020. 
 Realtoxicityprompts: Evaluating neural toxic degeneration in language   models. 
 \emph{arXiv preprint arXiv:2009.11462}."
2407.21058,goldfarb2023prompt,"[{Goldfarb-Tarrant et~al.(2023)Goldfarb-Tarrant, Ungless, Balkir, and   Blodgett}]{goldfarb2023prompt} Goldfarb-Tarrant, S.; Ungless, E.; Balkir, E.; and Blodgett, S.~L. 2023.",This prompt is measuring< mask>: evaluating bias evaluation in   language models.,This prompt is measuring< mask>: evaluating bias evaluation in   language models.,,"[{Goldfarb-Tarrant et~al.(2023)Goldfarb-Tarrant, Ungless, Balkir, and   Blodgett}]{goldfarb2023prompt} Goldfarb-Tarrant, S.; Ungless, E.; Balkir, E.; and Blodgett, S.~L. 2023. 
 This prompt is measuring< mask>: evaluating bias evaluation in   language models. 
 \emph{arXiv preprint arXiv:2305.12757}."
2407.21058,henighan2020scaling,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,   Jun, Brown, Dhariwal, Gray et~al.}]{henighan2020scaling} Henighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.; Jackson, J.; Jun, H.;   Brown, T.~B.; Dhariwal, P.; Gray, S.; et~al. 2020.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,   Jun, Brown, Dhariwal, Gray et~al.}]{henighan2020scaling} Henighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.; Jackson, J.; Jun, H.;   Brown, T.~B.; Dhariwal, P.; Gray, S.; et~al. 2020. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}."
2407.21058,hernandez2021scaling,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and   McCandlish}]{hernandez2021scaling} Hernandez, D.; Kaplan, J.; Henighan, T.; and McCandlish, S. 2021.",Scaling laws for transfer.,Scaling laws for transfer.,,"[{Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and   McCandlish}]{hernandez2021scaling} Hernandez, D.; Kaplan, J.; Henighan, T.; and McCandlish, S. 2021. 
 Scaling laws for transfer. 
 \emph{arXiv preprint arXiv:2102.01293}."
2407.21058,hestness2017deep,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,   Kianinejad, Patwary, Yang, and Zhou}]{hestness2017deep} Hestness, J.; Narang, S.; Ardalani, N.; Diamos, G.; Jun, H.; Kianinejad, H.;   Patwary, M. M.~A.; Yang, Y.; and Zhou, Y. 2017.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,   Kianinejad, Patwary, Yang, and Zhou}]{hestness2017deep} Hestness, J.; Narang, S.; Ardalani, N.; Diamos, G.; Jun, H.; Kianinejad, H.;   Patwary, M. M.~A.; Yang, Y.; and Zhou, Y. 2017. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}."
2407.21058,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford,   E.; Casas, D. d.~L.; Hendricks, L.~A.; Welbl, J.; Clark, A.; et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,   Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford,   E.; Casas, D. d.~L.; Hendricks, L.~A.; Welbl, J.; Clark, A.; et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2407.21058,hutchinson2020social,"[{Hutchinson et~al.(2020)Hutchinson, Prabhakaran, Denton, Webster,   Zhong, and Denuyl}]{hutchinson2020social} Hutchinson, B.; Prabhakaran, V.; Denton, E.; Webster, K.; Zhong, Y.; and   Denuyl, S. 2020.",Social biases in NLP models as barriers for persons with   disabilities.,Social biases in NLP models as barriers for persons with   disabilities.,,"[{Hutchinson et~al.(2020)Hutchinson, Prabhakaran, Denton, Webster,   Zhong, and Denuyl}]{hutchinson2020social} Hutchinson, B.; Prabhakaran, V.; Denton, E.; Webster, K.; Zhong, Y.; and   Denuyl, S. 2020. 
 Social biases in NLP models as barriers for persons with   disabilities. 
 \emph{arXiv preprint arXiv:2005.00813}."
2407.21058,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.~B.; Chess, B.; Child, R.;   Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.~B.; Chess, B.; Child, R.;   Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2407.21058,kurita2019measuring,"[{Kurita et~al.(2019)Kurita, Vyas, Pareek, Black, and   Tsvetkov}]{kurita2019measuring} Kurita, K.; Vyas, N.; Pareek, A.; Black, A.~W.; and Tsvetkov, Y. 2019.",Measuring bias in contextualized word representations.,Measuring bias in contextualized word representations.,,"[{Kurita et~al.(2019)Kurita, Vyas, Pareek, Black, and   Tsvetkov}]{kurita2019measuring} Kurita, K.; Vyas, N.; Pareek, A.; Black, A.~W.; and Tsvetkov, Y. 2019. 
 Measuring bias in contextualized word representations. 
 \emph{arXiv preprint arXiv:1906.07337}."
2407.21058,li2020unqovering,"[{Li et~al.(2020)Li, Khot, Khashabi, Sabharwal, and   Srikumar}]{li2020unqovering} Li, T.; Khot, T.; Khashabi, D.; Sabharwal, A.; and Srikumar, V. 2020.",UNQOVERing stereotyping biases via underspecified questions.,UNQOVERing stereotyping biases via underspecified questions.,,"[{Li et~al.(2020)Li, Khot, Khashabi, Sabharwal, and   Srikumar}]{li2020unqovering} Li, T.; Khot, T.; Khashabi, D.; Sabharwal, A.; and Srikumar, V. 2020. 
 UNQOVERing stereotyping biases via underspecified questions. 
 \emph{arXiv preprint arXiv:2010.02428}."
2407.21058,liu2019roberta,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,   Zettlemoyer, and Stoyanov}]{liu2019roberta} Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;   Zettlemoyer, L.; and Stoyanov, V. 2019.",Roberta: A robustly optimized bert pretraining approach.,Roberta: A robustly optimized bert pretraining approach.,,"[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,   Zettlemoyer, and Stoyanov}]{liu2019roberta} Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;   Zettlemoyer, L.; and Stoyanov, V. 2019. 
 Roberta: A robustly optimized bert pretraining approach. 
 \emph{arXiv preprint arXiv:1907.11692}."
2407.21058,luccioni2021s,"[{Luccioni and Viviano(2021)}]{luccioni2021s} Luccioni, A.~S.; and Viviano, J.~D. 2021.",What's in the Box? A Preliminary Analysis of Undesirable Content in   the Common Crawl Corpus.,What's in the Box? A Preliminary Analysis of Undesirable Content in   the Common Crawl Corpus.,,"[{Luccioni and Viviano(2021)}]{luccioni2021s} Luccioni, A.~S.; and Viviano, J.~D. 2021. 
 What's in the Box? A Preliminary Analysis of Undesirable Content in   the Common Crawl Corpus. 
 \emph{arXiv preprint arXiv:2105.02732}."
2407.21058,may-etal-2019-measuring,"[{May et~al.(2019)May, Wang, Bordia, Bowman, and   Rudinger}]{may-etal-2019-measuring} May, C.; Wang, A.; Bordia, S.; Bowman, S.~R.; and Rudinger, R. 2019.",On measuring social biases in sentence encoders.,On measuring social biases in sentence encoders.,,"[{May et~al.(2019)May, Wang, Bordia, Bowman, and   Rudinger}]{may-etal-2019-measuring} May, C.; Wang, A.; Bordia, S.; Bowman, S.~R.; and Rudinger, R. 2019. 
 On measuring social biases in sentence encoders. 
 \emph{arXiv preprint arXiv:1903.10561}."
2407.21058,nadeem2020stereoset,"[{Nadeem, Bethke, and Reddy(2020)}]{nadeem2020stereoset} Nadeem, M.; Bethke, A.; and Reddy, S. 2020.",StereoSet: Measuring stereotypical bias in pretrained language   models.,StereoSet: Measuring stereotypical bias in pretrained language   models.,,"[{Nadeem, Bethke, and Reddy(2020)}]{nadeem2020stereoset} Nadeem, M.; Bethke, A.; and Reddy, S. 2020. 
 StereoSet: Measuring stereotypical bias in pretrained language   models. 
 \emph{arXiv preprint arXiv:2004.09456}."
2407.21058,nangia2020crows,"[{Nangia et~al.(2020)Nangia, Vania, Bhalerao, and   Bowman}]{nangia2020crows} Nangia, N.; Vania, C.; Bhalerao, R.; and Bowman, S.~R. 2020.",CrowS-pairs: A challenge dataset for measuring social biases in   masked language models.,CrowS-pairs: A challenge dataset for measuring social biases in   masked language models.,,"[{Nangia et~al.(2020)Nangia, Vania, Bhalerao, and   Bowman}]{nangia2020crows} Nangia, N.; Vania, C.; Bhalerao, R.; and Bowman, S.~R. 2020. 
 CrowS-pairs: A challenge dataset for measuring social biases in   masked language models. 
 \emph{arXiv preprint arXiv:2010.00133}."
2407.21058,parrish2021bbq,"[{Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang,   Thompson, Htut, and Bowman}]{parrish2021bbq} Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V.; Phang, J.; Thompson, J.;   Htut, P.~M.; and Bowman, S.~R. 2021.",BBQ: A hand-built bias benchmark for question answering.,BBQ: A hand-built bias benchmark for question answering.,,"[{Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang,   Thompson, Htut, and Bowman}]{parrish2021bbq} Parrish, A.; Chen, A.; Nangia, N.; Padmakumar, V.; Phang, J.; Thompson, J.;   Htut, P.~M.; and Bowman, S.~R. 2021. 
 BBQ: A hand-built bias benchmark for question answering. 
 \emph{arXiv preprint arXiv:2110.08193}."
2407.21058,srivastava2023beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,   Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2023beyond} Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A.~M.; Abid, A.; Fisch, A.;   Brown, A.~R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,   Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2023beyond} Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A.~M.; Abid, A.; Fisch, A.;   Brown, A.~R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the   capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2407.21058,tay2022scaling,"[{Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang,   Tran, Yogatama, and Metzler}]{tay2022scaling} Tay, Y.; Dehghani, M.; Abnar, S.; Chung, H.~W.; Fedus, W.; Rao, J.; Narang, S.;   Tran, V.~Q.; Yogatama, D.; and Metzler, D. 2022.",Scaling laws vs model architectures: How does inductive bias   influence scaling?,Scaling laws vs model architectures: How does inductive bias   influence scaling?,,"[{Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang,   Tran, Yogatama, and Metzler}]{tay2022scaling} Tay, Y.; Dehghani, M.; Abnar, S.; Chung, H.~W.; Fedus, W.; Rao, J.; Narang, S.;   Tran, V.~Q.; Yogatama, D.; and Metzler, D. 2022. 
 Scaling laws vs model architectures: How does inductive bias   influence scaling? 
 \emph{arXiv preprint arXiv:2207.10551}."
2407.21058,turc2019well,"[{Turc et~al.(2019)Turc, Chang, Lee, and Toutanova}]{turc2019well} Turc, I.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.",Well-read students learn better: On the importance of pre-training   compact models.,Well-read students learn better: On the importance of pre-training   compact models.,,"[{Turc et~al.(2019)Turc, Chang, Lee, and Toutanova}]{turc2019well} Turc, I.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. 
 Well-read students learn better: On the importance of pre-training   compact models. 
 \emph{arXiv preprint arXiv:1908.08962}."
2407.21058,weerts2021introduction,"[{Weerts(2021)}]{weerts2021introduction} Weerts, H.~J. 2021.",An introduction to algorithmic fairness.,An introduction to algorithmic fairness.,,"[{Weerts(2021)}]{weerts2021introduction} Weerts, H.~J. 2021. 
 An introduction to algorithmic fairness. 
 \emph{arXiv preprint arXiv:2105.05595}."
2407.21058,wei2021finetuned,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and   Le}]{wei2021finetuned} Wei, J.; Bosma, M.; Zhao, V.~Y.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai,   A.~M.; and Le, Q.~V. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and   Le}]{wei2021finetuned} Wei, J.; Bosma, M.; Zhao, V.~Y.; Guu, K.; Yu, A.~W.; Lester, B.; Du, N.; Dai,   A.~M.; and Le, Q.~V. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2407.21058,wei2022emergent,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,   Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama,   D.; Bosma, M.; Zhou, D.; Metzler, D.; et~al. 2022.",Emergent abilities of large language models.,Emergent abilities of large language models.,,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,   Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama,   D.; Bosma, M.; Zhou, D.; Metzler, D.; et~al. 2022. 
 Emergent abilities of large language models. 
 \emph{arXiv preprint arXiv:2206.07682}."
2407.21058,wenzek2019ccnet,"[{Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n,   Joulin, and Grave}]{wenzek2019ccnet} Wenzek, G.; Lachaux, M.-A.; Conneau, A.; Chaudhary, V.; Guzm{\'a}n, F.; Joulin,   A.; and Grave, E. 2019.",CCNet: Extracting high quality monolingual datasets from web crawl   data.,CCNet: Extracting high quality monolingual datasets from web crawl   data.,,"[{Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n,   Joulin, and Grave}]{wenzek2019ccnet} Wenzek, G.; Lachaux, M.-A.; Conneau, A.; Chaudhary, V.; Guzm{\'a}n, F.; Joulin,   A.; and Grave, E. 2019. 
 CCNet: Extracting high quality monolingual datasets from web crawl   data. 
 \emph{arXiv preprint arXiv:1911.00359}."
2407.21058,zhao2018gender,"[{Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and   Chang}]{zhao2018gender} Zhao, J.; Wang, T.; Yatskar, M.; Ordonez, V.; and Chang, K.-W. 2018.",Gender bias in coreference resolution: Evaluation and debiasing   methods.,Gender bias in coreference resolution: Evaluation and debiasing   methods.,,"[{Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and   Chang}]{zhao2018gender} Zhao, J.; Wang, T.; Yatskar, M.; Ordonez, V.; and Chang, K.-W. 2018. 
 Gender bias in coreference resolution: Evaluation and debiasing   methods. 
 \emph{arXiv preprint arXiv:1804.06876}."
2407.21368,caffagni2024wikillava,"[Caffagni et~al.(2024)Caffagni, Cocchi, Moratelli, Sarto, Cornia, Baraldi, and Cucchiara]{caffagni2024wikillava} Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.",{Wiki-LLaVA}: Hierarchical retrieval-augmented generation for multimodal {LLMs}.,{Wiki-LLaVA}: Hierarchical retrieval-augmented generation for multimodal {LLMs}.,,"[Caffagni et~al.(2024)Caffagni, Cocchi, Moratelli, Sarto, Cornia, Baraldi, and Cucchiara]{caffagni2024wikillava} Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 
 {Wiki-LLaVA}: Hierarchical retrieval-augmented generation for multimodal {LLMs}. 
 \emph{arXiv Preprint arXiv:2404.15406}, 2024."
2407.21368,du2023improving,"[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch]{du2023improving} Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch.",Improving factuality and reasoning in language models through multiagent debate.,Improving factuality and reasoning in language models through multiagent debate.,,"[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch]{du2023improving} Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch. 
 Improving factuality and reasoning in language models through multiagent debate. 
 \emph{arXiv Preprint arXiv:2305.14325}, 2023."
2407.21368,favero2024multimodal,"[Favero et~al.(2024)Favero, Zancato, Trager, Choudhary, Perera, Achille, Swaminathan, and Soatto]{favero2024multimodal} Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto.",Multi-modal hallucination control by visual information grounding.,Multi-modal hallucination control by visual information grounding.,,"[Favero et~al.(2024)Favero, Zancato, Trager, Choudhary, Perera, Achille, Swaminathan, and Soatto]{favero2024multimodal} Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. 
 Multi-modal hallucination control by visual information grounding. 
 \emph{arXiv Preprint arXiv:2403.14003}, 2024."
2407.21368,he2020pathvqa,"[He et~al.(2020)He, Zhang, Mou, Xing, and Xie]{he2020pathvqa} Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.",{PathVQA}: 30000+ questions for medical visual question answering.,{PathVQA}: 30000+ questions for medical visual question answering.,,"[He et~al.(2020)He, Zhang, Mou, Xing, and Xie]{he2020pathvqa} Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 
 {PathVQA}: 30000+ questions for medical visual question answering. 
 \emph{arXiv Preprint arXiv:2003.10286}, 2020."
2407.21368,hu2023ciem,"[Hu et~al.(2023)Hu, Zhang, Zhao, and Sun]{hu2023ciem} Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun.",{CIEM}: Contrastive instruction evaluation method for better instruction tuning.,{CIEM}: Contrastive instruction evaluation method for better instruction tuning.,,"[Hu et~al.(2023)Hu, Zhang, Zhao, and Sun]{hu2023ciem} Hongyu Hu, Jiyuan Zhang, Minyi Zhao, and Zhenbang Sun. 
 {CIEM}: Contrastive instruction evaluation method for better instruction tuning. 
 \emph{arXiv Preprint arXiv:2309.02301}, 2023."
2407.21368,jain2023vcoder,"[Jain et~al.(2023)Jain, Yang, and Shi]{jain2023vcoder} Jitesh Jain, Jianwei Yang, and Humphrey Shi.",Vcoder: Versatile vision encoders for multimodal large language models.,Vcoder: Versatile vision encoders for multimodal large language models.,,"[Jain et~al.(2023)Jain, Yang, and Shi]{jain2023vcoder} Jitesh Jain, Jianwei Yang, and Humphrey Shi. 
 Vcoder: Versatile vision encoders for multimodal large language models. 
 \emph{arXiv Preprint arXiv:2312.14233}, 2023."
2407.21368,leng2023mitigating,"[Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing.",Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,Mitigating object hallucinations in large vision-language models through visual contrastive decoding.,,"[Leng et~al.(2023)Leng, Zhang, Chen, Li, Lu, Miao, and Bing]{leng2023mitigating} Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 
 Mitigating object hallucinations in large vision-language models through visual contrastive decoding. 
 \emph{arXiv Preprint arXiv:2311.16922}, 2023."
2407.21368,llavamed,"[Li et~al.(2023{\natexlab{a}})Li, Wong, Zhang, Usuyama, Liu, Yang, Naumann, Poon, and Gao]{llavamed} Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.",{LLaVA-Med}: Training a large language-and-vision assistant for biomedicine in one day.,{LLaVA-Med}: Training a large language-and-vision assistant for biomedicine in one day.,,"[Li et~al.(2023{\natexlab{a}})Li, Wong, Zhang, Usuyama, Liu, Yang, Naumann, Poon, and Gao]{llavamed} Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 
 {LLaVA-Med}: Training a large language-and-vision assistant for biomedicine in one day. 
 \emph{arXiv Preprint arXiv:2306.00890}, 2023{\natexlab{a}}."
2407.21368,vllmsurvey,"[Liu et~al.(2024)Liu, Xue, Chen, Chen, Zhao, Wang, Hou, Li, and Peng]{vllmsurvey} Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke~Wang, Liping Hou, Rongjun Li, and Wei Peng.",A survey on hallucination in large vision-language models.,A survey on hallucination in large vision-language models.,,"[Liu et~al.(2024)Liu, Xue, Chen, Chen, Zhao, Wang, Hou, Li, and Peng]{vllmsurvey} Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke~Wang, Liping Hou, Rongjun Li, and Wei Peng. 
 A survey on hallucination in large vision-language models. 
 \emph{arXiv Preprint arXiv:2402.00253}, 2024."
2407.21368,lovenia2023negative,"[Lovenia et~al.(2023)Lovenia, Dai, Cahyawijaya, Ji, and Fung]{lovenia2023negative} Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.",Negative object presence evaluation ({NOPE}) to measure object hallucination in vision-language models.,Negative object presence evaluation ({NOPE}) to measure object hallucination in vision-language models.,,"[Lovenia et~al.(2023)Lovenia, Dai, Cahyawijaya, Ji, and Fung]{lovenia2023negative} Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. 
 Negative object presence evaluation ({NOPE}) to measure object hallucination in vision-language models. 
 \emph{arXiv Preprint arXiv:2310.05338}, 2023."
2407.21368,openai2023gpt4,[OpenAI et~al.(2023)]{openai2023gpt4} OpenAI et~al.,{GPT-4} technical report.,{GPT-4} technical report.,,"[OpenAI et~al.(2023)]{openai2023gpt4} OpenAI et~al. 
 {GPT-4} technical report. 
 \emph{arXiv Preprint arXiv:2303.08774}, 2023."
2407.21368,deceive,"[Qian et~al.(2024)Qian, Zhang, Yang, and Gan]{deceive} Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan.",How easy is it to fool your multimodal {LLMs}? {An} empirical analysis on deceptive prompts.,How easy is it to fool your multimodal {LLMs}? {An} empirical analysis on deceptive prompts.,,"[Qian et~al.(2024)Qian, Zhang, Yang, and Gan]{deceive} Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan. 
 How easy is it to fool your multimodal {LLMs}? {An} empirical analysis on deceptive prompts. 
 \emph{arXiv Preprint arXiv:2402.13220}, 2024."
2407.21368,medpalm2,"[Singhal et~al.(2023)Singhal, Tu, Gottweis, Sayres, Wulczyn, Hou, Clark, Pfohl, Cole-Lewis, Neal, Schaekermann, Wang, Amin, Lachgar, Mansfield, Prakash, Green, Dominowska, y~Arcas, Tomasev, Liu, Wong, Semturs, Mahdavi, Barral, Webster, Corrado, Matias, Azizi, Karthikesalingam, and Natarajan]{medpalm2} Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le~Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise~Aguera y~Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S.~Sara Mahdavi, Joelle Barral, Dale Webster, Greg~S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.",Towards expert-level medical question answering with large language models.,Towards expert-level medical question answering with large language models.,,"[Singhal et~al.(2023)Singhal, Tu, Gottweis, Sayres, Wulczyn, Hou, Clark, Pfohl, Cole-Lewis, Neal, Schaekermann, Wang, Amin, Lachgar, Mansfield, Prakash, Green, Dominowska, y~Arcas, Tomasev, Liu, Wong, Semturs, Mahdavi, Barral, Webster, Corrado, Matias, Azizi, Karthikesalingam, and Natarajan]{medpalm2} Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le~Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise~Aguera y~Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S.~Sara Mahdavi, Joelle Barral, Dale Webster, Greg~S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 
 Towards expert-level medical question answering with large language models. 
 \emph{arXiv Preprint arXiv:2305.09617}, 2023."
2407.21368,2023llavarlhf,"[Sun et~al.(2023{\natexlab{a}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, Keutzer, and Darrell]{2023llavarlhf} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell.",Aligning large multimodal models with factually augmented {RLHF}.,Aligning large multimodal models with factually augmented {RLHF}.,,"[Sun et~al.(2023{\natexlab{a}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, Keutzer, and Darrell]{2023llavarlhf} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 
 Aligning large multimodal models with factually augmented {RLHF}. 
 \emph{arXiv Preprint arXiv:2309.14525}, 2023{\natexlab{a}}."
2407.21368,sun2023aligning,"[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, et~al.]{sun2023aligning} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et~al.",Aligning large multimodal models with factually augmented {RLHF}.,Aligning large multimodal models with factually augmented {RLHF}.,,"[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Cao, Liu, Li, Shen, Gan, Gui, Wang, Yang, et~al.]{sun2023aligning} Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et~al. 
 Aligning large multimodal models with factually augmented {RLHF}. 
 \emph{arXiv Preprint arXiv:2309.14525}, 2023{\natexlab{b}}."
2407.21368,touvron2023llama,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.",{LLaMA}: Open and efficient foundation language models.,{LLaMA}: Open and efficient foundation language models.,,"[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 
 {LLaMA}: Open and efficient foundation language models. 
 \emph{arXiv Preprint arXiv:2302.13971}, 2023."
2407.21368,zhang2023llamaadapter,"[Zhang et~al.(2023)Zhang, Han, Liu, Gao, Zhou, Hu, Yan, Lu, Li, and Qiao]{zhang2023llamaadapter} Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu~Qiao.",{LLaMA-Adapter}: Efficient fine-tuning of language models with zero-init attention.,{LLaMA-Adapter}: Efficient fine-tuning of language models with zero-init attention.,,"[Zhang et~al.(2023)Zhang, Han, Liu, Gao, Zhou, Hu, Yan, Lu, Li, and Qiao]{zhang2023llamaadapter} Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu~Qiao. 
 {LLaMA-Adapter}: Efficient fine-tuning of language models with zero-init attention. 
 \emph{arXiv Preprint arXiv:2303.16199}, 2023."
2407.21368,pmc15,"[Zhang et~al.(2024)Zhang, Xu, Usuyama, Xu, Bagga, Tinn, Preston, Rao, Wei, Valluri, Wong, Tupini, Wang, Mazzola, Shukla, Liden, Gao, Lungren, Naumann, Wang, and Poon]{pmc15} Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu~Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu~Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew~P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon.",{BiomedCLIP}: A multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,{BiomedCLIP}: A multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.,,"[Zhang et~al.(2024)Zhang, Xu, Usuyama, Xu, Bagga, Tinn, Preston, Rao, Wei, Valluri, Wong, Tupini, Wang, Mazzola, Shukla, Liden, Gao, Lungren, Naumann, Wang, and Poon]{pmc15} Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu~Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu~Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew~P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. 
 {BiomedCLIP}: A multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. 
 \emph{arXiv Preprint arXiv:2303.00915}, 2024."
2407.21368,zhao2024mitigating,"[Zhao et~al.(2024)Zhao, Deng, Zhang, and Gu]{zhao2024mitigating} Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu.",Mitigating object hallucination in large vision-language models via classifier-free guidance.,Mitigating object hallucination in large vision-language models via classifier-free guidance.,,"[Zhao et~al.(2024)Zhao, Deng, Zhang, and Gu]{zhao2024mitigating} Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. 
 Mitigating object hallucination in large vision-language models via classifier-free guidance. 
 \emph{arXiv Preprint arXiv:2402.08680}, 2024."
2407.21368,zhu2023minigpt,"[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",{MiniGPT-4}: Enhancing vision-language understanding with advanced large language models.,{MiniGPT-4}: Enhancing vision-language understanding with advanced large language models.,,"[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt} Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 
 {MiniGPT-4}: Enhancing vision-language understanding with advanced large language models. 
 \emph{arXiv Preprint arXiv:2304.10592}, 2023."
2407.2153,qwen,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2407.2153,allenai:arc,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{allenai:arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv:1803.05457v1}."
2407.2153,cobbe2021gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2407.2153,gliwa2019samsum,"[{Gliwa et~al.(2019)Gliwa, Mochol, Biesek, and Wawer}]{gliwa2019samsum} Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019.",Samsum corpus: A human-annotated dialogue dataset for abstractive summarization.,Samsum corpus: A human-annotated dialogue dataset for abstractive summarization.,,"[{Gliwa et~al.(2019)Gliwa, Mochol, Biesek, and Wawer}]{gliwa2019samsum} Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. 
 Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. 
 \emph{arXiv preprint arXiv:1911.12237}."
2407.2153,huang2023ceval,"[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, Fu, Sun, and He}]{huang2023ceval} Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023.",C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.,C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.,,"[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, Fu, Sun, and He}]{huang2023ceval} Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. 
 C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. 
 \emph{arXiv preprint arXiv:2305.08322}."
2407.2153,kornilova2019billsum,[{Kornilova and Eidelman(2019)}]{kornilova2019billsum} Anastassia Kornilova and Vlad Eidelman. 2019.,Billsum: A corpus for automatic summarization of us legislation.,Billsum: A corpus for automatic summarization of us legislation.,,"[{Kornilova and Eidelman(2019)}]{kornilova2019billsum} Anastassia Kornilova and Vlad Eidelman. 2019. 
 Billsum: A corpus for automatic summarization of us legislation. 
 \emph{arXiv preprint arXiv:1910.00523}."
2407.2153,Magnusson2023PalomaAB,"[{Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, Soldaini, Jha, Tafjord, Schwenk, Walsh, Elazar, Lo, Groeneveld, Beltagy, Hajishirzi, Smith, Richardson, and Dodge}]{Magnusson2023PalomaAB} Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A.~Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz~Beltagy, Hannaneh Hajishirzi, Noah~A. Smith, Kyle Richardson, and Jesse Dodge. 2023.",Paloma: A benchmark for evaluating language model fit.,Paloma: A benchmark for evaluating language model fit.,,"[{Magnusson et~al.(2023)Magnusson, Bhagia, Hofmann, Soldaini, Jha, Tafjord, Schwenk, Walsh, Elazar, Lo, Groeneveld, Beltagy, Hajishirzi, Smith, Richardson, and Dodge}]{Magnusson2023PalomaAB} Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A.~Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz~Beltagy, Hannaneh Hajishirzi, Noah~A. Smith, Kyle Richardson, and Jesse Dodge. 2023. 
 Paloma: A benchmark for evaluating language model fit. 
 \emph{arXiv preprint arXiv:2312.10523}."
2407.2153,ngram-novelty,"[{Merrill et~al.(2024)Merrill, Smith, and Elazar}]{ngram-novelty} William Merrill, Noah~A. Smith, and Yanai Elazar. 2024.",Evaluating $n$-gram novelty of language models using rusty-dawg.,Evaluating $n$-gram novelty of language models using rusty-dawg.,,"[{Merrill et~al.(2024)Merrill, Smith, and Elazar}]{ngram-novelty} William Merrill, Noah~A. Smith, and Yanai Elazar. 2024. 
 Evaluating $n$-gram novelty of language models using rusty-dawg. 
 \emph{arXiv preprint arXiv:2406.13069}."
2407.2153,muennighoff2022crosslingual,"[{Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf et~al.}]{muennighoff2022crosslingual} Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al. 2022.",Crosslingual generalization through multitask finetuning.,Crosslingual generalization through multitask finetuning.,,"[{Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf et~al.}]{muennighoff2022crosslingual} Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al. 2022. 
 Crosslingual generalization through multitask finetuning. 
 \emph{arXiv preprint arXiv:2211.01786}."
2407.2153,wang2017liar,[{Wang(2017)}]{wang2017liar} William~Yang Wang. 2017.,""" liar, liar pants on fire"": A new benchmark dataset for fake news detection.",""" liar, liar pants on fire"": A new benchmark dataset for fake news detection.",,"[{Wang(2017)}]{wang2017liar} William~Yang Wang. 2017. 
 "" liar, liar pants on fire"": A new benchmark dataset for fake news detection. 
 \emph{arXiv preprint arXiv:1705.00648}."
2408.00137,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2408.00137,belrose2023eliciting,"[{Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky, McKinney, Biderman, and Steinhardt}]{belrose2023eliciting} Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023.",Eliciting latent predictions from transformers with the tuned lens.,Eliciting latent predictions from transformers with the tuned lens.,,"[{Belrose et~al.(2023)Belrose, Furman, Smith, Halawi, Ostrovsky, McKinney, Biderman, and Steinhardt}]{belrose2023eliciting} Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. 
 Eliciting latent predictions from transformers with the tuned lens. 
 \emph{arXiv preprint arXiv:2303.08112}."
2408.00137,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2408.00137,huang2023survey,"[{Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin et~al.}]{huang2023survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.","A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.",,"[{Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin et~al.}]{huang2023survey} Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et~al. 2023. 
 A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 
 \emph{arXiv preprint arXiv:2311.05232}."
2408.00137,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.00137,kadavath2022language,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022.",Language models (mostly) know what they know.,Language models (mostly) know what they know.,,"[{Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tran-Johnson et~al.}]{kadavath2022language} Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et~al. 2022. 
 Language models (mostly) know what they know. 
 \emph{arXiv preprint arXiv:2207.05221}."
2408.00137,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2408.00137,xu2024hallucination,"[{Xu et~al.(2024)Xu, Jain, and Kankanhalli}]{xu2024hallucination} Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.",Hallucination is inevitable: An innate limitation of large language models.,Hallucination is inevitable: An innate limitation of large language models.,,"[{Xu et~al.(2024)Xu, Jain, and Kankanhalli}]{xu2024hallucination} Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. 
 Hallucination is inevitable: An innate limitation of large language models. 
 \emph{arXiv preprint arXiv:2401.11817}."
2408.00137,yang2024large,"[{Yang et~al.(2024)Yang, Gribovskaya, Kassner, Geva, and Riedel}]{yang2024large} Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024.",Do large language models latently perform multi-hop reasoning?,Do large language models latently perform multi-hop reasoning?,,"[{Yang et~al.(2024)Yang, Gribovskaya, Kassner, Geva, and Riedel}]{yang2024large} Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024. 
 Do large language models latently perform multi-hop reasoning? 
 \emph{arXiv preprint arXiv:2402.16837}."
2408.00137,yuan2024whispers,"[{Yuan et~al.(2024)Yuan, Cao, Jin, Chen, Zeng, Liu, and Zhao}]{yuan2024whispers} Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, and Jun Zhao. 2024.",Whispers that shake foundations: Analyzing and mitigating false premise hallucinations in large language models.,Whispers that shake foundations: Analyzing and mitigating false premise hallucinations in large language models.,,"[{Yuan et~al.(2024)Yuan, Cao, Jin, Chen, Zeng, Liu, and Zhao}]{yuan2024whispers} Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, and Jun Zhao. 2024. 
 Whispers that shake foundations: Analyzing and mitigating false premise hallucinations in large language models. 
 \emph{arXiv preprint arXiv:2402.19103}."
2408.00137,zhang2023siren,"[{Zhang et~al.(2023)Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen et~al.}]{zhang2023siren} Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al. 2023.",Siren's song in the ai ocean: a survey on hallucination in large language models.,Siren's song in the ai ocean: a survey on hallucination in large language models.,,"[{Zhang et~al.(2023)Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen et~al.}]{zhang2023siren} Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, et~al. 2023. 
 Siren's song in the ai ocean: a survey on hallucination in large language models. 
 \emph{arXiv preprint arXiv:2309.01219}."
2408.00331,jiang2019fantastic,"[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and Bengio]{jiang2019fantastic} Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio.",Fantastic generalization measures and where to find them.,Fantastic generalization measures and where to find them.,,"[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and Bengio]{jiang2019fantastic} Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. 
 Fantastic generalization measures and where to find them. 
 \emph{arXiv preprint arXiv:1912.02178}, 2019."
2408.00331,joshi2022all,"[Joshi et~al.(2022)Joshi, Pan, and He]{joshi2022all} Nitish Joshi, Xiang Pan, and He~He.",Are all spurious features in natural language alike? an analysis through a causal lens.,Are all spurious features in natural language alike? an analysis through a causal lens.,,"[Joshi et~al.(2022)Joshi, Pan, and He]{joshi2022all} Nitish Joshi, Xiang Pan, and He~He. 
 Are all spurious features in natural language alike? an analysis through a causal lens. 
 \emph{arXiv preprint arXiv:2210.14011}, 2022."
2408.00331,lee2022surgical,"[Lee et~al.(2022)Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{lee2022surgical} Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn.",Surgical fine-tuning improves adaptation to distribution shifts.,Surgical fine-tuning improves adaptation to distribution shifts.,,"[Lee et~al.(2022)Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{lee2022surgical} Yoonho Lee, Annie~S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. 
 Surgical fine-tuning improves adaptation to distribution shifts. 
 \emph{arXiv preprint arXiv:2210.11466}, 2022."
2408.00331,merullo2022linearly,"[Merullo et~al.(2022)Merullo, Castricato, Eickhoff, and Pavlick]{merullo2022linearly} Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick.",Linearly mapping from image to text space.,Linearly mapping from image to text space.,,"[Merullo et~al.(2022)Merullo, Castricato, Eickhoff, and Pavlick]{merullo2022linearly} Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. 
 Linearly mapping from image to text space. 
 \emph{arXiv preprint arXiv:2209.15162}, 2022."
2408.00331,michels2023contrastive,"[Michels et~al.(2023)Michels, Adaloglou, Kaiser, and Kollmann]{michels2023contrastive} Felix Michels, Nikolas Adaloglou, Tim Kaiser, and Markus Kollmann.",Contrastive language-image pretrained (clip) models are powerful out-of-distribution detectors.,Contrastive language-image pretrained (clip) models are powerful out-of-distribution detectors.,,"[Michels et~al.(2023)Michels, Adaloglou, Kaiser, and Kollmann]{michels2023contrastive} Felix Michels, Nikolas Adaloglou, Tim Kaiser, and Markus Kollmann. 
 Contrastive language-image pretrained (clip) models are powerful out-of-distribution detectors. 
 \emph{arXiv preprint arXiv:2303.05828}, 2023."
2408.00331,nakano2021webgpt,"[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt} Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et~al.",Webgpt: Browser-assisted question-answering with human feedback.,Webgpt: Browser-assisted question-answering with human feedback.,,"[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt} Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et~al. 
 Webgpt: Browser-assisted question-answering with human feedback. 
 \emph{arXiv preprint arXiv:2112.09332}, 2021."
2408.00331,ng2022predicting,"[Ng et~al.(2022)Ng, Cho, Hulkund, and Ghassemi]{ng2022predicting} Nathan Ng, Kyunghyun Cho, Neha Hulkund, and Marzyeh Ghassemi.",Predicting out-of-domain generalization with local manifold smoothness.,Predicting out-of-domain generalization with local manifold smoothness.,,"[Ng et~al.(2022)Ng, Cho, Hulkund, and Ghassemi]{ng2022predicting} Nathan Ng, Kyunghyun Cho, Neha Hulkund, and Marzyeh Ghassemi. 
 Predicting out-of-domain generalization with local manifold smoothness. 
 \emph{arXiv preprint arXiv:2207.02093}, 2022."
2408.00331,song2022clip,"[Song et~al.(2022)Song, Dong, Zhang, Liu, and Wei]{song2022clip} Haoyu Song, Li~Dong, Wei-Nan Zhang, Ting Liu, and Furu Wei.",Clip models are few-shot learners: Empirical studies on vqa and visual entailment.,Clip models are few-shot learners: Empirical studies on vqa and visual entailment.,,"[Song et~al.(2022)Song, Dong, Zhang, Liu, and Wei]{song2022clip} Haoyu Song, Li~Dong, Wei-Nan Zhang, Ting Liu, and Furu Wei. 
 Clip models are few-shot learners: Empirical studies on vqa and visual entailment. 
 \emph{arXiv preprint arXiv:2203.07190}, 2022."
2408.00331,subramanyam2023crepe,"[Subramanyam et~al.(2023)Subramanyam, Jayram, Anirudh, and Thiagarajan]{subramanyam2023crepe} Rakshith Subramanyam, TS~Jayram, Rushil Anirudh, and Jayaraman~J Thiagarajan.",Crepe: Learnable prompting with clip improves visual relationship prediction.,Crepe: Learnable prompting with clip improves visual relationship prediction.,,"[Subramanyam et~al.(2023)Subramanyam, Jayram, Anirudh, and Thiagarajan]{subramanyam2023crepe} Rakshith Subramanyam, TS~Jayram, Rushil Anirudh, and Jayaraman~J Thiagarajan. 
 Crepe: Learnable prompting with clip improves visual relationship prediction. 
 \emph{arXiv preprint arXiv:2307.04838}, 2023."
2408.00331,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2408.00427,courtiol2020classification,"[Courtiol et~al.(2018)Courtiol, Tramel, Sanselme, and Wainrib]{courtiol2020classification} Pierre Courtiol, Eric~W Tramel, Marc Sanselme, and Gilles Wainrib.",Classification and disease localization in histopathology using only global labels: A weakly-supervised approach.,Classification and disease localization in histopathology using only global labels: A weakly-supervised approach.,,"[Courtiol et~al.(2018)Courtiol, Tramel, Sanselme, and Wainrib]{courtiol2020classification} Pierre Courtiol, Eric~W Tramel, Marc Sanselme, and Gilles Wainrib. 
 Classification and disease localization in histopathology using only global labels: A weakly-supervised approach. 
 \emph{arXiv preprint arXiv:1802.02212}, 2018."
2408.00427,kazeminia2023topologicallyregularized,"[Kazeminia et~al.(2023)Kazeminia, Sadafi, Makhro, Bogdanova, Marr, and Rieck]{kazeminia2023topologicallyregularized} Salome Kazeminia, Ario Sadafi, Asya Makhro, Anna Bogdanova, Carsten Marr, and Bastian Rieck.",Topologically-regularized multiple instance learning for red blood cell disease classification.,Topologically-regularized multiple instance learning for red blood cell disease classification.,,"[Kazeminia et~al.(2023)Kazeminia, Sadafi, Makhro, Bogdanova, Marr, and Rieck]{kazeminia2023topologicallyregularized} Salome Kazeminia, Ario Sadafi, Asya Makhro, Anna Bogdanova, Carsten Marr, and Bastian Rieck. 
 Topologically-regularized multiple instance learning for red blood cell disease classification. 
 \emph{arXiv preprint arXiv:2307.14025}, 2023."
2408.00427,kingma2017adam,[Kingma and Ba(2014)]{kingma2017adam} Diederik~P Kingma and Jimmy Ba.,Adam: A method for stochastic optimization.,Adam: A method for stochastic optimization.,,"[Kingma and Ba(2014)]{kingma2017adam} Diederik~P Kingma and Jimmy Ba. 
 Adam: A method for stochastic optimization. 
 \emph{arXiv preprint arXiv:1412.6980}, 2014."
2408.00427,VGAE_kipf2016,[Kipf and Welling(2016{\natexlab{a}})]{VGAE_kipf2016} Thomas~N Kipf and Max Welling.,Variational graph auto-encoders.,Variational graph auto-encoders.,,"[Kipf and Welling(2016{\natexlab{a}})]{VGAE_kipf2016} Thomas~N Kipf and Max Welling. 
 Variational graph auto-encoders. 
 \emph{arXiv preprint arXiv:1611.07308}, 2016{\natexlab{a}}."
2408.00427,kipf2017semisupervised,[Kipf and Welling(2016{\natexlab{b}})]{kipf2017semisupervised} Thomas~N Kipf and Max Welling.,Semi-supervised classification with graph convolutional networks.,Semi-supervised classification with graph convolutional networks.,,"[Kipf and Welling(2016{\natexlab{b}})]{kipf2017semisupervised} Thomas~N Kipf and Max Welling. 
 Semi-supervised classification with graph convolutional networks. 
 \emph{arXiv preprint arXiv:1609.02907}, 2016{\natexlab{b}}."
2408.00724,azerbayev2023llemma,"[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma} Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.",Llemma: An open language model for mathematics.,Llemma: An open language model for mathematics.,,"[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma} Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 
 Llemma: An open language model for mathematics. 
 \emph{arXiv preprint arXiv:2310.10631}, 2023."
2408.00724,biderman2023pythia,"[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia} Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.",Pythia: A suite for analyzing large language models across training and scaling.,Pythia: A suite for analyzing large language models across training and scaling.,,"[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia} Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al. 
 Pythia: A suite for analyzing large language models across training and scaling. 
 \emph{arXiv preprint arXiv:2304.01373}, 2023."
2408.00724,chen2024tree,"[Chen et~al.(2024{\natexlab{b}})Chen, White, Mooney, Payani, Su, and Sun]{chen2024tree} Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu~Su, and Huan Sun.",When is tree search useful for llm planning? it depends on the discriminator.,When is tree search useful for llm planning? it depends on the discriminator.,,"[Chen et~al.(2024{\natexlab{b}})Chen, White, Mooney, Payani, Su, and Sun]{chen2024tree} Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu~Su, and Huan Sun. 
 When is tree search useful for llm planning? it depends on the discriminator. 
 \emph{arXiv preprint arXiv:2402.10890}, 2024{\natexlab{b}}."
2408.00724,chowdhery2022palm,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.",{PaLM}: Scaling language modeling with pathways.,{PaLM}: Scaling language modeling with pathways.,,"[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm} Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 
 {PaLM}: Scaling language modeling with pathways. 
 \emph{arXiv preprint arXiv:2204.02311}, 2022."
2408.00724,cobbe2021gsm8k,"[Cobbe et~al.(2021{\natexlab{a}})Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021{\natexlab{a}})Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021{\natexlab{a}}."
2408.00724,cobbe2021training,"[Cobbe et~al.(2021{\natexlab{b}})Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[Cobbe et~al.(2021{\natexlab{b}})Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}, 2021{\natexlab{b}}."
2408.00724,gudibande2023false,"[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel, Levine, and Song]{gudibande2023false} Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song.",The false promise of imitating proprietary llms.,The false promise of imitating proprietary llms.,,"[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel, Levine, and Song]{gudibande2023false} Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 
 The false promise of imitating proprietary llms. 
 \emph{arXiv preprint arXiv:2305.15717}, 2023."
2408.00724,hendrycks2021measuring,"[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, et~al.]{hendrycks2021measuring} Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al.",Measuring coding challenge competence with apps.,Measuring coding challenge competence with apps.,,"[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, et~al.]{hendrycks2021measuring} Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al. 
 Measuring coding challenge competence with apps. 
 \emph{arXiv preprint arXiv:2105.09938}, 2021{\natexlab{a}}."
2408.00724,henighan2020scaling,"[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, et~al.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, et~al. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}, 2020."
2408.00724,hestness2017deep,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Ali, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Ali, Yang, and Zhou]{hestness2017deep} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}, 2017."
2408.00724,hoffmann2022training,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}, 2022."
2408.00724,jiang2023mistral,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.",Mistral 7b.,Mistral 7b.,,"[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}, 2023."
2408.00724,jones2021scaling,[Jones(2021)]{jones2021scaling} Andy~L Jones.,Scaling scaling laws with board games.,Scaling scaling laws with board games.,,"[Jones(2021)]{jones2021scaling} Andy~L Jones. 
 Scaling scaling laws with board games. 
 \emph{arXiv preprint arXiv:2104.03113}, 2021."
2408.00724,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2408.00724,kojima2022zeroshotreasoner,"[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022zeroshotreasoner} Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.",Large language models are zero-shot reasoners.,Large language models are zero-shot reasoners.,,"[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022zeroshotreasoner} Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 
 Large language models are zero-shot reasoners. 
 \emph{arXiv preprint arXiv:2205.11916}, 2022."
2408.00724,lewkowycz2022solving,"[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving} Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et~al.",Solving quantitative reasoning problems with language models.,Solving quantitative reasoning problems with language models.,,"[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving} Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et~al. 
 Solving quantitative reasoning problems with language models. 
 \emph{arXiv preprint arXiv:2206.14858}, 2022."
2408.00724,lightman2023let,"[Lightman et~al.(2023{\natexlab{a}})Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.",Let's verify step by step.,Let's verify step by step.,,"[Lightman et~al.(2023{\natexlab{a}})Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let} Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 
 Let's verify step by step. 
 \emph{arXiv preprint arXiv:2305.20050}, 2023{\natexlab{a}}."
2408.00724,nye2021show,"[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show} Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al.",Show your work: Scratchpads for intermediate computation with language models.,Show your work: Scratchpads for intermediate computation with language models.,,"[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show} Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al. 
 Show your work: Scratchpads for intermediate computation with language models. 
 \emph{arXiv preprint arXiv:2112.00114}, 2021."
2408.00724,polu2020generative,[Polu and Sutskever(2020)]{polu2020generative} Stanislas Polu and Ilya Sutskever.,Generative language modeling for automated theorem proving.,Generative language modeling for automated theorem proving.,,"[Polu and Sutskever(2020)]{polu2020generative} Stanislas Polu and Ilya Sutskever. 
 Generative language modeling for automated theorem proving. 
 \emph{arXiv preprint arXiv:2009.03393}, 2020."
2408.00724,rosenfeld2019constructive,"[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and Shavit]{rosenfeld2019constructive} Jonathan~S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.",A constructive prediction of the generalization error across scales.,A constructive prediction of the generalization error across scales.,,"[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and Shavit]{rosenfeld2019constructive} Jonathan~S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 
 A constructive prediction of the generalization error across scales. 
 \emph{arXiv preprint arXiv:1909.12673}, 2019."
2408.00724,sun2024easy,"[Sun et~al.(2024)Sun, Yu, Shen, Liu, Yang, Welleck, and Gan]{sun2024easy} Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan.",Easy-to-hard generalization: Scalable alignment beyond human supervision.,Easy-to-hard generalization: Scalable alignment beyond human supervision.,,"[Sun et~al.(2024)Sun, Yu, Shen, Liu, Yang, Welleck, and Gan]{sun2024easy} Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. 
 Easy-to-hard generalization: Scalable alignment beyond human supervision. 
 \emph{arXiv preprint arXiv:2403.09472}, 2024."
2408.00724,tian2024toward,"[Tian et~al.(2024)Tian, Peng, Song, Jin, Yu, Mi, and Yu]{tian2024toward} Ye~Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu.","Toward self-improvement of llms via imagination, searching, and criticizing.","Toward self-improvement of llms via imagination, searching, and criticizing.",,"[Tian et~al.(2024)Tian, Peng, Song, Jin, Yu, Mi, and Yu]{tian2024toward} Ye~Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 
 Toward self-improvement of llms via imagination, searching, and criticizing. 
 \emph{arXiv preprint arXiv:2404.12253}, 2024."
2408.00724,uesato2022solvingmath,"[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solvingmath} Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.",Solving math word problems with process- and outcome-based feedback.,Solving math word problems with process- and outcome-based feedback.,,"[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solvingmath} Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 
 Solving math word problems with process- and outcome-based feedback. 
 \emph{arXiv preprint arXiv:2211.14275}, 2022."
2408.00724,wang2022self,"[Wang et~al.(2022{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self} Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.",Self-instruct: Aligning language model with self generated instructions.,Self-instruct: Aligning language model with self generated instructions.,,"[Wang et~al.(2022{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self} Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 
 Self-instruct: Aligning language model with self generated instructions. 
 \emph{arXiv preprint arXiv:2212.10560}, 2022{\natexlab{b}}."
2408.00724,yao2023tree,"[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree} Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan.",Tree of thoughts: Deliberate problem solving with large language models.,Tree of thoughts: Deliberate problem solving with large language models.,,"[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree} Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan. 
 Tree of thoughts: Deliberate problem solving with large language models. 
 \emph{arXiv preprint arXiv:2305.10601}, 2023."
2408.00724,yu2022scaling,"[Yu et~al.(2022)Yu, Xu, Koh, Luong, Baid, Wang, Vasudevan, Ku, Yang, Ayan, et~al.]{yu2022scaling} Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.",Scaling autoregressive models for content-rich text-to-image generation.,Scaling autoregressive models for content-rich text-to-image generation.,,"[Yu et~al.(2022)Yu, Xu, Koh, Luong, Baid, Wang, Vasudevan, Ku, Yang, Ayan, et~al.]{yu2022scaling} Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al. 
 Scaling autoregressive models for content-rich text-to-image generation. 
 \emph{arXiv preprint arXiv:2206.10789}, 2\penalty0 (3):\penalty0 5, 2022."
2408.00724,zhou2022large,"[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba]{zhou2022large} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.",Large language models are human-level prompt engineers.,Large language models are human-level prompt engineers.,,"[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and Ba]{zhou2022large} Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 
 Large language models are human-level prompt engineers. 
 \emph{arXiv preprint arXiv:2211.01910}, 2022."
2408.00802,anil2023palm,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos,   Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,   Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,   et~al. 2023.",Palm 2 technical report.,Palm 2 technical report.,,"[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos,   Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm} Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,   Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,   et~al. 2023. 
 Palm 2 technical report. 
 \emph{arXiv preprint arXiv:2305.10403}."
2408.00802,cobbe2021training,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,   Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz   Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,   et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,   Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz   Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,   et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2408.00802,cui2022m6,"[{Cui et~al.(2022)Cui, Ma, Zhou, Zhou, and Yang}]{cui2022m6} Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.",M6-rec: Generative pretrained language models are open-ended   recommender systems.,M6-rec: Generative pretrained language models are open-ended   recommender systems.,,"[{Cui et~al.(2022)Cui, Ma, Zhou, Zhou, and Yang}]{cui2022m6} Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. 
 M6-rec: Generative pretrained language models are open-ended   recommender systems. 
 \emph{arXiv preprint arXiv:2205.08084}."
2408.00802,friedman2023leveraging,"[{Friedman et~al.(2023)Friedman, Ahuja, Allen, Tan, Sidahmed, Long,   Xie, Schubiner, Patel, Lara et~al.}]{friedman2023leveraging} Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo   Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et~al. 2023.",Leveraging large language models in conversational recommender   systems.,Leveraging large language models in conversational recommender   systems.,,"[{Friedman et~al.(2023)Friedman, Ahuja, Allen, Tan, Sidahmed, Long,   Xie, Schubiner, Patel, Lara et~al.}]{friedman2023leveraging} Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo   Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et~al. 2023. 
 Leveraging large language models in conversational recommender   systems. 
 \emph{arXiv preprint arXiv:2305.07961}."
2408.00802,gao2020making,"[{Gao et~al.(2020)Gao, Fisch, and Chen}]{gao2020making} Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.",Making pre-trained language models better few-shot learners.,Making pre-trained language models better few-shot learners.,,"[{Gao et~al.(2020)Gao, Fisch, and Chen}]{gao2020making} Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. 
 Making pre-trained language models better few-shot learners. 
 \emph{arXiv preprint arXiv:2012.15723}."
2408.00802,gao2023chat,"[{Gao et~al.(2023)Gao, Sheng, Xiang, Xiong, Wang, and   Zhang}]{gao2023chat} Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang.   2023.",Chat-rec: Towards interactive and explainable llms-augmented   recommender system.,Chat-rec: Towards interactive and explainable llms-augmented   recommender system.,,"[{Gao et~al.(2023)Gao, Sheng, Xiang, Xiong, Wang, and   Zhang}]{gao2023chat} Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang.   2023. 
 Chat-rec: Towards interactive and explainable llms-augmented   recommender system. 
 \emph{arXiv preprint arXiv:2303.14524}."
2408.00802,golovneva2022roscoe,"[{Golovneva et~al.(2022)Golovneva, Chen, Poff, Corredor, Zettlemoyer,   Fazel-Zarandi, and Celikyilmaz}]{golovneva2022roscoe} Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer,   Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022.",Roscoe: A suite of metrics for scoring step-by-step reasoning.,Roscoe: A suite of metrics for scoring step-by-step reasoning.,,"[{Golovneva et~al.(2022)Golovneva, Chen, Poff, Corredor, Zettlemoyer,   Fazel-Zarandi, and Celikyilmaz}]{golovneva2022roscoe} Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer,   Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. 
 Roscoe: A suite of metrics for scoring step-by-step reasoning. 
 \emph{arXiv preprint arXiv:2212.07919}."
2408.00802,kang2023llms,"[{Kang et~al.(2023)Kang, Ni, Mehta, Sathiamoorthy, Hong, Chi, and   Cheng}]{kang2023llms} Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan   Hong, Ed~Chi, and Derek~Zhiyuan Cheng. 2023.",Do llms understand user preferences? evaluating llms on user rating   prediction.,Do llms understand user preferences? evaluating llms on user rating   prediction.,,"[{Kang et~al.(2023)Kang, Ni, Mehta, Sathiamoorthy, Hong, Chi, and   Cheng}]{kang2023llms} Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan   Hong, Ed~Chi, and Derek~Zhiyuan Cheng. 2023. 
 Do llms understand user preferences? evaluating llms on user rating   prediction. 
 \emph{arXiv preprint arXiv:2305.06474}."
2408.00802,liu2023llmrec,"[{Liu et~al.(2023)Liu, Liu, Zhou, Ye, Chong, Zhou, Xie, Cao, Wang, You   et~al.}]{liu2023llmrec} Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi   Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et~al. 2023.",Llmrec: Benchmarking large language models on recommendation task.,Llmrec: Benchmarking large language models on recommendation task.,,"[{Liu et~al.(2023)Liu, Liu, Zhou, Ye, Chong, Zhou, Xie, Cao, Wang, You   et~al.}]{liu2023llmrec} Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi   Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et~al. 2023. 
 Llmrec: Benchmarking large language models on recommendation task. 
 \emph{arXiv preprint arXiv:2308.12241}."
2408.00802,saparov2022language,[{Saparov and He(2022)}]{saparov2022language} Abulhair Saparov and He~He. 2022.,Language models are greedy reasoners: A systematic formal analysis of   chain-of-thought.,Language models are greedy reasoners: A systematic formal analysis of   chain-of-thought.,,"[{Saparov and He(2022)}]{saparov2022language} Abulhair Saparov and He~He. 2022. 
 Language models are greedy reasoners: A systematic formal analysis of   chain-of-thought. 
 \emph{arXiv preprint arXiv:2210.01240}."
2408.00802,wang2022iteratively,"[{Wang et~al.(2022{\natexlab{a}})Wang, Deng, and   Sun}]{wang2022iteratively} Boshi Wang, Xiang Deng, and Huan Sun. 2022{\natexlab{a}}.",Iteratively prompt pre-trained language models for chain of thought.,Iteratively prompt pre-trained language models for chain of thought.,,"[{Wang et~al.(2022{\natexlab{a}})Wang, Deng, and   Sun}]{wang2022iteratively} Boshi Wang, Xiang Deng, and Huan Sun. 2022{\natexlab{a}}. 
 Iteratively prompt pre-trained language models for chain of thought. 
 \emph{arXiv preprint arXiv:2203.08383}."
2408.00802,zhang2023recommendation,"[{Zhang et~al.(2023)Zhang, Xie, Hou, Zhao, Lin, and   Wen}]{zhang2023recommendation} Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne~Xin Zhao, Leyu Lin, and Ji-Rong   Wen. 2023.",Recommendation as instruction following: A large language model   empowered recommendation approach.,Recommendation as instruction following: A large language model   empowered recommendation approach.,,"[{Zhang et~al.(2023)Zhang, Xie, Hou, Zhao, Lin, and   Wen}]{zhang2023recommendation} Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne~Xin Zhao, Leyu Lin, and Ji-Rong   Wen. 2023. 
 Recommendation as instruction following: A large language model   empowered recommendation approach. 
 \emph{arXiv preprint arXiv:2305.07001}."
2408.00802,zhang2019bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.   2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and   Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.   2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
2408.00994,allal2023santacoder,"[{Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey et~al.}]{allal2023santacoder} Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al. 2023.",Santacoder: don't reach for the stars!,Santacoder: don't reach for the stars!,,"[{Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey et~al.}]{allal2023santacoder} Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al. 2023. 
 Santacoder: don't reach for the stars! 
 \emph{arXiv preprint arXiv:2301.03988}."
2408.00994,brown2020language,"[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language} Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.",Language models are few-shot learners.,Language models are few-shot learners.,,"[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language} Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020. 
 Language models are few-shot learners. 
 \emph{arXiv preprint arXiv:2005.14165}."
2408.00994,cassano2022multipl,"[{Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman et~al.}]{cassano2022multipl} Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, et~al. 2022.",Multipl-e: A scalable and extensible approach to benchmarking neural code generation.,Multipl-e: A scalable and extensible approach to benchmarking neural code generation.,,"[{Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman et~al.}]{cassano2022multipl} Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, et~al. 2022. 
 Multipl-e: A scalable and extensible approach to benchmarking neural code generation. 
 \emph{arXiv preprint arXiv:2208.08227}."
2408.00994,huang2023enhancing,"[{Huang et~al.(2023)Huang, Lu, Chen, Wan, and Duan}]{huang2023enhancing} Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2023.",Enhancing large language models in coding through multi-perspective self-consistency.,Enhancing large language models in coding through multi-perspective self-consistency.,,"[{Huang et~al.(2023)Huang, Lu, Chen, Wan, and Duan}]{huang2023enhancing} Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2023. 
 Enhancing large language models in coding through multi-perspective self-consistency. 
 \emph{arXiv preprint arXiv:2309.17272}."
2408.00994,li2023think,"[{Li et~al.(2023)Li, Xue, Xie, and Li}]{li2023think} Xin-Ye Li, Jiang-Tian Xue, Zheng Xie, and Ming Li. 2023.",Think outside the code: Brainstorming boosts large language models in code generation.,Think outside the code: Brainstorming boosts large language models in code generation.,,"[{Li et~al.(2023)Li, Xue, Xie, and Li}]{li2023think} Xin-Ye Li, Jiang-Tian Xue, Zheng Xie, and Ming Li. 2023. 
 Think outside the code: Brainstorming boosts large language models in code generation. 
 \emph{arXiv preprint arXiv:2305.10679}."
2408.00994,luo2023wizardcoder,"[{Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.",Wizardcoder: Empowering code large language models with evol-instruct.,Wizardcoder: Empowering code large language models with evol-instruct.,,"[{Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{luo2023wizardcoder} Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. 
 Wizardcoder: Empowering code large language models with evol-instruct. 
 \emph{arXiv preprint arXiv:2306.08568}."
2408.00994,madaan2023learning,"[{Madaan et~al.(2023)Madaan, Shypula, Alon, Hashemi, Ranganathan, Yang, Neubig, and Yazdanbakhsh}]{madaan2023learning} Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. 2023.",Learning performance-improving code edits.,Learning performance-improving code edits.,,"[{Madaan et~al.(2023)Madaan, Shypula, Alon, Hashemi, Ranganathan, Yang, Neubig, and Yazdanbakhsh}]{madaan2023learning} Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. 2023. 
 Learning performance-improving code edits. 
 \emph{arXiv preprint arXiv:2302.07867}."
2408.00994,wang2023voyager,"[{Wang et~al.(2023)Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar}]{wang2023voyager} Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023.",Voyager: An open-ended embodied agent with large language models.,Voyager: An open-ended embodied agent with large language models.,,"[{Wang et~al.(2023)Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar}]{wang2023voyager} Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. 
 Voyager: An open-ended embodied agent with large language models. 
 \emph{arXiv preprint arXiv:2305.16291}."
2408.00994,xia2023universal,"[{Xia et~al.(2023)Xia, Paltenghi, Tian, Pradel, and Zhang}]{xia2023universal} Chunqiu~Steven Xia, Matteo Paltenghi, Jia~Le Tian, Michael Pradel, and Lingming Zhang. 2023.",Universal fuzzing via large language models.,Universal fuzzing via large language models.,,"[{Xia et~al.(2023)Xia, Paltenghi, Tian, Pradel, and Zhang}]{xia2023universal} Chunqiu~Steven Xia, Matteo Paltenghi, Jia~Le Tian, Michael Pradel, and Lingming Zhang. 2023. 
 Universal fuzzing via large language models. 
 \emph{arXiv preprint arXiv:2308.04748}."
2408.00994,yuan2023evaluating,"[{Yuan et~al.(2023)Yuan, Liu, Zi, Liu, Peng, and Lou}]{yuan2023evaluating} Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023.",Evaluating instruction-tuned large language models on code comprehension and generation.,Evaluating instruction-tuned large language models on code comprehension and generation.,,"[{Yuan et~al.(2023)Yuan, Liu, Zi, Liu, Peng, and Lou}]{yuan2023evaluating} Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. 
 Evaluating instruction-tuned large language models on code comprehension and generation. 
 \emph{arXiv preprint arXiv:2308.01240}."
2408.01571,atad2022chexplaining,"[Atad et~al.(2022)Atad, Dmytrenko, Li, Zhang, Keicher, Kirschke, Wiestler, Khakzar, and Navab]{atad2022chexplaining} Matan Atad, Vitalii Dmytrenko, Yitong Li, Xinyue Zhang, Matthias Keicher, Jan Kirschke, Bene Wiestler, Ashkan Khakzar, and Nassir Navab.",Chexplaining in style: Counterfactual explanations for chest x-rays using stylegan.,Chexplaining in style: Counterfactual explanations for chest x-rays using stylegan.,,"[Atad et~al.(2022)Atad, Dmytrenko, Li, Zhang, Keicher, Kirschke, Wiestler, Khakzar, and Navab]{atad2022chexplaining} Matan Atad, Vitalii Dmytrenko, Yitong Li, Xinyue Zhang, Matthias Keicher, Jan Kirschke, Bene Wiestler, Ashkan Khakzar, and Nassir Navab. 
 Chexplaining in style: Counterfactual explanations for chest x-rays using stylegan. 
 \emph{arXiv:2207.07553}, 2022."
2408.01571,bedel2023dreamr,[Bedel and {\c{C}}ukur(2023)]{bedel2023dreamr} Hasan~Atakan Bedel and Tolga {\c{C}}ukur.,Dreamr: Diffusion-driven counterfactual explanation for functional mri.,Dreamr: Diffusion-driven counterfactual explanation for functional mri.,,"[Bedel and {\c{C}}ukur(2023)]{bedel2023dreamr} Hasan~Atakan Bedel and Tolga {\c{C}}ukur. 
 Dreamr: Diffusion-driven counterfactual explanation for functional mri. 
 \emph{arXiv preprint arXiv:2307.09547}, 2023."
2408.01571,fontanella2023diffusion,"[Fontanella et~al.(2023)Fontanella, Mair, Wardlaw, Trucco, and Storkey]{fontanella2023diffusion} Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, and Amos Storkey.",Diffusion models for counterfactual generation and anomaly detection in brain images.,Diffusion models for counterfactual generation and anomaly detection in brain images.,,"[Fontanella et~al.(2023)Fontanella, Mair, Wardlaw, Trucco, and Storkey]{fontanella2023diffusion} Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, and Amos Storkey. 
 Diffusion models for counterfactual generation and anomaly detection in brain images. 
 \emph{arXiv preprint arXiv:2308.02062}, 2023."
2408.01571,pegios2024diffusion,"[Pegios et~al.(2024)Pegios, Lin, Weng, Svendsen, Bashir, Bigdeli, Christensen, Tolsgaard, and Feragen]{pegios2024diffusion} Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo~S{\o}ndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders~Nymark Christensen, Martin Tolsgaard, and Aasa Feragen.",Diffusion-based iterative counterfactual explanations for fetal ultrasound image quality assessment.,Diffusion-based iterative counterfactual explanations for fetal ultrasound image quality assessment.,,"[Pegios et~al.(2024)Pegios, Lin, Weng, Svendsen, Bashir, Bigdeli, Christensen, Tolsgaard, and Feragen]{pegios2024diffusion} Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo~S{\o}ndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders~Nymark Christensen, Martin Tolsgaard, and Aasa Feragen. 
 Diffusion-based iterative counterfactual explanations for fetal ultrasound image quality assessment. 
 \emph{arXiv preprint arXiv:2403.08700}, 2024."
2408.01571,sankar2021glowin,"[Sankar et~al.(2021)Sankar, Keicher, Eisawy, Parida, Pfister, Kim, and Navab]{sankar2021glowin} Aadhithya Sankar, Matthias Keicher, Rami Eisawy, Abhijeet Parida, Franz Pfister, Seong~Tae Kim, and Nassir Navab.",Glowin: A flow-based invertible generative framework for learning disentangled feature representations in medical images.,Glowin: A flow-based invertible generative framework for learning disentangled feature representations in medical images.,,"[Sankar et~al.(2021)Sankar, Keicher, Eisawy, Parida, Pfister, Kim, and Navab]{sankar2021glowin} Aadhithya Sankar, Matthias Keicher, Rami Eisawy, Abhijeet Parida, Franz Pfister, Seong~Tae Kim, and Nassir Navab. 
 Glowin: A flow-based invertible generative framework for learning disentangled feature representations in medical images. 
 \emph{arXiv:2103.10868}, 2021."
2408.01571,schutte2021using,"[Schutte et~al.(2021)Schutte, Moindrot, H{\'e}rent, Schiratti, and J{\'e}gou]{schutte2021using} Kathryn Schutte, Olivier Moindrot, Paul H{\'e}rent, Jean-Baptiste Schiratti, and Simon J{\'e}gou.",Using stylegan for visual interpretability of deep learning models on medical images.,Using stylegan for visual interpretability of deep learning models on medical images.,,"[Schutte et~al.(2021)Schutte, Moindrot, H{\'e}rent, Schiratti, and J{\'e}gou]{schutte2021using} Kathryn Schutte, Olivier Moindrot, Paul H{\'e}rent, Jean-Baptiste Schiratti, and Simon J{\'e}gou. 
 Using stylegan for visual interpretability of deep learning models on medical images. 
 \emph{arXiv preprint arXiv:2101.07563}, 2021."
2408.01571,verma2020counterfactual,"[Verma et~al.(2020)Verma, Boonsanong, Hoang, Hines, Dickerson, and Shah]{verma2020counterfactual} Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan~E Hines, John~P Dickerson, and Chirag Shah.",Counterfactual explanations and algorithmic recourses for machine learning: A review.,Counterfactual explanations and algorithmic recourses for machine learning: A review.,,"[Verma et~al.(2020)Verma, Boonsanong, Hoang, Hines, Dickerson, and Shah]{verma2020counterfactual} Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan~E Hines, John~P Dickerson, and Chirag Shah. 
 Counterfactual explanations and algorithmic recourses for machine learning: A review. 
 \emph{arXiv preprint arXiv:2010.10596}, 2020."
2408.01571,yi2018unsupervised,"[Yi et~al.(2018)Yi, Walia, and Babyn]{yi2018unsupervised} Xin Yi, Ekta Walia, and Paul Babyn.",Unsupervised and semi-supervised learning with categorical generative adversarial networks assisted by wasserstein distance for dermoscopy image classification.,Unsupervised and semi-supervised learning with categorical generative adversarial networks assisted by wasserstein distance for dermoscopy image classification.,,"[Yi et~al.(2018)Yi, Walia, and Babyn]{yi2018unsupervised} Xin Yi, Ekta Walia, and Paul Babyn. 
 Unsupervised and semi-supervised learning with categorical generative adversarial networks assisted by wasserstein distance for dermoscopy image classification. 
 \emph{arXiv:1804.03700}, 2018."
2408.01963,bandel2024unitxt,"[{Bandel et~al.(2024)Bandel, Perlitz, Venezian, Friedman-Melamed, Arviv, Orbach, Don-Yehyia, Sheinwald, Gera, Choshen et~al.}]{bandel2024unitxt} Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, et~al. 2024.","Unitxt: Flexible, shareable and reusable data preparation and evaluation for generative ai.","Unitxt: Flexible, shareable and reusable data preparation and evaluation for generative ai.",,"[{Bandel et~al.(2024)Bandel, Perlitz, Venezian, Friedman-Melamed, Arviv, Orbach, Don-Yehyia, Sheinwald, Gera, Choshen et~al.}]{bandel2024unitxt} Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, et~al. 2024. 
 Unitxt: Flexible, shareable and reusable data preparation and evaluation for generative ai. 
 \emph{arXiv preprint arXiv:2401.14019}."
2408.01963,jiang2024mixtral,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.",Mixtral of experts.,Mixtral of experts.,,"[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}."
2408.01963,mizrahi2023stateofwhat,"[{Mizrahi et~al.(2023)Mizrahi, Kaplan, Malkin, Dror, Shahaf, and Stanovsky}]{mizrahi2023stateofwhat} Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2023.",State of what art? a call for multi-prompt llm evaluation.,State of what art? a call for multi-prompt llm evaluation.,,"[{Mizrahi et~al.(2023)Mizrahi, Kaplan, Malkin, Dror, Shahaf, and Stanovsky}]{mizrahi2023stateofwhat} Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2023. 
 State of what art? a call for multi-prompt llm evaluation. 
 \emph{arXiv preprint arXiv:2401.00595}."
2408.01963,shayegani2023survey,"[{Shayegani et~al.(2023)Shayegani, Mamun, Fu, Zaree, Dong, and Abu-Ghazaleh}]{shayegani2023survey} Erfan Shayegani, Md~Abdullah~Al Mamun, Yu~Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023.",Survey of vulnerabilities in large language models revealed by adversarial attacks.,Survey of vulnerabilities in large language models revealed by adversarial attacks.,,"[{Shayegani et~al.(2023)Shayegani, Mamun, Fu, Zaree, Dong, and Abu-Ghazaleh}]{shayegani2023survey} Erfan Shayegani, Md~Abdullah~Al Mamun, Yu~Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. 
 Survey of vulnerabilities in large language models revealed by adversarial attacks. 
 \emph{arXiv preprint arXiv:2310.10844}."
2408.01963,flan_ul2,"[{Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung, Shakeri, Bahri, Schuster et~al.}]{flan_ul2} Yi~Tay, Mostafa Dehghani, Vinh~Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung~Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et~al. 2022.",Ul2: Unifying language learning paradigms.,Ul2: Unifying language learning paradigms.,,"[{Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung, Shakeri, Bahri, Schuster et~al.}]{flan_ul2} Yi~Tay, Mostafa Dehghani, Vinh~Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung~Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et~al. 2022. 
 Ul2: Unifying language learning paradigms. 
 \emph{arXiv preprint arXiv:2205.05131}."
2408.01963,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2408.01963,voronov2024mind,"[{Voronov et~al.(2024)Voronov, Wolf, and Ryabinin}]{voronov2024mind} Anton Voronov, Lena Wolf, and Max Ryabinin. 2024.",Mind your format: Towards consistent evaluation of in-context learning improvements.,Mind your format: Towards consistent evaluation of in-context learning improvements.,,"[{Voronov et~al.(2024)Voronov, Wolf, and Ryabinin}]{voronov2024mind} Anton Voronov, Lena Wolf, and Max Ryabinin. 2024. 
 Mind your format: Towards consistent evaluation of in-context learning improvements. 
 \emph{arXiv preprint arXiv:2401.06766}."
2408.01963,zhu2023promptbench,"[{Zhu et~al.(2023)Zhu, Wang, Zhou, Wang, Chen, Wang, Yang, Ye, Zhang, Gong et~al.}]{zhu2023promptbench} Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil~Zhenqiang Gong, et~al. 2023.",Promptbench: {T}owards {E}valuating the {R}obustness of {L}arge {L}anguage {M}odels on {A}dversarial {P}rompts.,Promptbench: {T}owards {E}valuating the {R}obustness of {L}arge {L}anguage {M}odels on {A}dversarial {P}rompts.,,"[{Zhu et~al.(2023)Zhu, Wang, Zhou, Wang, Chen, Wang, Yang, Ye, Zhang, Gong et~al.}]{zhu2023promptbench} Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil~Zhenqiang Gong, et~al. 2023. 
 Promptbench: {T}owards {E}valuating the {R}obustness of {L}arge {L}anguage {M}odels on {A}dversarial {P}rompts. 
 \emph{arXiv preprint arXiv:2306.04528}."
2408.02103,deletang2023language,"[{Del{\'e}tang et~al.(2023)Del{\'e}tang, Ruoss, Duquenne, Catt, Genewein, Mattern, Grau-Moya, Wenliang, Aitchison, Orseau et~al.}]{deletang2023language} Gr{\'e}goire Del{\'e}tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li~Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et~al. 2023.",Language modeling is compression.,Language modeling is compression.,,"[{Del{\'e}tang et~al.(2023)Del{\'e}tang, Ruoss, Duquenne, Catt, Genewein, Mattern, Grau-Moya, Wenliang, Aitchison, Orseau et~al.}]{deletang2023language} Gr{\'e}goire Del{\'e}tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li~Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et~al. 2023. 
 Language modeling is compression. 
 \emph{arXiv preprint arXiv:2309.10668}."
2408.02103,lan2019albert,"[{Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}]{lan2019albert} Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.",Albert: A lite bert for self-supervised learning of language representations.,Albert: A lite bert for self-supervised learning of language representations.,,"[{Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}]{lan2019albert} Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. 
 Albert: A lite bert for self-supervised learning of language representations. 
 \emph{arXiv preprint arXiv:1909.11942}."
2408.02103,sorensen2022information,"[{Sorensen et~al.(2022)Sorensen, Robinson, Rytting, Shaw, Rogers, Delorey, Khalil, Fulda, and Wingate}]{sorensen2022information} Taylor Sorensen, Joshua Robinson, Christopher~Michael Rytting, Alexander~Glenn Shaw, Kyle~Jeffrey Rogers, Alexia~Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022.",An information-theoretic approach to prompt engineering without ground truth labels.,An information-theoretic approach to prompt engineering without ground truth labels.,,"[{Sorensen et~al.(2022)Sorensen, Robinson, Rytting, Shaw, Rogers, Delorey, Khalil, Fulda, and Wingate}]{sorensen2022information} Taylor Sorensen, Joshua Robinson, Christopher~Michael Rytting, Alexander~Glenn Shaw, Kyle~Jeffrey Rogers, Alexia~Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. 
 An information-theoretic approach to prompt engineering without ground truth labels. 
 \emph{arXiv preprint arXiv:2203.11364}."
2408.02103,wang2018glue,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{wang2018glue} Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman. 2018. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}."
2408.02103,williams2017broad,"[{Williams et~al.(2017)Williams, Nangia, and Bowman}]{williams2017broad} Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2017.",A broad-coverage challenge corpus for sentence understanding through inference.,A broad-coverage challenge corpus for sentence understanding through inference.,,"[{Williams et~al.(2017)Williams, Nangia, and Bowman}]{williams2017broad} Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2017. 
 A broad-coverage challenge corpus for sentence understanding through inference. 
 \emph{arXiv preprint arXiv:1704.05426}."
2408.02103,yu2022generate,"[{Yu et~al.(2022)Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang}]{yu2022generate} Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022.",Generate rather than retrieve: Large language models are strong context generators.,Generate rather than retrieve: Large language models are strong context generators.,,"[{Yu et~al.(2022)Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang}]{yu2022generate} Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. 
 Generate rather than retrieve: Large language models are strong context generators. 
 \emph{arXiv preprint arXiv:2209.10063}."
2408.02103,zellers2019hellaswag,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}."
2408.02632,holtzman2019curious,"[{Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi}]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 2019.",The curious case of neural text degeneration.,The curious case of neural text degeneration.,,"[{Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi}]{holtzman2019curious} Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi. 2019. 
 The curious case of neural text degeneration. 
 \emph{arXiv preprint arXiv:1904.09751}."
2408.02632,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.02632,lu2023mathvista,"[{Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023.",Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.,,"[{Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao}]{lu2023mathvista} Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. 
 Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. 
 \emph{arXiv preprint arXiv:2310.02255}."
2408.02632,qiao2024we,"[{Qiao et~al.(2024{\natexlab{a}})Qiao, Tan, Dong, Wu, Sun, Song, GongQue, Lei, Wei, Zhang et~al.}]{qiao2024we} Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et~al. 2024{\natexlab{a}}.",We-math: Does your large multimodal model achieve human-like mathematical reasoning?,We-math: Does your large multimodal model achieve human-like mathematical reasoning?,,"[{Qiao et~al.(2024{\natexlab{a}})Qiao, Tan, Dong, Wu, Sun, Song, GongQue, Lei, Wei, Zhang et~al.}]{qiao2024we} Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et~al. 2024{\natexlab{a}}. 
 We-math: Does your large multimodal model achieve human-like mathematical reasoning? 
 \emph{arXiv preprint arXiv:2407.01284}."
2408.02632,song2024cs,"[{Song et~al.(2024)Song, Diao, Dong, Wang, Fu, Qiao, Wang, Fu, Wu, Liang et~al.}]{song2024cs} Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, et~al. 2024.",Cs-bench: A comprehensive benchmark for large language models towards computer science mastery.,Cs-bench: A comprehensive benchmark for large language models towards computer science mastery.,,"[{Song et~al.(2024)Song, Diao, Dong, Wang, Fu, Qiao, Wang, Fu, Wu, Liang et~al.}]{song2024cs} Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, et~al. 2024. 
 Cs-bench: A comprehensive benchmark for large language models towards computer science mastery. 
 \emph{arXiv preprint arXiv:2406.08587}."
2408.02632,llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2408.03092,colombo2024saullm,"[Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado, and Desa]{colombo2024saullm} Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre~FT Martins, Fabrizio Esposito, Vera~L{\'u}cia Raposo, Sofia Morgado, and Michael Desa.",Saullm-7b: A pioneering large language model for law.,Saullm-7b: A pioneering large language model for law.,,"[Colombo et~al.(2024)Colombo, Pires, Boudiaf, Culver, Melo, Corro, Martins, Esposito, Raposo, Morgado, and Desa]{colombo2024saullm} Pierre Colombo, Telmo~Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre~FT Martins, Fabrizio Esposito, Vera~L{\'u}cia Raposo, Sofia Morgado, and Michael Desa. 
 Saullm-7b: A pioneering large language model for law. 
 \emph{arXiv preprint arXiv:2403.03883}, 2024."
2408.03297,asai2023self,"[{Asai et~al.(2023{\natexlab{b}})Asai, Wu, Wang, Sil, and Hajishirzi}]{asai2023self} Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2023{\natexlab{b}}.","Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.","Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.",,"[{Asai et~al.(2023{\natexlab{b}})Asai, Wu, Wang, Sil, and Hajishirzi}]{asai2023self} Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2023{\natexlab{b}}. 
 Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. 
 \emph{arXiv preprint arXiv:2310.11511}."
2408.03297,decao2021editingfactualknowledgelanguage,"[{Cao, Aziz, and Titov(2021)}]{decao2021editingfactualknowledgelanguage} Cao, N.~D.; Aziz, W.; and Titov, I. 2021.",Editing Factual Knowledge in Language Models.,Editing Factual Knowledge in Language Models.,,"[{Cao, Aziz, and Titov(2021)}]{decao2021editingfactualknowledgelanguage} Cao, N.~D.; Aziz, W.; and Titov, I. 2021. 
 Editing Factual Knowledge in Language Models. 
 arXiv:2104.08164."
2408.03297,chen2022richknowledgesourcesbring,"[{Chen, Zhang, and Choi(2022)}]{chen2022richknowledgesourcesbring} Chen, H.-T.; Zhang, M. J.~Q.; and Choi, E. 2022.",Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence.,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence.,,"[{Chen, Zhang, and Choi(2022)}]{chen2022richknowledgesourcesbring} Chen, H.-T.; Zhang, M. J.~Q.; and Choi, E. 2022. 
 Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence. 
 arXiv:2210.13701."
2408.03297,chen2023benchmarkinglargelanguagemodels,"[{Chen et~al.(2023)Chen, Lin, Han, and Sun}]{chen2023benchmarkinglargelanguagemodels} Chen, J.; Lin, H.; Han, X.; and Sun, L. 2023.",Benchmarking Large Language Models in Retrieval-Augmented Generation.,Benchmarking Large Language Models in Retrieval-Augmented Generation.,,"[{Chen et~al.(2023)Chen, Lin, Han, and Sun}]{chen2023benchmarkinglargelanguagemodels} Chen, J.; Lin, H.; Han, X.; and Sun, L. 2023. 
 Benchmarking Large Language Models in Retrieval-Augmented Generation. 
 arXiv:2309.01431."
2408.03297,chowdhery2022palm,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{chowdhery2022palm} Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.~W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A.~M.; Pillai, T.~S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022.",PaLM: Scaling Language Modeling with Pathways.,PaLM: Scaling Language Modeling with Pathways.,,"[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{chowdhery2022palm} Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H.~W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A.~M.; Pillai, T.~S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. 
 PaLM: Scaling Language Modeling with Pathways. 
 arXiv:2204.02311."
2408.03297,gao2022scalinglawsrewardmodel,"[{Gao, Schulman, and Hilton(2022)}]{gao2022scalinglawsrewardmodel} Gao, L.; Schulman, J.; and Hilton, J. 2022.",Scaling Laws for Reward Model Overoptimization.,Scaling Laws for Reward Model Overoptimization.,,"[{Gao, Schulman, and Hilton(2022)}]{gao2022scalinglawsrewardmodel} Gao, L.; Schulman, J.; and Hilton, J. 2022. 
 Scaling Laws for Reward Model Overoptimization. 
 arXiv:2210.10760."
2408.03297,gao2024RAGsurvey,"[{Gao et~al.(2024)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, Wang, and Wang}]{gao2024RAGsurvey} Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, M.; and Wang, H. 2024.",Retrieval-Augmented Generation for Large Language Models: A Survey.,Retrieval-Augmented Generation for Large Language Models: A Survey.,,"[{Gao et~al.(2024)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, Wang, and Wang}]{gao2024RAGsurvey} Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, M.; and Wang, H. 2024. 
 Retrieval-Augmented Generation for Large Language Models: A Survey. 
 arXiv:2312.10997."
2408.03297,he2022rethinking,"[{He, Zhang, and Roth(2022)}]{he2022rethinking} He, H.; Zhang, H.; and Roth, D. 2022.",Rethinking with Retrieval: Faithful Large Language Model Inference.,Rethinking with Retrieval: Faithful Large Language Model Inference.,,"[{He, Zhang, and Roth(2022)}]{he2022rethinking} He, H.; Zhang, H.; and Roth, D. 2022. 
 Rethinking with Retrieval: Faithful Large Language Model Inference. 
 arXiv:2301.00303."
2408.03297,izacard2022atlas,"[{Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave}]{izacard2022atlas} Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2022.",Atlas: Few-shot Learning with Retrieval Augmented Language Models.,Atlas: Few-shot Learning with Retrieval Augmented Language Models.,,"[{Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave}]{izacard2022atlas} Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave, E. 2022. 
 Atlas: Few-shot Learning with Retrieval Augmented Language Models. 
 arXiv:2208.03299."
2408.03297,jang2022continualknowledgelearninglanguage,"[{Jang et~al.(2022)Jang, Ye, Yang, Shin, Han, Kim, Choi, and Seo}]{jang2022continualknowledgelearninglanguage} Jang, J.; Ye, S.; Yang, S.; Shin, J.; Han, J.; Kim, G.; Choi, S.~J.; and Seo, M. 2022.",Towards Continual Knowledge Learning of Language Models.,Towards Continual Knowledge Learning of Language Models.,,"[{Jang et~al.(2022)Jang, Ye, Yang, Shin, Han, Kim, Choi, and Seo}]{jang2022continualknowledgelearninglanguage} Jang, J.; Ye, S.; Yang, S.; Shin, J.; Han, J.; Kim, G.; Choi, S.~J.; and Seo, M. 2022. 
 Towards Continual Knowledge Learning of Language Models. 
 arXiv:2110.03215."
2408.03297,jiang2024hykgehypothesisknowledgegraph,"[{Jiang et~al.(2024)Jiang, Zhang, Xu, Qiu, Fang, Wang, Tang, Ding, Chu, Zhao, and Wang}]{jiang2024hykgehypothesisknowledgegraph} Jiang, X.; Zhang, R.; Xu, Y.; Qiu, R.; Fang, Y.; Wang, Z.; Tang, J.; Ding, H.; Chu, X.; Zhao, J.; and Wang, Y. 2024.",HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses.,HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses.,,"[{Jiang et~al.(2024)Jiang, Zhang, Xu, Qiu, Fang, Wang, Tang, Ding, Chu, Zhao, and Wang}]{jiang2024hykgehypothesisknowledgegraph} Jiang, X.; Zhang, R.; Xu, Y.; Qiu, R.; Fang, Y.; Wang, Z.; Tang, J.; Ding, H.; Chu, X.; Zhao, J.; and Wang, Y. 2024. 
 HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses. 
 arXiv:2312.15883."
2408.03297,jin2024tugofwarknowledgeexploringresolving,"[{Jin et~al.(2024)Jin, Cao, Chen, Liu, Jiang, Xu, Li, and Zhao}]{jin2024tugofwarknowledgeexploringresolving} Jin, Z.; Cao, P.; Chen, Y.; Liu, K.; Jiang, X.; Xu, J.; Li, Q.; and Zhao, J. 2024.",Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models.,Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models.,,"[{Jin et~al.(2024)Jin, Cao, Chen, Liu, Jiang, Xu, Li, and Zhao}]{jin2024tugofwarknowledgeexploringresolving} Jin, Z.; Cao, P.; Chen, Y.; Liu, K.; Jiang, X.; Xu, J.; Li, Q.; and Zhao, J. 2024. 
 Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models. 
 arXiv:2402.14409."
2408.03297,kaplan2020scaling,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.~B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020.",Scaling Laws for Neural Language Models.,Scaling Laws for Neural Language Models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling} Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.~B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020. 
 Scaling Laws for Neural Language Models. 
 arXiv:2001.08361."
2408.03297,kojima2023largelanguagemodelszeroshot,"[{Kojima et~al.(2023)Kojima, Gu, Reid, Matsuo, and Iwasawa}]{kojima2023largelanguagemodelszeroshot} Kojima, T.; Gu, S.~S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2023.",Large Language Models are Zero-Shot Reasoners.,Large Language Models are Zero-Shot Reasoners.,,"[{Kojima et~al.(2023)Kojima, Gu, Reid, Matsuo, and Iwasawa}]{kojima2023largelanguagemodelszeroshot} Kojima, T.; Gu, S.~S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2023. 
 Large Language Models are Zero-Shot Reasoners. 
 arXiv:2205.11916."
2408.03297,lewis2021retrievalaugmented,"[{Lewis et~al.(2021)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, Küttler, Lewis, tau Yih, Rocktäschel, Riedel, and Kiela}]{lewis2021retrievalaugmented} Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; tau Yih, W.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2021.",Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.,,"[{Lewis et~al.(2021)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, Küttler, Lewis, tau Yih, Rocktäschel, Riedel, and Kiela}]{lewis2021retrievalaugmented} Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; tau Yih, W.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2021. 
 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 
 arXiv:2005.11401."
2408.03297,li2022largelanguagemodelscontrollable,"[{Li et~al.(2022)Li, Rawat, Zaheer, Wang, Lukasik, Veit, Yu, and Kumar}]{li2022largelanguagemodelscontrollable} Li, D.; Rawat, A.~S.; Zaheer, M.; Wang, X.; Lukasik, M.; Veit, A.; Yu, F.; and Kumar, S. 2022.",Large Language Models with Controllable Working Memory.,Large Language Models with Controllable Working Memory.,,"[{Li et~al.(2022)Li, Rawat, Zaheer, Wang, Lukasik, Veit, Yu, and Kumar}]{li2022largelanguagemodelscontrollable} Li, D.; Rawat, A.~S.; Zaheer, M.; Wang, X.; Lukasik, M.; Veit, A.; Yu, F.; and Kumar, S. 2022. 
 Large Language Models with Controllable Working Memory. 
 arXiv:2211.05110."
2408.03297,li2023chainofknowledge,"[{Li et~al.(2023)Li, Zhao, Chia, Ding, Joty, Poria, and Bing}]{li2023chainofknowledge} Li, X.; Zhao, R.; Chia, Y.~K.; Ding, B.; Joty, S.; Poria, S.; and Bing, L. 2023.",Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.,Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.,,"[{Li et~al.(2023)Li, Zhao, Chia, Ding, Joty, Poria, and Bing}]{li2023chainofknowledge} Li, X.; Zhao, R.; Chia, Y.~K.; Ding, B.; Joty, S.; Poria, S.; and Bing, L. 2023. 
 Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. 
 arXiv:2305.13269."
2408.03297,liu2022generatedknowledgepromptingcommonsense,"[{Liu et~al.(2022)Liu, Liu, Lu, Welleck, West, Bras, Choi, and Hajishirzi}]{liu2022generatedknowledgepromptingcommonsense} Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras, R.~L.; Choi, Y.; and Hajishirzi, H. 2022.",Generated Knowledge Prompting for Commonsense Reasoning.,Generated Knowledge Prompting for Commonsense Reasoning.,,"[{Liu et~al.(2022)Liu, Liu, Lu, Welleck, West, Bras, Choi, and Hajishirzi}]{liu2022generatedknowledgepromptingcommonsense} Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras, R.~L.; Choi, Y.; and Hajishirzi, H. 2022. 
 Generated Knowledge Prompting for Commonsense Reasoning. 
 arXiv:2110.08387."
2408.03297,liu2024untangleknotinterweavingconflicting,"[{Liu et~al.(2024)Liu, Yao, Lv, Fan, Cao, Yu, Hou, and Li}]{liu2024untangleknotinterweavingconflicting} Liu, Y.; Yao, Z.; Lv, X.; Fan, Y.; Cao, S.; Yu, J.; Hou, L.; and Li, J. 2024.",Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models.,Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models.,,"[{Liu et~al.(2024)Liu, Yao, Lv, Fan, Cao, Yu, Hou, and Li}]{liu2024untangleknotinterweavingconflicting} Liu, Y.; Yao, Z.; Lv, X.; Fan, Y.; Cao, S.; Yu, J.; Hou, L.; and Li, J. 2024. 
 Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models. 
 arXiv:2404.03577."
2408.03297,longpre2022entitybasedknowledgeconflictsquestion,"[{Longpre et~al.(2022)Longpre, Perisetla, Chen, Ramesh, DuBois, and Singh}]{longpre2022entitybasedknowledgeconflictsquestion} Longpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois, C.; and Singh, S. 2022.",Entity-Based Knowledge Conflicts in Question Answering.,Entity-Based Knowledge Conflicts in Question Answering.,,"[{Longpre et~al.(2022)Longpre, Perisetla, Chen, Ramesh, DuBois, and Singh}]{longpre2022entitybasedknowledgeconflictsquestion} Longpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois, C.; and Singh, S. 2022. 
 Entity-Based Knowledge Conflicts in Question Answering. 
 arXiv:2109.05052."
2408.03297,luo2023augmentedlargelanguagemodels,"[{Luo et~al.(2023)Luo, Xu, Zhao, Geng, Tao, Ma, Lin, and Jiang}]{luo2023augmentedlargelanguagemodels} Luo, Z.; Xu, C.; Zhao, P.; Geng, X.; Tao, C.; Ma, J.; Lin, Q.; and Jiang, D. 2023.",Augmented Large Language Models with Parametric Knowledge Guiding.,Augmented Large Language Models with Parametric Knowledge Guiding.,,"[{Luo et~al.(2023)Luo, Xu, Zhao, Geng, Tao, Ma, Lin, and Jiang}]{luo2023augmentedlargelanguagemodels} Luo, Z.; Xu, C.; Zhao, P.; Geng, X.; Tao, C.; Ma, J.; Lin, Q.; and Jiang, D. 2023. 
 Augmented Large Language Models with Parametric Knowledge Guiding. 
 arXiv:2305.04757."
2408.03297,meng2023locatingeditingfactualassociations,"[{Meng et~al.(2023)Meng, Bau, Andonian, and Belinkov}]{meng2023locatingeditingfactualassociations} Meng, K.; Bau, D.; Andonian, A.; and Belinkov, Y. 2023.",Locating and Editing Factual Associations in GPT.,Locating and Editing Factual Associations in GPT.,,"[{Meng et~al.(2023)Meng, Bau, Andonian, and Belinkov}]{meng2023locatingeditingfactualassociations} Meng, K.; Bau, D.; Andonian, A.; and Belinkov, Y. 2023. 
 Locating and Editing Factual Associations in GPT. 
 arXiv:2202.05262."
2408.03297,onoe2023lmslearnnewentities,"[{Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and Choi}]{onoe2023lmslearnnewentities} Onoe, Y.; Zhang, M. J.~Q.; Padmanabhan, S.; Durrett, G.; and Choi, E. 2023.",Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge.,Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge.,,"[{Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and Choi}]{onoe2023lmslearnnewentities} Onoe, Y.; Zhang, M. J.~Q.; Padmanabhan, S.; Durrett, G.; and Choi, E. 2023. 
 Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. 
 arXiv:2305.01651."
2408.03297,park2024disentanglinglengthqualitydirect,"[{Park et~al.(2024)Park, Rafailov, Ermon, and Finn}]{park2024disentanglinglengthqualitydirect} Park, R.; Rafailov, R.; Ermon, S.; and Finn, C. 2024.",Disentangling Length from Quality in Direct Preference Optimization.,Disentangling Length from Quality in Direct Preference Optimization.,,"[{Park et~al.(2024)Park, Rafailov, Ermon, and Finn}]{park2024disentanglinglengthqualitydirect} Park, R.; Rafailov, R.; Ermon, S.; and Finn, C. 2024. 
 Disentangling Length from Quality in Direct Preference Optimization. 
 arXiv:2403.19159."
2408.03297,pinter2023emptyingoceanspoonedit,"[{Pinter and Elhadad(2023)}]{pinter2023emptyingoceanspoonedit} Pinter, Y.; and Elhadad, M. 2023.",Emptying the Ocean with a Spoon: Should We Edit Models?,Emptying the Ocean with a Spoon: Should We Edit Models?,,"[{Pinter and Elhadad(2023)}]{pinter2023emptyingoceanspoonedit} Pinter, Y.; and Elhadad, M. 2023. 
 Emptying the Ocean with a Spoon: Should We Edit Models? 
 arXiv:2310.11958."
2408.03297,rafailov2024directpreferenceoptimizationlanguage,"[{Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn}]{rafailov2024directpreferenceoptimizationlanguage} Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C.~D.; and Finn, C. 2024.",Direct Preference Optimization: Your Language Model is Secretly a Reward Model.,Direct Preference Optimization: Your Language Model is Secretly a Reward Model.,,"[{Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn}]{rafailov2024directpreferenceoptimizationlanguage} Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C.~D.; and Finn, C. 2024. 
 Direct Preference Optimization: Your Language Model is Secretly a Reward Model. 
 arXiv:2305.18290."
2408.03297,rajpurkar2018knowdontknowunanswerable,"[{Rajpurkar, Jia, and Liang(2018)}]{rajpurkar2018knowdontknowunanswerable} Rajpurkar, P.; Jia, R.; and Liang, P. 2018.",Know What You Don't Know: Unanswerable Questions for SQuAD.,Know What You Don't Know: Unanswerable Questions for SQuAD.,,"[{Rajpurkar, Jia, and Liang(2018)}]{rajpurkar2018knowdontknowunanswerable} Rajpurkar, P.; Jia, R.; and Liang, P. 2018. 
 Know What You Don't Know: Unanswerable Questions for SQuAD. 
 arXiv:1806.03822."
2408.03297,shi2023largelanguagemodelseasily,"[{Shi et~al.(2023{\natexlab{a}})Shi, Chen, Misra, Scales, Dohan, Chi, Schärli, and Zhou}]{shi2023largelanguagemodelseasily} Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E.; Schärli, N.; and Zhou, D. 2023{\natexlab{a}}.",Large Language Models Can Be Easily Distracted by Irrelevant Context.,Large Language Models Can Be Easily Distracted by Irrelevant Context.,,"[{Shi et~al.(2023{\natexlab{a}})Shi, Chen, Misra, Scales, Dohan, Chi, Schärli, and Zhou}]{shi2023largelanguagemodelseasily} Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E.; Schärli, N.; and Zhou, D. 2023{\natexlab{a}}. 
 Large Language Models Can Be Easily Distracted by Irrelevant Context. 
 arXiv:2302.00093."
2408.03297,shi2023trustingevidencehallucinatecontextaware,"[{Shi et~al.(2023{\natexlab{b}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and tau Yih}]{shi2023trustingevidencehallucinatecontextaware} Shi, W.; Han, X.; Lewis, M.; Tsvetkov, Y.; Zettlemoyer, L.; and tau Yih, S.~W. 2023{\natexlab{b}}.",Trusting Your Evidence: Hallucinate Less with Context-aware Decoding.,Trusting Your Evidence: Hallucinate Less with Context-aware Decoding.,,"[{Shi et~al.(2023{\natexlab{b}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and tau Yih}]{shi2023trustingevidencehallucinatecontextaware} Shi, W.; Han, X.; Lewis, M.; Tsvetkov, Y.; Zettlemoyer, L.; and tau Yih, S.~W. 2023{\natexlab{b}}. 
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. 
 arXiv:2305.14739."
2408.03297,shi2023trusting,"[{Shi et~al.(2023{\natexlab{c}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and Yih}]{shi2023trusting} Shi, W.; Han, X.; Lewis, M.; Tsvetkov, Y.; Zettlemoyer, L.; and Yih, S. W.-t. 2023{\natexlab{c}}.",Trusting your evidence: Hallucinate less with context-aware decoding.,Trusting your evidence: Hallucinate less with context-aware decoding.,,"[{Shi et~al.(2023{\natexlab{c}})Shi, Han, Lewis, Tsvetkov, Zettlemoyer, and Yih}]{shi2023trusting} Shi, W.; Han, X.; Lewis, M.; Tsvetkov, Y.; Zettlemoyer, L.; and Yih, S. W.-t. 2023{\natexlab{c}}. 
 Trusting your evidence: Hallucinate less with context-aware decoding. 
 \emph{arXiv preprint arXiv:2305.14739}."
2408.03297,si2023promptinggpt3reliable,"[{Si et~al.(2023)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and Wang}]{si2023promptinggpt3reliable} Si, C.; Gan, Z.; Yang, Z.; Wang, S.; Wang, J.; Boyd-Graber, J.; and Wang, L. 2023.",Prompting GPT-3 To Be Reliable.,Prompting GPT-3 To Be Reliable.,,"[{Si et~al.(2023)Si, Gan, Yang, Wang, Wang, Boyd-Graber, and Wang}]{si2023promptinggpt3reliable} Si, C.; Gan, Z.; Yang, Z.; Wang, S.; Wang, J.; Boyd-Graber, J.; and Wang, L. 2023. 
 Prompting GPT-3 To Be Reliable. 
 arXiv:2210.09150."
2408.03297,singhal2024longwaygoinvestigating,"[{Singhal et~al.(2024)Singhal, Goyal, Xu, and Durrett}]{singhal2024longwaygoinvestigating} Singhal, P.; Goyal, T.; Xu, J.; and Durrett, G. 2024.",A Long Way to Go: Investigating Length Correlations in RLHF.,A Long Way to Go: Investigating Length Correlations in RLHF.,,"[{Singhal et~al.(2024)Singhal, Goyal, Xu, and Durrett}]{singhal2024longwaygoinvestigating} Singhal, P.; Goyal, T.; Xu, J.; and Durrett, G. 2024. 
 A Long Way to Go: Investigating Length Correlations in RLHF. 
 arXiv:2310.03716."
2408.03297,tan2024blindedgeneratedcontextslanguage,"[{Tan et~al.(2024)Tan, Sun, Yang, Wang, Cao, and Cheng}]{tan2024blindedgeneratedcontextslanguage} Tan, H.; Sun, F.; Yang, W.; Wang, Y.; Cao, Q.; and Cheng, X. 2024.",Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?,Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?,,"[{Tan et~al.(2024)Tan, Sun, Yang, Wang, Cao, and Cheng}]{tan2024blindedgeneratedcontextslanguage} Tan, H.; Sun, F.; Yang, W.; Wang, Y.; Cao, Q.; and Cheng, X. 2024. 
 Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts? 
 arXiv:2401.11911."
2408.03297,taylor2022galactica,"[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic}]{taylor2022galactica} Taylor, R.; Kardas, M.; Cucurull, G.; Scialom, T.; Hartshorn, A.; Saravia, E.; Poulton, A.; Kerkez, V.; and Stojnic, R. 2022.",Galactica: A Large Language Model for Science.,Galactica: A Large Language Model for Science.,,"[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic}]{taylor2022galactica} Taylor, R.; Kardas, M.; Cucurull, G.; Scialom, T.; Hartshorn, A.; Saravia, E.; Poulton, A.; Kerkez, V.; and Stojnic, R. 2022. 
 Galactica: A Large Language Model for Science. 
 arXiv:2211.09085."
2408.03297,touvron2023llama2openfoundation,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama2openfoundation} Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer, C.~C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P.~S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E.~M.; Subramanian, R.; Tan, X.~E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J.~X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.",Llama 2: Open Foundation and Fine-Tuned Chat Models.,Llama 2: Open Foundation and Fine-Tuned Chat Models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama2openfoundation} Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer, C.~C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P.~S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E.~M.; Subramanian, R.; Tan, X.~E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J.~X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023. 
 Llama 2: Open Foundation and Fine-Tuned Chat Models. 
 arXiv:2307.09288."
2408.03297,vu2024gptvoicetasker,"[{Vu et~al.(2024)Vu, Wang, Li, Chen, Zhao, Xing, and Chen}]{vu2024gptvoicetasker} Vu, M.~D.; Wang, H.; Li, Z.; Chen, J.; Zhao, S.; Xing, Z.; and Chen, C. 2024.",GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone.,GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone.,,"[{Vu et~al.(2024)Vu, Wang, Li, Chen, Zhao, Xing, and Chen}]{vu2024gptvoicetasker} Vu, M.~D.; Wang, H.; Li, Z.; Chen, J.; Zhao, S.; Xing, Z.; and Chen, C. 2024. 
 GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone. 
 arXiv:2401.14268."
2408.03297,wang2023knowledgedrivencotexploringfaithful,"[{Wang et~al.(2023{\natexlab{a}})Wang, Duan, Wang, Li, Xian, Yin, Rong, and Xiong}]{wang2023knowledgedrivencotexploringfaithful} Wang, K.; Duan, F.; Wang, S.; Li, P.; Xian, Y.; Yin, C.; Rong, W.; and Xiong, Z. 2023{\natexlab{a}}.",Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.,Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.,,"[{Wang et~al.(2023{\natexlab{a}})Wang, Duan, Wang, Li, Xian, Yin, Rong, and Xiong}]{wang2023knowledgedrivencotexploringfaithful} Wang, K.; Duan, F.; Wang, S.; Li, P.; Xian, Y.; Yin, C.; Rong, W.; and Xiong, Z. 2023{\natexlab{a}}. 
 Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. 
 arXiv:2308.13259."
2408.03297,cmb,"[{Wang et~al.(2023{\natexlab{c}})Wang, Chen, Song, Zhang, Chen, Xiao, Jiang, Li, Wan, Wang, and Li}]{cmb} Wang, X.; Chen, G.~H.; Song, D.; Zhang, Z.; Chen, Z.; Xiao, Q.; Jiang, F.; Li, J.; Wan, X.; Wang, B.; and Li, H. 2023{\natexlab{c}}.",CMB: A Comprehensive Medical Benchmark in Chinese.,CMB: A Comprehensive Medical Benchmark in Chinese.,,"[{Wang et~al.(2023{\natexlab{c}})Wang, Chen, Song, Zhang, Chen, Xiao, Jiang, Li, Wan, Wang, and Li}]{cmb} Wang, X.; Chen, G.~H.; Song, D.; Zhang, Z.; Chen, Z.; Xiao, Q.; Jiang, F.; Li, J.; Wan, X.; Wang, B.; and Li, H. 2023{\natexlab{c}}. 
 CMB: A Comprehensive Medical Benchmark in Chinese. 
 arXiv:2308.08833."
2408.03297,wang2023selfinstruct,"[{Wang et~al.(2023{\natexlab{d}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi}]{wang2023selfinstruct} Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N.~A.; Khashabi, D.; and Hajishirzi, H. 2023{\natexlab{d}}.",Self-Instruct: Aligning Language Models with Self-Generated Instructions.,Self-Instruct: Aligning Language Models with Self-Generated Instructions.,,"[{Wang et~al.(2023{\natexlab{d}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi}]{wang2023selfinstruct} Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N.~A.; Khashabi, D.; and Hajishirzi, H. 2023{\natexlab{d}}. 
 Self-Instruct: Aligning Language Models with Self-Generated Instructions. 
 arXiv:2212.10560."
2408.03297,wei2023chainofthought,"[{Wei et~al.(2023)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}]{wei2023chainofthought} Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023.",Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.,,"[{Wei et~al.(2023)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}]{wei2023chainofthought} Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. 
 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 
 arXiv:2201.11903."
2408.03297,wu2024clashevalquantifyingtugofwarllms,"[{Wu, Wu, and Zou(2024)}]{wu2024clashevalquantifyingtugofwarllms} Wu, K.; Wu, E.; and Zou, J. 2024.",ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence.,ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence.,,"[{Wu, Wu, and Zou(2024)}]{wu2024clashevalquantifyingtugofwarllms} Wu, K.; Wu, E.; and Zou, J. 2024. 
 ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence. 
 arXiv:2404.10198."
2408.03297,xie2024adaptivechameleonstubbornsloth,"[{Xie et~al.(2024)Xie, Zhang, Chen, Lou, and Su}]{xie2024adaptivechameleonstubbornsloth} Xie, J.; Zhang, K.; Chen, J.; Lou, R.; and Su, Y. 2024.",Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts.,Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts.,,"[{Xie et~al.(2024)Xie, Zhang, Chen, Lou, and Su}]{xie2024adaptivechameleonstubbornsloth} Xie, J.; Zhang, K.; Chen, J.; Lou, R.; and Su, Y. 2024. 
 Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts. 
 arXiv:2305.13300."
2408.03297,xu2024knowledgeconflictsllmssurvey,"[{Xu et~al.(2024)Xu, Qi, Guo, Wang, Wang, Zhang, and Xu}]{xu2024knowledgeconflictsllmssurvey} Xu, R.; Qi, Z.; Guo, Z.; Wang, C.; Wang, H.; Zhang, Y.; and Xu, W. 2024.",Knowledge Conflicts for LLMs: A Survey.,Knowledge Conflicts for LLMs: A Survey.,,"[{Xu et~al.(2024)Xu, Qi, Guo, Wang, Wang, Zhang, and Xu}]{xu2024knowledgeconflictsllmssurvey} Xu, R.; Qi, Z.; Guo, Z.; Wang, C.; Wang, H.; Zhang, Y.; and Xu, W. 2024. 
 Knowledge Conflicts for LLMs: A Survey. 
 arXiv:2403.08319."
2408.03297,xue2023improvingfactualconsistencyknowledgegrounded,"[{Xue et~al.(2023)Xue, Wang, Wang, Mi, Wang, Wang, Shang, Jiang, Liu, and Wong}]{xue2023improvingfactualconsistencyknowledgegrounded} Xue, B.; Wang, W.; Wang, H.; Mi, F.; Wang, R.; Wang, Y.; Shang, L.; Jiang, X.; Liu, Q.; and Wong, K.-F. 2023.",Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment.,Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment.,,"[{Xue et~al.(2023)Xue, Wang, Wang, Mi, Wang, Wang, Shang, Jiang, Liu, and Wong}]{xue2023improvingfactualconsistencyknowledgegrounded} Xue, B.; Wang, W.; Wang, H.; Mi, F.; Wang, R.; Wang, Y.; Shang, L.; Jiang, X.; Liu, Q.; and Wong, K.-F. 2023. 
 Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. 
 arXiv:2310.08372."
2408.03297,baichuan,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, Yang, Deng, Wang, Liu, Ai, Dong, Zhao, Xu, Sun, Zhang, Liu, Ji, Xie, Dai, Fang, Su, Song, Liu, Ru, Ma, Wang, Liu, Lin, Nie, Guo, Sun, Zhang, Li, Li, Cheng, Chen, Zeng, Wang, Chen, Men, Yu, Pan, Shen, Wang, Li, Jiang, Gao, Zhang, Zhou, and Wu}]{baichuan} Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; Yang, F.; Deng, F.; Wang, F.; Liu, F.; Ai, G.; Dong, G.; Zhao, H.; Xu, H.; Sun, H.; Zhang, H.; Liu, H.; Ji, J.; Xie, J.; Dai, J.; Fang, K.; Su, L.; Song, L.; Liu, L.; Ru, L.; Ma, L.; Wang, M.; Liu, M.; Lin, M.; Nie, N.; Guo, P.; Sun, R.; Zhang, T.; Li, T.; Li, T.; Cheng, W.; Chen, W.; Zeng, X.; Wang, X.; Chen, X.; Men, X.; Yu, X.; Pan, X.; Shen, Y.; Wang, Y.; Li, Y.; Jiang, Y.; Gao, Y.; Zhang, Y.; Zhou, Z.; and Wu, Z. 2023.",Baichuan 2: Open Large-scale Language Models.,Baichuan 2: Open Large-scale Language Models.,,"[{Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, Yang, Deng, Wang, Liu, Ai, Dong, Zhao, Xu, Sun, Zhang, Liu, Ji, Xie, Dai, Fang, Su, Song, Liu, Ru, Ma, Wang, Liu, Lin, Nie, Guo, Sun, Zhang, Li, Li, Cheng, Chen, Zeng, Wang, Chen, Men, Yu, Pan, Shen, Wang, Li, Jiang, Gao, Zhang, Zhou, and Wu}]{baichuan} Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; Yang, F.; Deng, F.; Wang, F.; Liu, F.; Ai, G.; Dong, G.; Zhao, H.; Xu, H.; Sun, H.; Zhang, H.; Liu, H.; Ji, J.; Xie, J.; Dai, J.; Fang, K.; Su, L.; Song, L.; Liu, L.; Ru, L.; Ma, L.; Wang, M.; Liu, M.; Lin, M.; Nie, N.; Guo, P.; Sun, R.; Zhang, T.; Li, T.; Li, T.; Cheng, W.; Chen, W.; Zeng, X.; Wang, X.; Chen, X.; Men, X.; Yu, X.; Pan, X.; Shen, Y.; Wang, Y.; Li, Y.; Jiang, Y.; Gao, Y.; Zhang, Y.; Zhou, Z.; and Wu, Z. 2023. 
 Baichuan 2: Open Large-scale Language Models. 
 arXiv:2309.10305."
2408.03297,yu2023generateretrievelargelanguage,"[{Yu et~al.(2023{\natexlab{a}})Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang}]{yu2023generateretrievelargelanguage} Yu, W.; Iter, D.; Wang, S.; Xu, Y.; Ju, M.; Sanyal, S.; Zhu, C.; Zeng, M.; and Jiang, M. 2023{\natexlab{a}}.",Generate rather than Retrieve: Large Language Models are Strong Context Generators.,Generate rather than Retrieve: Large Language Models are Strong Context Generators.,,"[{Yu et~al.(2023{\natexlab{a}})Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang}]{yu2023generateretrievelargelanguage} Yu, W.; Iter, D.; Wang, S.; Xu, Y.; Ju, M.; Sanyal, S.; Zhu, C.; Zeng, M.; and Jiang, M. 2023{\natexlab{a}}. 
 Generate rather than Retrieve: Large Language Models are Strong Context Generators. 
 arXiv:2209.10063."
2408.03297,yu2023chainofnote,"[{Yu et~al.(2023{\natexlab{b}})Yu, Zhang, Pan, Ma, Wang, and Yu}]{yu2023chainofnote} Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D. 2023{\natexlab{b}}.",Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.,Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.,,"[{Yu et~al.(2023{\natexlab{b}})Yu, Zhang, Pan, Ma, Wang, and Yu}]{yu2023chainofnote} Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D. 2023{\natexlab{b}}. 
 Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models. 
 arXiv:2311.09210."
2408.03297,zhao2023verifyandeditknowledgeenhancedchainofthoughtframework,"[{Zhao et~al.(2023{\natexlab{a}})Zhao, Li, Joty, Qin, and Bing}]{zhao2023verifyandeditknowledgeenhancedchainofthoughtframework} Zhao, R.; Li, X.; Joty, S.; Qin, C.; and Bing, L. 2023{\natexlab{a}}.",Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework.,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework.,,"[{Zhao et~al.(2023{\natexlab{a}})Zhao, Li, Joty, Qin, and Bing}]{zhao2023verifyandeditknowledgeenhancedchainofthoughtframework} Zhao, R.; Li, X.; Joty, S.; Qin, C.; and Bing, L. 2023{\natexlab{a}}. 
 Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. 
 arXiv:2305.03268."
2408.03297,zhao2023verify,"[{Zhao et~al.(2023{\natexlab{b}})Zhao, Li, Joty, Qin, and Bing}]{zhao2023verify} Zhao, R.; Li, X.; Joty, S.; Qin, C.; and Bing, L. 2023{\natexlab{b}}.",Verify-and-edit: A knowledge-enhanced chain-of-thought framework.,Verify-and-edit: A knowledge-enhanced chain-of-thought framework.,,"[{Zhao et~al.(2023{\natexlab{b}})Zhao, Li, Joty, Qin, and Bing}]{zhao2023verify} Zhao, R.; Li, X.; Joty, S.; Qin, C.; and Bing, L. 2023{\natexlab{b}}. 
 Verify-and-edit: A knowledge-enhanced chain-of-thought framework. 
 \emph{arXiv preprint arXiv:2305.03268}."
2408.03297,zhao2023survey,"[{Zhao et~al.(2023{\natexlab{c}})Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu, Nie, and Wen}]{zhao2023survey} Zhao, W.~X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R. 2023{\natexlab{c}}.",A Survey of Large Language Models.,A Survey of Large Language Models.,,"[{Zhao et~al.(2023{\natexlab{c}})Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu, Nie, and Wen}]{zhao2023survey} Zhao, W.~X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R. 2023{\natexlab{c}}. 
 A Survey of Large Language Models. 
 arXiv:2303.18223."
2408.03297,zhou2023contextfaithfulpromptinglargelanguage,"[{Zhou et~al.(2023{\natexlab{a}})Zhou, Zhang, Poon, and Chen}]{zhou2023contextfaithfulpromptinglargelanguage} Zhou, W.; Zhang, S.; Poon, H.; and Chen, M. 2023{\natexlab{a}}.",Context-faithful Prompting for Large Language Models.,Context-faithful Prompting for Large Language Models.,,"[{Zhou et~al.(2023{\natexlab{a}})Zhou, Zhang, Poon, and Chen}]{zhou2023contextfaithfulpromptinglargelanguage} Zhou, W.; Zhang, S.; Poon, H.; and Chen, M. 2023{\natexlab{a}}. 
 Context-faithful Prompting for Large Language Models. 
 arXiv:2303.11315."
2408.03297,zhou2023context,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Zhang, Poon, and Chen}]{zhou2023context} Zhou, W.; Zhang, S.; Poon, H.; and Chen, M. 2023{\natexlab{b}}.",Context-faithful prompting for large language models.,Context-faithful prompting for large language models.,,"[{Zhou et~al.(2023{\natexlab{b}})Zhou, Zhang, Poon, and Chen}]{zhou2023context} Zhou, W.; Zhang, S.; Poon, H.; and Chen, M. 2023{\natexlab{b}}. 
 Context-faithful prompting for large language models. 
 \emph{arXiv preprint arXiv:2303.11315}."
2408.03297,ziegler2020finetuning,"[{Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2020finetuning} Ziegler, D.~M.; Stiennon, N.; Wu, J.; Brown, T.~B.; Radford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2020.",Fine-Tuning Language Models from Human Preferences.,Fine-Tuning Language Models from Human Preferences.,,"[{Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2020finetuning} Ziegler, D.~M.; Stiennon, N.; Wu, J.; Brown, T.~B.; Radford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2020. 
 Fine-Tuning Language Models from Human Preferences. 
 arXiv:1909.08593."
2408.03402,textbooks2,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{textbooks2} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023.",Textbooks are all you need ii: \textbf{phi-1.5} technical report.,Textbooks are all you need ii: \textbf{phi-1.5} technical report.,,"[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{textbooks2} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023. 
 Textbooks are all you need ii: \textbf{phi-1.5} technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2408.0356,openai2024gpt4,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{openai2024gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{openai2024gpt4} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2408.0356,chen2024alpagasus,"[{Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang et~al.}]{chen2024alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 2023.",Alpagasus: Training a better alpaca with fewer data.,Alpagasus: Training a better alpaca with fewer data.,,"[{Chen et~al.(2023)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang et~al.}]{chen2024alpagasus} Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al. 2023. 
 Alpagasus: Training a better alpaca with fewer data. 
 \emph{arXiv preprint arXiv:2307.08701}."
2408.0356,devlin2019bert,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2019bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2019bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}."
2408.0356,ding2023enhancing,"[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou}]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.",Enhancing chat language models by scaling high-quality instructional conversations.,Enhancing chat language models by scaling high-quality instructional conversations.,,"[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou}]{ding2023enhancing} Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. 
 Enhancing chat language models by scaling high-quality instructional conversations. 
 \emph{arXiv preprint arXiv:2305.14233}."
2408.0356,goodfellow2015empirical,"[{Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and Bengio}]{goodfellow2015empirical} Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio. 2013.",An empirical investigation of catastrophic forgetting in gradient-based neural networks.,An empirical investigation of catastrophic forgetting in gradient-based neural networks.,,"[{Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and Bengio}]{goodfellow2015empirical} Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio. 2013. 
 An empirical investigation of catastrophic forgetting in gradient-based neural networks. 
 \emph{arXiv preprint arXiv:1312.6211}."
2408.0356,grosse2023studying,"[{Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez et~al.}]{grosse2023studying} Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et~al. 2023.",Studying large language model generalization with influence functions.,Studying large language model generalization with influence functions.,,"[{Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez et~al.}]{grosse2023studying} Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et~al. 2023. 
 Studying large language model generalization with influence functions. 
 \emph{arXiv preprint arXiv:2308.03296}."
2408.0356,guo2021fastif,"[{Guo et~al.(2020)Guo, Rajani, Hase, Bansal, and Xiong}]{guo2021fastif} Han Guo, Nazneen~Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. 2020.",Fastif: Scalable influence functions for efficient model interpretation and debugging.,Fastif: Scalable influence functions for efficient model interpretation and debugging.,,"[{Guo et~al.(2020)Guo, Rajani, Hase, Bansal, and Xiong}]{guo2021fastif} Han Guo, Nazneen~Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. 2020. 
 Fastif: Scalable influence functions for efficient model interpretation and debugging. 
 \emph{arXiv preprint arXiv:2012.15781}."
2408.0356,han2023understanding,"[{Han et~al.(2023)Han, Simig, Mihaylov, Tsvetkov, Celikyilmaz, and Wang}]{han2023understanding} Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023.",Understanding in-context learning via supportive pretraining data.,Understanding in-context learning via supportive pretraining data.,,"[{Han et~al.(2023)Han, Simig, Mihaylov, Tsvetkov, Celikyilmaz, and Wang}]{han2023understanding} Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang. 2023. 
 Understanding in-context learning via supportive pretraining data. 
 \emph{arXiv preprint arXiv:2306.15091}."
2408.0356,he2023simplifying,[{He and Hofmann(2023)}]{he2023simplifying} Bobby He and Thomas Hofmann. 2023.,Simplifying transformer blocks.,Simplifying transformer blocks.,,"[{He and Hofmann(2023)}]{he2023simplifying} Bobby He and Thomas Hofmann. 2023. 
 Simplifying transformer blocks. 
 \emph{arXiv preprint arXiv:2311.01906}."
2408.0356,hendrycks2021measuring,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",Measuring massive multitask language understanding.,Measuring massive multitask language understanding.,,"[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 
 Measuring massive multitask language understanding. 
 \emph{arXiv preprint arXiv:2009.03300}."
2408.0356,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2408.0356,jacovi2020towards,[{Jacovi and Goldberg(2020)}]{jacovi2020towards} Alon Jacovi and Yoav Goldberg. 2020.,Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?,Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness?,,"[{Jacovi and Goldberg(2020)}]{jacovi2020towards} Alon Jacovi and Yoav Goldberg. 2020. 
 Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? 
 \emph{arXiv preprint arXiv:2004.03685}."
2408.0356,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.0356,kwon2023datainf,"[{Kwon et~al.(2023)Kwon, Wu, Wu, and Zou}]{kwon2023datainf} Yongchan Kwon, Eric Wu, Kevin Wu, and James Zou. 2023.",Datainf: Efficiently estimating data influence in lora-tuned llms and diffusion models.,Datainf: Efficiently estimating data influence in lora-tuned llms and diffusion models.,,"[{Kwon et~al.(2023)Kwon, Wu, Wu, and Zou}]{kwon2023datainf} Yongchan Kwon, Eric Wu, Kevin Wu, and James Zou. 2023. 
 Datainf: Efficiently estimating data influence in lora-tuned llms and diffusion models. 
 \emph{arXiv preprint arXiv:2310.00902}."
2408.0356,li2023textbooks,"[{Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023{\natexlab{b}}.",Textbooks are all you need ii: phi-1.5 technical report.,Textbooks are all you need ii: phi-1.5 technical report.,,"[{Li et~al.(2023{\natexlab{b}})Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and Lee}]{li2023textbooks} Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee. 2023{\natexlab{b}}. 
 Textbooks are all you need ii: phi-1.5 technical report. 
 \emph{arXiv preprint arXiv:2309.05463}."
2408.0356,li2024shot,"[{Li et~al.(2023{\natexlab{c}})Li, Hui, Xia, Yang, Yang, Zhang, Si, Liu, Liu, Huang et~al.}]{li2024shot} Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et~al. 2023{\natexlab{c}}.",One shot learning as instruction data prospector for large language models.,One shot learning as instruction data prospector for large language models.,,"[{Li et~al.(2023{\natexlab{c}})Li, Hui, Xia, Yang, Yang, Zhang, Si, Liu, Liu, Huang et~al.}]{li2024shot} Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et~al. 2023{\natexlab{c}}. 
 One shot learning as instruction data prospector for large language models. 
 \emph{arXiv preprint arXiv:2312.10302}."
2408.0356,liang2023holistic,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2023holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022.",Holistic evaluation of language models.,Holistic evaluation of language models.,,"[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar et~al.}]{liang2023holistic} Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al. 2022. 
 Holistic evaluation of language models. 
 \emph{arXiv preprint arXiv:2211.09110}."
2408.0356,liu2024datasets,"[{Liu et~al.(2024{\natexlab{a}})Liu, Cao, Liu, Ding, and Jin}]{liu2024datasets} Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. 2024{\natexlab{a}}.",Datasets for large language models: A comprehensive survey.,Datasets for large language models: A comprehensive survey.,,"[{Liu et~al.(2024{\natexlab{a}})Liu, Cao, Liu, Ding, and Jin}]{liu2024datasets} Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. 2024{\natexlab{a}}. 
 Datasets for large language models: A comprehensive survey. 
 \emph{arXiv preprint arXiv:2402.18041}."
2408.0356,liu2024understanding,"[{Liu et~al.(2024{\natexlab{b}})Liu, He, Han, Zhang, Liu, Tian, Zhang, Wang, Gao, Zhong et~al.}]{liu2024understanding} Yiheng Liu, Hao He, Tianle Han, Xu~Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et~al. 2024{\natexlab{b}}.",Understanding llms: A comprehensive overview from training to inference.,Understanding llms: A comprehensive overview from training to inference.,,"[{Liu et~al.(2024{\natexlab{b}})Liu, He, Han, Zhang, Liu, Tian, Zhang, Wang, Gao, Zhong et~al.}]{liu2024understanding} Yiheng Liu, Hao He, Tianle Han, Xu~Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et~al. 2024{\natexlab{b}}. 
 Understanding llms: A comprehensive overview from training to inference. 
 \emph{arXiv preprint arXiv:2401.02038}."
2408.0356,park2023trak,"[{Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry}]{park2023trak} Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023.",Trak: Attributing model behavior at scale.,Trak: Attributing model behavior at scale.,,"[{Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry}]{park2023trak} Sung~Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. 
 Trak: Attributing model behavior at scale. 
 \emph{arXiv preprint arXiv:2303.14186}."
2408.0356,sanh2020distilbert,"[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf}]{sanh2020distilbert} Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.","Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.","Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",,"[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf}]{sanh2020distilbert} Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. 
 Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. 
 \emph{arXiv preprint arXiv:1910.01108}."
2408.0356,shumailov2023curse,"[{Shumailov et~al.(2023)Shumailov, Shumaylov, Zhao, Gal, Papernot, and Anderson}]{shumailov2023curse} Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023.",The curse of recursion: Training on generated data makes models forget.,The curse of recursion: Training on generated data makes models forget.,,"[{Shumailov et~al.(2023)Shumailov, Shumaylov, Zhao, Gal, Papernot, and Anderson}]{shumailov2023curse} Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. 
 The curse of recursion: Training on generated data makes models forget. 
 \emph{arXiv preprint arXiv:2305.17493}."
2408.0356,geminiteam2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{geminiteam2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{geminiteam2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2408.0356,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2408.0356,wang2023farewell,"[{Wang et~al.(2023)Wang, Zhou, Zhang, Zhou, Gao, Wang, Zhang, Gao, Chen, and Gui}]{wang2023farewell} Xiao Wang, Weikang Zhou, Qi~Zhang, Jie Zhou, Songyang Gao, Junzhe Wang, Menghan Zhang, Xiang Gao, Yunwen Chen, and Tao Gui. 2023.",Farewell to aimless large-scale pretraining: Influential subset selection for language model.,Farewell to aimless large-scale pretraining: Influential subset selection for language model.,,"[{Wang et~al.(2023)Wang, Zhou, Zhang, Zhou, Gao, Wang, Zhang, Gao, Chen, and Gui}]{wang2023farewell} Xiao Wang, Weikang Zhou, Qi~Zhang, Jie Zhou, Songyang Gao, Junzhe Wang, Menghan Zhang, Xiang Gao, Yunwen Chen, and Tao Gui. 2023. 
 Farewell to aimless large-scale pretraining: Influential subset selection for language model. 
 \emph{arXiv preprint arXiv:2305.12816}."
2408.0356,xia2024less,"[{Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen}]{xia2024less} Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024.",Less: Selecting influential data for targeted instruction tuning.,Less: Selecting influential data for targeted instruction tuning.,,"[{Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen}]{xia2024less} Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. 
 Less: Selecting influential data for targeted instruction tuning. 
 \emph{arXiv preprint arXiv:2402.04333}."
2408.0356,yang2023dataset,"[{Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li}]{yang2023dataset} Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2022.",Dataset pruning: Reducing training data by examining generalization influence.,Dataset pruning: Reducing training data by examining generalization influence.,,"[{Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li}]{yang2023dataset} Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2022. 
 Dataset pruning: Reducing training data by examining generalization influence. 
 \emph{arXiv preprint arXiv:2205.09329}."
2408.0356,zhang2023instruction,"[{Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu et~al.}]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al. 2023.",Instruction tuning for large language models: A survey.,Instruction tuning for large language models: A survey.,,"[{Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu et~al.}]{zhang2023instruction} Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al. 2023. 
 Instruction tuning for large language models: A survey. 
 \emph{arXiv preprint arXiv:2308.10792}."
2408.0356,zhang2020bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2020bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2020bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
2408.0356,zhao2024preliminary,"[{Zhao et~al.(2023)Zhao, Yu, Hui, Yu, Huang, Li, and Zhang}]{zhao2024preliminary} Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin~L Zhang. 2023.",A preliminary study of the intrinsic relationship between complexity and alignment.,A preliminary study of the intrinsic relationship between complexity and alignment.,,"[{Zhao et~al.(2023)Zhao, Yu, Hui, Yu, Huang, Li, and Zhang}]{zhao2024preliminary} Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin~L Zhang. 2023. 
 A preliminary study of the intrinsic relationship between complexity and alignment. 
 \emph{arXiv preprint arXiv:2308.05696}."
2408.03907,kim2023prometheus,"[{Kim et~al.(2023)Kim, Shin, Cho, Jang, Longpre, Lee, Yun, Shin, Kim,   Thorne et~al.}]{kim2023prometheus} Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee,   Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et~al. 2023.",Prometheus: Inducing fine-grained evaluation capability in language   models.,Prometheus: Inducing fine-grained evaluation capability in language   models.,,"[{Kim et~al.(2023)Kim, Shin, Cho, Jang, Longpre, Lee, Yun, Shin, Kim,   Thorne et~al.}]{kim2023prometheus} Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee,   Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et~al. 2023. 
 Prometheus: Inducing fine-grained evaluation capability in language   models. 
 \emph{arXiv preprint arXiv:2310.08491}."
2408.03907,liu2023calibrating,"[{Liu et~al.(2023)Liu, Yang, Huang, Zhang, Huang, Wei, Deng, Sun, and   Zhang}]{liu2023calibrating} Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei,   Weiwei Deng, Feng Sun, and Qi~Zhang. 2023.",Calibrating llm-based evaluator.,Calibrating llm-based evaluator.,,"[{Liu et~al.(2023)Liu, Yang, Huang, Zhang, Huang, Wei, Deng, Sun, and   Zhang}]{liu2023calibrating} Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei,   Weiwei Deng, Feng Sun, and Qi~Zhang. 2023. 
 Calibrating llm-based evaluator. 
 \emph{arXiv preprint arXiv:2309.13308}."
2408.03907,zhu2023judgelm,"[{Zhu et~al.(2023)Zhu, Wang, and Wang}]{zhu2023judgelm} Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023.",Judgelm: Fine-tuned large language models are scalable judges.,Judgelm: Fine-tuned large language models are scalable judges.,,"[{Zhu et~al.(2023)Zhu, Wang, and Wang}]{zhu2023judgelm} Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. 
 Judgelm: Fine-tuned large language models are scalable judges. 
 \emph{arXiv preprint arXiv:2310.17631}."
2408.04816,bamieh2018discovering,[Bamieh(2018)]{bamieh2018discovering} Bassam Bamieh.,"Discovering transforms: A tutorial on circulant matrices, circular convolution, and the discrete fourier transform.","Discovering transforms: A tutorial on circulant matrices, circular convolution, and the discrete fourier transform.",,"[Bamieh(2018)]{bamieh2018discovering} Bassam Bamieh. 
 Discovering transforms: A tutorial on circulant matrices, circular convolution, and the discrete fourier transform. 
 \emph{arXiv preprint arXiv:1805.05533}, 2018."
2408.04816,bapna2019simple,"[Bapna et~al.(2019)Bapna, Arivazhagan, and Firat]{bapna2019simple} Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.","Simple, scalable adaptation for neural machine translation.","Simple, scalable adaptation for neural machine translation.",,"[Bapna et~al.(2019)Bapna, Arivazhagan, and Firat]{bapna2019simple} Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 
 Simple, scalable adaptation for neural machine translation. 
 \emph{arXiv preprint arXiv:1909.08478}, 2019."
2408.04816,chao2023jailbreaking,"[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.",Jailbreaking black box large language models in twenty queries.,Jailbreaking black box large language models in twenty queries.,,"[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking} Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong. 
 Jailbreaking black box large language models in twenty queries. 
 \emph{arXiv preprint arXiv:2310.08419}, 2023."
2408.04816,chen2023unleashing,"[Chen et~al.(2023)Chen, Zhang, Langren{\'e}, and Zhu]{chen2023unleashing} Banghao Chen, Zhaofeng Zhang, Nicolas Langren{\'e}, and Shengxin Zhu.",Unleashing the potential of prompt engineering in large language models: a comprehensive review.,Unleashing the potential of prompt engineering in large language models: a comprehensive review.,,"[Chen et~al.(2023)Chen, Zhang, Langren{\'e}, and Zhu]{chen2023unleashing} Banghao Chen, Zhaofeng Zhang, Nicolas Langren{\'e}, and Shengxin Zhu. 
 Unleashing the potential of prompt engineering in large language models: a comprehensive review. 
 \emph{arXiv preprint arXiv:2310.14735}, 2023."
2408.04816,gu2023systematic,"[Gu et~al.(2023)Gu, Han, Chen, Beirami, He, Zhang, Liao, Qin, Tresp, and Torr]{gu2023systematic} Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr.",A systematic survey of prompt engineering on vision-language foundation models.,A systematic survey of prompt engineering on vision-language foundation models.,,"[Gu et~al.(2023)Gu, Han, Chen, Beirami, He, Zhang, Liao, Qin, Tresp, and Torr]{gu2023systematic} Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 
 A systematic survey of prompt engineering on vision-language foundation models. 
 \emph{arXiv preprint arXiv:2307.12980}, 2023."
2408.04816,he2021effectiveness,"[He et~al.(2021)He, Liu, Ye, Tan, Ding, Cheng, Low, Bing, and Si]{he2021effectiveness} Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si.",On the effectiveness of adapter-based tuning for pretrained language model adaptation.,On the effectiveness of adapter-based tuning for pretrained language model adaptation.,,"[He et~al.(2021)He, Liu, Ye, Tan, Ding, Cheng, Low, Bing, and Si]{he2021effectiveness} Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. 
 On the effectiveness of adapter-based tuning for pretrained language model adaptation. 
 \emph{arXiv preprint arXiv:2106.03164}, 2021."
2408.04816,li2022mplug,"[Li et~al.(2022)Li, Xu, Tian, Wang, Yan, Bi, Ye, Chen, Xu, Cao, et~al.]{li2022mplug} Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et~al.",mplug: Effective and efficient vision-language learning by cross-modal skip-connections.,mplug: Effective and efficient vision-language learning by cross-modal skip-connections.,,"[Li et~al.(2022)Li, Xu, Tian, Wang, Yan, Bi, Ye, Chen, Xu, Cao, et~al.]{li2022mplug} Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et~al. 
 mplug: Effective and efficient vision-language learning by cross-modal skip-connections. 
 \emph{arXiv preprint arXiv:2205.12005}, 2022."
2408.04816,mahajan2023prompting,"[Mahajan et~al.(2023)Mahajan, Rahman, Yi, and Sigal]{mahajan2023prompting} Shweta Mahajan, Tanzila Rahman, Kwang~Moo Yi, and Leonid Sigal.",Prompting hard or hardly prompting: Prompt inversion for text-to-image diffusion models.,Prompting hard or hardly prompting: Prompt inversion for text-to-image diffusion models.,,"[Mahajan et~al.(2023)Mahajan, Rahman, Yi, and Sigal]{mahajan2023prompting} Shweta Mahajan, Tanzila Rahman, Kwang~Moo Yi, and Leonid Sigal. 
 Prompting hard or hardly prompting: Prompt inversion for text-to-image diffusion models. 
 \emph{arXiv preprint arXiv:2312.12416}, 2023."
2408.04816,mokady2021clipcap,"[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{mokady2021clipcap} Ron Mokady, Amir Hertz, and Amit~H Bermano.",Clipcap: Clip prefix for image captioning.,Clipcap: Clip prefix for image captioning.,,"[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{mokady2021clipcap} Ron Mokady, Amir Hertz, and Amit~H Bermano. 
 Clipcap: Clip prefix for image captioning. 
 \emph{arXiv preprint arXiv:2111.09734}, 2021."
2408.04816,shi2022toward,"[Shi et~al.(2022)Shi, Han, Gonen, Holtzman, Tsvetkov, and Zettlemoyer]{shi2022toward} Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer.","Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?","Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too?",,"[Shi et~al.(2022)Shi, Han, Gonen, Holtzman, Tsvetkov, and Zettlemoyer]{shi2022toward} Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 
 Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? 
 \emph{arXiv preprint arXiv:2212.10539}, 2022."
2408.04816,shin2020autoprompt,"[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{shin2020autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh.",Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,,"[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{shin2020autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 
 Autoprompt: Eliciting knowledge from language models with automatically generated prompts. 
 \emph{arXiv preprint arXiv:2010.15980}, 2020."
2408.04816,tewel2021zero,"[Tewel et~al.(2021)Tewel, Shalev, Schwartz, and Wolf]{tewel2021zero} Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf.",Zero-shot image-to-text generation for visual-semantic arithmetic.,Zero-shot image-to-text generation for visual-semantic arithmetic.,,"[Tewel et~al.(2021)Tewel, Shalev, Schwartz, and Wolf]{tewel2021zero} Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. 
 Zero-shot image-to-text generation for visual-semantic arithmetic. 
 \emph{arXiv preprint arXiv:2111.14447}, 2, 2021."
2408.04816,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2408.04816,wang2020k,"[Wang et~al.(2020)Wang, Tang, Duan, Wei, Huang, Cao, Jiang, Zhou, et~al.]{wang2020k} Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et~al.",K-adapter: Infusing knowledge into pre-trained models with adapters.,K-adapter: Infusing knowledge into pre-trained models with adapters.,,"[Wang et~al.(2020)Wang, Tang, Duan, Wei, Huang, Cao, Jiang, Zhou, et~al.]{wang2020k} Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et~al. 
 K-adapter: Infusing knowledge into pre-trained models with adapters. 
 \emph{arXiv preprint arXiv:2002.01808}, 2020."
2408.04816,zhang2022latent,"[Zhang et~al.(2022)Zhang, Zhang, Wang, Chen, and Wei]{zhang2022latent} Yubo Zhang, Xingxing Zhang, Xun Wang, Si-qing Chen, and Furu Wei.",Latent prompt tuning for text summarization.,Latent prompt tuning for text summarization.,,"[Zhang et~al.(2022)Zhang, Zhang, Wang, Chen, and Wei]{zhang2022latent} Yubo Zhang, Xingxing Zhang, Xun Wang, Si-qing Chen, and Furu Wei. 
 Latent prompt tuning for text summarization. 
 \emph{arXiv preprint arXiv:2211.01837}, 2022."
2408.04816,zhu2023autodan,"[Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun]{zhu2023autodan} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun.",Autodan: Automatic and interpretable adversarial attacks on large language models.,Autodan: Automatic and interpretable adversarial attacks on large language models.,,"[Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun]{zhu2023autodan} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 
 Autodan: Automatic and interpretable adversarial attacks on large language models. 
 \emph{arXiv preprint arXiv:2310.15140}, 2023."
2408.04816,zou2023universal,"[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 
 Universal and transferable adversarial attacks on aligned language models. 
 \emph{arXiv preprint arXiv:2307.15043}, 2023."
2408.0645,gu2024cruxeval,"[Gu et~al.(2024)Gu, Rozi{\`e}re, Leather, Solar-Lezama, Synnaeve, and Wang]{gu2024cruxeval} Alex Gu, Baptiste Rozi{\`e}re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida~I Wang.","Cruxeval: A benchmark for code reasoning, understanding and execution.","Cruxeval: A benchmark for code reasoning, understanding and execution.",,"[Gu et~al.(2024)Gu, Rozi{\`e}re, Leather, Solar-Lezama, Synnaeve, and Wang]{gu2024cruxeval} Alex Gu, Baptiste Rozi{\`e}re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida~I Wang. 
 Cruxeval: A benchmark for code reasoning, understanding and execution. 
 \emph{arXiv preprint arXiv:2401.03065}, 2024."
2408.0645,huang2024effibench,"[Huang et~al.(2024)Huang, Zhang, Qing, and Cui]{huang2024effibench} Dong Huang, Jie~M Zhang, Yuhao Qing, and Heming Cui.",Effibench: Benchmarking the efficiency of automatically generated code.,Effibench: Benchmarking the efficiency of automatically generated code.,,"[Huang et~al.(2024)Huang, Zhang, Qing, and Cui]{huang2024effibench} Dong Huang, Jie~M Zhang, Yuhao Qing, and Heming Cui. 
 Effibench: Benchmarking the efficiency of automatically generated code. 
 \emph{arXiv preprint arXiv:2402.02037}, 2024."
2408.0645,jain2024livecodebench,"[Jain et~al.(2024)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar-Lezama, Sen, and Stoica]{jain2024livecodebench} Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.",Livecodebench: Holistic and contamination free evaluation of large language models for code.,Livecodebench: Holistic and contamination free evaluation of large language models for code.,,"[Jain et~al.(2024)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar-Lezama, Sen, and Stoica]{jain2024livecodebench} Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 
 Livecodebench: Holistic and contamination free evaluation of large language models for code. 
 \emph{arXiv preprint arXiv:2403.07974}, 2024."
2408.0645,kaplan2020scaling,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}, 2020."
2408.0645,liu2024codemind,"[Liu et~al.(2024)Liu, Zhang, and Jabbarvand]{liu2024codemind} Changshu Liu, Shizhuo~Dylan Zhang, and Reyhaneh Jabbarvand.",Codemind: A framework to challenge large language models for code reasoning.,Codemind: A framework to challenge large language models for code reasoning.,,"[Liu et~al.(2024)Liu, Zhang, and Jabbarvand]{liu2024codemind} Changshu Liu, Shizhuo~Dylan Zhang, and Reyhaneh Jabbarvand. 
 Codemind: A framework to challenge large language models for code reasoning. 
 \emph{arXiv preprint arXiv:2402.09664}, 2024."
2408.0645,repobench,"[Liu et~al.(2023{\natexlab{b}})Liu, Xu, and McAuley]{repobench} Tianyang Liu, Canwen Xu, and Julian McAuley.",Repobench: Benchmarking repository-level code auto-completion systems.,Repobench: Benchmarking repository-level code auto-completion systems.,,"[Liu et~al.(2023{\natexlab{b}})Liu, Xu, and McAuley]{repobench} Tianyang Liu, Canwen Xu, and Julian McAuley. 
 Repobench: Benchmarking repository-level code auto-completion systems. 
 \emph{arXiv preprint arXiv:2306.03091}, 2023{\natexlab{b}}."
2408.0645,starcoder2,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{starcoder2} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.",Starcoder 2 and the stack v2: The next generation.,Starcoder 2 and the stack v2: The next generation.,,"[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, et~al.]{starcoder2} Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al. 
 Starcoder 2 and the stack v2: The next generation. 
 \emph{arXiv preprint arXiv:2402.19173}, 2024."
2408.0645,qiu2024efficient,"[Qiu et~al.(2024)Qiu, Zeng, Tong, Ezick, and Lott]{qiu2024efficient} Ruizhong Qiu, Weiliang~Will Zeng, Hanghang Tong, James Ezick, and Christopher Lott.",How efficient is llm-generated code? a rigorous \& high-standard benchmark.,How efficient is llm-generated code? a rigorous \& high-standard benchmark.,,"[Qiu et~al.(2024)Qiu, Zeng, Tong, Ezick, and Lott]{qiu2024efficient} Ruizhong Qiu, Weiliang~Will Zeng, Hanghang Tong, James Ezick, and Christopher Lott. 
 How efficient is llm-generated code? a rigorous \& high-standard benchmark. 
 \emph{arXiv preprint arXiv:2406.06647}, 2024."
2408.0645,pie,"[Shypula et~al.(2023)Shypula, Madaan, Zeng, Alon, Gardner, Hashemi, Neubig, Ranganathan, Bastani, and Yazdanbakhsh]{pie} Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh.",Learning performance-improving code edits.,Learning performance-improving code edits.,,"[Shypula et~al.(2023)Shypula, Madaan, Zeng, Alon, Gardner, Hashemi, Neubig, Ranganathan, Bastani, and Yazdanbakhsh]{pie} Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. 
 Learning performance-improving code edits. 
 \emph{arXiv preprint arXiv:2302.07867}, 2023."
2408.0645,waghjale2024ecco,"[Waghjale et~al.(2024)Waghjale, Veerendranath, Wang, and Fried]{waghjale2024ecco} Siddhant Waghjale, Vishruth Veerendranath, Zora~Zhiruo Wang, and Daniel Fried.",Ecco: Can we improve model-generated code efficiency without sacrificing functional correctness?,Ecco: Can we improve model-generated code efficiency without sacrificing functional correctness?,,"[Waghjale et~al.(2024)Waghjale, Veerendranath, Wang, and Fried]{waghjale2024ecco} Siddhant Waghjale, Vishruth Veerendranath, Zora~Zhiruo Wang, and Daniel Fried. 
 Ecco: Can we improve model-generated code efficiency without sacrificing functional correctness? 
 \emph{arXiv preprint arXiv:2407.14044}, 2024."
2408.0645,arcade,"[Yin et~al.(2022)Yin, Li, Xiao, Rao, Wen, Shi, Howland, Bailey, Catasta, Michalewski, et~al.]{arcade} Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, et~al.",Natural language to code generation in interactive data science notebooks.,Natural language to code generation in interactive data science notebooks.,,"[Yin et~al.(2022)Yin, Li, Xiao, Rao, Wen, Shi, Howland, Bailey, Catasta, Michalewski, et~al.]{arcade} Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, et~al. 
 Natural language to code generation in interactive data science notebooks. 
 \emph{arXiv preprint arXiv:2212.09248}, 2022."
2408.0645,zan2022cert,"[Zan et~al.(2022)Zan, Chen, Yang, Lin, Kim, Guan, Wang, Chen, and Lou]{zan2022cert} Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou.",Cert: continual pre-training on sketches for library-oriented code generation.,Cert: continual pre-training on sketches for library-oriented code generation.,,"[Zan et~al.(2022)Zan, Chen, Yang, Lin, Kim, Guan, Wang, Chen, and Lou]{zan2022cert} Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. 
 Cert: continual pre-training on sketches for library-oriented code generation. 
 \emph{arXiv preprint arXiv:2206.06888}, 2022."
2408.0645,humanevalx,"[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang, Li, et~al.]{humanevalx} Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et~al.",Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x.,Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x.,,"[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang, Li, et~al.]{humanevalx} Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et~al. 
 Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. 
 \emph{arXiv preprint arXiv:2303.17568}, 2023."
2408.0645,zhuo2024bigcodebench,"[Zhuo et~al.(2024)Zhuo, Vu, Chim, Hu, Yu, Widyasari, Yusuf, Zhan, He, Paul, et~al.]{zhuo2024bigcodebench} Terry~Yue Zhuo, Minh~Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur~Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et~al.",Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions.,Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions.,,"[Zhuo et~al.(2024)Zhuo, Vu, Chim, Hu, Yu, Widyasari, Yusuf, Zhan, He, Paul, et~al.]{zhuo2024bigcodebench} Terry~Yue Zhuo, Minh~Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur~Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et~al. 
 Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. 
 \emph{arXiv preprint arXiv:2406.15877}, 2024."
2408.06663,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2408.06663,almazrouei2023falcon,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.",The falcon series of open language models.,The falcon series of open language models.,,"[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon} Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023. 
 The falcon series of open language models. 
 \emph{arXiv preprint arXiv:2311.16867}."
2408.06663,batsuren2024evaluating,"[{Batsuren et~al.(2024)Batsuren, Vylomova, Dankers, Delgerbaatar, Uzan, Pinter, and Bella}]{batsuren2024evaluating} Khuyagbaatar Batsuren, Ekaterina Vylomova, Verna Dankers, Tsetsuukhei Delgerbaatar, Omri Uzan, Yuval Pinter, and G{\'a}bor Bella. 2024.",Evaluating subword tokenization: Alien subword composition and oov generalization challenge.,Evaluating subword tokenization: Alien subword composition and oov generalization challenge.,,"[{Batsuren et~al.(2024)Batsuren, Vylomova, Dankers, Delgerbaatar, Uzan, Pinter, and Bella}]{batsuren2024evaluating} Khuyagbaatar Batsuren, Ekaterina Vylomova, Verna Dankers, Tsetsuukhei Delgerbaatar, Omri Uzan, Yuval Pinter, and G{\'a}bor Bella. 2024. 
 Evaluating subword tokenization: Alien subword composition and oov generalization challenge. 
 \emph{arXiv preprint arXiv:2404.13292}."
2408.06663,bianchi2023safety,"[{Bianchi et~al.(2023)Bianchi, Suzgun, Attanasio, R{\""o}ttger, Jurafsky, Hashimoto, and Zou}]{bianchi2023safety} Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R{\""o}ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2023.",Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.,Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.,,"[{Bianchi et~al.(2023)Bianchi, Suzgun, Attanasio, R{\""o}ttger, Jurafsky, Hashimoto, and Zou}]{bianchi2023safety} Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R{\""o}ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2023. 
 Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. 
 \emph{arXiv preprint arXiv:2309.07875}."
2408.06663,chen2023sudden,"[{Chen et~al.(2023)Chen, Schwartz-Ziv, Cho, Leavitt, and Saphra}]{chen2023sudden} Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew~L Leavitt, and Naomi Saphra. 2023.","Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms.","Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms.",,"[{Chen et~al.(2023)Chen, Schwartz-Ziv, Cho, Leavitt, and Saphra}]{chen2023sudden} Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew~L Leavitt, and Naomi Saphra. 2023. 
 Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms. 
 \emph{arXiv preprint arXiv:2309.07311}."
2408.06663,clark2018think,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2408.06663,groeneveld2024olmo,"[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang et~al.}]{groeneveld2024olmo} Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al. 2024.",Olmo: Accelerating the science of language models.,Olmo: Accelerating the science of language models.,,"[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang et~al.}]{groeneveld2024olmo} Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al. 2024. 
 Olmo: Accelerating the science of language models. 
 \emph{arXiv preprint arXiv:2402.00838}."
2408.06663,hassid2024larger,"[{Hassid et~al.(2024)Hassid, Remez, Gehring, Schwartz, and Adi}]{hassid2024larger} Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi. 2024.",The larger the better? improved llm code-generation via budget reallocation.,The larger the better? improved llm code-generation via budget reallocation.,,"[{Hassid et~al.(2024)Hassid, Remez, Gehring, Schwartz, and Adi}]{hassid2024larger} Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi. 2024. 
 The larger the better? improved llm code-generation via budget reallocation. 
 \emph{arXiv preprint arXiv:2404.00725}."
2408.06663,hoffmann2022training,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2408.06663,hu2021lora,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021.",Lora: Low-rank adaptation of large language models.,Lora: Low-rank adaptation of large language models.,,"[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2021lora} Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2021. 
 Lora: Low-rank adaptation of large language models. 
 \emph{arXiv preprint arXiv:2106.09685}."
2408.06663,ivison2023camels,"[{Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy et~al.}]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al. 2023.",Camels in a changing climate: Enhancing lm adaptation with tulu 2.,Camels in a changing climate: Enhancing lm adaptation with tulu 2.,,"[{Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy et~al.}]{ivison2023camels} Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al. 2023. 
 Camels in a changing climate: Enhancing lm adaptation with tulu 2. 
 \emph{arXiv preprint arXiv:2311.10702}."
2408.06663,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.06663,lee2023rlaif,"[{Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi}]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023.",Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,Rlaif: Scaling reinforcement learning from human feedback with ai feedback.,,"[{Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi}]{lee2023rlaif} Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. 
 Rlaif: Scaling reinforcement learning from human feedback with ai feedback. 
 \emph{arXiv preprint arXiv:2309.00267}."
2408.06663,liu2021p,"[{Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{liu2021p} Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021.",P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.,P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.,,"[{Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang}]{liu2021p} Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. 
 P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. 
 \emph{arXiv preprint arXiv:2110.07602}."
2408.06663,olsson2022context,"[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022.",In-context learning and induction heads.,In-context learning and induction heads.,,"[{Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen et~al.}]{olsson2022context} Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al. 2022. 
 In-context learning and induction heads. 
 \emph{arXiv preprint arXiv:2209.11895}."
2408.06663,rae2021scaling,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021. 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 \emph{arXiv preprint arXiv:2112.11446}."
2408.06663,team2024gemma,"[{Riviere et~al.(2024)Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma} Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024.",Gemma 2: Improving open language models at a practical size.,Gemma 2: Improving open language models at a practical size.,,"[{Riviere et~al.(2024)Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e} et~al.}]{team2024gemma} Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al. 2024. 
 Gemma 2: Improving open language models at a practical size. 
 \emph{arXiv preprint arXiv:2408.00118}."
2408.06663,schulman2017proximal,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",Proximal policy optimization algorithms.,Proximal policy optimization algorithms.,,"[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov}]{schulman2017proximal} John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 
 Proximal policy optimization algorithms. 
 \emph{arXiv preprint arXiv:1707.06347}."
2408.06663,sclar2023quantifying,"[{Sclar et~al.(2023)Sclar, Choi, Tsvetkov, and Suhr}]{sclar2023quantifying} Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.",Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.,Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.,,"[{Sclar et~al.(2023)Sclar, Choi, Tsvetkov, and Suhr}]{sclar2023quantifying} Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. 
 Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. 
 \emph{arXiv preprint arXiv:2310.11324}."
2408.06663,sharma2024critical,"[{Sharma et~al.(2024)Sharma, Keh, Mitchell, Finn, Arora, and Kollar}]{sharma2024critical} Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. 2024.",A critical evaluation of ai feedback for aligning large language models.,A critical evaluation of ai feedback for aligning large language models.,,"[{Sharma et~al.(2024)Sharma, Keh, Mitchell, Finn, Arora, and Kollar}]{sharma2024critical} Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. 2024. 
 A critical evaluation of ai feedback for aligning large language models. 
 \emph{arXiv preprint arXiv:2402.12366}."
2408.06663,shen2024bidirectional,"[{Shen et~al.(2024)Shen, Knearem, Ghosh, Alkiek, Krishna, Liu, Ma, Petridis, Peng, Qiwei, Rakshit, Si, Xie, Bigham, Bentley, Chai, Lipton, Mei, Mihalcea, Terry, Yang, Morris, Resnick, and Jurgens}]{shen2024bidirectional} Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li~Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey~P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith~Ringel Morris, Paul Resnick, and David Jurgens. 2024.","Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions.","Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions.",,"[{Shen et~al.(2024)Shen, Knearem, Ghosh, Alkiek, Krishna, Liu, Ma, Petridis, Peng, Qiwei, Rakshit, Si, Xie, Bigham, Bentley, Chai, Lipton, Mei, Mihalcea, Terry, Yang, Morris, Resnick, and Jurgens}]{shen2024bidirectional} Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li~Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey~P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith~Ringel Morris, Paul Resnick, and David Jurgens. 2024. 
 Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions. 
 \emph{arXiv preprint arXiv:2406.09264}."
2408.06663,singh2024tokenization,[{Singh and Strouse(2024)}]{singh2024tokenization} Aaditya~K Singh and DJ~Strouse. 2024.,Tokenization counts: the impact of tokenization on arithmetic in frontier llms.,Tokenization counts: the impact of tokenization on arithmetic in frontier llms.,,"[{Singh and Strouse(2024)}]{singh2024tokenization} Aaditya~K Singh and DJ~Strouse. 2024. 
 Tokenization counts: the impact of tokenization on arithmetic in frontier llms. 
 \emph{arXiv preprint arXiv:2402.14903}."
2408.06663,soldaini2024dolma,"[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma} Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024.",Dolma: An open corpus of three trillion tokens for language model pretraining research.,Dolma: An open corpus of three trillion tokens for language model pretraining research.,,"[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma} Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024. 
 Dolma: An open corpus of three trillion tokens for language model pretraining research. 
 \emph{arXiv preprint arXiv:2402.00159}."
2408.06663,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2408.06663,wang2024helpsteer2,"[{Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev}]{wang2024helpsteer2} Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J. Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024.",Helpsteer2: Open-source dataset for training top-performing reward models.,Helpsteer2: Open-source dataset for training top-performing reward models.,,"[{Wang et~al.(2024)Wang, Dong, Delalleau, Zeng, Shen, Egert, Zhang, Sreedhar, and Kuchaiev}]{wang2024helpsteer2} Zhilin Wang, Yi~Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy~J. Zhang, Makesh~Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024. 
 Helpsteer2: Open-source dataset for training top-performing reward models. 
 \emph{arXiv preprint arXiv:2406.08673}."
2408.06663,wei2021finetuned,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2408.06663,wei2022emergent,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al. 2022.",Emergent abilities of large language models.,Emergent abilities of large language models.,,"[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler et~al.}]{wei2022emergent} Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al. 2022. 
 Emergent abilities of large language models. 
 \emph{arXiv preprint arXiv:2206.07682}."
2408.06663,wu2023reasoning,"[{Wu et~al.(2023)Wu, Qiu, Ross, Aky{\""u}rek, Chen, Wang, Kim, Andreas, and Kim}]{wu2023reasoning} Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky{\""u}rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023.",Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.,Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.,,"[{Wu et~al.(2023)Wu, Qiu, Ross, Aky{\""u}rek, Chen, Wang, Kim, Andreas, and Kim}]{wu2023reasoning} Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky{\""u}rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. 
 Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. 
 \emph{arXiv preprint arXiv:2307.02477}."
2408.06663,xia2024less,"[{Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen}]{xia2024less} Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024.",Less: Selecting influential data for targeted instruction tuning.,Less: Selecting influential data for targeted instruction tuning.,,"[{Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen}]{xia2024less} Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. 
 Less: Selecting influential data for targeted instruction tuning. 
 \emph{arXiv preprint arXiv:2402.04333}."
2408.06663,xie2021explanation,"[{Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation} Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021.",An explanation of in-context learning as implicit bayesian inference.,An explanation of in-context learning as implicit bayesian inference.,,"[{Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation} Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. 
 An explanation of in-context learning as implicit bayesian inference. 
 \emph{arXiv preprint arXiv:2111.02080}."
2408.06663,xu2024contrastive,"[{Xu et~al.(2024)Xu, Sharaf, Chen, Tan, Shen, Van~Durme, Murray, and Kim}]{xu2024contrastive} Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van~Durme, Kenton Murray, and Young~Jin Kim. 2024.",Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.,Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.,,"[{Xu et~al.(2024)Xu, Sharaf, Chen, Tan, Shen, Van~Durme, Murray, and Kim}]{xu2024contrastive} Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van~Durme, Kenton Murray, and Young~Jin Kim. 2024. 
 Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. 
 \emph{arXiv preprint arXiv:2401.08417}."
2408.06663,yang2024unveiling,"[{Yang et~al.(2024)Yang, Zhang, Xu, Lu, Heng, and Lam}]{yang2024unveiling} Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng~Ann Heng, and Wai Lam. 2024.",Unveiling the generalization power of fine-tuned large language models.,Unveiling the generalization power of fine-tuned large language models.,,"[{Yang et~al.(2024)Yang, Zhang, Xu, Lu, Heng, and Lam}]{yang2024unveiling} Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng~Ann Heng, and Wai Lam. 2024. 
 Unveiling the generalization power of fine-tuned large language models. 
 \emph{arXiv preprint arXiv:2403.09162}."
2408.06663,zhang2022opt,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.",Opt: Open pre-trained transformer language models.,Opt: Open pre-trained transformer language models.,,"[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt} Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022. 
 Opt: Open pre-trained transformer language models. 
 \emph{arXiv preprint arXiv:2205.01068}."
2408.06663,ziegler2019fine,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.",Fine-tuning language models from human preferences.,Fine-tuning language models from human preferences.,,"[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving}]{ziegler2019fine} Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. 
 Fine-tuning language models from human preferences. 
 \emph{arXiv preprint arXiv:1909.08593}."
2408.0831,meta-mm-scalinglaw,"[{Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer}]{meta-mm-scalinglaw} Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 2023.",Scaling laws for generative mixed-modal language models.,Scaling laws for generative mixed-modal language models.,,"[{Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer}]{meta-mm-scalinglaw} Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 2023. 
 Scaling laws for generative mixed-modal language models. 
 \emph{arXiv preprint arXiv:2301.03728}."
2408.0831,deepseekllm,"[{Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu et~al.}]{deepseekllm} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al. 2024.",Deepseek llm: Scaling open-source language models with longtermism.,Deepseek llm: Scaling open-source language models with longtermism.,,"[{Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu et~al.}]{deepseekllm} Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al. 2024. 
 Deepseek llm: Scaling open-source language models with longtermism. 
 \emph{arXiv preprint arXiv:2401.02954}."
2408.0831,boolq,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.",Boolq: Exploring the surprising difficulty of natural yes/no questions.,Boolq: Exploring the surprising difficulty of natural yes/no questions.,,"[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{boolq} Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. 
 Boolq: Exploring the surprising difficulty of natural yes/no questions. 
 \emph{arXiv preprint arXiv:1905.10044}."
2408.0831,arc,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{arc} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}."
2408.0831,crossling-scalinglaw,"[{Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}]{crossling-scalinglaw} Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",Unsupervised cross-lingual representation learning at scale.,Unsupervised cross-lingual representation learning at scale.,,"[{Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov}]{crossling-scalinglaw} Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. 
 Unsupervised cross-lingual representation learning at scale. 
 \emph{arXiv preprint arXiv:1911.02116}."
2408.0831,cerebras,"[{Dey et~al.(2023)Dey, Gosal, Khachane, Marshall, Pathria, Tom, Hestness et~al.}]{cerebras} Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et~al. 2023.",Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster.,Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster.,,"[{Dey et~al.(2023)Dey, Gosal, Khachane, Marshall, Pathria, Tom, Hestness et~al.}]{cerebras} Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et~al. 2023. 
 Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. 
 \emph{arXiv preprint arXiv:2304.03208}."
2408.0831,vendi,[{Friedman and Dieng(2022)}]{vendi} Dan Friedman and Adji~Bousso Dieng. 2022.,The vendi score: A diversity evaluation metric for machine learning.,The vendi score: A diversity evaluation metric for machine learning.,,"[{Friedman and Dieng(2022)}]{vendi} Dan Friedman and Adji~Bousso Dieng. 2022. 
 The vendi score: A diversity evaluation metric for machine learning. 
 \emph{arXiv preprint arXiv:2210.02410}."
2408.0831,pile,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima et~al.}]{pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020.",The pile: An 800gb dataset of diverse text for language modeling.,The pile: An 800gb dataset of diverse text for language modeling.,,"[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima et~al.}]{pile} Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020. 
 The pile: An 800gb dataset of diverse text for language modeling. 
 \emph{arXiv preprint arXiv:2101.00027}."
2408.0831,openai-mm-scalinglaw,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{openai-mm-scalinglaw} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020.",Scaling laws for autoregressive generative modeling.,Scaling laws for autoregressive generative modeling.,,"[{Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray et~al.}]{openai-mm-scalinglaw} Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al. 2020. 
 Scaling laws for autoregressive generative modeling. 
 \emph{arXiv preprint arXiv:2010.14701}."
2408.0831,2017scalinglaw,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{2017scalinglaw} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017.","Deep learning scaling is predictable, empirically.","Deep learning scaling is predictable, empirically.",,"[{Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}]{2017scalinglaw} Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md~Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. 
 Deep learning scaling is predictable, empirically. 
 \emph{arXiv preprint arXiv:1712.00409}."
2408.0831,chinchilla,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{chinchilla} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.",Training compute-optimal large language models.,Training compute-optimal large language models.,,"[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{chinchilla} Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022. 
 Training compute-optimal large language models. 
 \emph{arXiv preprint arXiv:2203.15556}."
2408.0831,openai-scalinglaw,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{openai-scalinglaw} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling laws for neural language models.,Scaling laws for neural language models.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{openai-scalinglaw} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling laws for neural language models. 
 \emph{arXiv preprint arXiv:2001.08361}."
2408.0831,data-pruning,"[{Marion et~al.(2023)Marion, {\""U}st{\""u}n, Pozzobon, Wang, Fadaee, and Hooker}]{data-pruning} Max Marion, Ahmet {\""U}st{\""u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023.",When less is more: Investigating data pruning for pretraining llms at scale.,When less is more: Investigating data pruning for pretraining llms at scale.,,"[{Marion et~al.(2023)Marion, {\""U}st{\""u}n, Pozzobon, Wang, Fadaee, and Hooker}]{data-pruning} Max Marion, Ahmet {\""U}st{\""u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. 2023. 
 When less is more: Investigating data pruning for pretraining llms at scale. 
 \emph{arXiv preprint arXiv:2309.04564}."
2408.0831,openbookqa,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{openbookqa} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.",Can a suit of armor conduct electricity? a new dataset for open book question answering.,Can a suit of armor conduct electricity? a new dataset for open book question answering.,,"[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{openbookqa} Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. 
 Can a suit of armor conduct electricity? a new dataset for open book question answering. 
 \emph{arXiv preprint arXiv:1809.02789}."
2408.0831,lambada,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez. 2016.",The lambada dataset: Word prediction requiring a broad discourse context.,The lambada dataset: Word prediction requiring a broad discourse context.,,"[{Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez}]{lambada} Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez. 2016. 
 The lambada dataset: Word prediction requiring a broad discourse context. 
 \emph{arXiv preprint arXiv:1606.06031}."
2408.0831,refinedweb,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.","The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.",,"[{Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay}]{refinedweb} Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. 
 The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 
 \emph{arXiv preprint arXiv:2306.01116}."
2408.0831,fp8lm,"[{Peng et~al.(2023)Peng, Wu, Wei, Zhao, Yang, Liu, Xiong, Yang, Ni, Hu et~al.}]{fp8lm} Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze~Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et~al. 2023.",Fp8-lm: Training fp8 large language models.,Fp8-lm: Training fp8 large language models.,,"[{Peng et~al.(2023)Peng, Wu, Wei, Zhao, Yang, Liu, Xiong, Yang, Ni, Hu et~al.}]{fp8lm} Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze~Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et~al. 2023. 
 Fp8-lm: Training fp8 large language models. 
 \emph{arXiv preprint arXiv:2310.18313}."
2408.0831,gopher,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{gopher} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.","Scaling language models: Methods, analysis \& insights from training gopher.","Scaling language models: Methods, analysis \& insights from training gopher.",,"[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{gopher} Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021. 
 Scaling language models: Methods, analysis \& insights from training gopher. 
 \emph{arXiv preprint arXiv:2112.11446}."
2408.0831,megatron,"[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{megatron} Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.",Megatron-lm: Training multi-billion parameter language models using model parallelism.,Megatron-lm: Training multi-billion parameter language models using model parallelism.,,"[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{megatron} Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. 
 Megatron-lm: Training multi-billion parameter language models using model parallelism. 
 \emph{arXiv preprint arXiv:1909.08053}."
2408.0831,mdeep,"[{Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti et~al.}]{mdeep} Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et~al. 2022.","Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.","Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.",,"[{Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti et~al.}]{mdeep} Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et~al. 2022. 
 Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. 
 \emph{arXiv preprint arXiv:2201.11990}."
2408.0831,llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2408.0831,ccnet,"[{Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave}]{ccnet} Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave. 2019.",Ccnet: Extracting high quality monolingual datasets from web crawl data.,Ccnet: Extracting high quality monolingual datasets from web crawl data.,,"[{Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave}]{ccnet} Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave. 2019. 
 Ccnet: Extracting high quality monolingual datasets from web crawl data. 
 \emph{arXiv preprint arXiv:1911.00359}."
2408.0831,qurating,"[{Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen}]{qurating} Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024.",Qurating: Selecting high-quality data for training language models.,Qurating: Selecting high-quality data for training language models.,,"[{Wettig et~al.(2024)Wettig, Gupta, Malik, and Chen}]{qurating} Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. 
 Qurating: Selecting high-quality data for training language models. 
 \emph{arXiv preprint arXiv:2402.09739}."
2408.0831,hellaswag,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.",Hellaswag: Can a machine really finish your sentence?,Hellaswag: Can a machine really finish your sentence?,,"[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{hellaswag} Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. 
 Hellaswag: Can a machine really finish your sentence? 
 \emph{arXiv preprint arXiv:1905.07830}."
2408.08631,gsm8k,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.",Training verifiers to solve math word problems.,Training verifiers to solve math word problems.,,"[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{gsm8k} Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021. 
 Training verifiers to solve math word problems. 
 \emph{arXiv preprint arXiv:2110.14168}."
2408.08631,gupta2023bias,"[{Gupta et~al.(2023)Gupta, Shrivastava, Deshpande, Kalyan, Clark, Sabharwal, and Khot}]{gupta2023bias} Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2023.",Bias runs deep: Implicit reasoning biases in persona-assigned llms.,Bias runs deep: Implicit reasoning biases in persona-assigned llms.,,"[{Gupta et~al.(2023)Gupta, Shrivastava, Deshpande, Kalyan, Clark, Sabharwal, and Khot}]{gupta2023bias} Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2023. 
 Bias runs deep: Implicit reasoning biases in persona-assigned llms. 
 \emph{arXiv preprint arXiv:2311.04892}."
2408.08631,li2023split,"[{Li et~al.(2023)Li, Wang, Ma, Wu, Wang, Gao, and Liu}]{li2023split} Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023.",Split and merge: Aligning position biases in large language model based evaluators.,Split and merge: Aligning position biases in large language model based evaluators.,,"[{Li et~al.(2023)Li, Wang, Ma, Wu, Wang, Gao, and Liu}]{li2023split} Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023. 
 Split and merge: Aligning position biases in large language model based evaluators. 
 \emph{arXiv preprint arXiv:2310.01432}."
2408.08631,srivastava2022beyond,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022.",Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.,,"[{Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso et~al.}]{srivastava2022beyond} Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al. 2022. 
 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 
 \emph{arXiv preprint arXiv:2206.04615}."
2408.08631,wang2023large,"[{Wang et~al.(2023)Wang, Li, Chen, Cai, Zhu, Lin, Cao, Liu, Liu, and Sui}]{wang2023large} Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui. 2023.",Large language models are not fair evaluators.,Large language models are not fair evaluators.,,"[{Wang et~al.(2023)Wang, Li, Chen, Cai, Zhu, Lin, Cao, Liu, Liu, and Sui}]{wang2023large} Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui. 2023. 
 Large language models are not fair evaluators. 
 \emph{arXiv preprint arXiv:2305.17926}."
2408.08631,zheng2023helpful,"[{Zheng et~al.(2023)Zheng, Pei, and Jurgens}]{zheng2023helpful} Mingqian Zheng, Jiaxin Pei, and David Jurgens. 2023.","Is"" a helpful assistant"" the best role for large language models? a systematic evaluation of social roles in system prompts.","Is"" a helpful assistant"" the best role for large language models? a systematic evaluation of social roles in system prompts.",,"[{Zheng et~al.(2023)Zheng, Pei, and Jurgens}]{zheng2023helpful} Mingqian Zheng, Jiaxin Pei, and David Jurgens. 2023. 
 Is"" a helpful assistant"" the best role for large language models? a systematic evaluation of social roles in system prompts. 
 \emph{arXiv preprint arXiv:2311.10054}."
2408.09743,qwen,"[Bai et~al.(2023)Bai, Bai, Chu, and et~al.]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, and et al.",Qwen technical report.,Qwen technical report.,,"[Bai et~al.(2023)Bai, Bai, Chu, and et~al.]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, and et al. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}, 2023."
2408.09743,chambon2024chexpertPLUS,"[Chambon et~al.(2024)Chambon, Delbrouck, Sounack, Huang, Chen, Varma,   Truong, Chuong, and Langlotz]{chambon2024chexpertPLUS} Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang,   Zhihong Chen, Maya Varma, Steven~QH Truong, Chu~The Chuong, and Curtis~P   Langlotz.","Chexpert plus: Augmenting a large chest x-ray dataset with text   radiology reports, patient demographics and additional image formats.","Chexpert plus: Augmenting a large chest x-ray dataset with text   radiology reports, patient demographics and additional image formats.",,"[Chambon et~al.(2024)Chambon, Delbrouck, Sounack, Huang, Chen, Varma,   Truong, Chuong, and Langlotz]{chambon2024chexpertPLUS} Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang,   Zhihong Chen, Maya Varma, Steven~QH Truong, Chu~The Chuong, and Curtis~P   Langlotz. 
 Chexpert plus: Augmenting a large chest x-ray dataset with text   radiology reports, patient demographics and additional image formats. 
 \emph{arXiv preprint arXiv:2405.19538}, 2024."
2408.09743,dong2022ContextLearnsurvey,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui]{dong2022ContextLearnsurvey} Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,   Jingjing Xu, and Zhifang Sui.",A survey on in-context learning.,A survey on in-context learning.,,"[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and   Sui]{dong2022ContextLearnsurvey} Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,   Jingjing Xu, and Zhifang Sui. 
 A survey on in-context learning. 
 \emph{arXiv preprint arXiv:2301.00234}, 2022."
2408.09743,dubey2024llama3,"[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman,   Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama3} Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad   Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,   et~al.",The llama 3 herd of models.,The llama 3 herd of models.,,"[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman,   Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama3} Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad   Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,   et~al. 
 The llama 3 herd of models. 
 \emph{arXiv preprint arXiv:2407.21783}, 2024."
2408.09743,gu2023mamba,[Gu and Dao(2023)]{gu2023mamba} Albert Gu and Tri Dao.,Mamba: Linear-time sequence modeling with selective state spaces.,Mamba: Linear-time sequence modeling with selective state spaces.,,"[Gu and Dao(2023)]{gu2023mamba} Albert Gu and Tri Dao. 
 Mamba: Linear-time sequence modeling with selective state spaces. 
 \emph{arXiv preprint arXiv:2312.00752}, 2023."
2408.09743,liu2024vmamba,"[Liu et~al.(2024{\natexlab{b}})Liu, Tian, Zhao, Yu, Xie, Wang, Ye, and   Liu]{liu2024vmamba} Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang,   Qixiang Ye, and Yunfan Liu.",Vmamba: Visual state space model.,Vmamba: Visual state space model.,,"[Liu et~al.(2024{\natexlab{b}})Liu, Tian, Zhao, Yu, Xie, Wang, Ye, and   Liu]{liu2024vmamba} Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang,   Qixiang Ye, and Yunfan Liu. 
 Vmamba: Visual state space model. 
 \emph{arXiv preprint arXiv:2401.10166}, 2024{\natexlab{b}}."
2408.09743,wang2024XrayHDMAE,"[Wang et~al.(2024{\natexlab{b}})Wang, Li, Wu, Jin, Rong, Jiang, Li, and   Tang]{wang2024XrayHDMAE} Xiao Wang, Yuehang Li, Wentao Wu, Jiandong Jin, Yao Rong, Bo Jiang, Chuanfu Li,   and Jin Tang.",Pre-training on high definition x-ray images: An experimental study.,Pre-training on high definition x-ray images: An experimental study.,,"[Wang et~al.(2024{\natexlab{b}})Wang, Li, Wu, Jin, Rong, Jiang, Li, and   Tang]{wang2024XrayHDMAE} Xiao Wang, Yuehang Li, Wentao Wu, Jiandong Jin, Yao Rong, Bo Jiang, Chuanfu Li,   and Jin Tang. 
 Pre-training on high definition x-ray images: An experimental study. 
 \emph{arXiv preprint arXiv:2404.17926}, 2024{\natexlab{b}}."
2408.09743,wang2024SSMSurvey,"[Wang et~al.(2024{\natexlab{c}})Wang, Wang, Ding, Li, Wu, Rong, Kong,   Huang, Li, Yang, et~al.]{wang2024SSMSurvey} Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong,   Ju Huang, Shihao Li, Haoxiang Yang, et~al.",State space model for new-generation network alternative to   transformers: A survey.,State space model for new-generation network alternative to   transformers: A survey.,,"[Wang et~al.(2024{\natexlab{c}})Wang, Wang, Ding, Li, Wu, Rong, Kong,   Huang, Li, Yang, et~al.]{wang2024SSMSurvey} Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong,   Ju Huang, Shihao Li, Haoxiang Yang, et~al. 
 State space model for new-generation network alternative to   transformers: A survey. 
 \emph{arXiv preprint arXiv:2404.09516}, 2024{\natexlab{c}}."
2408.09743,zhao2024RAGSurvey,"[Zhao et~al.(2024)Zhao, Zhang, Yu, Wang, Geng, Fu, Yang, Zhang, and   Cui]{zhao2024RAGSurvey} Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng   Fu, Ling Yang, Wentao Zhang, and Bin Cui.",Retrieval-augmented generation for ai-generated content: A survey.,Retrieval-augmented generation for ai-generated content: A survey.,,"[Zhao et~al.(2024)Zhao, Zhang, Yu, Wang, Geng, Fu, Yang, Zhang, and   Cui]{zhao2024RAGSurvey} Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng   Fu, Ling Yang, Wentao Zhang, and Bin Cui. 
 Retrieval-augmented generation for ai-generated content: A survey. 
 \emph{arXiv preprint arXiv:2402.19473}, 2024."
2408.11219,clark2018think,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.","Think you have solved question answering? try arc, the ai2 reasoning challenge.","Think you have solved question answering? try arc, the ai2 reasoning challenge.",,"[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think} Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 
 Think you have solved question answering? try arc, the ai2 reasoning challenge. 
 \emph{arXiv preprint arXiv:1803.05457}, 2018."
2408.11219,longpre2023flan,"[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan} Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.",The flan collection: Designing data and methods for effective instruction tuning.,The flan collection: Designing data and methods for effective instruction tuning.,,"[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan} Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 
 The flan collection: Designing data and methods for effective instruction tuning. 
 \emph{arXiv preprint arXiv:2301.13688}, 2023."
2408.11219,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2408.11219,xia2023sheared,"[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared} Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.",Sheared llama: Accelerating language model pre-training via structured pruning.,Sheared llama: Accelerating language model pre-training via structured pruning.,,"[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{xia2023sheared} Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 
 Sheared llama: Accelerating language model pre-training via structured pruning. 
 \emph{arXiv preprint arXiv:2310.06694}, 2023."
2408.11219,xu2023wizardlm,"[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.",Wizardlm: Empowering large language models to follow complex instructions.,Wizardlm: Empowering large language models to follow complex instructions.,,"[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm} Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 
 Wizardlm: Empowering large language models to follow complex instructions. 
 \emph{arXiv preprint arXiv:2304.12244}, 2023."
2408.11843,bartl2020unmasking,"[Bartl et~al.(2020)Bartl, Nissim, and Gatt]{bartl2020unmasking} M.~Bartl, M.~Nissim, and A.~Gatt.",Unmasking contextual stereotypes: Measuring and mitigating bert's gender bias.,Unmasking contextual stereotypes: Measuring and mitigating bert's gender bias.,,"[Bartl et~al.(2020)Bartl, Nissim, and Gatt]{bartl2020unmasking} M.~Bartl, M.~Nissim, and A.~Gatt. 
 Unmasking contextual stereotypes: Measuring and mitigating bert's gender bias. 
 \emph{arXiv preprint arXiv:2010.14534}, 2020."
2408.11843,bommasani2022trustworthy,[Bommasani and Liang(2022)]{bommasani2022trustworthy} R.~Bommasani and P.~Liang.,Trustworthy social bias measurement.,Trustworthy social bias measurement.,,"[Bommasani and Liang(2022)]{bommasani2022trustworthy} R.~Bommasani and P.~Liang. 
 Trustworthy social bias measurement. 
 \emph{arXiv preprint arXiv:2212.11672}, 2022."
2408.11843,chen2023fast,"[Chen et~al.(2023)Chen, Yang, Xiong, Bai, Hu, Hao, Feng, Zhou, Wu, and Liu]{chen2023fast} R.~Chen, J.~Yang, H.~Xiong, J.~Bai, T.~Hu, J.~Hao, Y.~Feng, J.~T. Zhou, J.~Wu, and Z.~Liu.",Fast model debias with machine unlearning.,Fast model debias with machine unlearning.,,"[Chen et~al.(2023)Chen, Yang, Xiong, Bai, Hu, Hao, Feng, Zhou, Wu, and Liu]{chen2023fast} R.~Chen, J.~Yang, H.~Xiong, J.~Bai, T.~Hu, J.~Hao, Y.~Feng, J.~T. Zhou, J.~Wu, and Z.~Liu. 
 Fast model debias with machine unlearning. 
 \emph{arXiv preprint arXiv:2310.12560}, 2023."
2408.11843,chen2024learnable,"[Chen et~al.(2024{\natexlab{a}})Chen, Hu, Feng, and Liu]{chen2024learnable} R.~Chen, T.~Hu, Y.~Feng, and Z.~Liu.",Learnable privacy neurons localization in language models.,Learnable privacy neurons localization in language models.,,"[Chen et~al.(2024{\natexlab{a}})Chen, Hu, Feng, and Liu]{chen2024learnable} R.~Chen, T.~Hu, Y.~Feng, and Z.~Liu. 
 Learnable privacy neurons localization in language models. 
 \emph{arXiv preprint arXiv:2405.10989}, 2024{\natexlab{a}}."
2408.11843,chen2024large,"[Chen et~al.(2024{\natexlab{b}})Chen, Li, Xiao, and Liu]{chen2024large} R.~Chen, Y.~Li, Z.~Xiao, and Z.~Liu.",Large language model bias mitigation from the perspective of knowledge editing.,Large language model bias mitigation from the perspective of knowledge editing.,,"[Chen et~al.(2024{\natexlab{b}})Chen, Li, Xiao, and Liu]{chen2024large} R.~Chen, Y.~Li, Z.~Xiao, and Z.~Liu. 
 Large language model bias mitigation from the perspective of knowledge editing. 
 \emph{arXiv preprint arXiv:2405.09341}, 2024{\natexlab{b}}."
2408.11843,cheng2021fairfil,"[Cheng et~al.(2021)Cheng, Hao, Yuan, Si, and Carin]{cheng2021fairfil} P.~Cheng, W.~Hao, S.~Yuan, S.~Si, and L.~Carin.",Fairfil: Contrastive neural debiasing method for pretrained text encoders.,Fairfil: Contrastive neural debiasing method for pretrained text encoders.,,"[Cheng et~al.(2021)Cheng, Hao, Yuan, Si, and Carin]{cheng2021fairfil} P.~Cheng, W.~Hao, S.~Yuan, S.~Si, and L.~Carin. 
 Fairfil: Contrastive neural debiasing method for pretrained text encoders. 
 \emph{arXiv preprint arXiv:2103.06413}, 2021."
2408.11843,dai2021knowledge,"[Dai et~al.(2021)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2021knowledge} D.~Dai, L.~Dong, Y.~Hao, Z.~Sui, B.~Chang, and F.~Wei.",Knowledge neurons in pretrained transformers.,Knowledge neurons in pretrained transformers.,,"[Dai et~al.(2021)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2021knowledge} D.~Dai, L.~Dong, Y.~Hao, Z.~Sui, B.~Chang, and F.~Wei. 
 Knowledge neurons in pretrained transformers. 
 \emph{arXiv preprint arXiv:2104.08696}, 2021."
2408.11843,de2021editing,"[De~Cao et~al.(2021)De~Cao, Aziz, and Titov]{de2021editing} N.~De~Cao, W.~Aziz, and I.~Titov.",Editing factual knowledge in language models.,Editing factual knowledge in language models.,,"[De~Cao et~al.(2021)De~Cao, Aziz, and Titov]{de2021editing} N.~De~Cao, W.~Aziz, and I.~Titov. 
 Editing factual knowledge in language models. 
 \emph{arXiv preprint arXiv:2104.08164}, 2021."
2408.11843,devlin2018bert,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.",Bert: Pre-training of deep bidirectional transformers for language understanding.,Bert: Pre-training of deep bidirectional transformers for language understanding.,,"[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert} J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova. 
 Bert: Pre-training of deep bidirectional transformers for language understanding. 
 \emph{arXiv preprint arXiv:1810.04805}, 2018."
2408.11843,dong2022calibrating,"[Dong et~al.(2022)Dong, Dai, Song, Xu, Sui, and Li]{dong2022calibrating} Q.~Dong, D.~Dai, Y.~Song, J.~Xu, Z.~Sui, and L.~Li.",Calibrating factual knowledge in pretrained language models.,Calibrating factual knowledge in pretrained language models.,,"[Dong et~al.(2022)Dong, Dai, Song, Xu, Sui, and Li]{dong2022calibrating} Q.~Dong, D.~Dai, Y.~Song, J.~Xu, Z.~Sui, and L.~Li. 
 Calibrating factual knowledge in pretrained language models. 
 \emph{arXiv preprint arXiv:2210.03329}, 2022."
2408.11843,dong2023co,"[Dong et~al.(2023)Dong, Zhu, Wang, Teleki, and Caverlee]{dong2023co} X.~Dong, Z.~Zhu, Z.~Wang, M.~Teleki, and J.~Caverlee.",Co $^{2}$ pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning.,Co $^{2}$ pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning.,,"[Dong et~al.(2023)Dong, Zhu, Wang, Teleki, and Caverlee]{dong2023co} X.~Dong, Z.~Zhu, Z.~Wang, M.~Teleki, and J.~Caverlee. 
 Co $^{2}$ pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning. 
 \emph{arXiv preprint arXiv:2310.12490}, 2023."
2408.11843,fan2024biasalert,"[Fan et~al.(2024)Fan, Chen, Xu, and Liu]{fan2024biasalert} Z.~Fan, R.~Chen, R.~Xu, and Z.~Liu.",Biasalert: A plug-and-play tool for social bias detection in llms.,Biasalert: A plug-and-play tool for social bias detection in llms.,,"[Fan et~al.(2024)Fan, Chen, Xu, and Liu]{fan2024biasalert} Z.~Fan, R.~Chen, R.~Xu, and Z.~Liu. 
 Biasalert: A plug-and-play tool for social bias detection in llms. 
 \emph{arXiv preprint arXiv:2407.10241}, 2024."
2408.11843,finlayson2021causal,"[Finlayson et~al.(2021)Finlayson, Mueller, Gehrmann, Shieber, Linzen, and Belinkov]{finlayson2021causal} M.~Finlayson, A.~Mueller, S.~Gehrmann, S.~Shieber, T.~Linzen, and Y.~Belinkov.",Causal analysis of syntactic agreement mechanisms in neural language models.,Causal analysis of syntactic agreement mechanisms in neural language models.,,"[Finlayson et~al.(2021)Finlayson, Mueller, Gehrmann, Shieber, Linzen, and Belinkov]{finlayson2021causal} M.~Finlayson, A.~Mueller, S.~Gehrmann, S.~Shieber, T.~Linzen, and Y.~Belinkov. 
 Causal analysis of syntactic agreement mechanisms in neural language models. 
 \emph{arXiv preprint arXiv:2106.06087}, 2021."
2408.11843,gallegos2023bias,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} I.~O. Gallegos, R.~A. Rossi, J.~Barrow, M.~M. Tanjim, S.~Kim, F.~Dernoncourt, T.~Yu, R.~Zhang, and N.~K. Ahmed.",Bias and fairness in large language models: A survey.,Bias and fairness in large language models: A survey.,,"[Gallegos et~al.(2023)Gallegos, Rossi, Barrow, Tanjim, Kim, Dernoncourt, Yu, Zhang, and Ahmed]{gallegos2023bias} I.~O. Gallegos, R.~A. Rossi, J.~Barrow, M.~M. Tanjim, S.~Kim, F.~Dernoncourt, T.~Yu, R.~Zhang, and N.~K. Ahmed. 
 Bias and fairness in large language models: A survey. 
 \emph{arXiv preprint arXiv:2309.00770}, 2023."
2408.11843,gehman2020realtoxicityprompts,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith.",Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,Realtoxicityprompts: Evaluating neural toxic degeneration in language models.,,"[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and Smith]{gehman2020realtoxicityprompts} S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith. 
 Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 
 \emph{arXiv preprint arXiv:2009.11462}, 2020."
2408.11843,geva2020transformer,"[Geva et~al.(2020)Geva, Schuster, Berant, and Levy]{geva2020transformer} M.~Geva, R.~Schuster, J.~Berant, and O.~Levy.",Transformer feed-forward layers are key-value memories.,Transformer feed-forward layers are key-value memories.,,"[Geva et~al.(2020)Geva, Schuster, Berant, and Levy]{geva2020transformer} M.~Geva, R.~Schuster, J.~Berant, and O.~Levy. 
 Transformer feed-forward layers are key-value memories. 
 \emph{arXiv preprint arXiv:2012.14913}, 2020."
2408.11843,geva2022transformer,"[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer} M.~Geva, A.~Caciularu, K.~R. Wang, and Y.~Goldberg.",Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.,Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.,,"[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer} M.~Geva, A.~Caciularu, K.~R. Wang, and Y.~Goldberg. 
 Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. 
 \emph{arXiv preprint arXiv:2203.14680}, 2022."
2408.11843,gupta2023editing,"[Gupta et~al.(2023)Gupta, Mondal, Sheshadri, Zhao, Li, Wiegreffe, and Tandon]{gupta2023editing} A.~Gupta, D.~Mondal, A.~K. Sheshadri, W.~Zhao, X.~L. Li, S.~Wiegreffe, and N.~Tandon.",Editing commonsense knowledge in gpt.,Editing commonsense knowledge in gpt.,,"[Gupta et~al.(2023)Gupta, Mondal, Sheshadri, Zhao, Li, Wiegreffe, and Tandon]{gupta2023editing} A.~Gupta, D.~Mondal, A.~K. Sheshadri, W.~Zhao, X.~L. Li, S.~Wiegreffe, and N.~Tandon. 
 Editing commonsense knowledge in gpt. 
 \emph{arXiv preprint arXiv:2305.14956}, 2023."
2408.11843,han2021diverse,"[Han et~al.(2021)Han, Baldwin, and Cohn]{han2021diverse} X.~Han, T.~Baldwin, and T.~Cohn.",Diverse adversaries for mitigating bias in training.,Diverse adversaries for mitigating bias in training.,,"[Han et~al.(2021)Han, Baldwin, and Cohn]{han2021diverse} X.~Han, T.~Baldwin, and T.~Cohn. 
 Diverse adversaries for mitigating bias in training. 
 \emph{arXiv preprint arXiv:2101.10001}, 2021."
2408.11843,hartvigsen2022aging,"[Hartvigsen et~al.(2022)Hartvigsen, Sankaranarayanan, Palangi, Kim, and Ghassemi]{hartvigsen2022aging} T.~Hartvigsen, S.~Sankaranarayanan, H.~Palangi, Y.~Kim, and M.~Ghassemi.",Aging with grace: Lifelong model editing with discrete key-value adaptors.,Aging with grace: Lifelong model editing with discrete key-value adaptors.,,"[Hartvigsen et~al.(2022)Hartvigsen, Sankaranarayanan, Palangi, Kim, and Ghassemi]{hartvigsen2022aging} T.~Hartvigsen, S.~Sankaranarayanan, H.~Palangi, Y.~Kim, and M.~Ghassemi. 
 Aging with grace: Lifelong model editing with discrete key-value adaptors. 
 \emph{arXiv preprint arXiv:2211.11031}, 2022."
2408.11843,hase2021language,"[Hase et~al.(2021)Hase, Diab, Celikyilmaz, Li, Kozareva, Stoyanov, Bansal, and Iyer]{hase2021language} P.~Hase, M.~Diab, A.~Celikyilmaz, X.~Li, Z.~Kozareva, V.~Stoyanov, M.~Bansal, and S.~Iyer.","Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs.","Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs.",,"[Hase et~al.(2021)Hase, Diab, Celikyilmaz, Li, Kozareva, Stoyanov, Bansal, and Iyer]{hase2021language} P.~Hase, M.~Diab, A.~Celikyilmaz, X.~Li, Z.~Kozareva, V.~Stoyanov, M.~Bansal, and S.~Iyer. 
 Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. 
 \emph{arXiv preprint arXiv:2111.13654}, 2021."
2408.11843,he2022mabel,"[He et~al.(2022)He, Xia, Fellbaum, and Chen]{he2022mabel} J.~He, M.~Xia, C.~Fellbaum, and D.~Chen.",Mabel: Attenuating gender bias using textual entailment data.,Mabel: Attenuating gender bias using textual entailment data.,,"[He et~al.(2022)He, Xia, Fellbaum, and Chen]{he2022mabel} J.~He, M.~Xia, C.~Fellbaum, and D.~Chen. 
 Mabel: Attenuating gender bias using textual entailment data. 
 \emph{arXiv preprint arXiv:2210.14975}, 2022."
2408.11843,huang2023transformer,"[Huang et~al.(2023)Huang, Shen, Zhang, Zhou, Rong, and Xiong]{huang2023transformer} Z.~Huang, Y.~Shen, X.~Zhang, J.~Zhou, W.~Rong, and Z.~Xiong.",Transformer-patcher: One mistake worth one neuron.,Transformer-patcher: One mistake worth one neuron.,,"[Huang et~al.(2023)Huang, Shen, Zhang, Zhou, Rong, and Xiong]{huang2023transformer} Z.~Huang, Y.~Shen, X.~Zhang, J.~Zhou, W.~Rong, and Z.~Xiong. 
 Transformer-patcher: One mistake worth one neuron. 
 \emph{arXiv preprint arXiv:2301.09785}, 2023."
2408.11843,kaneko2021debiasing,[Kaneko and Bollegala(2021)]{kaneko2021debiasing} M.~Kaneko and D.~Bollegala.,Debiasing pre-trained contextualised embeddings.,Debiasing pre-trained contextualised embeddings.,,"[Kaneko and Bollegala(2021)]{kaneko2021debiasing} M.~Kaneko and D.~Bollegala. 
 Debiasing pre-trained contextualised embeddings. 
 \emph{arXiv preprint arXiv:2101.09523}, 2021."
2408.11843,kumar2022language,"[Kumar et~al.(2022)Kumar, Balachandran, Njoo, Anastasopoulos, and Tsvetkov]{kumar2022language} S.~Kumar, V.~Balachandran, L.~Njoo, A.~Anastasopoulos, and Y.~Tsvetkov.",Language generation models can cause harm: So what can we do about it? an actionable survey.,Language generation models can cause harm: So what can we do about it? an actionable survey.,,"[Kumar et~al.(2022)Kumar, Balachandran, Njoo, Anastasopoulos, and Tsvetkov]{kumar2022language} S.~Kumar, V.~Balachandran, L.~Njoo, A.~Anastasopoulos, and Y.~Tsvetkov. 
 Language generation models can cause harm: So what can we do about it? an actionable survey. 
 \emph{arXiv preprint arXiv:2210.07700}, 2022."
2408.11843,lauscher2021sustainable,"[Lauscher et~al.(2021)Lauscher, Lueken, and Glava{\v{s}}]{lauscher2021sustainable} A.~Lauscher, T.~Lueken, and G.~Glava{\v{s}}.",Sustainable modular debiasing of language models.,Sustainable modular debiasing of language models.,,"[Lauscher et~al.(2021)Lauscher, Lueken, and Glava{\v{s}}]{lauscher2021sustainable} A.~Lauscher, T.~Lueken, and G.~Glava{\v{s}}. 
 Sustainable modular debiasing of language models. 
 \emph{arXiv preprint arXiv:2109.03646}, 2021."
2408.11843,levy2017zero,"[Levy et~al.(2017)Levy, Seo, Choi, and Zettlemoyer]{levy2017zero} O.~Levy, M.~Seo, E.~Choi, and L.~Zettlemoyer.",Zero-shot relation extraction via reading comprehension.,Zero-shot relation extraction via reading comprehension.,,"[Levy et~al.(2017)Levy, Seo, Choi, and Zettlemoyer]{levy2017zero} O.~Levy, M.~Seo, E.~Choi, and L.~Zettlemoyer. 
 Zero-shot relation extraction via reading comprehension. 
 \emph{arXiv preprint arXiv:1706.04115}, 2017."
2408.11843,li2023pmet,"[Li et~al.(2023{\natexlab{a}})Li, Li, Song, Yang, Ma, and Yu]{li2023pmet} X.~Li, S.~Li, S.~Song, J.~Yang, J.~Ma, and J.~Yu.",Pmet: Precise model editing in a transformer.,Pmet: Precise model editing in a transformer.,,"[Li et~al.(2023{\natexlab{a}})Li, Li, Song, Yang, Ma, and Yu]{li2023pmet} X.~Li, S.~Li, S.~Song, J.~Yang, J.~Ma, and J.~Yu. 
 Pmet: Precise model editing in a transformer. 
 \emph{arXiv preprint arXiv:2308.08742}, 2023{\natexlab{a}}."
2408.11843,li2023prompt,"[Li et~al.(2023{\natexlab{b}})Li, Du, Wang, and Wang]{li2023prompt} Y.~Li, M.~Du, X.~Wang, and Y.~Wang.","Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases.","Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases.",,"[Li et~al.(2023{\natexlab{b}})Li, Du, Wang, and Wang]{li2023prompt} Y.~Li, M.~Du, X.~Wang, and Y.~Wang. 
 Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases. 
 \emph{arXiv preprint arXiv:2307.01595}, 2023{\natexlab{b}}."
2408.11843,liang2020towards,"[Liang et~al.(2020)Liang, Li, Zheng, Lim, Salakhutdinov, and Morency]{liang2020towards} P.~P. Liang, I.~M. Li, E.~Zheng, Y.~C. Lim, R.~Salakhutdinov, and L.-P. Morency.",Towards debiasing sentence representations.,Towards debiasing sentence representations.,,"[Liang et~al.(2020)Liang, Li, Zheng, Lim, Salakhutdinov, and Morency]{liang2020towards} P.~P. Liang, I.~M. Li, E.~Zheng, Y.~C. Lim, R.~Salakhutdinov, and L.-P. Morency. 
 Towards debiasing sentence representations. 
 \emph{arXiv preprint arXiv:2007.08100}, 2020."
2408.11843,may2019measuring,"[May et~al.(2019)May, Wang, Bordia, Bowman, and Rudinger]{may2019measuring} C.~May, A.~Wang, S.~Bordia, S.~R. Bowman, and R.~Rudinger.",On measuring social biases in sentence encoders.,On measuring social biases in sentence encoders.,,"[May et~al.(2019)May, Wang, Bordia, Bowman, and Rudinger]{may2019measuring} C.~May, A.~Wang, S.~Bordia, S.~R. Bowman, and R.~Rudinger. 
 On measuring social biases in sentence encoders. 
 \emph{arXiv preprint arXiv:1903.10561}, 2019."
2408.11843,meng2022mass,"[Meng et~al.(2022{\natexlab{b}})Meng, Sharma, Andonian, Belinkov, and Bau]{meng2022mass} K.~Meng, A.~S. Sharma, A.~Andonian, Y.~Belinkov, and D.~Bau.",Mass-editing memory in a transformer.,Mass-editing memory in a transformer.,,"[Meng et~al.(2022{\natexlab{b}})Meng, Sharma, Andonian, Belinkov, and Bau]{meng2022mass} K.~Meng, A.~S. Sharma, A.~Andonian, Y.~Belinkov, and D.~Bau. 
 Mass-editing memory in a transformer. 
 \emph{arXiv preprint arXiv:2210.07229}, 2022{\natexlab{b}}."
2408.11843,mitchell2021fast,"[Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2021fast} E.~Mitchell, C.~Lin, A.~Bosselut, C.~Finn, and C.~D. Manning.",Fast model editing at scale.,Fast model editing at scale.,,"[Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2021fast} E.~Mitchell, C.~Lin, A.~Bosselut, C.~Finn, and C.~D. Manning. 
 Fast model editing at scale. 
 \emph{arXiv preprint arXiv:2110.11309}, 2021."
2408.11843,murty2022fixing,"[Murty et~al.(2022)Murty, Manning, Lundberg, and Ribeiro]{murty2022fixing} S.~Murty, C.~D. Manning, S.~Lundberg, and M.~T. Ribeiro.",Fixing model bugs with natural language patches.,Fixing model bugs with natural language patches.,,"[Murty et~al.(2022)Murty, Manning, Lundberg, and Ribeiro]{murty2022fixing} S.~Murty, C.~D. Manning, S.~Lundberg, and M.~T. Ribeiro. 
 Fixing model bugs with natural language patches. 
 \emph{arXiv preprint arXiv:2211.03318}, 2022."
2408.11843,nadeem2020stereoset,"[Nadeem et~al.(2020{\natexlab{a}})Nadeem, Bethke, and Reddy]{nadeem2020stereoset} M.~Nadeem, A.~Bethke, and S.~Reddy.",Stereoset: Measuring stereotypical bias in pretrained language models.,Stereoset: Measuring stereotypical bias in pretrained language models.,,"[Nadeem et~al.(2020{\natexlab{a}})Nadeem, Bethke, and Reddy]{nadeem2020stereoset} M.~Nadeem, A.~Bethke, and S.~Reddy. 
 Stereoset: Measuring stereotypical bias in pretrained language models. 
 \emph{arXiv preprint arXiv:2004.09456}, 2020{\natexlab{a}}."
2408.11843,nangia2020crows,"[Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman]{nangia2020crows} N.~Nangia, C.~Vania, R.~Bhalerao, and S.~R. Bowman.",Crows-pairs: A challenge dataset for measuring social biases in masked language models.,Crows-pairs: A challenge dataset for measuring social biases in masked language models.,,"[Nangia et~al.(2020)Nangia, Vania, Bhalerao, and Bowman]{nangia2020crows} N.~Nangia, C.~Vania, R.~Bhalerao, and S.~R. Bowman. 
 Crows-pairs: A challenge dataset for measuring social biases in masked language models. 
 \emph{arXiv preprint arXiv:2010.00133}, 2020."
2408.11843,petroni2019language,"[Petroni et~al.(2019)Petroni, Rockt{\""a}schel, Lewis, Bakhtin, Wu, Miller, and Riedel]{petroni2019language} F.~Petroni, T.~Rockt{\""a}schel, P.~Lewis, A.~Bakhtin, Y.~Wu, A.~H. Miller, and S.~Riedel.",Language models as knowledge bases?,Language models as knowledge bases?,,"[Petroni et~al.(2019)Petroni, Rockt{\""a}schel, Lewis, Bakhtin, Wu, Miller, and Riedel]{petroni2019language} F.~Petroni, T.~Rockt{\""a}schel, P.~Lewis, A.~Bakhtin, Y.~Wu, A.~H. Miller, and S.~Riedel. 
 Language models as knowledge bases? 
 \emph{arXiv preprint arXiv:1909.01066}, 2019."
2408.11843,ravfogel2020null,"[Ravfogel et~al.(2020)Ravfogel, Elazar, Gonen, Twiton, and Goldberg]{ravfogel2020null} S.~Ravfogel, Y.~Elazar, H.~Gonen, M.~Twiton, and Y.~Goldberg.",Null it out: Guarding protected attributes by iterative nullspace projection.,Null it out: Guarding protected attributes by iterative nullspace projection.,,"[Ravfogel et~al.(2020)Ravfogel, Elazar, Gonen, Twiton, and Goldberg]{ravfogel2020null} S.~Ravfogel, Y.~Elazar, H.~Gonen, M.~Twiton, and Y.~Goldberg. 
 Null it out: Guarding protected attributes by iterative nullspace projection. 
 \emph{arXiv preprint arXiv:2004.07667}, 2020."
2408.11843,rudinger2018gender,"[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{rudinger2018gender} R.~Rudinger, J.~Naradowsky, B.~Leonard, and B.~Van~Durme.",Gender bias in coreference resolution.,Gender bias in coreference resolution.,,"[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and Van~Durme]{rudinger2018gender} R.~Rudinger, J.~Naradowsky, B.~Leonard, and B.~Van~Durme. 
 Gender bias in coreference resolution. 
 \emph{arXiv preprint arXiv:1804.09301}, 2018."
2408.11843,sahoo2022detecting,"[Sahoo et~al.(2022)Sahoo, Gupta, and Bhattacharyya]{sahoo2022detecting} N.~Sahoo, H.~Gupta, and P.~Bhattacharyya.",Detecting unintended social bias in toxic language datasets.,Detecting unintended social bias in toxic language datasets.,,"[Sahoo et~al.(2022)Sahoo, Gupta, and Bhattacharyya]{sahoo2022detecting} N.~Sahoo, H.~Gupta, and P.~Bhattacharyya. 
 Detecting unintended social bias in toxic language datasets. 
 \emph{arXiv preprint arXiv:2210.11762}, 2022."
2408.11843,sheng2021societal,"[Sheng et~al.(2021)Sheng, Chang, Natarajan, and Peng]{sheng2021societal} E.~Sheng, K.-W. Chang, P.~Natarajan, and N.~Peng.",Societal biases in language generation: Progress and challenges.,Societal biases in language generation: Progress and challenges.,,"[Sheng et~al.(2021)Sheng, Chang, Natarajan, and Peng]{sheng2021societal} E.~Sheng, K.-W. Chang, P.~Natarajan, and N.~Peng. 
 Societal biases in language generation: Progress and challenges. 
 \emph{arXiv preprint arXiv:2105.04054}, 2021."
2408.11843,sinitsin2020editable,"[Sinitsin et~al.(2020)Sinitsin, Plokhotnyuk, Pyrkin, Popov, and Babenko]{sinitsin2020editable} A.~Sinitsin, V.~Plokhotnyuk, D.~Pyrkin, S.~Popov, and A.~Babenko.",Editable neural networks.,Editable neural networks.,,"[Sinitsin et~al.(2020)Sinitsin, Plokhotnyuk, Pyrkin, Popov, and Babenko]{sinitsin2020editable} A.~Sinitsin, V.~Plokhotnyuk, D.~Pyrkin, S.~Popov, and A.~Babenko. 
 Editable neural networks. 
 \emph{arXiv preprint arXiv:2004.00345}, 2020."
2408.11843,touvron2023llama,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama} H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}, 2023."
2408.11843,wang2018glue,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.",Glue: A multi-task benchmark and analysis platform for natural language understanding.,Glue: A multi-task benchmark and analysis platform for natural language understanding.,,"[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue} A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman. 
 Glue: A multi-task benchmark and analysis platform for natural language understanding. 
 \emph{arXiv preprint arXiv:1804.07461}, 2018."
2408.11843,wang2023decodingtrust,"[Wang et~al.(2023)Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} B.~Wang, W.~Chen, H.~Pei, C.~Xie, M.~Kang, C.~Zhang, C.~Xu, Z.~Xiong, R.~Dutta, R.~Schaeffer, et~al.",Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.,,"[Wang et~al.(2023)Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust} B.~Wang, W.~Chen, H.~Pei, C.~Xie, M.~Kang, C.~Zhang, C.~Xu, Z.~Xiong, R.~Dutta, R.~Schaeffer, et~al. 
 Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. 
 \emph{arXiv preprint arXiv:2306.11698}, 2023."
2408.11843,webster2020measuring,"[Webster et~al.(2020)Webster, Wang, Tenney, Beutel, Pitler, Pavlick, Chen, Chi, and Petrov]{webster2020measuring} K.~Webster, X.~Wang, I.~Tenney, A.~Beutel, E.~Pitler, E.~Pavlick, J.~Chen, E.~Chi, and S.~Petrov.",Measuring and reducing gendered correlations in pre-trained models.,Measuring and reducing gendered correlations in pre-trained models.,,"[Webster et~al.(2020)Webster, Wang, Tenney, Beutel, Pitler, Pavlick, Chen, Chi, and Petrov]{webster2020measuring} K.~Webster, X.~Wang, I.~Tenney, A.~Beutel, E.~Pitler, E.~Pavlick, J.~Chen, E.~Chi, and S.~Petrov. 
 Measuring and reducing gendered correlations in pre-trained models. 
 \emph{arXiv preprint arXiv:2010.06032}, 2020."
2408.11843,xie2023empirical,[Xie and Lukasiewicz(2023)]{xie2023empirical} Z.~Xie and T.~Lukasiewicz.,An empirical analysis of parameter-efficient methods for debiasing pre-trained language models.,An empirical analysis of parameter-efficient methods for debiasing pre-trained language models.,,"[Xie and Lukasiewicz(2023)]{xie2023empirical} Z.~Xie and T.~Lukasiewicz. 
 An empirical analysis of parameter-efficient methods for debiasing pre-trained language models. 
 \emph{arXiv preprint arXiv:2306.04067}, 2023."
2408.11843,yu2022hate,"[Yu et~al.(2022)Yu, Blanco, and Hong]{yu2022hate} X.~Yu, E.~Blanco, and L.~Hong.",Hate speech and counter speech detection: Conversational context does matter.,Hate speech and counter speech detection: Conversational context does matter.,,"[Yu et~al.(2022)Yu, Blanco, and Hong]{yu2022hate} X.~Yu, E.~Blanco, and L.~Hong. 
 Hate speech and counter speech detection: Conversational context does matter. 
 \emph{arXiv preprint arXiv:2206.06423}, 2022."
2408.11843,zhao2018gender,"[Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and Chang]{zhao2018gender} J.~Zhao, T.~Wang, M.~Yatskar, V.~Ordonez, and K.-W. Chang.",Gender bias in coreference resolution: Evaluation and debiasing methods.,Gender bias in coreference resolution: Evaluation and debiasing methods.,,"[Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and Chang]{zhao2018gender} J.~Zhao, T.~Wang, M.~Yatskar, V.~Ordonez, and K.-W. Chang. 
 Gender bias in coreference resolution: Evaluation and debiasing methods. 
 \emph{arXiv preprint arXiv:1804.06876}, 2018."
2408.11843,zhao2019gender,"[Zhao et~al.(2019)Zhao, Wang, Yatskar, Cotterell, Ordonez, and Chang]{zhao2019gender} J.~Zhao, T.~Wang, M.~Yatskar, R.~Cotterell, V.~Ordonez, and K.-W. Chang.",Gender bias in contextualized word embeddings.,Gender bias in contextualized word embeddings.,,"[Zhao et~al.(2019)Zhao, Wang, Yatskar, Cotterell, Ordonez, and Chang]{zhao2019gender} J.~Zhao, T.~Wang, M.~Yatskar, R.~Cotterell, V.~Ordonez, and K.-W. Chang. 
 Gender bias in contextualized word embeddings. 
 \emph{arXiv preprint arXiv:1904.03310}, 2019."
2408.11843,zheng2023can,"[Zheng et~al.(2023)Zheng, Li, Dong, Fan, Wu, Xu, and Chang]{zheng2023can} C.~Zheng, L.~Li, Q.~Dong, Y.~Fan, Z.~Wu, J.~Xu, and B.~Chang.",Can we edit factual knowledge by in-context learning?,Can we edit factual knowledge by in-context learning?,,"[Zheng et~al.(2023)Zheng, Li, Dong, Fan, Wu, Xu, and Chang]{zheng2023can} C.~Zheng, L.~Li, Q.~Dong, Y.~Fan, Z.~Wu, J.~Xu, and B.~Chang. 
 Can we edit factual knowledge by in-context learning? 
 \emph{arXiv preprint arXiv:2305.12740}, 2023."
2408.11843,zmigrod2019counterfactual,"[Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell]{zmigrod2019counterfactual} R.~Zmigrod, S.~J. Mielke, H.~Wallach, and R.~Cotterell.",Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology.,,"[Zmigrod et~al.(2019)Zmigrod, Mielke, Wallach, and Cotterell]{zmigrod2019counterfactual} R.~Zmigrod, S.~J. Mielke, H.~Wallach, and R.~Cotterell. 
 Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. 
 \emph{arXiv preprint arXiv:1906.04571}, 2019."
2408.11854,behnamghader2024llm2vec,"[{BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy}]{behnamghader2024llm2vec} Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024.",Llm2vec: Large language models are secretly powerful text encoders.,Llm2vec: Large language models are secretly powerful text encoders.,,"[{BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy}]{behnamghader2024llm2vec} Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. 
 Llm2vec: Large language models are secretly powerful text encoders. 
 \emph{arXiv preprint arXiv:2404.05961}."
2408.11854,chen2023meditron,"[{Chen et~al.(2023)Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{chen2023meditron} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023.",Meditron-70b: Scaling medical pretraining for large language models.,Meditron-70b: Scaling medical pretraining for large language models.,,"[{Chen et~al.(2023)Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\""o}pf, Mohtashami et~al.}]{chen2023meditron} Zeming Chen, Alejandro~Hern{\'a}ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K{\""o}pf, Amirkeivan Mohtashami, et~al. 2023. 
 Meditron-70b: Scaling medical pretraining for large language models. 
 \emph{arXiv preprint arXiv:2311.16079}."
2408.11854,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.11854,lee2024gecko,"[{Lee et~al.(2024)Lee, Dai, Ren, Chen, Cer, Cole, Hui, Boratko, Kapadia, Ding et~al.}]{lee2024gecko} Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy~R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et~al. 2024.",Gecko: Versatile text embeddings distilled from large language models.,Gecko: Versatile text embeddings distilled from large language models.,,"[{Lee et~al.(2024)Lee, Dai, Ren, Chen, Cer, Cole, Hui, Boratko, Kapadia, Ding et~al.}]{lee2024gecko} Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy~R Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et~al. 2024. 
 Gecko: Versatile text embeddings distilled from large language models. 
 \emph{arXiv preprint arXiv:2403.20327}."
2408.11854,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2408.11854,zhu2024prompting,"[{Zhu et~al.(2024)Zhu, Wang, Gao, Tong, An, Liao, Harrison, Ma, and Pan}]{zhu2024prompting} Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen~M Harrison, Liantao Ma, and Chengwei Pan. 2024.",Prompting large language models for zero-shot clinical prediction with structured longitudinal electronic health record data.,Prompting large language models for zero-shot clinical prediction with structured longitudinal electronic health record data.,,"[{Zhu et~al.(2024)Zhu, Wang, Gao, Tong, An, Liao, Harrison, Ma, and Pan}]{zhu2024prompting} Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen~M Harrison, Liantao Ma, and Chengwei Pan. 2024. 
 Prompting large language models for zero-shot clinical prediction with structured longitudinal electronic health record data. 
 \emph{arXiv preprint arXiv:2402.01713}."
2408.12337,phi3,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla,   Bach, Bahree, Bakhtiari, Behl et~al.}]{phi3} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,   Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,   et~al. 2024.",{Phi-3 Technical Report: A Highly Capable Language Model Locally on   Your Phone}.,{Phi-3 Technical Report: A Highly Capable Language Model Locally on   Your Phone}.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla,   Bach, Bahree, Bakhtiari, Behl et~al.}]{phi3} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,   Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,   et~al. 2024. 
 {Phi-3 Technical Report: A Highly Capable Language Model Locally on   Your Phone}. 
 \emph{arXiv preprint arXiv:2404.14219}."
2408.12337,evaluating_llm_code,"[{Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Ponde,   Kaplan, Edwards, Burda et~al.}]{evaluating_llm_code} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan,   Harrison Edwards, Yura Burda, et~al. 2021{\natexlab{a}}.",Evaluating {L}arge {L}anguage {M}odels {T}rained on {C}ode.,Evaluating {L}arge {L}anguage {M}odels {T}rained on {C}ode.,,"[{Chen et~al.(2021{\natexlab{a}})Chen, Tworek, Jun, Yuan, Ponde,   Kaplan, Edwards, Burda et~al.}]{evaluating_llm_code} Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan,   Harrison Edwards, Yura Burda, et~al. 2021{\natexlab{a}}. 
 Evaluating {L}arge {L}anguage {M}odels {T}rained on {C}ode. 
 \emph{arXiv preprint arXiv:2107.03374}."
2408.12337,chen,[{Chen(2022)}]{chen} Wenhu Chen. 2022.,Large {L}anguage {M}odels are few (1)-shot {T}able {R}easoners.,Large {L}anguage {M}odels are few (1)-shot {T}able {R}easoners.,,"[{Chen(2022)}]{chen} Wenhu Chen. 2022. 
 Large {L}anguage {M}odels are few (1)-shot {T}able {R}easoners. 
 \emph{arXiv preprint arXiv:2210.06710}."
2408.12337,convfinqa,"[{Chen et~al.(2022)Chen, Li, Smiley, Ma, Shah, and Wang}]{convfinqa} Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and   William~Yang Wang. 2022.",{ConvFinQA}: {E}xploring the {C}hain of {N}umerical {R}easoning in   {C}onversational {F}inance {Q}uestion {A}nswering.,{ConvFinQA}: {E}xploring the {C}hain of {N}umerical {R}easoning in   {C}onversational {F}inance {Q}uestion {A}nswering.,,"[{Chen et~al.(2022)Chen, Li, Smiley, Ma, Shah, and Wang}]{convfinqa} Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and   William~Yang Wang. 2022. 
 {ConvFinQA}: {E}xploring the {C}hain of {N}umerical {R}easoning in   {C}onversational {F}inance {Q}uestion {A}nswering. 
 \emph{arXiv preprint arXiv:2210.03849}."
2408.12337,improving_alignment,"[{Glaese et~al.(2022)Glaese, McAleese, Tr{\k{e}}bacz, Aslanides,   Firoiu, Ewalds, Rauh, Weidinger, Chadwick, Thacker   et~al.}]{improving_alignment} Amelia Glaese, Nat McAleese, Maja Tr{\k{e}}bacz, John Aslanides, Vlad Firoiu,   Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker,   et~al. 2022.",Improving alignment of dialogue agents via targeted human judgements.,Improving alignment of dialogue agents via targeted human judgements.,,"[{Glaese et~al.(2022)Glaese, McAleese, Tr{\k{e}}bacz, Aslanides,   Firoiu, Ewalds, Rauh, Weidinger, Chadwick, Thacker   et~al.}]{improving_alignment} Amelia Glaese, Nat McAleese, Maja Tr{\k{e}}bacz, John Aslanides, Vlad Firoiu,   Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker,   et~al. 2022. 
 Improving alignment of dialogue agents via targeted human judgements. 
 \emph{arXiv preprint arXiv:2209.14375}."
2408.12337,tora,"[{Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen   et~al.}]{tora} Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan,   Weizhu Chen, et~al. 2023.",To{RA}: {A} {T}ool-{I}ntegrated {R}easoning {A}gent for   {M}athematical {P}roblem {S}olving.,Tool-{Integrated {Reasoning {Agent for   {Mathematical {Problem {Solving.,,"[{Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen   et~al.}]{tora} Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan,   Weizhu Chen, et~al. 2023. 
 To{RA}: {A} {T}ool-{I}ntegrated {R}easoning {A}gent for   {M}athematical {P}roblem {S}olving. 
 \emph{arXiv preprint arXiv:2309.17452}."
2408.12337,distill_step_by_step,"[{Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna,   Lee, and Pfister}]{distill_step_by_step} Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,   Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.",Distilling {S}tep-by-{S}tep! {O}utperforming {L}arger {L}anguage   {M}odels with {L}ess {T}raining {D}ata and {S}maller {M}odel {S}izes.,Distilling {S}tep-by-{S}tep! {O}utperforming {L}arger {L}anguage   {M}odels with {L}ess {T}raining {D}ata and {S}maller {M}odel {S}izes.,,"[{Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna,   Lee, and Pfister}]{distill_step_by_step} Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,   Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. 
 Distilling {S}tep-by-{S}tep! {O}utperforming {L}arger {L}anguage   {M}odels with {L}ess {T}raining {D}ata and {S}maller {M}odel {S}izes. 
 \emph{arXiv preprint arXiv:2305.02301}."
2408.12337,scaling_laws,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei}]{scaling_laws} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",Scaling {L}aws for {N}eural {L}anguage {M}odels.,Scaling {L}aws for {N}eural {L}anguage {M}odels.,,"[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,   Gray, Radford, Wu, and Amodei}]{scaling_laws} Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon   Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 
 Scaling {L}aws for {N}eural {L}anguage {M}odels. 
 \emph{arXiv preprint arXiv:2001.08361}."
2408.12337,orca2,"[{Mitra et~al.(2023)Mitra, Del~Corro, Mahajan, Codas, Simoes, Agarwal,   Chen, Razdaibiedina, Jones, Aggarwal et~al.}]{orca2} Arindam Mitra, Luciano Del~Corro, Shweti Mahajan, Andres Codas, Clarisse   Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti   Aggarwal, et~al. 2023.",Orca 2: {T}eaching {S}mall {L}anguage {M}odels {H}ow to {R}eason.,Orca 2: {T}eaching {S}mall {L}anguage {M}odels {H}ow to {R}eason.,,"[{Mitra et~al.(2023)Mitra, Del~Corro, Mahajan, Codas, Simoes, Agarwal,   Chen, Razdaibiedina, Jones, Aggarwal et~al.}]{orca2} Arindam Mitra, Luciano Del~Corro, Shweti Mahajan, Andres Codas, Clarisse   Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti   Aggarwal, et~al. 2023. 
 Orca 2: {T}eaching {S}mall {L}anguage {M}odels {H}ow to {R}eason. 
 \emph{arXiv preprint arXiv:2311.11045}."
2408.12337,orca,"[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,   and Awadallah}]{orca} Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid   Palangi, and Ahmed Awadallah. 2023.",Orca: {P}rogressive {L}earning from {C}omplex {E}xplanation {T}races   of {GPT}-4.,Orca: {P}rogressive {L}earning from {C}omplex {E}xplanation {T}races   of {GPT}-4.,,"[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,   and Awadallah}]{orca} Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid   Palangi, and Ahmed Awadallah. 2023. 
 Orca: {P}rogressive {L}earning from {C}omplex {E}xplanation {T}races   of {GPT}-4. 
 \emph{arXiv preprint arXiv:2306.02707}."
2408.12337,zspot,"[{Phogat et~al.(2023)Phogat, Harsha, Dasaratha, Ramakrishna, and   Puranam}]{zspot} Karmvir~Singh Phogat, Chetan Harsha, Sridhar Dasaratha, Shashishekar   Ramakrishna, and Sai~Akhil Puranam. 2023.",Zero-{S}hot {Q}uestion {A}nswering over {F}inancial {D}ocuments using   {L}arge {L}anguage {M}odels.,Zero-{S}hot {Q}uestion {A}nswering over {F}inancial {D}ocuments using   {L}arge {L}anguage {M}odels.,,"[{Phogat et~al.(2023)Phogat, Harsha, Dasaratha, Ramakrishna, and   Puranam}]{zspot} Karmvir~Singh Phogat, Chetan Harsha, Sridhar Dasaratha, Shashishekar   Ramakrishna, and Sai~Akhil Puranam. 2023. 
 Zero-{S}hot {Q}uestion {A}nswering over {F}inancial {D}ocuments using   {L}arge {L}anguage {M}odels. 
 \emph{arXiv preprint arXiv:2311.14722}."
2408.12337,assess_fqa,"[{Srivastava et~al.(2024)Srivastava, Malik, and Ganu}]{assess_fqa} Pragya Srivastava, Manuj Malik, and Tanuja Ganu. 2024.",{Assessing LLMs' Mathematical Reasoning in Financial Document   Question Answering}.,{Assessing LLMs' Mathematical Reasoning in Financial Document   Question Answering}.,,"[{Srivastava et~al.(2024)Srivastava, Malik, and Ganu}]{assess_fqa} Pragya Srivastava, Manuj Malik, and Tanuja Ganu. 2024. 
 {Assessing LLMs' Mathematical Reasoning in Financial Document   Question Answering}. 
 \emph{arXiv preprint arXiv:2402.11194}."
2408.12337,llm_tool,[{Theuma and Shareghi(2024)}]{llm_tool} Adrian Theuma and Ehsan Shareghi. 2024.,{Equipping Language Models with Tool Use Capability for Tabular Data   Analysis in Finance}.,{Equipping Language Models with Tool Use Capability for Tabular Data   Analysis in Finance}.,,"[{Theuma and Shareghi(2024)}]{llm_tool} Adrian Theuma and Ehsan Shareghi. 2024. 
 {Equipping Language Models with Tool Use Capability for Tabular Data   Analysis in Finance}. 
 \emph{arXiv preprint arXiv:2401.15328}."
2408.12337,lamda,"[{Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,   Kulshreshtha, Cheng, Jin, Bos, Baker, Du et~al.}]{lamda} Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv   Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,   et~al. 2022.",La{MDA}: {L}anguage {M}odels for {D}ialog {A}pplications.,La{MDA}: {L}anguage {M}odels for {D}ialog {A}pplications.,,"[{Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,   Kulshreshtha, Cheng, Jin, Bos, Baker, Du et~al.}]{lamda} Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv   Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,   et~al. 2022. 
 La{MDA}: {L}anguage {M}odels for {D}ialog {A}pplications. 
 \emph{arXiv preprint arXiv:2201.08239}."
2408.12337,openmath_instruct,"[{Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia,   and Gitman}]{openmath_instruct} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and   Igor Gitman. 2024.",Open{M}ath{I}nstruct-1: {A} 1.8 {M}illion {M}ath {I}nstruction   {T}uning {D}ataset.,Open{M}ath{I}nstruct-1: {A} 1.8 {M}illion {M}ath {I}nstruction   {T}uning {D}ataset.,,"[{Toshniwal et~al.(2024)Toshniwal, Moshkov, Narenthiran, Gitman, Jia,   and Gitman}]{openmath_instruct} Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and   Igor Gitman. 2024. 
 Open{M}ath{I}nstruct-1: {A} 1.8 {M}illion {M}ath {I}nstruction   {T}uning {D}ataset. 
 \emph{arXiv preprint arXiv:2402.10176}."
2408.12337,mathcoder,"[{Wang et~al.(2023{\natexlab{a}})Wang, Ren, Zhou, Lu, Luo, Shi, Zhang,   Song, Zhan, and Li}]{mathcoder} Ke~Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui   Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023{\natexlab{a}}.",Math{C}oder: {S}eamless {C}ode {I}ntegration in {LLM}s for {E}nhanced   {M}athematical {R}easoning.,Math{C}oder: {S}eamless {C}ode {I}ntegration in {LLM}s for {E}nhanced   {M}athematical {R}easoning.,,"[{Wang et~al.(2023{\natexlab{a}})Wang, Ren, Zhou, Lu, Luo, Shi, Zhang,   Song, Zhan, and Li}]{mathcoder} Ke~Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui   Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023{\natexlab{a}}. 
 Math{C}oder: {S}eamless {C}ode {I}ntegration in {LLM}s for {E}nhanced   {M}athematical {R}easoning. 
 \emph{arXiv preprint arXiv:2310.03731}."
2408.12337,codet5,"[{Wang et~al.(2023{\natexlab{c}})Wang, Le, Gotmare, Bui, Li, and   Hoi}]{codet5} Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi~DQ Bui, Junnan Li, and   Steven~CH Hoi. 2023{\natexlab{c}}.",{CodeT5+: Open Code Large Language Models for Code Understanding and   Generation}.,{CodeT5+: Open Code Large Language Models for Code Understanding and   Generation}.,,"[{Wang et~al.(2023{\natexlab{c}})Wang, Le, Gotmare, Bui, Li, and   Hoi}]{codet5} Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi~DQ Bui, Junnan Li, and   Steven~CH Hoi. 2023{\natexlab{c}}. 
 {CodeT5+: Open Code Large Language Models for Code Understanding and   Generation}. 
 \emph{arXiv preprint arXiv:2305.07922}."
2408.12337,llm_judge,"[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,   Li, Xing, Zhang, Gonzalez, and Stoica}]{llm_judge} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao   Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric~P. Xing, Hao Zhang, Joseph~E.   Gonzalez, and Ion Stoica. 2023.",{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}.,{Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}.,,"[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,   Li, Xing, Zhang, Gonzalez, and Stoica}]{llm_judge} Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao   Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric~P. Xing, Hao Zhang, Joseph~E.   Gonzalez, and Ion Stoica. 2023. 
 {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}. 
 \emph{arXiv preprint arXiv:2306.05685}."
2408.13654,abdin2024phi,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.",Phi-3 technical report: A highly capable language model locally on your phone.,Phi-3 technical report: A highly capable language model locally on your phone.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024. 
 Phi-3 technical report: A highly capable language model locally on your phone. 
 \emph{arXiv preprint arXiv:2404.14219}."
2408.13654,berglund2023reversal,"[{Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans}]{berglund2023reversal} Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023.","The reversal curse: Llms trained on"" a is b"" fail to learn"" b is a"".","The reversal curse: Llms trained on"" a is b"" fail to learn"" b is a"".",,"[{Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans}]{berglund2023reversal} Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. 
 The reversal curse: Llms trained on"" a is b"" fail to learn"" b is a"". 
 \emph{arXiv preprint arXiv:2309.12288}."
2408.13654,chen2023learning,"[{Chen et~al.(2023)Chen, Ma, Song, Cao, Zhang, and Li}]{chen2023learning} Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, and Dongsheng Li. 2023.",Learning to teach large language models logical reasoning.,Learning to teach large language models logical reasoning.,,"[{Chen et~al.(2023)Chen, Ma, Song, Cao, Zhang, and Li}]{chen2023learning} Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, and Dongsheng Li. 2023. 
 Learning to teach large language models logical reasoning. 
 \emph{arXiv preprint arXiv:2310.09158}."
2408.13654,chen2024premise,"[{Chen et~al.(2024)Chen, Chi, Wang, and Zhou}]{chen2024premise} Xinyun Chen, Ryan~A Chi, Xuezhi Wang, and Denny Zhou. 2024.",Premise order matters in reasoning with large language models.,Premise order matters in reasoning with large language models.,,"[{Chen et~al.(2024)Chen, Chi, Wang, and Zhou}]{chen2024premise} Xinyun Chen, Ryan~A Chi, Xuezhi Wang, and Denny Zhou. 2024. 
 Premise order matters in reasoning with large language models. 
 \emph{arXiv preprint arXiv:2402.08939}."
2408.13654,creswell2022selection,"[{Creswell et~al.(2022)Creswell, Shanahan, and Higgins}]{creswell2022selection} Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022.",Selection-inference: Exploiting large language models for interpretable logical reasoning.,Selection-inference: Exploiting large language models for interpretable logical reasoning.,,"[{Creswell et~al.(2022)Creswell, Shanahan, and Higgins}]{creswell2022selection} Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. 
 Selection-inference: Exploiting large language models for interpretable logical reasoning. 
 \emph{arXiv preprint arXiv:2205.09712}."
2408.13654,guo2023empowering,"[{Guo et~al.(2023)Guo, Li, Qi, Yang, Li, Feng, Zhang, and Xu}]{guo2023empowering} Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si~Zhang, and Ming Xu. 2023.",Empowering working memory for large language model agents.,Empowering working memory for large language model agents.,,"[{Guo et~al.(2023)Guo, Li, Qi, Yang, Li, Feng, Zhang, and Xu}]{guo2023empowering} Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si~Zhang, and Ming Xu. 2023. 
 Empowering working memory for large language model agents. 
 \emph{arXiv preprint arXiv:2312.17259}."
2408.13654,hu2023chatdb,"[{Hu et~al.(2023)Hu, Fu, Du, Luo, Zhao, and Zhao}]{hu2023chatdb} Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023.",Chatdb: Augmenting llms with databases as their symbolic memory.,Chatdb: Augmenting llms with databases as their symbolic memory.,,"[{Hu et~al.(2023)Hu, Fu, Du, Luo, Zhao, and Zhao}]{hu2023chatdb} Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. 
 Chatdb: Augmenting llms with databases as their symbolic memory. 
 \emph{arXiv preprint arXiv:2306.03901}."
2408.13654,kim2023entity,[{Kim and Schuster(2023)}]{kim2023entity} Najoung Kim and Sebastian Schuster. 2023.,Entity tracking in language models.,Entity tracking in language models.,,"[{Kim and Schuster(2023)}]{kim2023entity} Najoung Kim and Sebastian Schuster. 2023. 
 Entity tracking in language models. 
 \emph{arXiv preprint arXiv:2305.02363}."
2408.13654,lee2024symba,[{Lee and Hwang(2024)}]{lee2024symba} Jinu Lee and Wonseok Hwang. 2024.,Symba: Symbolic backward chaining for multi-step natural language reasoning.,Symba: Symbolic backward chaining for multi-step natural language reasoning.,,"[{Lee and Hwang(2024)}]{lee2024symba} Jinu Lee and Wonseok Hwang. 2024. 
 Symba: Symbolic backward chaining for multi-step natural language reasoning. 
 \emph{arXiv preprint arXiv:2402.12806}."
2408.13654,lee2024human,"[{Lee et~al.(2024)Lee, Chen, Furuta, Canny, and Fischer}]{lee2024human} Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024.",A human-inspired reading agent with gist memory of very long contexts.,A human-inspired reading agent with gist memory of very long contexts.,,"[{Lee et~al.(2024)Lee, Chen, Furuta, Canny, and Fischer}]{lee2024human} Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024. 
 A human-inspired reading agent with gist memory of very long contexts. 
 \emph{arXiv preprint arXiv:2402.09727}."
2408.13654,lu2024longheads,"[{Lu et~al.(2024)Lu, Zhou, He, Zhao, Ji, Gui, Zhang, and Huang}]{lu2024longheads} Yi~Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2024.",Longheads: Multi-head attention is secretly a long context processor.,Longheads: Multi-head attention is secretly a long context processor.,,"[{Lu et~al.(2024)Lu, Zhou, He, Zhao, Ji, Gui, Zhang, and Huang}]{lu2024longheads} Yi~Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2024. 
 Longheads: Multi-head attention is secretly a long context processor. 
 \emph{arXiv preprint arXiv:2402.10685}."
2408.13654,nye2021show,"[{Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan et~al.}]{nye2021show} Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al. 2021.",Show your work: Scratchpads for intermediate computation with language models.,Show your work: Scratchpads for intermediate computation with language models.,,"[{Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan et~al.}]{nye2021show} Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al. 2021. 
 Show your work: Scratchpads for intermediate computation with language models. 
 \emph{arXiv preprint arXiv:2112.00114}."
2408.13654,pan2023logic,"[{Pan et~al.(2023)Pan, Albalak, Wang, and Wang}]{pan2023logic} Liangming Pan, Alon Albalak, Xinyi Wang, and William~Yang Wang. 2023.",Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.,Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.,,"[{Pan et~al.(2023)Pan, Albalak, Wang, and Wang}]{pan2023logic} Liangming Pan, Alon Albalak, Xinyi Wang, and William~Yang Wang. 2023. 
 Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. 
 \emph{arXiv preprint arXiv:2305.12295}."
2408.13654,qiu2023phenomenal,"[{Qiu et~al.(2023)Qiu, Jiang, Lu, Sclar, Pyatkin, Bhagavatula, Wang, Kim, Choi, Dziri et~al.}]{qiu2023phenomenal} Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et~al. 2023.",Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.,Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.,,"[{Qiu et~al.(2023)Qiu, Jiang, Lu, Sclar, Pyatkin, Bhagavatula, Wang, Kim, Choi, Dziri et~al.}]{qiu2023phenomenal} Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et~al. 2023. 
 Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. 
 \emph{arXiv preprint arXiv:2310.08559}."
2408.13654,sanyal2022fairr,"[{Sanyal et~al.(2022)Sanyal, Singh, and Ren}]{sanyal2022fairr} Soumya Sanyal, Harman Singh, and Xiang Ren. 2022.",Fairr: Faithful and robust deductive reasoning over natural language.,Fairr: Faithful and robust deductive reasoning over natural language.,,"[{Sanyal et~al.(2022)Sanyal, Singh, and Ren}]{sanyal2022fairr} Soumya Sanyal, Harman Singh, and Xiang Ren. 2022. 
 Fairr: Faithful and robust deductive reasoning over natural language. 
 \emph{arXiv preprint arXiv:2203.10261}."
2408.13654,sinha2019clutrr,"[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and Hamilton}]{sinha2019clutrr} Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L Hamilton. 2019.",Clutrr: A diagnostic benchmark for inductive reasoning from text.,Clutrr: A diagnostic benchmark for inductive reasoning from text.,,"[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and Hamilton}]{sinha2019clutrr} Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L Hamilton. 2019. 
 Clutrr: A diagnostic benchmark for inductive reasoning from text. 
 \emph{arXiv preprint arXiv:1908.06177}."
2408.13654,sun2023indeterminacy,"[{Sun et~al.(2023)Sun, Xu, Liu, Luan, Wang, Shang, Wen, and Yan}]{sun2023indeterminacy} Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. 2023.",From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models.,From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models.,,"[{Sun et~al.(2023)Sun, Xu, Liu, Luan, Wang, Shang, Wen, and Yan}]{sun2023indeterminacy} Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. 2023. 
 From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. 
 \emph{arXiv preprint arXiv:2310.18659}."
2408.13654,tafjord2020proofwriter,"[{Tafjord et~al.(2020)Tafjord, Mishra, and Clark}]{tafjord2020proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark. 2020.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.","Proofwriter: Generating implications, proofs, and abductive statements over natural language.",,"[{Tafjord et~al.(2020)Tafjord, Mishra, and Clark}]{tafjord2020proofwriter} Oyvind Tafjord, Bhavana~Dalvi Mishra, and Peter Clark. 2020. 
 Proofwriter: Generating implications, proofs, and abductive statements over natural language. 
 \emph{arXiv preprint arXiv:2012.13048}."
2408.13654,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2408.13654,touvron2023llama,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2408.13654,wang2024can,"[{Wang et~al.(2024{\natexlab{a}})Wang, Wei, Choi, and Ren}]{wang2024can} Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024{\natexlab{a}}.",Can llms reason with rules? logic scaffolding for stress-testing and improving llms.,Can llms reason with rules? logic scaffolding for stress-testing and improving llms.,,"[{Wang et~al.(2024{\natexlab{a}})Wang, Wei, Choi, and Ren}]{wang2024can} Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024{\natexlab{a}}. 
 Can llms reason with rules? logic scaffolding for stress-testing and improving llms. 
 \emph{arXiv preprint arXiv:2402.11442}."
2408.13654,wang2021logic,"[{Wang et~al.(2021)Wang, Zhong, Tang, Wei, Fan, Jiang, Zhou, and Duan}]{wang2021logic} Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2021.",Logic-driven context extension and data augmentation for logical reasoning of text.,Logic-driven context extension and data augmentation for logical reasoning of text.,,"[{Wang et~al.(2021)Wang, Zhong, Tang, Wei, Fan, Jiang, Zhou, and Duan}]{wang2021logic} Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2021. 
 Logic-driven context extension and data augmentation for logical reasoning of text. 
 \emph{arXiv preprint arXiv:2105.03659}."
2408.13654,wang2022self,"[{Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022{\natexlab{b}}.",Self-consistency improves chain of thought reasoning in language models.,Self-consistency improves chain of thought reasoning in language models.,,"[{Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self} Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022{\natexlab{b}}. 
 Self-consistency improves chain of thought reasoning in language models. 
 \emph{arXiv preprint arXiv:2203.11171}."
2408.13654,xu2024faithful,"[{Xu et~al.(2024)Xu, Fei, Pan, Liu, Lee, and Hsu}]{xu2024faithful} Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. 2024.",Faithful logical reasoning via symbolic chain-of-thought.,Faithful logical reasoning via symbolic chain-of-thought.,,"[{Xu et~al.(2024)Xu, Fei, Pan, Liu, Lee, and Hsu}]{xu2024faithful} Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. 2024. 
 Faithful logical reasoning via symbolic chain-of-thought. 
 \emph{arXiv preprint arXiv:2405.18357}."
2408.13654,yoneda2023statler,"[{Yoneda et~al.(2023)Yoneda, Fang, Li, Zhang, Jiang, Lin, Picker, Yunis, Mei, and Walter}]{yoneda2023statler} Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew~R Walter. 2023.",Statler: State-maintaining language models for embodied reasoning.,Statler: State-maintaining language models for embodied reasoning.,,"[{Yoneda et~al.(2023)Yoneda, Fang, Li, Zhang, Jiang, Lin, Picker, Yunis, Mei, and Walter}]{yoneda2023statler} Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew~R Walter. 2023. 
 Statler: State-maintaining language models for embodied reasoning. 
 \emph{arXiv preprint arXiv:2306.17840}."
2408.13654,yue2024fragrel,"[{Yue et~al.(2024)Yue, Zhu, and Yang}]{yue2024fragrel} Xihang Yue, Linchao Zhu, and Yi~Yang. 2024.",Fragrel: Exploiting fragment-level relations in the external memory of large language models.,Fragrel: Exploiting fragment-level relations in the external memory of large language models.,,"[{Yue et~al.(2024)Yue, Zhu, and Yang}]{yue2024fragrel} Xihang Yue, Linchao Zhu, and Yi~Yang. 2024. 
 Fragrel: Exploiting fragment-level relations in the external memory of large language models. 
 \emph{arXiv preprint arXiv:2406.03092}."
2408.13654,zhong2021ar,"[{Zhong et~al.(2021)Zhong, Wang, Tang, Xu, Guo, Wang, Yin, Zhou, and Duan}]{zhong2021ar} Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021.",Ar-lsat: Investigating analytical reasoning of text.,Ar-lsat: Investigating analytical reasoning of text.,,"[{Zhong et~al.(2021)Zhong, Wang, Tang, Xu, Guo, Wang, Yin, Zhou, and Duan}]{zhong2021ar} Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021. 
 Ar-lsat: Investigating analytical reasoning of text. 
 \emph{arXiv preprint arXiv:2104.06598}."
2408.13654,zhu2023large,"[{Zhu et~al.(2023)Zhu, Xue, Chen, Zhou, Tang, Schuurmans, and Dai}]{zhu2023large} Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023.",Large language models can learn rules.,Large language models can learn rules.,,"[{Zhu et~al.(2023)Zhu, Xue, Chen, Zhou, Tang, Schuurmans, and Dai}]{zhu2023large} Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023. 
 Large language models can learn rules. 
 \emph{arXiv preprint arXiv:2310.07064}."
2408.13933,llmqat,"[{Liu et~al.(2023)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra}]{llmqat} Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023.",{LLM-QAT}: Data-free quantization aware training for large language models.,{LLM-QAT}: Data-free quantization aware training for large language models.,,"[{Liu et~al.(2023)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra}]{llmqat} Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. 
 {LLM-QAT}: Data-free quantization aware training for large language models. 
 \emph{arXiv preprint arXiv:2307.06281}."
2408.14866,align3,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{align3} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022.",Training a helpful and harmless assistant with reinforcement learning from human feedback.,Training a helpful and harmless assistant with reinforcement learning from human feedback.,,"[{Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{align3} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al. 2022. 
 Training a helpful and harmless assistant with reinforcement learning from human feedback. 
 \emph{arXiv preprint arXiv:2204.05862}."
2408.14866,instruction,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{instruction} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.",Scaling instruction-finetuned language models.,Scaling instruction-finetuned language models.,,"[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma et~al.}]{instruction} Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022. 
 Scaling instruction-finetuned language models. 
 \emph{arXiv preprint arXiv:2210.11416}."
2408.14866,multilingual,"[{Deng et~al.(2023)Deng, Zhang, Pan, and Bing}]{multilingual} Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing. 2023.",Multilingual jailbreak challenges in large language models.,Multilingual jailbreak challenges in large language models.,,"[{Deng et~al.(2023)Deng, Zhang, Pan, and Bing}]{multilingual} Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing. 2023. 
 Multilingual jailbreak challenges in large language models. 
 \emph{arXiv preprint arXiv:2310.06474}."
2408.14866,hazell2023large,[{Hazell(2023)}]{hazell2023large} Julian Hazell. 2023.,Large language models can be used to effectively scale spear phishing campaigns.,Large language models can be used to effectively scale spear phishing campaigns.,,"[{Hazell(2023)}]{hazell2023large} Julian Hazell. 2023. 
 Large language models can be used to effectively scale spear phishing campaigns. 
 \emph{arXiv preprint arXiv:2305.06972}."
2408.14866,mi,"[{Huang et~al.(2023)Huang, Gupta, Xia, Li, and Chen}]{mi} Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023.",Catastrophic jailbreak of open-source llms via exploiting generation.,Catastrophic jailbreak of open-source llms via exploiting generation.,,"[{Huang et~al.(2023)Huang, Gupta, Xia, Li, and Chen}]{mi} Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. 
 Catastrophic jailbreak of open-source llms via exploiting generation. 
 \emph{arXiv preprint arXiv:2310.06987}."
2408.14866,mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.",Mistral 7b.,Mistral 7b.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{mistral} Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023. 
 Mistral 7b. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.14866,kang2023exploiting,"[{Kang et~al.(2023)Kang, Li, Stoica, Guestrin, Zaharia, and Hashimoto}]{kang2023exploiting} Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023.",Exploiting programmatic behavior of llms: Dual-use through standard security attacks.,Exploiting programmatic behavior of llms: Dual-use through standard security attacks.,,"[{Kang et~al.(2023)Kang, Li, Stoica, Guestrin, Zaharia, and Hashimoto}]{kang2023exploiting} Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. 
 Exploiting programmatic behavior of llms: Dual-use through standard security attacks. 
 \emph{arXiv preprint arXiv:2302.05733}."
2408.14866,sesame,"[{Lapid et~al.(2023)Lapid, Langberg, and Sipper}]{sesame} Raz Lapid, Ron Langberg, and Moshe Sipper. 2023.",Open sesame! universal black box jailbreaking of large language models.,Open sesame! universal black box jailbreaking of large language models.,,"[{Lapid et~al.(2023)Lapid, Langberg, and Sipper}]{sesame} Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. 
 Open sesame! universal black box jailbreaking of large language models. 
 \emph{arXiv preprint arXiv:2309.01446}."
2408.14866,autodan,"[{Liu et~al.(2023{\natexlab{a}})Liu, Xu, Chen, and Xiao}]{autodan} Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023{\natexlab{a}}.",Autodan: Generating stealthy jailbreak prompts on aligned large language models.,Autodan: Generating stealthy jailbreak prompts on aligned large language models.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Xu, Chen, and Xiao}]{autodan} Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023{\natexlab{a}}. 
 Autodan: Generating stealthy jailbreak prompts on aligned large language models. 
 \emph{arXiv preprint arXiv:2310.04451}."
2408.14866,jailbreakingchatgpt,"[{Liu et~al.(2023{\natexlab{b}})Liu, Deng, Xu, Li, Zheng, Zhang, Zhao, Zhang, and Liu}]{jailbreakingchatgpt} Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023{\natexlab{b}}.",Jailbreaking chatgpt via prompt engineering: An empirical study.,Jailbreaking chatgpt via prompt engineering: An empirical study.,,"[{Liu et~al.(2023{\natexlab{b}})Liu, Deng, Xu, Li, Zheng, Zhang, Zhao, Zhang, and Liu}]{jailbreakingchatgpt} Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023{\natexlab{b}}. 
 Jailbreaking chatgpt via prompt engineering: An empirical study. 
 \emph{arXiv preprint arXiv:2305.13860}."
2408.14866,harmbench,"[{Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li et~al.}]{harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al. 2024.",Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.,,"[{Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li et~al.}]{harmbench} Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, et~al. 2024. 
 Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. 
 \emph{arXiv preprint arXiv:2402.04249}."
2408.14866,meade2024universal,"[{Meade et~al.(2024)Meade, Patel, and Reddy}]{meade2024universal} Nicholas Meade, Arkil Patel, and Siva Reddy. 2024.",Universal adversarial triggers are not universal.,Universal adversarial triggers are not universal.,,"[{Meade et~al.(2024)Meade, Patel, and Reddy}]{meade2024universal} Nicholas Meade, Arkil Patel, and Siva Reddy. 2024. 
 Universal adversarial triggers are not universal. 
 \emph{arXiv preprint arXiv:2404.16020}."
2408.14866,autoprompt,"[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh}]{autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 2020.",Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,Autoprompt: Eliciting knowledge from language models with automatically generated prompts.,,"[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh}]{autoprompt} Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer Singh. 2020. 
 Autoprompt: Eliciting knowledge from language models with automatically generated prompts. 
 \emph{arXiv preprint arXiv:2010.15980}."
2408.14866,llama2,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2408.14866,openchat,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023.",Openchat: Advancing open-source language models with mixed-quality data.,Openchat: Advancing open-source language models with mixed-quality data.,,"[{Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu}]{openchat} Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. 
 Openchat: Advancing open-source language models with mixed-quality data. 
 \emph{arXiv preprint arXiv:2309.11235}."
2408.14866,inst-tuning,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{inst-tuning} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{inst-tuning} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}."
2408.14866,cipher,"[{Yuan et~al.(2023)Yuan, Jiao, Wang, Huang, He, Shi, and Tu}]{cipher} Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023.",Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.,Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.,,"[{Yuan et~al.(2023)Yuan, Jiao, Wang, Huang, He, Shi, and Tu}]{cipher} Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023. 
 Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. 
 \emph{arXiv preprint arXiv:2308.06463}."
2408.14866,autodan2,"[{Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun}]{autodan2} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 2023.",Autodan: Automatic and interpretable adversarial attacks on large language models.,Autodan: Automatic and interpretable adversarial attacks on large language models.,,"[{Zhu et~al.(2023)Zhu, Zhang, An, Wu, Barrow, Wang, Huang, Nenkova, and Sun}]{autodan2} Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 2023. 
 Autodan: Automatic and interpretable adversarial attacks on large language models. 
 \emph{arXiv preprint arXiv:2310.15140}."
2408.14866,gcg,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{gcg} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023.",Universal and transferable adversarial attacks on aligned language models.,Universal and transferable adversarial attacks on aligned language models.,,"[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{gcg} Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023. 
 Universal and transferable adversarial attacks on aligned language models. 
 \emph{arXiv preprint arXiv:2307.15043}."
2408.15971,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F.~L.;   Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,   Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F.~L.;   Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2408.15971,R:33,"[{Agashe, Fan, and Wang(2023)}]{R:33} Agashe, S.; Fan, Y.; and Wang, X.~E. 2023.",Evaluating multi-agent coordination abilities in large language   models.,Evaluating multi-agent coordination abilities in large language   models.,,"[{Agashe, Fan, and Wang(2023)}]{R:33} Agashe, S.; Fan, Y.; and Wang, X.~E. 2023. 
 Evaluating multi-agent coordination abilities in large language   models. 
 \emph{arXiv preprint arXiv:2310.03903}."
2408.15971,R:30,"[{Akata et~al.(2023)Akata, Schulz, Coda-Forno, Oh, Bethge, and   Schulz}]{R:30} Akata, E.; Schulz, L.; Coda-Forno, J.; Oh, S.~J.; Bethge, M.; and Schulz, E.   2023.",Playing repeated games with large language models.,Playing repeated games with large language models.,,"[{Akata et~al.(2023)Akata, Schulz, Coda-Forno, Oh, Bethge, and   Schulz}]{R:30} Akata, E.; Schulz, L.; Coda-Forno, J.; Oh, S.~J.; Bethge, M.; and Schulz, E.   2023. 
 Playing repeated games with large language models. 
 \emph{arXiv preprint arXiv:2305.16867}."
2408.15971,R:20,"[{BAAI(2023)}]{R:20} BAAI, P. 2023.",Plan4mc: Skill reinforcement learning and planning for open-world   minecraft tasks.,Plan4mc: Skill reinforcement learning and planning for open-world   minecraft tasks.,,"[{BAAI(2023)}]{R:20} BAAI, P. 2023. 
 Plan4mc: Skill reinforcement learning and planning for open-world   minecraft tasks. 
 \emph{arXiv preprint arXiv:2303.16563}."
2408.15971,R:18,"[{Chen et~al.(2024)Chen, Jiang, Lu, and Zhang}]{R:18} Chen, J.; Jiang, Y.; Lu, J.; and Zhang, L. 2024.",S-Agents: self-organizing agents in open-ended environment.,S-Agents: self-organizing agents in open-ended environment.,,"[{Chen et~al.(2024)Chen, Jiang, Lu, and Zhang}]{R:18} Chen, J.; Jiang, Y.; Lu, J.; and Zhang, L. 2024. 
 S-Agents: self-organizing agents in open-ended environment. 
 \emph{arXiv preprint arXiv:2402.04578}."
2408.15971,R:29,"[{de~Wynter(2024)}]{R:29} de~Wynter, A. 2024.",Will GPT-4 Run DOOM?,Will GPT-4 Run DOOM?,,"[{de~Wynter(2024)}]{R:29} de~Wynter, A. 2024. 
 Will GPT-4 Run DOOM? 
 \emph{arXiv preprint arXiv:2403.05468}."
2408.15971,R:34,"[{Feng et~al.(2023)Feng, Wang, Liu, Zheng, and Lu}]{R:34} Feng, Y.; Wang, Y.; Liu, J.; Zheng, S.; and Lu, Z. 2023.",Llama rider: Spurring large language models to explore the open   world.,Llama rider: Spurring large language models to explore the open   world.,,"[{Feng et~al.(2023)Feng, Wang, Liu, Zheng, and Lu}]{R:34} Feng, Y.; Wang, Y.; Liu, J.; Zheng, S.; and Lu, Z. 2023. 
 Llama rider: Spurring large language models to explore the open   world. 
 \emph{arXiv preprint arXiv:2310.08922}."
2408.15971,glm2024chatglm,"[{GLM et~al.(2024)GLM, Zeng, Xu, Wang, Zhang, Yin, Rojas, Feng, Zhao,   Lai et~al.}]{glm2024chatglm} GLM, T.; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; Rojas, D.; Feng, G.;   Zhao, H.; Lai, H.; et~al. 2024.",ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All   Tools.,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All   Tools.,,"[{GLM et~al.(2024)GLM, Zeng, Xu, Wang, Zhang, Yin, Rojas, Feng, Zhao,   Lai et~al.}]{glm2024chatglm} GLM, T.; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; Rojas, D.; Feng, G.;   Zhao, H.; Lai, H.; et~al. 2024. 
 ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All   Tools. 
 \emph{arXiv preprint arXiv:2406.12793}."
2408.15971,R:5,"[{Gong et~al.(2023)Gong, Huang, Ma, Vo, Durante, Noda, Zheng, Zhu,   Terzopoulos, Fei-Fei et~al.}]{R:5} Gong, R.; Huang, Q.; Ma, X.; Vo, H.; Durante, Z.; Noda, Y.; Zheng, Z.; Zhu,   S.-C.; Terzopoulos, D.; Fei-Fei, L.; et~al. 2023.",Mindagent: Emergent gaming interaction.,Mindagent: Emergent gaming interaction.,,"[{Gong et~al.(2023)Gong, Huang, Ma, Vo, Durante, Noda, Zheng, Zhu,   Terzopoulos, Fei-Fei et~al.}]{R:5} Gong, R.; Huang, Q.; Ma, X.; Vo, H.; Durante, Z.; Noda, Y.; Zheng, Z.; Zhu,   S.-C.; Terzopoulos, D.; Fei-Fei, L.; et~al. 2023. 
 Mindagent: Emergent gaming interaction. 
 \emph{arXiv preprint arXiv:2309.09971}."
2408.15971,R:10,"[{Guo et~al.(2023)Guo, Yang, Yoo, Lin, Iwasawa, and Matsuo}]{R:10} Guo, J.; Yang, B.; Yoo, P.; Lin, B.~Y.; Iwasawa, Y.; and Matsuo, Y. 2023.",Suspicion-agent: Playing imperfect information games with theory of   mind aware gpt-4.,Suspicion-agent: Playing imperfect information games with theory of   mind aware gpt-4.,,"[{Guo et~al.(2023)Guo, Yang, Yoo, Lin, Iwasawa, and Matsuo}]{R:10} Guo, J.; Yang, B.; Yoo, P.; Lin, B.~Y.; Iwasawa, Y.; and Matsuo, Y. 2023. 
 Suspicion-agent: Playing imperfect information games with theory of   mind aware gpt-4. 
 \emph{arXiv preprint arXiv:2309.17277}."
2408.15971,R:2,"[{Guo et~al.(2024)Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, and   Zhang}]{R:2} Guo, T.; Chen, X.; Wang, Y.; Chang, R.; Pei, S.; Chawla, N.~V.; Wiest, O.; and   Zhang, X. 2024.",Large language model based multi-agents: A survey of progress and   challenges.,Large language model based multi-agents: A survey of progress and   challenges.,,"[{Guo et~al.(2024)Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, and   Zhang}]{R:2} Guo, T.; Chen, X.; Wang, Y.; Chang, R.; Pei, S.; Chawla, N.~V.; Wiest, O.; and   Zhang, X. 2024. 
 Large language model based multi-agents: A survey of progress and   challenges. 
 \emph{arXiv preprint arXiv:2402.01680}."
2408.15971,gur2023real,"[{Gur et~al.(2023)Gur, Furuta, Huang, Safdari, Matsuo, Eck, and   Faust}]{gur2023real} Gur, I.; Furuta, H.; Huang, A.; Safdari, M.; Matsuo, Y.; Eck, D.; and Faust, A.   2023.","A real-world webagent with planning, long context understanding, and   program synthesis.","A real-world webagent with planning, long context understanding, and   program synthesis.",,"[{Gur et~al.(2023)Gur, Furuta, Huang, Safdari, Matsuo, Eck, and   Faust}]{gur2023real} Gur, I.; Furuta, H.; Huang, A.; Safdari, M.; Matsuo, Y.; Eck, D.; and Faust, A.   2023. 
 A real-world webagent with planning, long context understanding, and   program synthesis. 
 \emph{arXiv preprint arXiv:2307.12856}."
2408.15971,R:7,"[{Hu et~al.(2024)Hu, Huang, Ilhan, Tekin, Liu, Kompella, and Liu}]{R:7} Hu, S.; Huang, T.; Ilhan, F.; Tekin, S.; Liu, G.; Kompella, R.; and Liu, L.   2024.",A survey on large language model-based game agents.,A survey on large language model-based game agents.,,"[{Hu et~al.(2024)Hu, Huang, Ilhan, Tekin, Liu, Kompella, and Liu}]{R:7} Hu, S.; Huang, T.; Ilhan, F.; Tekin, S.; Liu, G.; Kompella, R.; and Liu, L.   2024. 
 A survey on large language model-based game agents. 
 \emph{arXiv preprint arXiv:2404.02039}."
2408.15971,R:27,"[{Hu, Huang, and Liu(2024)}]{R:27} Hu, S.; Huang, T.; and Liu, L. 2024.",Pok$\backslash$'eLLMon: A Human-Parity Agent for Pok$\backslash$'emon   Battles with Large Language Models.,Pok$\backslash$'eLLMon: A Human-Parity Agent for Pok$\backslash$'emon   Battles with Large Language Models.,,"[{Hu, Huang, and Liu(2024)}]{R:27} Hu, S.; Huang, T.; and Liu, L. 2024. 
 Pok$\backslash$'eLLMon: A Human-Parity Agent for Pok$\backslash$'emon   Battles with Large Language Models. 
 \emph{arXiv preprint arXiv:2402.01118}."
2408.15971,R:31,"[{Hua et~al.(2023)Hua, Fan, Li, Mei, Ji, Ge, Hemphill, and   Zhang}]{R:31} Hua, W.; Fan, L.; Li, L.; Mei, K.; Ji, J.; Ge, Y.; Hemphill, L.; and Zhang, Y.   2023.",War and peace (waragent): Large language model-based multi-agent   simulation of world wars.,War and peace (waragent): Large language model-based multi-agent   simulation of world wars.,,"[{Hua et~al.(2023)Hua, Fan, Li, Mei, Ji, Ge, Hemphill, and   Zhang}]{R:31} Hua, W.; Fan, L.; Li, L.; Mei, K.; Ji, J.; Ge, Y.; Hemphill, L.; and Zhang, Y.   2023. 
 War and peace (waragent): Large language model-based multi-agent   simulation of world wars. 
 \emph{arXiv preprint arXiv:2311.17227}."
2408.15971,R:28,"[{Huang et~al.(2024)Huang, Cao, Wen, Zhou, and Zhang}]{R:28} Huang, C.; Cao, Y.; Wen, Y.; Zhou, T.; and Zhang, Y. 2024.",PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas   Hold'em via Large Language Model.,PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas   Hold'em via Large Language Model.,,"[{Huang et~al.(2024)Huang, Cao, Wen, Zhou, and Zhang}]{R:28} Huang, C.; Cao, Y.; Wen, Y.; Zhou, T.; and Zhang, Y. 2024. 
 PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas   Hold'em via Large Language Model. 
 \emph{arXiv preprint arXiv:2401.06781}."
2408.15971,jiang2023mistral,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Jiang, A.~Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D.~S.; Casas,   D. d.~l.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; et~al. 2023.",Mistral 7B.,Mistral 7B.,,"[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,   Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral} Jiang, A.~Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D.~S.; Casas,   D. d.~l.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; et~al. 2023. 
 Mistral 7B. 
 \emph{arXiv preprint arXiv:2310.06825}."
2408.15971,R:3,"[{Li et~al.(2023{\natexlab{b}})Li, Chong, Stepputtis, Campbell, Hughes,   Lewis, and Sycara}]{R:3} Li, H.; Chong, Y.~Q.; Stepputtis, S.; Campbell, J.; Hughes, D.; Lewis, M.; and   Sycara, K. 2023{\natexlab{b}}.",Theory of mind for multi-agent collaboration via large language   models.,Theory of mind for multi-agent collaboration via large language   models.,,"[{Li et~al.(2023{\natexlab{b}})Li, Chong, Stepputtis, Campbell, Hughes,   Lewis, and Sycara}]{R:3} Li, H.; Chong, Y.~Q.; Stepputtis, S.; Campbell, J.; Hughes, D.; Lewis, M.; and   Sycara, K. 2023{\natexlab{b}}. 
 Theory of mind for multi-agent collaboration via large language   models. 
 \emph{arXiv preprint arXiv:2310.10701}."
2408.15971,R:35,"[{Liu et~al.(2023{\natexlab{a}})Liu, Yu, Gao, Xie, Liao, Wu, and   Wang}]{R:35} Liu, J.; Yu, C.; Gao, J.; Xie, Y.; Liao, Q.; Wu, Y.; and Wang, Y.   2023{\natexlab{a}}.",Llm-powered hierarchical language agent for real-time human-ai   coordination.,Llm-powered hierarchical language agent for real-time human-ai   coordination.,,"[{Liu et~al.(2023{\natexlab{a}})Liu, Yu, Gao, Xie, Liao, Wu, and   Wang}]{R:35} Liu, J.; Yu, C.; Gao, J.; Xie, Y.; Liao, Q.; Wu, Y.; and Wang, Y.   2023{\natexlab{a}}. 
 Llm-powered hierarchical language agent for real-time human-ai   coordination. 
 \emph{arXiv preprint arXiv:2312.15224}."
2408.15971,liu2023agentbench,"[{Liu et~al.(2023{\natexlab{b}})Liu, Yu, Zhang, Xu, Lei, Lai, Gu, Ding,   Men, Yang et~al.}]{liu2023agentbench} Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men,   K.; Yang, K.; et~al. 2023{\natexlab{b}}.",Agentbench: Evaluating llms as agents.,Agentbench: Evaluating llms as agents.,,"[{Liu et~al.(2023{\natexlab{b}})Liu, Yu, Zhang, Xu, Lei, Lai, Gu, Ding,   Men, Yang et~al.}]{liu2023agentbench} Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men,   K.; Yang, K.; et~al. 2023{\natexlab{b}}. 
 Agentbench: Evaluating llms as agents. 
 \emph{arXiv preprint arXiv:2308.03688}."
2408.15971,R:16,"[{Ma et~al.(2023)Ma, Mi, Yan, Wu, Lin, Zhang, and Wang}]{R:16} Ma, W.; Mi, Q.; Yan, X.; Wu, Y.; Lin, R.; Zhang, H.; and Wang, J. 2023.",Large language models play starcraft ii: Benchmarks and a chain of   summarization approach.,Large language models play starcraft ii: Benchmarks and a chain of   summarization approach.,,"[{Ma et~al.(2023)Ma, Mi, Yan, Wu, Lin, Zhang, and Wang}]{R:16} Ma, W.; Mi, Q.; Yan, X.; Wu, Y.; Lin, R.; Zhang, H.; and Wang, J. 2023. 
 Large language models play starcraft ii: Benchmarks and a chain of   summarization approach. 
 \emph{arXiv preprint arXiv:2312.11865}."
2408.15971,R:17,"[{Shao et~al.(2024)Shao, Jiang, Zuo, and Liu}]{R:17} Shao, X.; Jiang, W.; Zuo, F.; and Liu, M. 2024.",SwarmBrain: Embodied agent for real-time strategy game StarCraft II   via large language models.,SwarmBrain: Embodied agent for real-time strategy game StarCraft II   via large language models.,,"[{Shao et~al.(2024)Shao, Jiang, Zuo, and Liu}]{R:17} Shao, X.; Jiang, W.; Zuo, F.; and Liu, M. 2024. 
 SwarmBrain: Embodied agent for real-time strategy game StarCraft II   via large language models. 
 \emph{arXiv preprint arXiv:2401.17749}."
2408.15971,R:14,"[{Shi et~al.(2023)Shi, Fang, Zheng, Deng, Chen, and Du}]{R:14} Shi, Z.; Fang, M.; Zheng, S.; Deng, S.; Chen, L.; and Du, Y. 2023.",Cooperation on the fly: Exploring language agents for ad hoc teamwork   in the avalon game.,Cooperation on the fly: Exploring language agents for ad hoc teamwork   in the avalon game.,,"[{Shi et~al.(2023)Shi, Fang, Zheng, Deng, Chen, and Du}]{R:14} Shi, Z.; Fang, M.; Zheng, S.; Deng, S.; Chen, L.; and Du, Y. 2023. 
 Cooperation on the fly: Exploring language agents for ad hoc teamwork   in the avalon game. 
 \emph{arXiv preprint arXiv:2312.17515}."
2408.15971,R:25,"[{Tan et~al.(2024)Tan, Ding, Zhang, Li, Zhou, Yue, Xia, Jiang, Zheng,   Xu et~al.}]{R:25} Tan, W.; Ding, Z.; Zhang, W.; Li, B.; Zhou, B.; Yue, J.; Xia, H.; Jiang, J.;   Zheng, L.; Xu, X.; et~al. 2024.",Towards general computer control: A multimodal agent for red dead   redemption ii as a case study.,Towards general computer control: A multimodal agent for red dead   redemption ii as a case study.,,"[{Tan et~al.(2024)Tan, Ding, Zhang, Li, Zhou, Yue, Xia, Jiang, Zheng,   Xu et~al.}]{R:25} Tan, W.; Ding, Z.; Zhang, W.; Li, B.; Zhou, B.; Yue, J.; Xia, H.; Jiang, J.;   Zheng, L.; Xu, X.; et~al. 2024. 
 Towards general computer control: A multimodal agent for red dead   redemption ii as a case study. 
 \emph{arXiv preprint arXiv:2403.03186}."
2408.15971,team2024gemma,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,   Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Team, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.;   Sifre, L.; Rivi{\`e}re, M.; Kale, M.~S.; Love, J.; et~al. 2024.",Gemma: Open models based on gemini research and technology.,Gemma: Open models based on gemini research and technology.,,"[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,   Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma} Team, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhupatiraju, S.; Pathak, S.;   Sifre, L.; Rivi{\`e}re, M.; Kale, M.~S.; Love, J.; et~al. 2024. 
 Gemma: Open models based on gemini research and technology. 
 \emph{arXiv preprint arXiv:2403.08295}."
2408.15971,R:32,"[{Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,   Yeo, Makhzani, K{\""u}ttler, Agapiou, Schrittwieser et~al.}]{R:32} Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A.~S.; Yeo,   M.; Makhzani, A.; K{\""u}ttler, H.; Agapiou, J.; Schrittwieser, J.; et~al.   2017.",Starcraft ii: A new challenge for reinforcement learning.,Starcraft ii: A new challenge for reinforcement learning.,,"[{Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,   Yeo, Makhzani, K{\""u}ttler, Agapiou, Schrittwieser et~al.}]{R:32} Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A.~S.; Yeo,   M.; Makhzani, A.; K{\""u}ttler, H.; Agapiou, J.; Schrittwieser, J.; et~al.   2017. 
 Starcraft ii: A new challenge for reinforcement learning. 
 \emph{arXiv preprint arXiv:1708.04782}."
2408.15971,R:15,"[{Wang et~al.(2023{\natexlab{a}})Wang, Liu, Zheng, Qi, Chen, Yang,   Zhao, Wang, Song, and Huang}]{R:15} Wang, S.; Liu, C.; Zheng, Z.; Qi, S.; Chen, S.; Yang, Q.; Zhao, A.; Wang, C.;   Song, S.; and Huang, G. 2023{\natexlab{a}}.",Avalon's game of thoughts: Battle against deception through recursive   contemplation.,Avalon's game of thoughts: Battle against deception through recursive   contemplation.,,"[{Wang et~al.(2023{\natexlab{a}})Wang, Liu, Zheng, Qi, Chen, Yang,   Zhao, Wang, Song, and Huang}]{R:15} Wang, S.; Liu, C.; Zheng, Z.; Qi, S.; Chen, S.; Yang, Q.; Zhao, A.; Wang, C.;   Song, S.; and Huang, G. 2023{\natexlab{a}}. 
 Avalon's game of thoughts: Battle against deception through recursive   contemplation. 
 \emph{arXiv preprint arXiv:2310.01320}."
2408.15971,R:21,"[{Wang et~al.(2023{\natexlab{b}})Wang, Cai, Chen, Liu, Ma, and   Liang}]{R:21} Wang, Z.; Cai, S.; Chen, G.; Liu, A.; Ma, X.; and Liang, Y. 2023{\natexlab{b}}.","Describe, explain, plan and select: Interactive planning with large   language models enables open-world multi-task agents.","Describe, explain, plan and select: Interactive planning with large   language models enables open-world multi-task agents.",,"[{Wang et~al.(2023{\natexlab{b}})Wang, Cai, Chen, Liu, Ma, and   Liang}]{R:21} Wang, Z.; Cai, S.; Chen, G.; Liu, A.; Ma, X.; and Liang, Y. 2023{\natexlab{b}}. 
 Describe, explain, plan and select: Interactive planning with large   language models enables open-world multi-task agents. 
 \emph{arXiv preprint arXiv:2302.01560}."
2408.15971,R:1,"[{Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Zhang, Zhu, Li, Jiang, Zhang,   and Wang}]{R:1} Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Zhang, S.; Zhu, E.; Li, B.; Jiang, L.;   Zhang, X.; and Wang, C. 2023.",Autogen: Enabling next-gen llm applications via multi-agent   conversation framework.,Autogen: Enabling next-gen llm applications via multi-agent   conversation framework.,,"[{Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Zhang, Zhu, Li, Jiang, Zhang,   and Wang}]{R:1} Wu, Q.; Bansal, G.; Zhang, J.; Wu, Y.; Zhang, S.; Zhu, E.; Li, B.; Jiang, L.;   Zhang, X.; and Wang, C. 2023. 
 Autogen: Enabling next-gen llm applications via multi-agent   conversation framework. 
 \emph{arXiv preprint arXiv:2308.08155}."
2408.15971,wu2024avatar,"[{Wu et~al.(2024{\natexlab{a}})Wu, Zhao, Huang, Huang, Yasunaga, Cao,   Ioannidis, Subbian, Leskovec, and Zou}]{wu2024avatar} Wu, S.; Zhao, S.; Huang, Q.; Huang, K.; Yasunaga, M.; Cao, K.; Ioannidis,   V.~N.; Subbian, K.; Leskovec, J.; and Zou, J. 2024{\natexlab{a}}.",AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval.,AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval.,,"[{Wu et~al.(2024{\natexlab{a}})Wu, Zhao, Huang, Huang, Yasunaga, Cao,   Ioannidis, Subbian, Leskovec, and Zou}]{wu2024avatar} Wu, S.; Zhao, S.; Huang, Q.; Huang, K.; Yasunaga, M.; Cao, K.; Ioannidis,   V.~N.; Subbian, K.; Leskovec, J.; and Zou, J. 2024{\natexlab{a}}. 
 AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval. 
 \emph{arXiv preprint arXiv:2406.11200}."
2408.15971,R:26,"[{Wu et~al.(2024{\natexlab{b}})Wu, Zhu, Yang, Xu, Fu, Wei, and   Fu}]{R:26} Wu, S.; Zhu, L.; Yang, T.; Xu, S.; Fu, Q.; Wei, Y.; and Fu, H.   2024{\natexlab{b}}.",Enhance reasoning for large language models in the game werewolf.,Enhance reasoning for large language models in the game werewolf.,,"[{Wu et~al.(2024{\natexlab{b}})Wu, Zhu, Yang, Xu, Fu, Wei, and   Fu}]{R:26} Wu, S.; Zhu, L.; Yang, T.; Xu, S.; Fu, Q.; Wei, Y.; and Fu, H.   2024{\natexlab{b}}. 
 Enhance reasoning for large language models in the game werewolf. 
 \emph{arXiv preprint arXiv:2402.02330}."
2408.15971,yang2024qwen2,"[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang   et~al.}]{yang2024qwen2} Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li, C.; Li, C.; Liu,   D.; Huang, F.; et~al. 2024.",Qwen2 technical report.,Qwen2 technical report.,,"[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang   et~al.}]{yang2024qwen2} Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li, C.; Li, C.; Liu,   D.; Huang, F.; et~al. 2024. 
 Qwen2 technical report. 
 \emph{arXiv preprint arXiv:2407.10671}."
2408.15971,young2024yi,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu,   Chen, Chang et~al.}]{young2024yi} Young, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang, G.; Li, H.; Zhu, J.;   Chen, J.; Chang, J.; et~al. 2024.",Yi: Open foundation models by 01. ai.,Yi: Open foundation models by 01. ai.,,"[{Young et~al.(2024)Young, Chen, Li, Huang, Zhang, Zhang, Li, Zhu,   Chen, Chang et~al.}]{young2024yi} Young, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang, G.; Li, H.; Zhu, J.;   Chen, J.; Chang, J.; et~al. 2024. 
 Yi: Open foundation models by 01. ai. 
 \emph{arXiv preprint arXiv:2403.04652}."
2408.15971,R:22,"[{Zhang et~al.(2023)Zhang, Cai, Fu, Yuan, and Lu}]{R:22} Zhang, C.; Cai, P.; Fu, Y.; Yuan, H.; and Lu, Z. 2023.",Creative agents: Empowering agents with imagination for creative   tasks.,Creative agents: Empowering agents with imagination for creative   tasks.,,"[{Zhang et~al.(2023)Zhang, Cai, Fu, Yuan, and Lu}]{R:22} Zhang, C.; Cai, P.; Fu, Y.; Yuan, H.; and Lu, Z. 2023. 
 Creative agents: Empowering agents with imagination for creative   tasks. 
 \emph{arXiv preprint arXiv:2312.02519}."
2408.15971,zhang2024sciglm,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Hu, Zhoubian, Du, Yang, Wang,   Yue, Dong, and Tang}]{zhang2024sciglm} Zhang, D.; Hu, Z.; Zhoubian, S.; Du, Z.; Yang, K.; Wang, Z.; Yue, Y.; Dong, Y.;   and Tang, J. 2024{\natexlab{a}}.",Sciglm: Training scientific language models with self-reflective   instruction annotation and tuning.,Sciglm: Training scientific language models with self-reflective   instruction annotation and tuning.,,"[{Zhang et~al.(2024{\natexlab{a}})Zhang, Hu, Zhoubian, Du, Yang, Wang,   Yue, Dong, and Tang}]{zhang2024sciglm} Zhang, D.; Hu, Z.; Zhoubian, S.; Du, Z.; Yang, K.; Wang, Z.; Yue, Y.; Dong, Y.;   and Tang, J. 2024{\natexlab{a}}. 
 Sciglm: Training scientific language models with self-reflective   instruction annotation and tuning. 
 \emph{arXiv preprint arXiv:2401.07950}."
2408.15971,zhang2024rest,"[{Zhang et~al.(2024{\natexlab{b}})Zhang, Zhoubian, Yue, Dong, and   Tang}]{zhang2024rest} Zhang, D.; Zhoubian, S.; Yue, Y.; Dong, Y.; and Tang, J. 2024{\natexlab{b}}.",ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search.,ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search.,,"[{Zhang et~al.(2024{\natexlab{b}})Zhang, Zhoubian, Yue, Dong, and   Tang}]{zhang2024rest} Zhang, D.; Zhoubian, S.; Yue, Y.; Dong, Y.; and Tang, J. 2024{\natexlab{b}}. 
 ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. 
 \emph{arXiv preprint arXiv:2406.03816}."
2408.15971,R:8,"[{Zhang et~al.(2024{\natexlab{d}})Zhang, Tang, Wu, Wang, Shen, Hou,   Tan, Li, Zhuang, and Lu}]{R:8} Zhang, W.; Tang, K.; Wu, H.; Wang, M.; Shen, Y.; Hou, G.; Tan, Z.; Li, P.;   Zhuang, Y.; and Lu, W. 2024{\natexlab{d}}.",Agent-pro: Learning to evolve via policy-level reflection and   optimization.,Agent-pro: Learning to evolve via policy-level reflection and   optimization.,,"[{Zhang et~al.(2024{\natexlab{d}})Zhang, Tang, Wu, Wang, Shen, Hou,   Tan, Li, Zhuang, and Lu}]{R:8} Zhang, W.; Tang, K.; Wu, H.; Wang, M.; Shen, Y.; Hou, G.; Tan, Z.; Li, P.;   Zhuang, Y.; and Lu, W. 2024{\natexlab{d}}. 
 Agent-pro: Learning to evolve via policy-level reflection and   optimization. 
 \emph{arXiv preprint arXiv:2402.17574}."
2409.01483,shazeer2017outrageously,"[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.",Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.,,"[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017outrageously} Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 
 \emph{arXiv preprint arXiv:1701.06538}, 2017."
2409.01483,xue2024openmoe,"[Xue et~al.(2024)Xue, Zheng, Fu, Ni, Zheng, Zhou, and You]{xue2024openmoe} Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You.",Openmoe: An early effort on open mixture-of-experts language models.,Openmoe: An early effort on open mixture-of-experts language models.,,"[Xue et~al.(2024)Xue, Zheng, Fu, Ni, Zheng, Zhou, and You]{xue2024openmoe} Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. 
 Openmoe: An early effort on open mixture-of-experts language models. 
 \emph{arXiv preprint arXiv:2402.01739}, 2024."
2409.01483,jiang2024mixtral,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.",Mixtral of experts.,Mixtral of experts.,,"[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral} Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 
 Mixtral of experts. 
 \emph{arXiv preprint arXiv:2401.04088}, 2024."
2409.01483,dai2024deepseekmoe,"[Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, et~al.]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al.",Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.,Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.,,"[Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, et~al.]{dai2024deepseekmoe} Damai Dai, Chengqi Deng, Chenggang Zhao, RX~Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y~Wu, et~al. 
 Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. 
 \emph{arXiv preprint arXiv:2401.06066}, 2024."
2409.01483,huang2023towards,"[Huang et~al.(2023)Huang, Ardalani, Sun, Ke, Lee, Sridhar, Bhosale, Wu, and Lee]{huang2023towards} Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin~S Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee.",Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference.,Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference.,,"[Huang et~al.(2023)Huang, Ardalani, Sun, Ke, Lee, Sridhar, Bhosale, Wu, and Lee]{huang2023towards} Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin~S Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee. 
 Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference. 
 \emph{arXiv preprint arXiv:2303.06182}, 2023."
2409.01483,li2023merge,"[Li et~al.(2023)Li, Zhang, Yadav, Sung, Cheng, Bansal, and Chen]{li2023merge} Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu~Cheng, Mohit Bansal, and Tianlong Chen.","Merge, then compress: Demystify efficient smoe with hints from its routing policy.","Merge, then compress: Demystify efficient smoe with hints from its routing policy.",,"[Li et~al.(2023)Li, Zhang, Yadav, Sung, Cheng, Bansal, and Chen]{li2023merge} Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu~Cheng, Mohit Bansal, and Tianlong Chen. 
 Merge, then compress: Demystify efficient smoe with hints from its routing policy. 
 \emph{arXiv preprint arXiv:2310.01334}, 2023."
2409.01483,eliseev2023fast,[Eliseev and Mazur(2023)]{eliseev2023fast} Artyom Eliseev and Denis Mazur.,Fast inference of mixture-of-experts language models with offloading.,Fast inference of mixture-of-experts language models with offloading.,,"[Eliseev and Mazur(2023)]{eliseev2023fast} Artyom Eliseev and Denis Mazur. 
 Fast inference of mixture-of-experts language models with offloading. 
 \emph{arXiv preprint arXiv:2312.17238}, 2023."
2409.01483,kim2023mixture,"[Kim et~al.(2023)Kim, Fahim, and Awadalla]{kim2023mixture} Young~Jin Kim, Raffy Fahim, and Hany~Hassan Awadalla.",Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness.,Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness.,,"[Kim et~al.(2023)Kim, Fahim, and Awadalla]{kim2023mixture} Young~Jin Kim, Raffy Fahim, and Hany~Hassan Awadalla. 
 Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness. 
 \emph{arXiv preprint arXiv:2310.02410}, 2023."
2409.01483,shen2023mixture,"[Shen et~al.(2023{\natexlab{a}})Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph, Fedus, Chen, et~al.]{shen2023mixture} Sheng Shen, Le~Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung~Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et~al.",Mixture-of-experts meets instruction tuning: A winning combination for large language models.,Mixture-of-experts meets instruction tuning: A winning combination for large language models.,,"[Shen et~al.(2023{\natexlab{a}})Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph, Fedus, Chen, et~al.]{shen2023mixture} Sheng Shen, Le~Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung~Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et~al. 
 Mixture-of-experts meets instruction tuning: A winning combination for large language models. 
 \emph{arXiv preprint arXiv:2305.14705}, 2023{\natexlab{a}}."
2409.01483,zadouri2023pushing,"[Zadouri et~al.(2023)Zadouri, {\""U}st{\""u}n, Ahmadian, Ermi{\c{s}}, Locatelli, and Hooker]{zadouri2023pushing} Ted Zadouri, Ahmet {\""U}st{\""u}n, Arash Ahmadian, Beyza Ermi{\c{s}}, Acyr Locatelli, and Sara Hooker.",Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.,Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.,,"[Zadouri et~al.(2023)Zadouri, {\""U}st{\""u}n, Ahmadian, Ermi{\c{s}}, Locatelli, and Hooker]{zadouri2023pushing} Ted Zadouri, Ahmet {\""U}st{\""u}n, Arash Ahmadian, Beyza Ermi{\c{s}}, Acyr Locatelli, and Sara Hooker. 
 Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. 
 \emph{arXiv preprint arXiv:2309.05444}, 2023."
2409.01483,gururangan2021demix,"[Gururangan et~al.(2021)Gururangan, Lewis, Holtzman, Smith, and Zettlemoyer]{gururangan2021demix} Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah~A Smith, and Luke Zettlemoyer.",Demix layers: Disentangling domains for modular language modeling.,Demix layers: Disentangling domains for modular language modeling.,,"[Gururangan et~al.(2021)Gururangan, Lewis, Holtzman, Smith, and Zettlemoyer]{gururangan2021demix} Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah~A Smith, and Luke Zettlemoyer. 
 Demix layers: Disentangling domains for modular language modeling. 
 \emph{arXiv preprint arXiv:2108.05036}, 2021."
2409.01483,sukhbaatar2024branch,"[Sukhbaatar et~al.(2024)Sukhbaatar, Golovneva, Sharma, Xu, Lin, Rozi{\`e}re, Kahn, Li, Yih, Weston, et~al.]{sukhbaatar2024branch} Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu~Xu, Xi~Victoria Lin, Baptiste Rozi{\`e}re, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, et~al.",Branch-train-mix: Mixing expert llms into a mixture-of-experts llm.,Branch-train-mix: Mixing expert llms into a mixture-of-experts llm.,,"[Sukhbaatar et~al.(2024)Sukhbaatar, Golovneva, Sharma, Xu, Lin, Rozi{\`e}re, Kahn, Li, Yih, Weston, et~al.]{sukhbaatar2024branch} Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu~Xu, Xi~Victoria Lin, Baptiste Rozi{\`e}re, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, et~al. 
 Branch-train-mix: Mixing expert llms into a mixture-of-experts llm. 
 \emph{arXiv preprint arXiv:2403.07816}, 2024."
2409.01483,lepikhin2020gshard,"[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.",Gshard: Scaling giant models with conditional computation and automatic sharding.,Gshard: Scaling giant models with conditional computation and automatic sharding.,,"[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin2020gshard} Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 
 Gshard: Scaling giant models with conditional computation and automatic sharding. 
 \emph{arXiv preprint arXiv:2006.16668}, 2020."
2409.01483,zoph2022st,"[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{zoph2022st} Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus.",St-moe: Designing stable and transferable sparse expert models.,St-moe: Designing stable and transferable sparse expert models.,,"[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and Fedus]{zoph2022st} Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 
 St-moe: Designing stable and transferable sparse expert models. 
 \emph{arXiv preprint arXiv:2202.08906}, 2022."
2409.01483,wei2021finetuned,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.",Finetuned language models are zero-shot learners.,Finetuned language models are zero-shot learners.,,"[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned} Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 
 Finetuned language models are zero-shot learners. 
 \emph{arXiv preprint arXiv:2109.01652}, 2021."
2409.01483,ainsworth2022git,"[Ainsworth et~al.(2022)Ainsworth, Hayase, and Srinivasa]{ainsworth2022git} Samuel~K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.",Git re-basin: Merging models modulo permutation symmetries.,Git re-basin: Merging models modulo permutation symmetries.,,"[Ainsworth et~al.(2022)Ainsworth, Hayase, and Srinivasa]{ainsworth2022git} Samuel~K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. 
 Git re-basin: Merging models modulo permutation symmetries. 
 \emph{arXiv preprint arXiv:2209.04836}, 2022."
2409.01483,shen2023moduleformer,"[Shen et~al.(2023{\natexlab{b}})Shen, Zhang, Cao, Tan, Chen, and Gan]{shen2023moduleformer} Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.",Moduleformer: Learning modular large language models from uncurated data.,Moduleformer: Learning modular large language models from uncurated data.,,"[Shen et~al.(2023{\natexlab{b}})Shen, Zhang, Cao, Tan, Chen, and Gan]{shen2023moduleformer} Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan. 
 Moduleformer: Learning modular large language models from uncurated data. 
 \emph{arXiv preprint arXiv:2306.04640}, 2023{\natexlab{b}}."
2409.01483,muqeeth2023soft,"[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{muqeeth2023soft} Mohammed Muqeeth, Haokun Liu, and Colin Raffel.",Soft merging of experts with adaptive routing.,Soft merging of experts with adaptive routing.,,"[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{muqeeth2023soft} Mohammed Muqeeth, Haokun Liu, and Colin Raffel. 
 Soft merging of experts with adaptive routing. 
 \emph{arXiv preprint arXiv:2306.03745}, 2023."
2409.01483,dai2022stablemoe,"[Dai et~al.(2022)Dai, Dong, Ma, Zheng, Sui, Chang, and Wei]{dai2022stablemoe} Damai Dai, Li~Dong, Shuming Ma, Bo~Zheng, Zhifang Sui, Baobao Chang, and Furu Wei.",Stablemoe: Stable routing strategy for mixture of experts.,Stablemoe: Stable routing strategy for mixture of experts.,,"[Dai et~al.(2022)Dai, Dong, Ma, Zheng, Sui, Chang, and Wei]{dai2022stablemoe} Damai Dai, Li~Dong, Shuming Ma, Bo~Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. 
 Stablemoe: Stable routing strategy for mixture of experts. 
 \emph{arXiv preprint arXiv:2204.08396}, 2022."
2409.01483,fedus2022review,"[Fedus et~al.(2022{\natexlab{b}})Fedus, Dean, and Zoph]{fedus2022review} William Fedus, Jeff Dean, and Barret Zoph.",A review of sparse expert models in deep learning.,A review of sparse expert models in deep learning.,,"[Fedus et~al.(2022{\natexlab{b}})Fedus, Dean, and Zoph]{fedus2022review} William Fedus, Jeff Dean, and Barret Zoph. 
 A review of sparse expert models in deep learning. 
 \emph{arXiv preprint arXiv:2209.01667}, 2022{\natexlab{b}}."
2409.01483,li2022branch,"[Li et~al.(2022)Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and Zettlemoyer]{li2022branch} Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah~A Smith, and Luke Zettlemoyer.",Branch-train-merge: Embarrassingly parallel training of expert language models.,Branch-train-merge: Embarrassingly parallel training of expert language models.,,"[Li et~al.(2022)Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and Zettlemoyer]{li2022branch} Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah~A Smith, and Luke Zettlemoyer. 
 Branch-train-merge: Embarrassingly parallel training of expert language models. 
 \emph{arXiv preprint arXiv:2208.03306}, 2022."
2409.01483,kudugunta2021beyond,"[Kudugunta et~al.(2021)Kudugunta, Huang, Bapna, Krikun, Lepikhin, Luong, and Firat]{kudugunta2021beyond} Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat.",Beyond distillation: Task-level mixture-of-experts for efficient inference.,Beyond distillation: Task-level mixture-of-experts for efficient inference.,,"[Kudugunta et~al.(2021)Kudugunta, Huang, Bapna, Krikun, Lepikhin, Luong, and Firat]{kudugunta2021beyond} Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. 
 Beyond distillation: Task-level mixture-of-experts for efficient inference. 
 \emph{arXiv preprint arXiv:2110.03742}, 2021."
2409.01483,kim2021scalable,"[Kim et~al.(2021)Kim, Awan, Muzio, Salinas, Lu, Hendy, Rajbhandari, He, and Awadalla]{kim2021scalable} Young~Jin Kim, Ammar~Ahmad Awan, Alexandre Muzio, Andres Felipe~Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany~Hassan Awadalla.",Scalable and efficient moe training for multitask multilingual models.,Scalable and efficient moe training for multitask multilingual models.,,"[Kim et~al.(2021)Kim, Awan, Muzio, Salinas, Lu, Hendy, Rajbhandari, He, and Awadalla]{kim2021scalable} Young~Jin Kim, Ammar~Ahmad Awan, Alexandre Muzio, Andres Felipe~Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany~Hassan Awadalla. 
 Scalable and efficient moe training for multitask multilingual models. 
 \emph{arXiv preprint arXiv:2109.10465}, 2021."
2409.01483,sarkar2023testing,[Sarkar and Lausen(2023)]{sarkar2023testing} Soumajyoti Sarkar and Leonard Lausen.,Testing the limits of unified sequence to sequence llm pretraining on diverse table data tasks.,Testing the limits of unified sequence to sequence llm pretraining on diverse table data tasks.,,"[Sarkar and Lausen(2023)]{sarkar2023testing} Soumajyoti Sarkar and Leonard Lausen. 
 Testing the limits of unified sequence to sequence llm pretraining on diverse table data tasks. 
 \emph{arXiv preprint arXiv:2310.00789}, 2023."
2409.01483,ahn2024large,"[Ahn et~al.(2024)Ahn, Verma, Lou, Liu, Zhang, and Yin]{ahn2024large} Janice Ahn, Rishu Verma, Renze Lou, Di~Liu, Rui Zhang, and Wenpeng Yin.",Large language models for mathematical reasoning: Progresses and challenges.,Large language models for mathematical reasoning: Progresses and challenges.,,"[Ahn et~al.(2024)Ahn, Verma, Lou, Liu, Zhang, and Yin]{ahn2024large} Janice Ahn, Rishu Verma, Renze Lou, Di~Liu, Rui Zhang, and Wenpeng Yin. 
 Large language models for mathematical reasoning: Progresses and challenges. 
 \emph{arXiv preprint arXiv:2402.00157}, 2024."
2409.01483,sarkar2022parameter,"[Sarkar et~al.(2022)Sarkar, Lin, Sengupta, Lausen, Zha, and Mansour]{sarkar2022parameter} Soumajyoti Sarkar, Kaixiang Lin, Sailik Sengupta, Leonard Lausen, Sheng Zha, and Saab Mansour.",Parameter and data efficient continual pre-training for robustness to dialectal variance in arabic.,Parameter and data efficient continual pre-training for robustness to dialectal variance in arabic.,,"[Sarkar et~al.(2022)Sarkar, Lin, Sengupta, Lausen, Zha, and Mansour]{sarkar2022parameter} Soumajyoti Sarkar, Kaixiang Lin, Sailik Sengupta, Leonard Lausen, Sheng Zha, and Saab Mansour. 
 Parameter and data efficient continual pre-training for robustness to dialectal variance in arabic. 
 \emph{arXiv preprint arXiv:2211.03966}, 2022."
2409.01483,conneau2019unsupervised,"[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2019unsupervised} Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.",Unsupervised cross-lingual representation learning at scale.,Unsupervised cross-lingual representation learning at scale.,,"[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek, Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2019unsupervised} Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 
 Unsupervised cross-lingual representation learning at scale. 
 \emph{arXiv preprint arXiv:1911.02116}, 2019."
2409.01483,xue2020mt5,"[Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel]{xue2020mt5} Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.",mt5: A massively multilingual pre-trained text-to-text transformer.,mt5: A massively multilingual pre-trained text-to-text transformer.,,"[Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel]{xue2020mt5} Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 
 mt5: A massively multilingual pre-trained text-to-text transformer. 
 \emph{arXiv preprint arXiv:2010.11934}, 2020."
2409.01584,abdin2024phi,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024.",Phi-3 technical report: A highly capable language model locally on your phone.,Phi-3 technical report: A highly capable language model locally on your phone.,,"[{Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl et~al.}]{abdin2024phi} Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al. 2024. 
 Phi-3 technical report: A highly capable language model locally on your phone. 
 \emph{arXiv preprint arXiv:2404.14219}."
2409.01584,achiam2023gpt,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.",Gpt-4 technical report.,Gpt-4 technical report.,,"[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt} Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023. 
 Gpt-4 technical report. 
 \emph{arXiv preprint arXiv:2303.08774}."
2409.01584,qwen,"[{Bai et~al.(2023{\natexlab{a}})Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023{\natexlab{a}}.",Qwen technical report.,Qwen technical report.,,"[{Bai et~al.(2023{\natexlab{a}})Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu}]{qwen} Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023{\natexlab{a}}. 
 Qwen technical report. 
 \emph{arXiv preprint arXiv:2309.16609}."
2409.01584,Qwen-VL,"[{Bai et~al.(2023{\natexlab{b}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{Qwen-VL} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023{\natexlab{b}}.","Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.","Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",,"[{Bai et~al.(2023{\natexlab{b}})Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou}]{Qwen-VL} Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023{\natexlab{b}}. 
 Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 
 \emph{arXiv preprint arXiv:2308.12966}."
2409.01584,chen2020simple,"[{Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton}]{chen2020simple} Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.",A simple framework for contrastive learning of visual representations.,A simple framework for contrastive learning of visual representations.,,"[{Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton}]{chen2020simple} Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. 
 A simple framework for contrastive learning of visual representations. 
 \emph{arXiv preprint arXiv:2002.05709}."
2409.01584,dong2024internlm,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang, Duan, Cao et~al.}]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et~al. 2024.",Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.,Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.,,"[{Dong et~al.(2024)Dong, Zhang, Zang, Cao, Wang, Ouyang, Wei, Zhang, Duan, Cao et~al.}]{dong2024internlm} Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et~al. 2024. 
 Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. 
 \emph{arXiv preprint arXiv:2401.16420}."
2409.01584,dosovitskiy2020image,"[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al. 2020.",An image is worth 16x16 words: Transformers for image recognition at scale.,An image is worth 16x16 words: Transformers for image recognition at scale.,,"[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}]{dosovitskiy2020image} Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al. 2020. 
 An image is worth 16x16 words: Transformers for image recognition at scale. 
 \emph{arXiv preprint arXiv:2010.11929}."
2409.01584,dubey2024llama,"[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama} Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.",The llama 3 herd of models.,The llama 3 herd of models.,,"[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama} Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024. 
 The llama 3 herd of models. 
 \emph{arXiv preprint arXiv:2407.21783}."
2409.01584,he2015deep,"[{He et~al.(2015)He, Zhang, Ren, and Sun}]{he2015deep} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.",Deep residual learning for image recognition. arxiv e-prints.,Deep residual learning for image recognition. arxiv e-prints.,,"[{He et~al.(2015)He, Zhang, Ren, and Sun}]{he2015deep} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. 
 Deep residual learning for image recognition. arxiv e-prints. 
 \emph{arXiv preprint arXiv:1512.03385}, 10."
2409.01584,li2023seed,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}.",Seed-bench: Benchmarking multimodal llms with generative comprehension.,Seed-bench: Benchmarking multimodal llms with generative comprehension.,,"[{Li et~al.(2023{\natexlab{a}})Li, Wang, Wang, Ge, Ge, and Shan}]{li2023seed} Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023{\natexlab{a}}. 
 Seed-bench: Benchmarking multimodal llms with generative comprehension. 
 \emph{arXiv preprint arXiv:2307.16125}."
2409.01584,liu2023mmbench,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023{\natexlab{c}}.",Mmbench: Is your multi-modal model an all-around player?,Mmbench: Is your multi-modal model an all-around player?,,"[{Liu et~al.(2023{\natexlab{c}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu et~al.}]{liu2023mmbench} Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al. 2023{\natexlab{c}}. 
 Mmbench: Is your multi-modal model an all-around player? 
 \emph{arXiv preprint arXiv:2307.06281}."
2409.01584,nguyen2023vlsp2022,"[{Nguyen et~al.(2023)Nguyen, Nguyen, Vo, Tran, and Van~Nguyen}]{nguyen2023vlsp2022} Ngan Luu-Thuy Nguyen, Nghia~Hieu Nguyen, Duong~TD Vo, Khanh~Quoc Tran, and Kiet Van~Nguyen. 2023.",Vlsp2022-evjvqa challenge: Multilingual visual question answering.,Vlsp2022-evjvqa challenge: Multilingual visual question answering.,,"[{Nguyen et~al.(2023)Nguyen, Nguyen, Vo, Tran, and Van~Nguyen}]{nguyen2023vlsp2022} Ngan Luu-Thuy Nguyen, Nghia~Hieu Nguyen, Duong~TD Vo, Khanh~Quoc Tran, and Kiet Van~Nguyen. 2023. 
 Vlsp2022-evjvqa challenge: Multilingual visual question answering. 
 \emph{arXiv preprint arXiv:2302.11752}."
2409.01584,putri2024can,"[{Putri et~al.(2024)Putri, Haznitrama, Adhista, and Oh}]{putri2024can} Rifki~Afina Putri, Faiz~Ghifari Haznitrama, Dea Adhista, and Alice Oh. 2024.",Can llm generate culturally relevant commonsense qa data? case study in indonesian and sundanese.,Can llm generate culturally relevant commonsense qa data? case study in indonesian and sundanese.,,"[{Putri et~al.(2024)Putri, Haznitrama, Adhista, and Oh}]{putri2024can} Rifki~Afina Putri, Faiz~Ghifari Haznitrama, Dea Adhista, and Alice Oh. 2024. 
 Can llm generate culturally relevant commonsense qa data? case study in indonesian and sundanese. 
 \emph{arXiv preprint arXiv:2402.17302}."
2409.01584,reid2024gemini,"[{Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser et~al.}]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 2024.",Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.,,"[{Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser et~al.}]{reid2024gemini} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al. 2024. 
 Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 
 \emph{arXiv preprint arXiv:2403.05530}."
2409.01584,sakai2024mcsqa,"[{Sakai et~al.(2024)Sakai, Kamigaito, and Watanabe}]{sakai2024mcsqa} Yusuke Sakai, Hidetaka Kamigaito, and Taro Watanabe. 2024.",mcsqa: Multilingual commonsense reasoning dataset with unified creation strategy by language models and humans.,mcsqa: Multilingual commonsense reasoning dataset with unified creation strategy by language models and humans.,,"[{Sakai et~al.(2024)Sakai, Kamigaito, and Watanabe}]{sakai2024mcsqa} Yusuke Sakai, Hidetaka Kamigaito, and Taro Watanabe. 2024. 
 mcsqa: Multilingual commonsense reasoning dataset with unified creation strategy by language models and humans. 
 \emph{arXiv preprint arXiv:2406.04215}."
2409.01584,team2023gemini,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.",Gemini: a family of highly capable multimodal models.,Gemini: a family of highly capable multimodal models.,,"[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini} Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023. 
 Gemini: a family of highly capable multimodal models. 
 \emph{arXiv preprint arXiv:2312.11805}."
2409.01584,touvron2023llama,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.",Llama: Open and efficient foundation language models.,Llama: Open and efficient foundation language models.,,"[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama} Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}. 
 Llama: Open and efficient foundation language models. 
 \emph{arXiv preprint arXiv:2302.13971}."
2409.01584,touvron2023llama2,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.",Llama 2: Open foundation and fine-tuned chat models.,Llama 2: Open foundation and fine-tuned chat models.,,"[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2} Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}. 
 Llama 2: Open foundation and fine-tuned chat models. 
 \emph{arXiv preprint arXiv:2307.09288}."
2409.01584,qwen2,"[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, Dong, Wei, Lin, Tang, Wang, Yang, Tu, Zhang, Ma, Xu, Zhou, Bai, He, Lin, Dang, Lu, Chen, Yang, Li, Xue, Ni, Zhang, Wang, Peng, Men, Gao, Lin, Wang, Bai, Tan, Zhu, Li, Liu, Ge, Deng, Zhou, Ren, Zhang, Wei, Ren, Fan, Yao, Zhang, Wan, Chu, Liu, Cui, Zhang, and Fan}]{qwen2} An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu~Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024.",Qwen2 technical report.,Qwen2 technical report.,,"[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, Dong, Wei, Lin, Tang, Wang, Yang, Tu, Zhang, Ma, Xu, Zhou, Bai, He, Lin, Dang, Lu, Chen, Yang, Li, Xue, Ni, Zhang, Wang, Peng, Men, Gao, Lin, Wang, Bai, Tan, Zhu, Li, Liu, Ge, Deng, Zhou, Ren, Zhang, Wei, Ren, Fan, Yao, Zhang, Wan, Chu, Liu, Cui, Zhang, and Fan}]{qwen2} An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu~Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. 
 Qwen2 technical report. 
 \emph{arXiv preprint arXiv:2407.10671}."
2409.01584,zhang2019bertscore,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019.",Bertscore: Evaluating text generation with bert.,Bertscore: Evaluating text generation with bert.,,"[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi}]{zhang2019bertscore} Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi. 2019. 
 Bertscore: Evaluating text generation with bert. 
 \emph{arXiv preprint arXiv:1904.09675}."
