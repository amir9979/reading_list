In this work, we focus on two broad categories of language tasks: text classification and natural language inference (NLI). For text classification we use two datasets: IMDB~ for sentiment classification and AG News for news topic classification. For NLI, we use the SNLI dataset~. This variety of datasets allows us to evaluate the LLM-generated counterfactuals over a variety of label situations from binary to multi-class. 

The  dataset consists of a total of 50k highly polar movie reviews from IMDB (Internet Movie Database). Each data instance consists of a text string comprising the review text, and a label, either `negative' or `positive'. The  dataset consists of over 120k news articles, belonging to one of four news topics: `world', `sports', `business' and `science/technology'. The  dataset consists of 570k sentence pairs consisting of a premise and a hypothesis. Each premise-hypothesis pair is labeled with one of `entailment', `contradiction' or `neutral' labels.  %For all of these datasets, due to resource constraints, we use 500 samples from the test set to evaluate our proposed pipeline. For GPT-4 we use 250 samples.All experiments on open-source models were performed on two A100 GPUs with a total of 80G memory. For 13B Llama 2 models, we use 4-bit quantization using the optimal `nf4' datatype~. For all LLMs, we use top sampling with , temperature  and a repetition penalty of . We use PyTorch and fine-tuned models hosted on Huggingface for in both Sections  and .

To generate counterfactual explanations for a black-box text classifier  that predicts , we use the tuple  in the generation step, thereby replacing the ground truth label in Figure  by the model-predicted label, since we aim to explain why the model predicted  for the input sample . In our experiments, we use a DistilBERT model ~ , fine-tuned on the specific task dataset as the black-box model we aim to explain. Note that since our counterfactual generation process is model-agnostic, the same procedure can be applied to any black-box classifier in place of DistilBERT. Inspired by prior work~, we develop and experiment with two variants of : (1) : which directly generates the counterfactual explanation, and (2) : which first extracts words that may have caused the predicted label, and then uses those selected words to generate a counterfactual explanation, in a two-step manner. We hypothesize that the two-step generation may result in more effective and better quality counterfactual explanations, due to the additional guidance provided to the LLM, analogous to prior work such as Chain of Thought~. We show the prompts used in both the variants in Table . % we experiment with two variants (i) naive and (ii) guided, as shown in figure % . 

To evaluate the goodness of the counterfactual explanations generated by our zero-shot LLM-guided pipeline, we use a variety of evaluation metrics following prior work ~. Ideally, the generated counterfactual explanations should be able to flip the label of the classifier, thereby showcasing what  changed in the input that would flip the label of the classifier. Furthermore, counterfactual explanations should also be minimally edited samples of the input text, i.e., they should be as close as possible to the input sample both in the token space and the semantic space. To capture and evaluate these criteria, we use the following metrics:

%  This quantifies the percentage of samples for which the generated counterfactual could successfully flip the label predicted by the classifier. We formulate this as:% %     %         LFS = {n}\sum_{i=1}^n [f(x_i) \neq f(x_i^{cf})] \times 100% % For the text similarity, we compute this both in the token space via a normalized Levenshtein distance, as well as in the semantic space via  We use Label Flip Score to measure the effectiveness of the generated counterfactual explanations. For each input text  in the test split of the dataset, with correctly predicted label , we evaluate the corresponding LLM-generated counterfactual  using the same black-box classifier  and obtain a label for the counterfactual. For an effective counterfactual, the obtained label should be different from the original label . Then Label Flip Score (LFS) is computed as:\\

where  is the number of samples in the test set and  is the identity function.

 Counterfactual explanations generated by the LLMs should ideally be as `similar' to the original input text as possible. To evaluate this similarity, we use two metrics: similarity of the text embeddings using the Universal Sentence Encoder (USE)~ in the latent space, and a normalized Levenshtein distance~ to measure word edits in the token space. The semantic similarity using the embeddings of the original input and the generated counterfactual is computed as the inner product of the original and the counterfactual embeddings, averaged over the test dataset: 

where  refers to the Universal Sentence Encoder,  is the number of samples in the test set.

Levenshtein distance~ between two strings is defined as the minimum number of single character edits that are required to convert one string to another. To measure the distance between the original input text and the generated counterfactual in the token space we use a normalized Levenshtein distance, further averaged over the test dataset. This is computed as:

where  and  refer to the length of  and  respectively,  refers to the Levenshtein distance, and  is the number of samples in the test set.

% Here we elaborate the experimental settings, datasets or tasks, and the baselines used for evaluating FIZLE generated counterfactual explanations.

Similar to other counterfactual generation methods~, we compare our proposed  pipeline with three representative baselines from three categories of similar works: (i) BAE~ is a recent adversarial attack method that uses masked language modeling with BERT to perturb the input text by replacing masked words; (ii) CheckList ~ is a method for behavioral testing of NLP models via test cases generated by template-based methods as well as masked language models like RoBERTa; (iii)  Polyjuice ~ is a recent counterfactual generation method that uses an auxiliary language model (such as GPT-2) to generate diverse counterfactuals. Note that unlike these baselines, our  pipeline does not require any auxiliary model, language model or dataset, thereby enabling a completely zero-shot generation.

Following the experimental setup described above, we evaluate the generated counterfactuals to explain black-box classifiers for the three datasets, and compare to baselines. We show these quantitative results in Table . For each LLM, we evaluate both variants of our framework:  and . For effective and good quality counterfactual explanations, ideally we would expect high values of LFS and semantic similarity with low values of edit distance. Overall, we see varied performance of the LLMs and the two variants across the different tasks. Similar to other counterfactual generation works~, we see an obvious trade-off between the Label Flip Score and the semantic similarity. This is intuitive since the more the generated counterfactual deviates from the original input text, higher the chances are for it to be a successful counterfactual for the original input (i.e, it would result in a label flip). %However, ideally we want counterfactuals that are semantically as close to the input as possible, that is, higher values of semantic similarity are desirable.  Among the three baselines, we see CheckList fails completely in generating counterfactual explanations. We see satisfactory performance by BAE, except for the AG News dataset. For Polyjuice, even though the LFS scores are high, the poor textual similarity scores imply that the counterfactuals generated are not good quality and deviate from the input text significantly.

For most of the LLMs we evaluated,  outperforms  on the IMDB and AG News datasets. This may imply that the additional `guidance' provided by identifying the input words before the counterfactual explanation generation step enables the generation of better counterfactuals. Interestingly, we do not see this trend for GPT-4, where the  variant performs better than the  one. This might be due to the fact that GPT-4 is extremely good at understanding complex tasks and instructions~, and the additional feature extraction step in the  variant does not provide additional useful guidance, and perhaps even confuses the LLM. GPT-4 when used in the  variant of our pipeline, has the best performance for zero-shot counterfactual explanation generation, in terms of LFS.  %successfully generates the Furthermore, in terms of LFS, For the IMDB dataset, we see that counterfactual explanations generated by text-davinci-003 are very effective, with the  variant being marginally better than the other two. However, the best performance is achieved by GPT-4 in the  variant. In fact, the  variant of GPT-4 has better performance than the other two variants in both the IMDB and SNLI tasks. This might be due to the fact that GPT-4 is extremely good at understanding complex tasks and instructions, and the two-step feature extraction step in the full  variant does not provide additional useful guidance. We see a similar trend with ChatGPT 3.5 where the  variant outperforms the other two for the AG News and SNLI tasks, while retaining high semantic similarity values. For natural language inference on the SNLI dataset, we see all LLMs struggle to generate good counterfactual explanations. GPT-4 performs well, possibly owing to its instruction and textual understanding capabilities~, but the best performance is by the Polyjuice baseline. This poor performance of LLMs particularly on the SNLI dataset is further evidence towards LLMs struggling with inference and reasoning. This gap in the capabilities of recent LLMs on reasoning tasks has been observed by several recent efforts as well~.

Lastly, we see the open-source models Llama 2 7B and 13B struggle to generate zero-shot counterfactual explanations with small number of edits, thus resulting in very high edit distances. The Llama 2 models struggle to keep the generated counterfactuals semantically similar to the original input, implying they either make too many edits to the input text, or output some unrelated, low-quality text that does not conform to the instructions provided in the prompt.  %We see an example of this in Table  along with an accompanying discussion in Section . % % Note that the only way we control for the textual similarity here is simply via the prompt, without performing any thresholding on similarity values or edit distances. Thus a higher edit distance may also imply that the LLM, in this case Llama 2, is struggling to fully understand and follow complex instructions in the input prompt.% On the other hand, automated methods for generating counterfactual explanations, or broadly, counterfactual examples, often rely on auxiliary models such as pre-trained language models which need to be further fine-tuned with task-specific and/or control-code specific data~. Extending such counterfactual explanation generation to other NLP tasks or specific domains is also expensive due to the requirement of additional data and subsequent training. To alleviate these issues, in this work, we perform .% Please add the following required packages to your document preamble:% % % 

For generating the contrast sets, we use the same LLMs as used in Section , except , and prompt the LLM to generate counterfactuals in a zero-shot manner using the input text and ground truth label tuple . Unlike ~, we do not use human annotators to label the generated counterfactuals. Therefore, differing from ~, we only focus on counterfactuals that have the same label as the original input, and use these as contrast sets. We make this choice since the lack of human annotation and lack of step-by-step guidance (such as in ) would make it harder to validate whether the edits performed by the LLM are actually label flipping or not. Instead, we guide the generation process via the instruction in the prompt. We use the following prompt to perform the generation:

where,  is the description of the task, such as ``sentiment classification'',  is the input text, and  is the ground truth label. %Note that for the purposes of this work, we only evaluate on same label counterfactuals as contrast sets, since the lack of human annotation would have it harder to validate whether the edits performed by the LLM are actually label flipping or not.

For evaluating the goodness of the generated counterfactuals as contrast sets, we compare the accuracy of the target model  on both the original test set and the generated contrast set. We also measure the consistency, 

where  is the LLM-generated contrast set for the original input ,  is the ground truth label for the contrast set example and  is the number of test samples. Consistency measures the percentage of times when the model correctly classifies both the original and the contrast set example. Like the previous set of experiments, we want the generated counterfactuals (or contrast sets) to be as close to the original text input as possible, i.e., the edits should ideally be minimal. Therefore, we capture the textual similarity again in the token space via Equation  and the latent space via Equation .

Since there is not much work on contrast sets, we have a limited set of baselines here. We use the original expert-created contrast sets for the IMDB dataset from the original work~. This consists of 488 original test data samples, and 488 contrast samples created by the dataset experts. Furthermore, we use Polyjuice-generated counterfactuals~ as contrast sets for comparison. 

We use the same DistilBERT models as in Section  that are fine-tuned for each of the 3 tasks (IMDB, SNLI and AG News). We evaluate each of these 3 models on both the original test set and the counterfactual one (i.e., the contrast sets) and show these results in Table . We obtain the performance values for Polyjuice-generated contrast sets from the original paper ~. For the `Expert' baseline, the IMDB contrast sets are created by human experts in ~. Unfortunately, there does not exist any expert created contrast set for SNLI and AG News datasets. As evident from the test accuracies on both the original test set and the counterfactual one, we see a consistent decrease in performance on the generated counterfactuals over the original samples. The performance drop for the AG News dataset seems to be the least while interestingly, we see the highest performance drop on contrast sets for the SNLI dataset. Furthermore, we see GPT-3.5 and GPT-4 are able to create contrast sets with high degree of semantic similarity and low edit distance, thus being more desirable over Llama 2 generated contrast sets. Overall, the drop in accuracy and the consistency values seem analogous to similar results in literature (average drop in classification accuracy of around 6.8\% according to ~) that use human-generated contrast sets for evaluation ~.

While this is promising, we do note the ethical concerns surrounding this: LLM-generated contrast sets may induce pre-existing biases that can propagate further bias and errors through evaluation and subsequent model improvement steps. One way to effectively use LLM-generated contrast sets is by broadly identifying the failure models of the model via probing the model using the LLM-generated contrast sets, and  employing human annotators or data creators to hone in on that specific failure mode to either generate more contrast sets or counterfactually augmented training data to fill the identified gap. Such a combined method would greatly reduce costs while still being effective in terms of model evaluation and development. %