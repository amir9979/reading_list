Given -th sequence of tokens  in the pretraining dataset , causal language modeling minimizes the negative log-likelihood loss: 

Assuming that the update occurred for each sequence, and without considering the learning rate, we define the update step as

where  denotes the parameters which is updated for each sequence on . Notably, the pretrained model  is equal to , as both are trained on  token sequences. Subsequently, our unlearning objective is to approximate the optimal parameters achievable from complete retraining, i.e., , from the pretrained model . Concretely,  refers to the parameters before unlearning the target sequence , where  contains the target sequence, and  denotes the optimal parameters obtained from retraining on the remaining data .

In this section, we elaborate on the details of POP and its derivations for the optimal parameter updates for sequence unlearning.

Suppose that the arbitrary sequence  for  is the target sequence . Then,  is updated on  except for  from the randomly initialized parameters :

where  refers to the parameters trained on  without the target sequence  for .  In other words,  is equal to , since it is trained on  except for . By leveraging the equations above, we can derive the equation where  is represented by :

Although the derived equation above is reasonable, we cannot compute the   Equation~ because s during training are intractable. To address this, we constrain , where we suppose the target sequence  is trained just before the last sequence:

where  refers to remaining data  without the target sequence , and we can say that  has more knowledge of the target sequence than  does. 

Using Equations~ and~, we initialize  with , which is iteratively updated to unlearn the target sequence . To assure the relationship between  and , we fix  as , where the parameters remain frozen during unlearning. Then, the iterative update equation for unlearning the target sequence is

where  is trainable parameters initialized with , and is unlearned until convergence to . % , which is initialized with  and will be updated % In the experiment, we cast aspects of gradient as loss and use the following loss terms for ease of implementation: For training, we use the following losses corresponding to the derived gradient terms: % % _{} = _{} + \lambda _{} \\% _{asc} = _[(p_{\theta}())] \\% _{} = _[(p_{\theta^F}())] - _[(p_{\theta}())] %   where  refers to the loss for unlearning the target sequence , while  denotes the loss associated with retaining the remaining data  performance. Putting everything together, the overall training objective for sequence unlearning is minimizing the following loss: 

where  is a loss scaling hyperparameter. In , the first term is ignored by the optimization, even though it contains the initial state of the pretrained LM. Since this leads to underutilization of the pretrained LM for retaining the remaining data, we use the probability distribution over the vocabulary of the pretrained LM as the soft labels.  This is quite intuitive, as the objective of POP is to unlearn the target token sequence  deviating too much from the initial state of the pretrained LM.

%  Given a sequence of tokens , previous studies have proposed metrics to assess ``how well a model remembers a specific sequence of tokens'', and unlearning can be achieved by decreasing the value of these metrics for the forgetting data.  and  suggested Memorization Accuracy (MA) and Extraction Likelihood (EL), respectively:

where  in EL represents the list of n-grams in the given sequence, and  represents the output sequence from the LM. As unlearning metrics are often utilized to determine the thresholds for unlearning, thereby setting the stopping point of the unlearning process, it is important that they accurately portray the privacy risk of LM post-unlearning.  MA and EL, however, disregard the probabilities of tokens within the sequence. In other words, they do not consider the situation where the target token has the second highest probability in the probability distribution for the next token prediction. When these metrics are used to determine the stopping point of the unlearning process, the resulting LM can be vulnerable to various attacks that could extract the target token through sampling methods.

To alleviate this limitation, we propose Remnant Memorization Accuracy (RMA):

Unlike other unlearning metrics, RMA considers the probabilities of tokens to better represent the privacy risk.  Models unlearned until they satisfy the forgetting thresholds for RMA are significantly less likely to be vulnerable to extraction attacks. When utilized individually, RMA is a more stringent unlearning metric, as it is more difficult to satisfy the forgetting threshold.  Figure  shows an example of how RMA can provide a stronger privacy protection compared to other unlearning metrics.  The process for obtaining the forgetting thresholds is in Section , and metric comparisons can be found in Section . 

We experiment on two LMs for model sizes 125M, 1.3B, 2.7B: GPT-Neo LMs~ initially pretrained on the Pile~ corpus, and OPT LMs~, which are pretrained on a deduplicated version of the Pile, along with other corpora.   We perform experiments with the following unlearning methods: 

In Equation~, we set the  as 1 for simplicity.

We source the target sequence data from the Training Data Extraction Challenge.  This data consists of 15,000 examples, each not exceeding 200 tokens in length.  In our experiments, we construct 19 target sequence datasets, each with 32 sequences. Due to copyright issues, we randomly sample the remaining data from the uncopyrighted Pile corpus, without the target sequence. 

Although POP is focused on unlearning a specific sequence of tokens, it is vital that the model performs well in all settings.  Therefore, to ensure that the model is still capable of its original language modeling abilities post-unlearning, we evaluate the model on commonsense reasoning (Winogrande~ and COPA~), linguistic reasoning (Hellaswag~ and Lambada~), and scientific reasoning (ARC-Easy~, ARC-Challenge~, Piqa~, MathQA~ PubmedQA~) tasks.  We also evaluate the model on dialogue tasks (Blended Skill Talk~, Empathetic Dialogues~, Wizard of Internet~, and Wizard of Wikipedia~) to assess the generation capabilities of the model. 

We utilize EL, MA, and RMA to determine when to stop the unlearning process.  More specifically, we consider a token sequence  to be forgotten when all three unlearning metrics fall below the average value on token sequences of Pile's evaluation set that were not seen during the pretraining. This setting was also utilized in~, where they utilized thresholds for EL and MA. Table~ shows the threshold values for each metric, and the detailed process for calculating the thresholds can be found in Appendix~.

We perform unlearning with 5 different random datasets of 32 target sequences, and report the averaged results for various OPT and GPT-Neo models in Table~.  Individual results can be found in Appendix~. Unlearning is performed until the model reaches the forgetting thresholds of all three metrics.  The thresholds can be found in Table~. Here are our observations:

 Deduplicating the pretraining corpora can reduce the privacy risks, as OPT LMs show much smaller EL, MA, and RMA values compared to the corresponding GPT-Neo models.  However, deduplicating the corpora alone is not a valid unlearning solution, as the inherent privacy risk represented by EL, MA, and RMA values are not significantly lower than that of GPT-Neo. 

 UL reaches the threshold much faster than the other two methods, demonstrated by the lower number of epochs required to reach the forgetting threshold.  This is quite intuitive, as it only utilizes a single gradient ascent term, while the other two methods employ additional loss terms. % in the gradient update process.  The actual EL, MA, and RMA values for each model do not follow any pattern; that is, lower values do not necessarily indicate better performance. Instead, they serve as a stopping threshold to confirm the completion of unlearning target tokens.

 UL performs the worst in both LMs for 9 classification and 4 dialogue benchmarks, showing degradation from the initial performance.  This is even more evident in the OPT models, where the drop in performance is significant for dialogue tasks, potentially showing catastrophic forgetting.  POP demonstrates the least amount of degradation, representing a remarkable retention of general language modeling capabilities. 

 UL demonstrates the largest variance in almost all benchmarks, which undermines its reliability and accentuates its dependence on the target token sequence to be unlearned. 

 We believe that the deduplication of Pile corpus on OPT models, along with the inclusion of other corpus in the training data, contributed to the extreme degradation in UL for OPT models.  As GPT-Neo is trained solely on the Pile corpus, the duplicate instances might have contributed to the retention of LM performance after unlearning with UL.  As most LMs include a wide range of corpora in their training sets, we believe that this further proves the strength of POP in demonstrating optimal unlearning  retention of LM performance. 

 Although POP  outperforms UL in most benchmarks, it fails to match the performance of POP. This highlights the essential role of introducing the probability distribution over the vocabulary of the pretrained LM within . %The goal of sequence unlearning methods is to unlearn token sequences without significant degradation in language modeling performance. 

There are two ways to apply unlearning: batch unlearning and sequential unlearning. The results shown in Table~ demonstrate batch unlearning results, in which all target sequences are unlearned at once. In sequential unlearning, target sequences are split into smaller batches, which are unlearned in succession. Although batch unlearning is important to consider, sequential unlearning is a more likely real-world scenario, as unlearning requests will follow a sporadic pattern, requiring a more flexible solution. 

% To assess the practicality of POP,  We sequentially unlearn 320 target sequences, split into 10 batches.  Results for other models are available in Appendix .  As shown in Fig.~, POP demonstrates better retention of performance in both classification and dialogue tasks compared to UL.  After unlearning all 320 target sequences in 10 batches with UL, the performance of the OPT 2.7B model dropped over 18\% in average classification accuracy, and 13\% in average dialogue F1 score.  The performance degradation in the dialogue task is extreme, as the average F1 score dropped to 0.47\%, demonstrating catastrophic forgetting of general LM capabilities.  Furthermore, the performance in both sets of benchmarks reaches the minimum value after 2 batches, demonstrating the major flaw in UL. POP, however, only demonstrates a moderate drop, demonstrating a decrease of 9.6\% for the average classification accuracy and 7.14\% for the average dialogue F1 score. Fig.~ illustrates a qualitative example of the degradation in LM from UL.   After the sequential unlearning of 10 batches with UL and POP, sequences are generated for a given prefix. The generated sequence from the LM unlearned with the UL method demonstrates catastrophic degradation, while the LM unlearned with POP generates an acceptable response.  UL is not a viable option, as repeated unlearning in succession with UL results in a catastrophic failure of LMs.  On the other hand, POP successfully induces the LM to unlearn the target sequences  does not significantly impact the LM performance. 

%  We compare EL, MA, and RMA by unlearning 3 separate GPT-Neo 2.7B models with POP, and stopping the unlearning process once they reach the forgetting thresholds for each metric.  We generate 50 sequences for 1 target sequence using p-sampling with probabilities of p=0.9, 0.7, and 0.5, and use the first half of the sequence as a prefix to generate the second half as a suffix.  Lastly, we compare the generated and the original sequences with BLEU~ and CHRF~, where a lower score is favorable in the context of unlearning, as it indicates less overlap between the sequences. As shown in Fig.~, models unlearned until the RMA threshold demonstrate the lowest BLEU and CHRF scores.  This proves that in the context of unlearning, RMA provides the most privacy protection, as models that satisfy the RMA threshold are less likely to generate the original sequence.  We also perform a qualitative analysis, which is shown on Fig. .  It is clear that the model unlearned until the RMA threshold demonstrates the least amount of overlap between the sequences.  Models unlearned until the EL and MA thresholds, however, demonstrate some overlap in sequences, providing only partial unlearning.  RMA provides the optimal privacy protection, demonstrating apt threshold for unlearning.