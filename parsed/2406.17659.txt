Automated planning algorithms have a long-standing history in the literature of symbolic AI and have been widely used in robot systems.  Shakey is the first robot that was equipped with a planning component, which was constructed using STRIPS~.  Recent classical planning systems designed for robotics commonly employ Planning Domain Description Language (PDDL) or Answer Set Programming (ASP) as the underlying action language for planners~. % Researchers have utilized classical planning algorithms for various robotic applications, including sequencing actions for a mobile robot on delivery tasks~, reasoning about safe and efficient urban driving behaviors for autonomous vehicles~, planning actions for a team of mobile robots~, learning to ground properties for unknown objects~, and completing service tasks in open-world scenarios~.%  Most classical planning algorithms that are designed for robot planning do not consider perception. Though some recent works have already shown that training vision-based models from robot sensory data can be effective in plan feasibility evaluation~, their methods did not tightly bond with language symbols which are the state representations for classical planning systems.   %  The most relevant work to our study is probably the research by~, which trained domain-specific predicate classifiers from webscale data and deployed on a robot planning system~. We propose  that investigates how off-the-shelf Vision-language Models connect perception with symbolic language which is used to represent robot knowledge. %  In the light of the recent advancement in artificial intelligence, many LLMs have been developed in recent years~. These LLMs can encode a large amount of common sense~ and have been widely applied to robot task planning~.  % For instance,  showed that LLMs can be used for task planning in household domains by iteratively augmenting prompts. % SayCan is another approach that enabled robot planning with affordance functions to account for action feasibility, where the service requests are specified in natural language~.  However, a major drawback of existing LLMs is their lack of long-horizon reasoning/planning abilities for complex tasks~.  Specifically, the output they produce when presented with such a task is often incorrect in the sense that following the output plan will not actually solve the task.   As a result, a wide range of studies have investigated approaches that combine the classical planning methodology with LLMs in robotic domains~. However, neither LLMs nor classical planners are inherently , often necessitating complex interfaces to bridge the symbolic-continuous gap between language and robot perception.  Our approach seeks to ground classical planners by utilizing pre-trained VLMs through a novel but straightforward domain knowledge prompting strategy.

VLMs have emerged as powerful methods that integrate visual and linguistic information for complex AI tasks~.  % These models fuse data from different modalities enabling complex tasks like image captioning, visual question answering and scene generation from text such as CLIP~, GPT-4V~, Gemini% ~ and Claude 3~. Researchers have started to employ such models in robot systems~, where these models have shown effectiveness in, for example, semantic scene understanding~, open-ended agent learning~, guiding robot navigation~ and manipulation behaviors~. Recent VLMs have also been used for building  frameworks~. Adaptive planning significantly improve task performance through better environment awareness and fault recovery, and language understanding allows robots to seek human assistance in handling uncertainty~.  % For example, the work of~ improves model performance with fewer training steps and optimizes the mixing of pre-trained data domains to improve downstream task accuracy;~ integrates high-level language policy with reinforcement learning to solve long-period robotic tasks.  There have been recent methods, similar to us, that query VLMs for action success, failures, and affordances~.  % SuccessVQA has investigated how VLMs enable robots to detect action outcomes and model action rewards.% They also treat success/failure detections as VQA tasks, but did not consider affordances before action execution. % PaLM-E is a large embodied VLM that is trained to predict robot action sequences as well as solve other downstream vision-language tasks. % PaLM-E has demonstrated its effectiveness in both failure detection and affordance prediction. Different from their work,  uses classical planners to generate executable symbolic plans rather than solely relying on pre-trained models. Additionally,  integrates domain knowledge into the prompts, enhancing the grounded connection between the VLMs and the symbolic planner.

% Researchers have developed a novel framework for robot planning using Multimodal Language Models (MLMs), motivated by advanced foundational models~. % % Closed-loop  reasoning and % Other studies have fine-tuned MLM and introduced it as a robot planner; DoReMi improves model performance with fewer training steps and optimizes the mixing of pre-trained data domains to improve downstream task accuracy; Plan-Seq-Learn (PSL) is a high-level language policy integrated with reinforcement learning to solve long-period robotic tasks and achieve state-of-the-art results without the need for pre-defined skill libraries~. % % In this section, we briefly summarize basic concepts in classical planning and Vision-Language Models, which serve as the two main building blocks of this research.% % Formally, the input of a planning problem  is defined by a tuple . %  is a finite and discrete set of states used to describe the world's state (i.e., state space).% We assume a factored state space such that each state  is defined by the values of a fixed set of variables.%  is an initial world state.%  is a set of goal states.  are usually specified as a list of , all of which must hold in a goal state.%  is a set of symbolic actions. Actions are defined by their preconditions and effects.%  is the underlying state transition function. % %  takes the current state and an action as input and outputs the corresponding next state. % State transitions are usually deterministic in classical planning problems but are not in real-world scenarios.% % % %     \item  is a finite and discrete set of states used to describe the world's state (i.e., state space).% %     We assume a factored state space such that each state  is defined by the values of a fixed set of variables.% %     \item  is an initial world state.% %     \item  is a set of goal states.  are usually specified as a list of , all of which must hold in a goal state.% %     \item  is a set of symbolic actions. Actions are defined by their preconditions and effects.% %     \item  is the underlying state transition function.  takes the current state and an action as input and outputs the corresponding next state. % %     State transitions are usually deterministic in classical planning problems but are not in real-world scenarios.% % % A solution to a classical planning problem  is a symbolic plan  in the form of , such that the preconditions of  hold in , the preconditions of  hold in the state that results from applying , and so on, with the goal conditions all holding in the state that results after applying .  % % A general definition for VLMs is that they are models that combine both vision and language modalities.% Most VLMs require encoders for both vision and language so as to train joint feature embeddings.% One typical training strategy is by using Contrastive Learning~.% Pre-trained VLMs have shown impressive capabilities in downstream tasks such as image captioning~, open-vocabulary object detection~, and visual question answering~.% In this work, we relate robot actions with VQA queries for grounding long-horizon planning. % We use the ViLBERT model~ pre-trained on the VQA v2.0 dataset~ which is publicly available in the AllenNLP platform~.% [ht]%     \centering%     {1.1}%     .%     % https://docs.google.com/spreadsheets/d/1nLJ_mQmA9BGhe31Dnu9ZZYO9vfj0gcttv6AZnrXEo0Y/edit#gid=0%     }%     {!}{%%      &%         \def &%         \def &%         \def &%         \def & avg. (\%)\\%         \midrule%         {l}{{}} & \\%          &  (w/ GPT-4) & 12/18 & 18/20 & 14/23 & 15/20 & 8/20 & \bf 66.5\\%         \midrule%         {l}{{}} & \\%          & Open-loop planning & 5/35 & 3/11 & 4/30 & 8/30 & 1/25 & 17.1\\%          &  (ask affordance) & 4/28 & 11/20 & 7/20 & 10/20 & 5/20 & 35.9\\%          &  (ask success) & 7/18 & 18/20 & 10/20 & 12/20 & 4/20 & 51.8\\%          & (ask both) & 8/20 & 12/16 & 11/20 & 13/20 & 7/20 & 54.0\\%         \midrule%         {l}{}} & \\%          & Eff. only & 15/30 & 15/16 & 4/15 & 10/15 & 7/25 & 53.0\\%          & Pre. only & 3/17 & 15/20 & 7/20 & 11/20 & 5/20 & 41.5\\%         \midrule%         {l}{{}} & \\%          & Gemini-1.5 & 7/20 & 14/20 & 6/20 & 9/20 & 8/20 & 44.0\\%          & Claude-3 & 5/20 & 12/28 & 4/14 & 10/20 & 3/13 & 33.9\\%         \bottomrule%     %%     }% % % %     \includegraphics[width=0.75\textwidth]{figures/exp_figs/Ablation_accuracy.pdf}%     % \includegraphics[width=0.43\textwidth]{figures/num_of_success.pdf}%     %     }% % % % %     \includegraphics[width=0.75\textwidth]{figures/exp_figs/Ablation 2_accuracy.pdf}%     % \includegraphics[width=0.43\textwidth]{figures/num_of_success.pdf}%     %     }% % % % %     \includegraphics[width=0.75\textwidth]{figures/exp_figs/radar_Baselines.pdf}%     % \includegraphics[width=0.43\textwidth]{figures/num_of_success.pdf}%     %     }% % % % %     \includegraphics[width=0.75\textwidth]{figures/exp_figs/radar_Ablation.pdf}%     % \includegraphics[width=0.43\textwidth]{figures/num_of_success.pdf}%     %     }% % % % %     \includegraphics[width=0.75\textwidth]{figures/exp_figs/radar_Ablation 2.pdf}%     % \includegraphics[width=0.43\textwidth]{figures/num_of_success.pdf}%     %     }%     % % %  Our system considers ten actions (as listed in Table~), including basic navigation and manipulation. Situations occur after actions are successfully triggered by the agent. Table~ also provides examples of situations that happen following specific actions. Some of these situations impact the world states, while others do not. For example, the robot may fail on a ``grasp'' action, resulting in the target object, originally on the table, to fall on the floor nearby (changing the state from  to ). On the other hand, the object might also remain on the table with the world states being unchanged. % The frequency of most of the situations are controlled by probabilities as hyperparameters in our experiments. To quantify the openness of different environments, we created the simulation platform in such a way that one can easily adjust the probability of a situation's occurrence.  The source code of our benchmark system will be made available in our project website. 

Actions are implemented in a discrete manner for simplification purposes, since continuous action execution is not this paper's focus. For instance, ``find'' action is implemented by teleporting the agent from its initial position to a randomly-sampled obstacle-free goal position near the target, and ``fill'' action is by adding fluid particles directly into the container that the robot is holding.

Actions are subject to several constraints. For example, ``grasp'' action is deemed executable only if the target object is in the agent's view (assuming vision-based manipulation) and the agent's hand is empty. Similarly, ``cut'' action is considered executable only if the object to be cut is in the agent's view and the agent is currently holding a knife. Calling an action with at least one unsatisfied constraint will result in an action failure, but without any changes to the world states. Note that such constraints are not made available to agents, instead, they are partially encoded as domain (action) knowledge that the agent possesses. % (detailed in ).

We assume that situations can only happen during action execution, but are only observable by agents either before or after the action execution phase.  This assumption indicates that situations are solely caused by actions, and we are aware of a few recent robotic research that have started to consider more generalized situation handling~.  We leave situations that caused by external environmental factors (human or other embodiments) to future work.

%  A single action is usually defined by multiple preconditions and effects in the domain knowledge. VLMs, especially for those that are not trained using domain-specific data, frequently produce inaccurate answers that cause disagreements among the given preconditions~(or effects). For instance, the VLM might answer ``Yes'' to both  and  after the robot picks up an apple from the table. In this paper,  categorizes predicates into three: , , and .  will only ask about  predicates. Intuitively, we believe VLMs should be and will be only good at visually-perceptible predicates.  The robot will then have ground truth access to  predicates (this assumption also applies to all other baselines). We leave identifying these predicates using more advanced Multimodal Language Models to future work. As for the remaining  predicates, the  agent maintains a positive attitude and assumes they are always True.  This suggests that  believes these predicates will never be affected by any situation.

Before every action execution,  ~extracts knowledge about action preconditions from the planner's domain description. For instance, as indicated in Figure~, action  has preconditions of , , , and , meaning that to grasp an object  from a receptacle ,  should be open (not closed),  should be in the agent's current first person view,  should be inside , and the agent's hand should be empty. Then, we simply convert each action precondition into a natural language query by using manually defined templates, though it has been evident that LLMs can be used for the translation between PDDL and natural language~. Examples include  and  Paring each natural language query with the current observation from the robot's first-person view, we call the VLM to get answers indicating if the precondition is satisfied.

According to the results (i.e., ``yes'', ``no'', or ``skip'' if unsure) from the VLM,  will update the current state information in the classical planning system. Figure~ (Left) shows an example where the robot wants to  but fails to detect ``cabinet is open'', ``cup is inview of agent'', and is suspecious about if ``cup is in the cabinet'' (the VLM answers ``skip'' to this question) given the current observation. As a result,  will update the current state by changing  to , and removing .  will remain the same because we do not update the state if the VLM answers ``skip'', indicating the agent holds a positive attitude that situations will not commonly occur. We then provide the updated world state to the classical planner as the ``new'' initial state to re-generate a plan.  In the above example, instead of , the robot will now take the action of  again according to the newly-generated action plan. After every action execution,  extracts knowledge about action effects from the planner's domain description, illustrated in Figure~ (Right).  It queries action effects by using the VLM.  If the effects are not satisfied, the robot will update its belief on the current states and re-plan accordingly. The knowledge-based automated prompting strategy of VLMs enables our planning system to adaptively capture and handle unforeseen situations at execution time.  % % Before every action execution, \tpvqa ~extracts knowledge about action preconditions from the planner's domain description.% For instance, as indicated in the tcolorbox, action  has preconditions of , , and , meaning that to grasp an object  from a receptacle ,  should be in the agent's current first person view,  should be on inside , and the agent's hand should be empty.% Then, we simply convert each action precondition into a natural language query by using manually defined templates, such as  and % % The conversions are implemented by using some manually defined templates.% Paring each natural language query with the current observation from the robot's first-person view, we call the VLM to get answers indicating if the precondition is satisfied.% According to the results from the VLM,  will update the current state information in the classical planning system.% Fig.~ shows an example where the robot wants to  but fails to detect ``cabinet is open'', ``cup is inview of agent'', and is suspecious about ``cup is in the cabinet'', (the VLM answers ``skip'' to this question) given the current observation.% Because the classical planner always assumes perfect action executions, it will incorrectly believe all previous actions are successful, including the preceding action .% However, the cabinet is only half-opened causing the next grasp action to be not executable. % As a result,  will update the current state by changing  to , and removing .%  will remain the same because we do not update the state if the VLM answers ``skip'', indicating the agent holds a positive attitude that situations will not commonly occur.% We then provide the updated world state to the classical planner as the ``new'' initial state to re-generate a plan. % In the above example, instead of , the robot will now take the action of  again according to the newly-generated action plan.% After every action execution,  extracts knowledge about action effects from the planner's domain description. % Similar to how  asks about preconditions, it queries action effects by using the VLM. % If the effects are not satisfied, the robot will also update its belief on the current states and re-plan accordingly.% % \newline% % \noindent% % A single action is usually defined by multiple preconditions and effects.% VLMs, especially for those that are not trained using domain-specific data, frequently produce inaccurate answers that cause disagreements among the given preconditions~(or effects).% For instance, the VLM might answer ``Yes'' to both  and  after the robot picks up an apple from the table.% In this paper, we query the VLM about all the listed effects~(preconditions), and determine to re-execute~(re-plan) if the majority of them are not satisfied. Quantitative evaluation results are collected in the OmniGibson simulator~.  The agent is equipped with a set of skills, and aims to use its skills to interact with the environment, completing long-horizon tasks autonomously. In the experiment, we consider five everyday tasks that are ``boil water in the microwave'', ``bring in empty bottle'',``cook a frozen pie'', ``halve an egg'', and ``store firewood''. Their detailed descriptions are shown in Table~. These five tasks are originally from the Behavior 1K benchmark~ that are accompanied with the simulator. Task descriptions including initial and goal states are written in PDDL and symbolic plans are generated using the fast-downward planner~.

% % % To quantitatively evaluate the performance of \tpvqa ~in dealing with imperfect perception and uncertain action outcomes, we build a simulator using web-scale diffusion models.% % We first took images from real environments and use the image variation API provided by DALL-E~ to augment the original dataset.% For a small portion of the actions for which real photos are difficult to get, such as a robot washing a plate, we manually design prompts as inputs to DALL-E. % Each action is paired with 10 successful observations and 10 failed ones.% Overall, our image dataset consists of 20\% real photos, 60\% images from real photo variations, and 20\% images directly generated from text prompts.% Fig.~ shows example images from our dataset.% At each time step, an observation for the current action is sampled from the dataset.% We assume that there is a probability of 25\% that an action may fail which will result in a failed action observation. % We also assume there is another 25\% chance that a failed action may cause changes to previous states.% For instance, when the robot fails on the action , there is a chance that the apple~(or the knife) is not in the robot's hand anymore.% To model this uncertainty, we re-sample one of the previous observations to let the robot estimate the current world state.% The robot needs to successfully execute all the actions so as to complete the task, leading the system to the desired goal state.% % \tpvqa ~is compared with the following four baselines:% %     \item EffectVQA: An ablative version of ours where VLMs are only used for action effect monitoring.%     \item TP: A task planning baseline without perception.%     \item PaLMEVQA: PaLM-E~ is robust to most of the vision-language downstream tasks.%     Among those, we are more interested in affordance prediction and failure detection.%     To this end, PaLMEVQA is a baseline that is designed with prompts provided in the original PaLM-E paper, which are  and %     \item SuccessVQA~: We use the same query provided in their paper, which is  SuccessVQA does not consider affordance.% % Note that neither PaLM-E nor Flamingo~~(as used in the original SuccessVQA paper) is open-sourced, so we use the same VLM as ours~ for implementing their corresponding baselines.% As discussed in Section~, both PaLM-E and SuccessVQA are trained on robotics data, but all evaluations in this paper do not involve any altering for the VLM itself. Figure~ presents the main experimental results and details the comparative success rates of various methods from the literature. The methods include:

When using VLM itself as the planner, the agent frequently fails in finding an executable plan, resulting in the lowest success rate. This finding is consistent with recent work~ and motivates the development of other research that combines classical planning with large models~.  , which operates without visual feedback during task execution, shows the second lowest success rate across five tasks compared to other evaluated methods, highlighting its limited effectiveness in handling situations and recovering from potential action failures.  In contrast, methods that involve querying for action affordances, success probabilities, or both, acheive much higher success rates as compared to the ``blind'' classical planning approach.  This improvement demonstrates the general advantage of incorporating visual feedback and high-level reasoning in task planning systems. While it is always a good practice to verify both before and after an action (like ), we found that  also surpasses the performance of , indicating that there is a greater positive impact on task completion from action failure recovery, and VLMs have better zero-shot reasoning capabilities on the direct effects caused by actions. 

We observed that  consistently outperforms baselines in task completion rates, which supports our hypothesis. By incorporating domain knowledge~(i.e., action preconditions and effects) for prompting,  is significantly better than other methods, including  that also cares about affordance prediction and failure detection. However,  queries about actions solely by their names, which provides less information than the detailed domain knowledge used by , indicating that action knowledge is more informative for pretrained VLMs to reason over.

Table~ presents an ablation study comparing the performance of different versions of our approach across the same set of tasks.  integrates both action effects and action preconditions, while we are also curious to know how they affect the overall task completion independently.   achieves an average success rate of 66.5\%.  For ablation methods where only action effects are considered (), the average success rate drops to 53.0\%, and for methods considering only preconditions (), it further decreases to 41.5\%.  This suggests that the integration of both effects and preconditions in  significantly enhances task performance compared to considering these components separately.

%  We also run experiments on various VLMs, including GPT-4 (as being used in the original implementation of ) from OpenAI~, Gemini 1.5 from Google~, and Claude 3 from Anthropic. According to Figure~, GPT-4 consistently performs better than Gemini and Claude. By looking at the highest accuracy among all the VLMs (i.e., less than 65\%), our evaluation benchmark (designed with challenging open-world situations and rich domain knowledge) presents a simulation platform, dataset and success criteria that other researchers working on AI planning, VLMs or both might find useful.  We will open source the benchmark including software and data to the public after the anonymous review phase. 

% presents a significant challenge, both for the community focused on robotic planning (see the  baseline) and for assessing the vision-language understanding abilities of large-scale models % In future work, we would also like to expand and open-source our test benchmark, aiming to benefit practitioners involved in training VLMs as well as the robotic planning community by providing them with embodied data and simulation for improving their models and systems.% % We observe that \tpvqa ~consistently outperforms baselines in task completion rate, which supports our hypothesis.% As the number of required action steps of tasks increases, the success rates of all the methods decrease as expected.% By considering action knowledge~(i.e., preconditions and effects), \tpvqa ~and EffectVQA are significantly better than others, especially the ones~(PaLMEVQA and SuccessVQA) that only query about actions by their names.   % We can also tell that methods additionally considering action affordances~(\tpvqa ~and PaLMEVQA) perform better than the methods that only detect action failures~(EffectVQA and SuccessVQA).  % % % % % %     \includegraphics[width=0.9\textwidth]{figures/side.pdf}% %     % % % % % Another interesting finding is that TP, a baseline that does not include any perception, produces a higher success rate than PaLMEVQA and SuccessVQA, which are two other baselines that are capable of interacting with VLMs.% % That is because false positives and false negatives from VLMs will greatly impact the plan execution, easily leading the robot to failure cases.% TABLE~ shows that when querying about action knowledge, the VLM is more accurate in failure detection and affordance prediction.% If directly querying about action names, prediction accuracies for most of the actions are lower than a random guess, which is why SuccessVQA and PaLMEVQA perform poorly. % A straightforward example is when detecting if the robot has successfully washed the plate, instead of asking , \tpvqa ~will query .% We observe that the latter type of queries~(ours) is easier for VLMs to understand.% % %     \includegraphics[width=0.7\textwidth]{figures/short_example.pdf}%     .}% % % 

We also deployed  ~on real robot hardware to perform object rearrangement tasks~(Figure~), where the goal is to ``collect'' toys using a container and place them in the middle of the table~(i.e., goal area). Our real-robot setup includes a UR5e Arm with a Hand-E gripper mounted on a Segway base, and an overhead RGB-D camera (relatively fixed to the robot) for perception. We assume that the robot has a predefined set of skills, including , , and .  and  actions are implemented using GG-CNN~, and  action simply uses base rotation for capturing tabletop images from different angles.

Given the task description, the robot first decided to execute ``find container'' and ``pick up container''. These two actions were successfully executed as shown in Figure~(a),~(b). When the robot was preparing for the next action~(i.e., ``Place container into the goal area''), the blue container accidentally dropped from the robot's gripper to the ground~(Figure~(c)). Instead of directly executing the next action,  ~enabled the robot to check preconditions by querying the VLM  After receiving negative feedback from the VLM,  ~updated the world state by removing  and called the planner to generate a new plan that started the task again by finding another container~(Figure~(d)). Then the robot picked up the cyan container and placed it in the middle of the table as shown in Figure~(e),~(f). The subsequent actions in the plan were to find and pick up a toy, but the  action failed~(Figure~(g)).  ~managed to detect the failure by querying 1) , and 2) , and receiving Yes and No answers respectively. As a result, our system suggested the robot repeat the  action again~(Figure~(h)). Finally, the robot successfully collected the toy by putting it into the cyan container that was previously placed in the goal area~(Figure~(i)).