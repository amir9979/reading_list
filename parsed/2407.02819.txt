%We show how the  The above design of  allows us to tackle the core computational inefficiency of .  Instead of incurring the CPU overheads of trie-lookups during training, we propose to pre-enrich the training corpus with  the compact  distributions  stored along with the corpus.  %We only need %to store the %%Top- frequent tokens and their probabilities for computing . Assuming sequence length , this will incur additional cost of storing  \{token, fraction\} pairs per sequence. If we represent the fractions in same number of bits as token IDs,  The total storage cost becomes only  times the original storage cost but we avoid the expensive trie lookup operations during training. Also,  % Since we %have stored store the  distribution with the dataset itself, we no longer need to do %an expensive  a (slow) trie lookup operation on CPU.%%Since a trie lookup is not required at training time, this also eliminates the need to store the trie at all. Rather,  we can build the entire trie in-memory only once during the pre-processing phase and discard after the data enrichment phase.   %use it to ``enrich'' the dataset (i.e. store the queried -gram distribution with the data itself) and then completely free the trie memory. %.% with few modifications.  We implement our trie in C, similar to . Each TrieNode consists of the count and a HashMap where key is the next token ID and value is a pointer to the next TrieNode. The HashMap is implemented using an AVL tree. We implement a Top- query method which returns Top- token IDs and Top- probabilities for a given prefix of upto  tokens using Eq. .%First we use a custom HashMap implementation to store children nodes and their counts. The key in this map is token ID and values are counts. The counts represent the number of times prefix corresponding to path from root node () to that node () appears in the dataset. This HashMap is implemented with a self balancing tree (specifically an AVL tree) with 2 different sets of pointers. One set of pointers ensures that the map is sorted on keys i.e. token IDs allowing lookup in  while the other set of pointers ensures that map is sorted on values i.e. count values.%  Concretely, we first construct the trie by reading sequences of  tokens from the dataset and inserting it in the trie. At every level , we increment the count by 1 to implicitly record prefix . Once the trie is constructed, we start reading (disjoint) sequences of  tokens from the dataset and writing the ``enriched'' sequence back to disk. To ``enrich'' a sequence , we first look up the prefix in the trie and get pointers to  nodes i.e. one at each level. At each of these nodes, the HashMap is storing . We can efficiently traverse this HashMap in descending order of fractions to get . Once we get , we can write the ``enriched'' sequence to disk. Note that, this operation still requires disk storage of  tokens where  is the total number of sequences. We present additional discussion and a sample walkthrough of this procedure for more clarity in Appendix . %After the enriching is done, the original dataset may be deleted to make space for other training artifacts e.g. model checkpoints. To build the mini-batch of size  in standard NTL, one simply reads sequences of  tokens  times and concatenates them. In standard NTL, the sequence of  tokens itself is both input and target but in , we have 2 targets that need to be built. In  we need to read sequence of  tokens from the disk and use the first  tokens to form  similar to NTL. Next  ``tokens'' correspond to  where the first token is the actual token ID and the next token is really the count of that token appearing after prefix . The  tokens following this would correspond to  which encodes counts and tokens appearing after prefix . This would repeat for  times to give  distributions as targets for KL divergence in Eq. . We provide additional discussion on memory footprint in Appendix .

We explore the effectiveness of  fine-tuning existing models on WikiText-103 , MiniPile  and a subset of PubMed-Abstracts . The WikiText-103 training split consists of M tokens while MiniPile and PubMed splits consist of  1.6B and 2.6B tokens respectively.

For our full fine-tuning experiments, we compare  and NTL objectives on WikiText-103. We compare against D2GPO baseline  as well since they also use a KL divergence based augmentation of training loss. 

%D2GPO performs label smoothing by minimizing KL divergence similar to Eq.  but the target distribution it uses is based on (corpus-derived) word2vec  similarity. We finetune 3 different pretrained models on respective datasets for 40k steps with learning rate of  and Adam  optimizer.  Since  raised concerns about isotropy of the base gpt2-125m  model, we also study effects of  on gpt-neo-125m  and opt-125m, opt-1.3B .% models. % we use GPT-2 , GPT-Neo  and OPT  models and obtain their pretrained checkpoints from HuggingFace,  ,   respectively}}. Following , we evaluate each fine-tuning method several on model quality and text quality metrics. 

We also compare the Zipf coefficient of the generated text to gold text as suggested by . For generating text, we either use greedy decoding or nucleus  sampling. To compare training efficiency, we use following metrics:

% In Table , we compare performance of  various other objectives. We find that  able to provide consistent gains over the NTL baseline. We also notice that  D2GPO  which indicates that count based conditional -gram models are able to provide a stronger training signal as compared to word embedding similarity. Maximum gains with  observed on gpt2-125m, however this could be related to isotropy of gpt2-125m checkpoint as discussed by . Across all metrics and models,  generally the best performing model while  close to it.  gains (albeit modest as compared to small models) to larger models (opt-1.3B) as well. %We continue to see improvements over baseline NTL even at larger model sizes as seen by results on OPT-1.3B model. %% Figure  compares our training efficiency metrics across various training methods on WikiText-103 and PubMed datasets.  Python's  to implement the trie HashMap while  our efficient implementation of HashMap as described in Sec. .  the resultant trie to disk using the Python's  library. We also explore using an existing -gram implementation  with Python FFI as denoted by -CFFI. Because  the -gram model on disk for later retrieval, its disk overhead grows with the size of the dataset as opposed 's constant (dependant on  only) disk overhead. Measuring the total wall clock time, we find that () is  than () which highlights the benefits of our approximation and pre-enriching of the dataset. 

Results on PubMed demonstrate challenges in scaling up vanilla  large datasets. On our machine with 256GB RAM, the prefix trie for  with vanilla  not fit in memory. It is possible that -CFFI is able to fit everything in memory similar to  we could not explore in depth since our -gram implementation  kept crashing on our system. While -CFFI saves disk and RAM, it still is not as efficient as  WikiText-103. Moreover, the trie loading time as well as the (somewhat) slow CFFI interface serialization overheads significantly increase total wall time of -CFFI over . 

% The results on PubMed demonstrate difficulties in scaling \AllNTL\ to larger datasets. On our machine with 256GB RAM, the prefix trie for  did not fit in memory. It is likely that Python's garbage collector does not kick in properly when creating such large indices since we find that \sysname\ is able to fit the index in memory. Most of storage, RAM and wall clock overheads in \AllNTL\ are due to creation or handling of the prefix trie. Since \sysname\ does not need to store prefix trie explicitly, we see benefits across all metrics. We also observe effects of poorer  approximation on training efficiency on PubMed dataset where the support of  is greater than  even after  resulting in somewhat delayed (in comparison to \AllNTL) convergence of \sysname\ as measured by Steps-to-PPL. As compared to datasets used in this work, modern LLMs  are often trained on far bigger web corpora  for which building an -gram model in-memory may not be feasible. In such cases, we show that such datasets can be sharded into multiple small datasets of several few billion tokens with each shard being enriched with its own -gram index. We study the effect of sharding by simulating it on MiniPile and PubMed datasets and seeing effect on perplexity as shown in Fig. . We find that after a certain threshold of shards, the number of tokens per shard decreases so much that KL penalty can become overly optimistic and result in worse perplexity. In general, we found that having more than billion tokens per shard was sufficient to get results close to  still using modest amount of RAM.

%To study the effect of  and  on model quality,  By default we choose . We report ablations on these values  when fine-tuning gpt2-125m on WikiText-103 and compare using validation perplexity. As indicated by trends in Fig. , we find that increasing  and  leads to predictable improvements in perplexity as compared to the NTL baseline. We do notice a significant difference in perplexities between  lower values of . This could be potentially due to poorer  approximation since the fan-out is expected to be significantly higher for initial few tokens. Empirically, in the first few levels of the trie, the branching factor is the highest often leading to  that has a support size much larger than . This is further supported by the significant improvement observed when going from  to  for a fixed . Moreover, as  increases, the support of  naturally decreases. In fact for , the average support size is less than 4 on WikiText-103. This implies that  in Eq.  leading to  which effectively reduces .

% [!ht]%     \centering%     \footnotesize%     %     %     %  We show that using  -gram statistics from alternative corpus is also useful. To study this, we augmented our existing WikiText-103 -gram trie with -grams from MiniPile and PubMed datasets independently. As shown in Table , model trained with  WikiText-103 + MiniPile index improves perplexity on both WikiText-103  MiniPile. On the other hand, if the augmenting dataset (PubMed) is both large  domain specific, the resultant model improves %performance  on augmenting %(PubMed)  dataset  of performance on the original (WikiText-103) dataset.

% We show that even though we impose the  only in a small prefix of an overall sequence of length 256, the improvement in model quality is throughout the length of the sequence. For this, we measure the negative log-likelihood (NLL) separately at each position from 1 to 256 in the test data and show our findings in Figure~.% we show the average NLL against the position in sequence averaged over sampled batches in the test dataset for baseline (\NT), SimCTG, AllNTs and CoCoNTs with .   We observe that we reduce NLL at all sequence positions over the two existing methods.   % Also,  accounts for a large part of the gain, and with a larger prefix , we get a modest boost.   This suggests that in today's NLM architectures with shared parameters, enforcing the correct loss in only a subset of the positions is sufficient for overall gains.

 We study that pretraining for more steps flattens the benefits of CoCoNTs slightly but not completely. We continued pretraining of gpt-neo-125m from our experiments (Sec. ) for 20k more steps (50\% more than original pretraining steps) and report our findings in Table . Looking at the validation perplexities, we do see that the difference between baseline and CoCoNTs perplexities does decrease from 2.5 to 2.3 over the 20k additional steps. To correctly assess if this difference goes to zero, significantly longer training is required.

Parameter efficient fine-tuning (PEFT) of LLMs  has become a popular method to adapt general purpose LLMs such as LLaMA-2  on a specific domain. To study benefits of  in this direction, we first train a LoRA  to fine-tune LLaMA-2-7B on our split of the PubMed dataset using each of the fine-tuning method. Both   while  . With this LoRA as the starting point, we fine-tune the resultant (Medical LLaMA) model on 2 medical QA (PubMedQA  and MedQA ) tasks independently. As summarized by accuracy of these QA tasks in Table , we find results similar to full fine-tuning i.e.  better than NTL but worse than . However,  an almost 60\% increase in pre-training time, whereas our methods reduces the overheads by half as seen by the last total wallclock time (TWT) column in the table.  %Similar to results on opt-1.3B, the gains here are far more modest in comparison.%Motivated by promising results from fine-tuning experiments, we explore whether pre-training an LM from scratch with \sysname\ can lead to a better LM for downstream tasks.  Can  used to pre-train a better language model for downstream tasks? We pre-train an opt-125m architecture model from scratch on the data from the BabyLM challenge . We use the nearly 100M word data from the ``strict'' track with standard preprocessing. Once all (NTL, , ) the models are pre-trained, we finetune each on 3 downstream tasks (with recommended hyperparameters) from the evaluation suite -- CoLA , MRPC  and RTE  which are subsets of the SuperGLUE benchmark  for the BabyLM challenge. Task specific fine-tuning does not use any custom objective. Table  show metrics on respective tasks. %The CoLA task is about linguistic acceptibility i.e. determining whether the given sentence is grammatical or ungrammatical.  Both  base LMs show significant improvement over standard NTL in CoLA which is about linguistic acceptability. This could be due to grammatically incorrect sentences/completions automatically being suppressed in the -gram index. On other downstream tasks such as paraphrase detection (MRPC) or entailment (RTE), the performance of all models is much closer in comparison.  Also, observe that the total wall clock time (TWT) for  35\% higher than baseline, and  the overhead down to 19\%. 

% Model performance section% Training efficiency section% LLaMA-LoRA pretrain and finetune% BabyLM pretrain and finetune Most of our experiments are performed on a single TPUv2-8 core VM with TPU VM architecture. We also sometimes used 4x NVIDIA A100 GPUs with FlashAttention for a fraction of all experiments. 

For fine-tuning experiments (Sec. ), we start with publicly available checkpoints for gpt2-125m, gpt-neo-125m, opt-125m and opt-1.3B. Each 125m parameter model is trained for 40k steps with AdamW  optimizer with effective batch size of 192. The maximum learning rate was set to  with 10\% of maximum steps as warmup followed by cosine decay to zero. For the 1.3B parameter model, we set the maximum steps to 10k and reduce the batch size to 32. Each fine-tuning run took roughly 5-6 hours on TPUs and 8-10 hours on GPUs.

For pre-training on the BabyLM challenge (Sec. ), we set the batch size, optimizer and learning rate schedule similar to fine-tuning and trained for total of 5 epochs. For downstream tasks, we use the hyperparameters mentioned in the BabyLM evaluation pipeline. The pre-training took 5.5 hrs on TPUs while fine-tuning on downstream tasks took 1-1.5hrs each.

For parameter-efficient fine-tuning of LLaMA (Sec. ), we set the batch size to 32 and fine-tune on PubMed with hyperparameters similar to WikiText-103 fine-tuning. We use the same parameters as BabyLM downstream tasks for LLaMA PEFT downstream tasks as well. The pre-training took close to 6 hours on TPUs while fine-tuning on downstream tasks took 2-2.5hrs each.

 We show a simple example to illustrate how  manages to reduce variance in the next-token distribution across sampled mini-batches. Assume vocab size is 5, and . Let .  Assume .  For this case we have . Everytime we sample a mini-batch where next token is from the top-2 set: , we supervise with  which has a distance of 0.166 from . Whereas, when we sample the last three tokens we are at most 0.51 from .  Contrast this to the baseline NT case where for rare token the distance to  could be as high as 1-0.05=0.95!  With  we reduce this distance to 0.51.  Even for frequent tokens the distance has been reduced from a maximum of 0.7 to 0.166. 

 A crucial hyperparameter in our approximation is . While we see the effects of  on overall model quality in Sec. , we study the effect of  in a more controlled way when learning a single 10-class multinomial. By increasing , as shown in Fig. , we find that both convergence rate as well as KL divergence between learned and actual multinomial consistently improves.

In this section, we provide a step-by-step walkthrough of Pre-enriching process for the dataset.

First, we assume that the prefix-trie is already created and available in-memory for . This trie has a crucial property: the ``count'' attribute (64 bit unsigned integer in our implementation) of each node  indicates how many times the prefix, which corresponds to the path from the root to , appears in the training dataset . The root node stores the count of total prefixes in the dataset. Figure  shows an example trie, where the node associated with the token ``United'' has a count of 1000, while the root node has a count of 40000. This means that there are 1000 sequences in total that begin with the word ``United''. Additionally, the child node ``States'' has a count of 526, indicating that there are 526 sequences that start with ``United States''.

Let's assume that a block (length ) beginning with "United States of America '' is selected in the pre-enriching second pass over the dataset. %In our notation this corresponds to inputs  and labels .  Since , we retrieve from the prefix trie with prefix ``United States'' as shown in Figure . At every level (i.e. highlighted ), first  will be created in floating point representation with the same bitwidth as token IDs. In our case, this was fp16. Then, we sort the distribution to get top  token IDs and top  probability values. Storing these on disk will require space equivalent to  more tokens. 

After a block of  tokens is read from the input file, it is immediately written as is to the output file. Then we find top  token IDs and probabilities for each  and sequentially write these values to the output file. After all  such distributions are written, we would have written equivalent of  tokens to the output file.

Since we know that the maximum support of all the  distributions is , we can ideally easily pass them as key-value pairs to the training loop and calculate KL divergence more efficiently. This causes only  increase in memory footprint of a batch. However, this can be slightly inefficient since we need to run ``'' operation to obtain correct components of the predicted distribution  based on token indices. ``'' operation is often slow on TPU/XLA devices which rely on a predictable dataflow in order to optimize their compute graph. Prior works such as BigBird  have resorted to special resizing and converted the operation to continuous memory access. 

Such tricks are harder to implement here without ascertaining an upper bound on the maximum token ID in the support of . Ideally, obtaining such bounds may be useful and possible since the tokenizer (such as WordPiece or BPE) are expected  to assign lower token IDs (earlier ``merges'') to frequent tokens anyway. In our implementation, we initialize a  size zero vector and use the ``''  operation to populate counts at correct places. While the  ``'' operation is also slow, we perform it during batch creation on CPU which is latency optimized as opposed to previous proposal which was doing ``'' operation on accelerators which are throughput optimized. While this increases the memory footprint of the batch by , we found that using such dense vectors for KL divergence resulted in the model running slightly faster on both TPUs and GPUs.

Since we pack the  directly into the batch as -dimensional vectors, the memory used by the labels in the  is considerably higher than in the baseline. Despite this, we occupy only 0.5\% of the total TPU/GPU RAM used by the trainer. The baseline () method takes only  of memory to store labels for a sequence length of 256 and a batch size of 128, assuming%as it only requires  16-bit integer token IDs. In the , we provide additional  distributions. Assuming each number in the distribution is a 16-bit float, and considering  and GPT2's vocabulary size of , we occupy approximately  of additional GPU/TPU RAM per batch during training. On our GPUs, the  utilizes around 72GB of RAM out of the total available 80GB, leaving more than enough room to accommodate all the  extra distributions per sequence per batch. 

Maximizing the likelihood of the next token is an established, statistically sound objective for pre-training language models.  In this paper we show that we can train better models faster by pre-aggregating the corpus with a collapsed n-gram distribution. Previous studies have proposed corpus-level -gram statistics as a regularizer; however, the construction and querying of such -grams, if done naively, prove to be costly and significantly impede training speed, thereby limiting their application in modern large language model pre-training.

We introduce an alternative compact representation of the next token distribution that, in expectation, aligns with the complete -gram distribution while markedly reducing variance across mini-batches compared to the standard next-token loss. Empirically, we demonstrate that both the -gram regularized model and our approximation yield substantial improvements in model quality and convergence rate compared to existing methods. Furthermore, our approximation facilitates scalability of gains to larger datasets and models compared to the straightforward -gram regularization method.

%propose a method to harness collapsed n-gram statistics  and scale the regularization benefits to larger datasets and models. Our proposed method truncates the -gram estimated next-token distribution to always have a support of fixed size. This enables efficient retrieval of these distributions during training. With this, we were able to train large models on large datasets and study the benefits of -gram augmented training against standard single-next-token objective. Our experiments show that such -gram augmented training can be useful for both fine-tuning as well as pre-training of modern LLMs.% TODO, make more impactful?%Specifically, we find that our method, named ``\sysname'' is upto 50\% faster to reach same validation perplexity as standard cross-entropy. This comes at cost of upto 25\% increase in disk usage. Thorough experimentation on language modeling shows that our approach consistently outperforms existing models in terms of diversity, naturalness and faithfulness.Introductionbengio2011neural,mikolov2013distributedneubig-dyer-2016-generalizing, zhao-etal-2017-ngram2vec, yang-etal-2019-usingszegedy2016rethinking, muller2019whensec:model_performanceconstantsec:training_efficiencysec:training_efficiencysec:babylmsec:model_performancesec:peftsec:approximationsec:preenrichingsec:minibatchsec:training_efficiencysec:experiments\vocabsize|\vocab|\ynt\vy^\textRelated WorkLanguage modelingbengio2011neuralmikolov2013distributedvaswani2017attentiongpt-neox-20b, zhang2022opt, touvron2023llamaWelleck2020Neuralsu2022a,Jain2023ContraCLMAugmenting language model trainingmikolov2011extensions, chelba2014billion, jozefowicz2016exploringbengio2011neural, mikolov2013distributedneubig-dyer-2016-generalizing, zhao-etal-2017-ngram2vec, yang-etal-2019-usingfrydenlund2022languageli2020d2gpoyang-etal-2019-usingyang-etal-2019-usingli2020d2gpoLanguage Modeling

    _D = \argmax_\theta \sum_{n=1}^L \log P_\theta(x_{j+n+1} | x_{j},\ldots, x_{j+n}) fig:concept

    &P_{_D}( t | \vt) \longrightarrow , ~~~~, \\ &[w] = \delta(\vx_{j:n}=\vt, x_{j+n+1}=w)}}{\sum_{j \in D} \delta(\vx_{j:n}=\vt)} spliteq:conditionalThe \AllNTL\ objective


      \min_\theta &\sum_{n=k+1}^L   -\log P_\theta(x_{j+n+1} | \vx_{j:n}) \\  %{(\ynt_{j,n}; P_\theta( \cdot | \vx_{j:j+n}))} \\     & + \sum_{n=1}^k {(}; P_\theta(\cdot | \vx_{j:n}))} splityang-etal-2019-using, neubig-dyer-2016-generalizingBenefits of \AllNTLeq:ntlOverheads of \AllNTLheafield-2011-kenlm, heafield-etal-2013-scalablesec:training_efficiency\cciCompact and Consistent Next Tokens: \sysnamesec:approximation % text overflow could be desk rejected?      =          v\ytopr_\cci \quad\quad\quad\quad x_{j+n+1} \in \ytopr_\cci\\         u\ytopr_\cci + } \quad     casesChoosing fig:concept ([t]) & = \ytopr[t](vp + u(1-p)) = \ytopr[t] \\ %                     & \implies vp + (1-p)u = 1 \\                      & \implies v = {p}  sec:appendix_effectivenessPre-enriching the dataset with sec:preenrichingheafield-2011-kenlm, heafield-etal-2013-scalableeq:conditionalsec:appendix_preenrichingBuilding the mini-batch with sec:minibatcheq:allntlsec:appendix_minibatchExperimentssec:model_performancesec:training_efficiencysec:babylmsec:peftsec:appendix_hparamssec:experimentsDatasets, Baselines and Metricsmerity2017pointerkaddour2023minipile, gao2020pileluo2022biogptyang-etal-2019-usingli2020d2gposu2023contrastiveradford2019languageblack2021gptneozhang2022optMetricssu2022aPerplexity () of the model on the test set.% of Wikitext-103     pplPrediction accuracy () of the model. Given a sample with inputs  and labels  from the test set, we take argmax of each of the  predicted distribution at time step  to get top-1 predicted token and match it against  to calculate the prediction accuracy.      accRepetition () measures the fraction of top-1 next-token predictions that occur in the prefix.     repExpected calibration error () measures how over/underconfident is the model when making correct/incorrect predictions. Lower ECE indicates better calibrated models.     ECE~ measures     the similarity between the generated text and reference text using the embeddings of another large pretrained model.     MAUVE (MVE)pillutla2021mauveRepetition within a generated single text sequence: ()  .     %\item  \todo[inline]{Please describe.}     rep-nDiversity () measures repetition at different -gram levels: .     div.Number of unique bigrams () in the generated text.     %\item Perplexity of the generated text ().     %\item Inter-sequence diversity ()~ calculated on 1000 samples set aside from all generations. All the remaining 3000+ generations are considered as references.

\#uniqMeister2023holtzman2019curiousNumber of optimization steps () taken to reach NTL's perplexity on the val set.     steps-to-pplTotal wall clock time () to finish the entire training. We exclude the time for preprocessing (trie building and storage) as the preprocessing is a one-time operation which many times did not take very long. Additionally, one could always start with datasets that are preprocessed by someone else. We do include the time it takes to load the trie and training time retrieval as these operations will often stall the XLA devices. %     TWTTotal disk usage () required for training (includes storage of prefix trie).     diskMaximum CPU RAM () used for pretraining i.e. loading and using the trie. max-RAM\textbf We find that \sysname\ is competitive with far more expensive (as we show in Sec. \ref) \AllNTL\ objective. For both \AllNTL\ and \sysname,  was used to build the prefix trie. \sysname\ additionally used . Best results are highlighted with green while second-best are highlighted with yellow.tab:wikitext103Model performancesec:model_performance\sysname\ is comparable to \AllNTL\ and better than NTLtab:wikitext103li2020d2gposu2023contrastiveTraining efficiencysec:training_efficiencywidth=\textwidthfigs/efficiency_plots_with_c.pdf\textbf \AllNTL\ with higher values of  can easily go out of memory from a naive implementation. Both \AllNTL\ and \sysname\ converge faster to NTL's validation perplexity as compared to NTL. The total wall time (TWT) to finish the entire training is also significantly lower with \sysname\ as compared to \AllNTL\ due to lack of any -gram querying during training. gpt2-125m model is used for all experiments with  for \sysname.fig:efficiency\sysname\ is significantly more efficient than \AllNTLfig:efficiencydefaultdict\tinysec:preenrichingpickle\tinyheafield-2011-kenlmfasterheafield-2011-kenlmwidth=\textwidthfigs/shards_vs_ppl.pdf        \textbf Oversharding can make the -gram distribution unreasonably sparse. This can lead to overly optimistic approximation and KL penalty which can hurt the performance on extremely small indices.     fig:shardingEffects of shardinggpt-neox-20b, touvron2023llama, groeneveld2024olmogao2020pile, together2023redpajama,dolmafig:shardingUnderstanding \sysnamesec:ablationswidth=\textwidthfigs/ablations_adjustedntl-2.pdf        \textbf All experiments fine-tune a gpt2-125m model on WikiText-103. Higher values of both  and  improve perplexity before plateauing.  is fixed when varying  and  is fixed when varying  for \sysname.     fig:ablationsAblation studies on  and fig:ablationseq:approximatingUsing larger/domain specific data sources to build -gram models can helptab:extra-dataset-ablationandandat the costPerplexity reduction as a function of prefix lengthfig:nll_vs_seq_lenEffect of longer pretrainingsec:experimentstab:pretraining_longerCase Study: PEFT Domain Adaptationsec:pefthu2022lora, dettmers2023qloratouvron2023llamahu2022lorajin-etal-2019-pubmedqajin2021diseasetab:llama-lora-resultsCase Study: The BabyLM Challengesec:babylmwarstadt-etal-2023-findingswarstadt-etal-2019-neuraldolan-brockett-2005-automaticallydzikovska-etal-2013-semevalwang2018glue, wang2019supergluetab:babylm-resultsConclusionLimitations and Future WorkJurasfsky2023misra1982heavy,woodruff2016new,braverman2017bptreeJurasfsky2023gao2020piletogether2023redpajamadolmatouvron2023llamaEthics Statementanthology,custom,refsacl_natbibAppendixsec:appendixHyperparameterssec:appendix_hparamssec:model_performance\tiny\tiny\tiny\tiny\urlloshchilov2018decoupledsec:babylm\tinysec:peftEffectiveness of sec:appendix_effectivenessExampleEffect of sec:ablationsfig:coconts_multinomialAdditional Discussion on Pre-enriching the Datasetsec:appendix_preenrichingfig:trielookupfig:trielookupAdditional Discussion on Minibatch building with sec:appendix_minibatchgathergather\tinyzaheer2020bigbirdscatter\tinyscattergather