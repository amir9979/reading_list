To generate the two target sections  % BHC and DI, the most straightforward approach is to leverage the other readily available free-text sections in the discharge summary as the input for the generation stage. But using them all for one-stage generation is overwhelming and prone to hallucination because  % some sections are irrelevant or  % contain thousand words of nonessential information, making key aspects of the patient's record often be omitted. % We thereby design sets of heuristics (e.g., regular expressions) to selectively extract clinical notes information from 13 relevant sections of the discharge summaries (e.g, , , ), with definition of each section described in Figure~. %%%%%%%%%%%%% We report  % data distribution of these sections in Table~ (Appendix~)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Through exploring the format of different sections of the discharge summary, we notice a great complications in the structure and content of the  section,  % likely due to note bloat and information overload. This section, intended to highlight key findings of radiologist to the patient's treatment, is often cluttered with excessive laboratory and imaging data (e.g., blood tests, CT scans). These extraneous details can lead to challenges such as hallucination and high resource demands in generative tasks. %%%%%%%%%%%%%%%%%% Consequently, we explored using radiology reports as a viable alternative. These reports, often duplicated partially or entirely in the Pertinent Results section, succinctly convey diagnoses corresponding to specific lab results. We selected radiology reports with similar Impressions to those in the Pertinent Results and used these as a substitute, streamlining the content effectively. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In this framework, we performed instruction-finetuning on LLM to adapt the model to DSD. % For computational feasiblity, we employed Low-Rank Adaptation (LoRA)~, a parameter-efficient fine-tuning method that adds a small number of trainable parameters to the model while freezing the model's original weight, resulting in standalone adapters. % The adapters, specifically fine-tuned for each generation task in DSD, adjust important weight of LLMs to capture  %%  and generate clinical information in the corresponding form. %%%%%%%%%%%% Following OpenAI's prompt engineering guidelines~, we structured our prompts into five parts, detailed in Table~ (Appendix~): 1) Context of the discharge summary input to be summarized 2) Definition of the generation task and the specific section for documenting the discharge summary 3) Structure of the expected output of the generating section, infused with 4) Set of Chain-of-Thought (CoT) questions expected to be answered by the LLMs to capture and generate the information in each subsection of the output. % Of those, our primary strategy is Part 5,  which involved curating effective and generalizable CoT questions based on analysis of numerous samples. This manual effort helped in designing templates and questions that effectively guide the LLMs to focus on critical information amidst the extensive data and noise in the discharge summaries. We  %% analyzed the medical questionnaire essential for each section, based on hundreds of samples, in Appendix  , which underpins our CoT questions and prompt design. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To showcase the utility of prompt designing  % for adaptation to the DSD task, we  % developed three baselines, corresponding to three prompt variants for instruction-finetuning LLMs. %%  was fine-tuned with no instruction, but only the discharge summaries as input and the respective target section as output.  was fine-tuned with additional natural langauge instructions as prefix to the  discharge summary to provide the context and definition of the task's input/output. Finally,  was fine-tuned using prompts  % outlining the structure of the respective generating target section. Along the structure, we embed some CoT questions to elicit LLMs to generate output aligned with the questions. %%%%%%%%%%%%%%%%%%%%%%%%

We choose ~~ as our LM. The LLM was fine-tuned on a NVIDIA RTX 4090 GPU, and took 10 hours for fine-tuning each generation task. %%% The following hyperparameters were used: 1 sample per device,  % a LoRA rank and alpha of 128 and 64 for parameter-efficient fine-tuning, a learning rate of . %%% We keep other hyperparameters to their default values. %% We followed the organizers to measure textual similarity and factual correctness of the generated text based on several metrics, including BLEU-4~, ROUGE~, BERTScore~, Meteor~, AlignScore~, and MEDCON~. % The dataset for this task was sourced from the MIMIC-IV~ dataset, including 109,168 emergency department (ED) admissions and were split into a training (68,785), a validation (14,719), a phase I testing (14,702), and a phase II testing (10,962) subsets. % To address the variation in discharge summary length, we select data within the interquartile range (Q1-Q3) for training and validation.  % We further ensure consistency by selecting only samples with discharge summaries containing all 13 common sections and their target sections follow the most common format, as outlined in Figure~. % Overall, 11.1k and 8.7k samples were selected for training of  % BHC and  % DI generation, respectively. For experiment, due to runtime and computational limitations, we sample 250 hidden entries from each phase's testing data, totaling 500 samples  for evaluation of each generation task.

%%%%%%%%%%%%%%%%%%%%%%% Table~ presents the performance of models fine-tuned by different prompt variants. Overall, in both generation tasks, natural language instructions plays a critical role in  % guiding the LLM with comprehensive knowledge to understand the task. Providing well-described context of the generation task already helps the model  % achieves up to 14\% of performance gain across the metrics and tasks. Further, infusing CoT questions into the instructions effectively elicit LLM to think better,  % providing an additional 9\% performance increase. %% Notably, a reasonable improvement on MEDCON score also indicates better accuracy and consistency of clinical concepts in the generated text. %%%%%%%%%%%%%%%%%%% Table~ summarizes our framework's overall performance on the phase 2 test set of the shared task, alongside the best-ranked system~. We notice there is still a gap between our  framework and the best ranked system, of which the Overall score  % is 0.332. %%%% This performance dip is common across submissions, likely due to prevalent data quality issues in the Discharge Summary Documentation (DSD) task. %% DSD, a real-world summarization challenge, involves processing actual  % discharge summary information with significant variability in formatting and length. % Figure ~ shows word distribution variances in the target sections. %%%%%%%%%%%%%%%%% It is noticeable our models are trying to set a common length for the target sections, and are struggling to converge to the wide range of lengths of the reference text, highlighted by Table~. %%%%%%%%%%%%%%%%%%%%%%%% We note slight performance variation of  in terms of Meteor, AlignScore and MEDCON, in between Table~ and~. A possible reason for such variation may be that hidden test data in the shared task has distributions significantly deviate from the publicly released data for model development. %%%%%%%%%%%%%%%%%%%%% Furthermore, due to computational constraints and a focus on high-quality data, we utilized only a subset of the available training data. With more comprehensive training, we anticipate improved model convergence. %