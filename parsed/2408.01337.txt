consists of  multiple-choice questions aimed at testing the understanding of  unique music tracks sourced from the MusicCaps  and the Song Describer Dataset . We adopt a multiple-choice format in order to standardise evaluation and  follow widespread practice in LLM-centric evaluation scenarios . As illustrated in Figure , each question has four possible answers. One option is the correct answer, the other three are distractors. Inspired by , we structure these as follows: one does not fit the track of interest but is related to the question (), one correctly fits the audio, but does not address the question (), and one does not apply to the track and is also irrelevant to the question ().

 is built from a diverse set of musical works and their detailed descriptions, and serves as a foundation for evaluating Audio LLMs across various dimensions of music comprehension. To delineate the specific evaluation dimensions encompassed by our benchmark, we develop a taxonomy consisting of two primary categories:  and . Each category is further divided into several dimensions, informed by insights from national music education programs and existing research on music folksonomies  . This structured approach allows us to assess the depth and breadth of music-related competencies systematically, offering a holistic view of models' capabilities in the music domain.

In the  category, questions probe a model's ability to recognise pre-acquired knowledge across various musical aspects

Questions that test  are instead designed to require the synthesis and analytical processing of multiple musical concepts

An example of reasoning might involve using an understanding of tempo, chord quality, and instrumentation in concert to ascertain the mood of a music piece. Each question can cover multiple dimensions and their categorisation is obtained automatically, as described in Section . Figure  shows the coverage of the two categories and their respective dimensions within the benchmark. Over half the questions test at least one aspect of musical knowledge, such as features relating to instrumentation or performance characteristics, while 44\% are dedicated to probing reasoning skills. While the distribution of dimensions within each category is not balanced, we note that this reflects the distribution of different musical concepts within music captions , resulting in categories such as instrumentation, mood and genre appearing more frequently.

To build our dataset, we automatically transform human-written music captions into multiple-choice questions. These are then carefully validated by multiple human annotators, alongside the associated audio, in order to filter out invalid, ambiguous or irrelevant questions resulting from inaccuracies or hallucinations in the model output.

We source our data from music caption datasets as we aim for elaborate and linguistically diverse information about the music. Currently, only two captioning datasets provide sufficiently detailed music descriptions, namely the Song Describer Dataset (SDD) and MusicCaps. SDD contains 2-minute-long music clips with single-sentence captions crowd-sourced from music enthusiasts, while the captions in MusicCaps, describing 10-second audio snippets, are written by professional musicians. From SDD, we select all tracks that have at least two captions, to ensure enough information is provided to the model to be able to formulate interesting and challenging questions. While this is not possible for the MusicCaps dataset, where only one caption is available for each track, we note that descriptions are, on average, longer than in SDD and designed to be more comprehensive.  From the genre-balanced subset of the MusicCaps test split, we exclude all tracks for which the labels indicate a low recording quality, to prevent differences in audio quality from affecting the results. For both datasets, we employ a state-of-the-art genre tagging model  to identify non-musical tracks and to sub-sample songs from the most common genres (e.g. rock and electronic).  Through this curation process, we select 227 unique tracks from SDD and 497 from MusicCaps. We supplement the descriptions with short text labels taken from the dataset itself in the case of MusicCaps and from a related dataset for SDD .

We generate the question-answer sets by instructing Gemini 1.0 Pro  to formulate question and answer options for a given human-written caption.  To leverage the model's in-context learning capability, we prompt it with a detailed task description and three examples of input (description and tags) and expected output. In addition to the question and answer pairs, we ask the model to start its output with a summary of the provided information about the music recording and to interleave the distractor answer options with explanations of their suitability. This way of prompting is inspired by the chain-of-thought methodology and helps to elicit the best model responses . This way, we obtain three multiple choice questions from each description on average and collect a total of 2,091 question-answer pairs. An example of the generated questions is shown in Figure . 

In order to ensure that questions and answers in our benchmark are factually accurate, aptly written and that each question can be correctly answered based on the available audio, we validate all sets of questions via human annotators. For this step, we recruit 222 participants via the Prolific platform (www.prolific.com). During annotation, a question, the corresponding audio clip, and all four answer options are presented to the participants in random order, for a total of 30 to 50 question items. Participants are then asked to select all options that correctly answer the question or skip the question by indicating that they are unable to provide an answer or that the question is not valid. Following this procedure, for each question, we collect three to five annotations, stopping early if different annotators are in agreement. % Appendix  presents further information about the validation procedure, including the instructions provided to the annotators. This task setup is intended to vet questions and detect those that do not adhere to the intended multiple-choice format, either because the expected correct answer is not the only plausible option or because any one of the distractors is more likely. Consequently, we exclude questions from our final dataset for which i) less than 50\% of the annotations indicate the intended correct answer or ii) more than 50\% of the annotations mark any of the disctractors as a plausible answer. The final dataset comprises 858 questions from MusicCaps descriptions and the remaining 329 from SDD captions.

Once questions are validated, we categorise them according to our taxonomy outlined in Section . To achieve this, we employ Gemini 1.0 Pro, this time prompting it to automatically label each question with one or more of the evaluation dimensions. The prompt includes the full taxonomy including detailed descriptions of all dimensions, a chain-of-thought instruction, and a single question with only the correct answer. The produced output contains an explanation of the categories and dimensions assigned to each question.

In multiple-choice-based evaluation, a model is provided with a question and a set of answer options, and is then tasked with selecting the most suitable answer. In practice, this can be accomplished in different ways . In our experiments, we adopt  evaluation: given a music clip and an associated question-answer set, the language output produced by the model is mapped to one of the candidate options by string matching.  Another common approach in MC evaluation is to determine the selected answer through the conditional log likelihood scores of the tokens forming each of the different options. While this can help estimate uncertainty and confidence in the model predictions, in our experiments, we explore only the output-based setting, for three reasons: (1) this corresponds to real-world use of the models, as interactions usually take the form of a conversation; (2) it has a lower computational cost; (3) prior work has demonstrated that sentence probabilities are not necessarily indicative of the probabilities assigned to the answers . To extract the selected answer from the generated outputs, we match either the option identifier (, ,  or ) or the full answer text, if one and only one is given in the output. 

We look at two main metrics to measure model performance on our benchmark: accuracy and  instruction following rate (IFR). Accuracy is given by the percentage of correctly answered questions out of the total set of questions. IFR is given by the percentage of generated answers that correspond to one of the given options. In both cases, finegrained scores can be obtained by considering only the subset of questions covering at least one of the available evaluation dimensions shown in Figure .

An important design factor in the evaluation of LLM-based models is adaptation , the process of adapting the input to a suitable format. While the format of the audio input is typically fixed by the model design, text inputs allow for more flexibility and different prompting techniques have been shown to significantly influence model's behaviour . Beyond simply passing the question and answer options as the input text, corresponding to , an effective alternative strategy is to leverage  (ICL), whereby the model is presented with a set of reference inputs that exemplify the task prior to being shown the question of interest. We experiment with in-context learning in our experiments, providing between 0 and 5 examples in the text input. In the interest of standardisation and to ensure a fair comparison between the models, unless otherwise specified, we keep the prompt selection fixed in our final experiments, following an initial exploration.

In our evaluation, we consider three music-specific models, MuLLaMA , MusiLingo , and M2UGen , and two general-audio LLMs which can be applied to music, as reported in their respective papers, SALMONN  and Qwen-Audio . To the best of our knowledge, these are all the existing Audio LLMs which can be applied to music and for which open-source weights are available. These all share a similar architectural design and are composed of a backbone LLM, an audio encoder and a lightweight learnable adapter module to align embeddings produced by the audio encoder to the input space of the LLM, based on either the LLaMA-adapter  (MuLLaMA, MusiLingo, M2UGen) or a Q-Former network  (SALMONN). An overview of the backbones used in each model is provided in Table . All systems are trained via instruction tuning  and all employ a combination of different instruction datasets, often in multiple training stages including pre-training and fine-tuning. For all models, we follow the official implementation and use default inference settings. We repeat all experiments 3 times, randomly shuffling the order in which answer options are presented, and report average performance across all runs.

We report results for all models in Table , showing the overall accuracy score alongside detailed scores on knowledge and reasoning questions, and the instruction following rate (IFR). Figure  presents a breakdown of accuracy scores along all reasoning and knowledge dimensions. Unless otherwise specified, we show one-shot performance for all models, as we find this to be the overall optimal setting, as we discuss in more detail in Section . From this, we observe that current models generally perform poorly across all settings and along all evaluation dimensions. Among these, Qwen-Audio stands out with a score of 51.4\%. Surprisingly, with the exception of M2UGen, music-specialised models generally perform worse than general-audio ones, in some cases performing only marginally above or even below random performance. As evidenced by the IFR, these models struggle to output answers in the correct format, which in turn negatively impacts their accuracy score. As shown later in Section , we find that, when none of the answer options is selected by the model, this is often due to ,  or . 

We now investigate factors influencing performance along different axes by using our benchmark as a diagnostic tool.

 We first study the effect of varying the number of in-context examples. As shown in Figure , providing a single example is occasionally beneficial to accuracy and IFR, but with both the difference magnitude and overall impact differing between models. Additionally, this trend does not hold after the one-shot setting, and we see no consistent improvement when using a larger number of examples. Interestingly, we observe that, for M2UGen, Qwen-Audio and MuLLaMa, changes in accuracy from zero- to one-shot prompts are accompanied by a reduction in variance, suggesting that ICL can help minimise variability in the model output. While we do not explore this in our experiments, we also hypothesise that the advantages of ICL may become more prominent through multimodal few-shot prompting , which we leave for future work.

Next, we shift our attention to examining how distractors in our benchmark influence the difficulty of the task. To this end, we ablate answer options corresponding to the different kinds of distractors described in Section , and present the model with only two or three answer options. In Figure (a) we show how performance is affected when using only one distractor alongside the correct option, always randomising their order. From this, we observe that the two distractors containing information which is not related to the question (CU and IU) have a similar effect, while including the  (IR) option consistently makes the task more challenging. This phenomenon persists when adding a second distractor (not shown here), with combinations which include IR invariably leading to worse performance. Intuitively, the two  options can be ruled out based on the text input only, while selecting the correct answer between two options that appear relevant requires engaging multimodal understanding to relate information in the audio content to the text in the question. Crucially, this indicates that models particularly struggle to discern between options that are equally plausible based on the text input only, suggesting that less attention is given to the audio content. This forms the basis of our hypothesis that current Audio LLMs are characterised by a strong language bias, leading to poor performance in tasks that are more audio-dependent. We test this hypothesis in the next section.

 In order to verify whether the audio input is effectively being ignored or is overshadowed by its text counterpart, we devise a simple test, which we call , where we replace the audio clip corresponding to a given question with either white Gaussian noise or a randomly chosen track from the dataset. In order to pass this test, a model should display a statistically significant drop in performance when either form of audio perturbation is used, compared to its baseline performance. We showcase results on this test in Figure (b). From this, we clearly see that, with the exception of SALMONN and Qwen-Audio, all models fail the audio attention test, and the severity of this failure is often negatively correlated to their overall performance on the benchmark (see Table ). This confirms that current Audio LLMs are biased towards textual information, often choosing answers that score well under their language prior. Additionally, it provides an explanation for their low performance on the benchmark, as this is effectively bounded by the maximum score they can attain mostly based on the language input. We argue that this constitutes a major pitfall in the design and training procedure of these models, which results in music understanding abilities that do not match the expected performance, as obtained through prior evaluations. 

While the core goal of our benchmark is to provide standardised automatic evaluation to objectively measure general music understanding capabilities, we argue that it can also constitute a useful tool for qualitative assessment. We showcase three examples here, focusing on the two lowest-performing models. While this is not an exhaustive analysis, these examples offer a bird's-eye view of how language pre-training biases percolate through multimodal training, resulting in failures to attend to the inputs in our evaluation. To describe these, we borrow terminology from .

 One of the ways models fail to provide a suitable answer falls under the category of , whereby a response includes references to musical elements that are not present in the audio. For example, when asked about an accompaniment instrument, models with this type of hallucination may ignore any suitable option provided ( or ), instead answering , when the audio clip clearly contains no piano.

 Another instance of hallucination concerns mundane statements that deviate from the topic of the question altogether. Among others, an observed case of this failure mode is a statement of the form  to a question specifically asking about the .

The last failure mode we encounter is related to a bias towards frequent patterns occurring in the training data. While some of the benchmarked models undergo a stage of training that includes instruction-tuning examples with questions and answers, occasionally they still produce trivial outputs. For example, when asked , a model with this type of bias may answer .  Reviewing MusicQA, used in training MuLLaMa and MusiLingo, reveals that a high number of the LLM-generated training examples mention similar phrases, thus likely biasing the model towards this type of uninformative but highly likely output.

Prior to participation, the annotation experiment described in Section  was approved by the Queen Mary Ethics of Research Committee to ensure alignment with ethical guidelines and protections for human subjects in research. We did not collect any personal data from our annotators, safeguarding their privacy and confidentiality. Annotators were fully informed about the objectives of the research, the nature of their tasks, and the use of their annotations, underpinning their informed consent before contributing to the project. In an effort to provide a fair compensation for their contributions, annotators were paid Â£9 per hour.

In constructing the  benchmark, our data collection strategy included sourcing music tracks from a variety of backgrounds, acknowledging the inherent challenges in representing the rich diversity of global music cultures within our dataset. We recognise that our initiative does not fully balance the benchmark across all genres, languages, and cultural backgrounds, and annotations were conducted exclusively in English due to logistical constraints, highlighting areas for future expansion and improvement.