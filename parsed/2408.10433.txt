First, we probe the effectiveness of CLIP in accurately ranking correct captions over those with hallucinated content. This assessment will indicate the efficacy of CLIP-style models in reducing hallucinations with DPO-based training. To this end, initially, we select 1K captions from the Detailed Caption~ dataset, chosen for its high-quality labels. Then, we instruct GPT-4~ to create three hallucinated captions for each image, corresponding to the following types of hallucination: (i) : new elements or objects are added to the caption that were not mentioned originally, (ii) : the attributes, characteristics, or features of the original caption's elements are altered, and (iii) : the spatial, contextual, or interactive relationships of the original caption's elements are altered. See Fig.~ for an example of the injected hallucinations.

Next, we use LLaVA-1.5 7B to compute the likelihood of all captions, including the original and the hallucinated captions.  We then retain only the hallucinated captions for which the model predicts a higher likelihood than the original, yielding 61 for existence, 76 for attribute, and 119 for relationship hallucinations out of the initial 1K captions. Finally, we compute the CLIP

 image-text scores for these likely yet incorrect captions. As illustrated in Fig., CLIP manages to accurately rank the original caption over the hallucinated ones in at least 59\% of cases, reaching up to 83\% for the existence type of hallucinations. These findings underscore the potential of using VL embedding models to provide a reliable training signal for hallucination reduction in LVLMs.

The standard pipeline for LLM and LVLM alignment consists of three steps: generation, annotation, and optimization. First, a set of  prompts , each one consisting of an image and a text component in the case of LVLMs, are used to generate a set of response pairs  and  obtained from a pool of pre-trained LVLMs for each prompt . Then, either human annotators or another set of LLMs ( AI annotators) are used to rank the responses, resulting in a preferred  and a less preferred response  for each prompt ,  and thus a final preference dataset . Finally, Direct Preference Optimization~ (DPO) can be applied to update the target policy  parameterized by  directly using the preference dataset . Specifically, the DPO optimization objective is defined as follows:

with  as the policy to be learned,  as the reference SFT policy, and  as a hyperparameter to control the Kullback-Leibler (KL) divergence between the learned  and reference  policies~. The main benefit of DPO-based optimization is the direct alignment of the LVLM towards the preferences implicit in the preference data .

In contrast to previous DPO-based LVLM preference optimization works~, next, we will introduce the proposed , which (i) simplifies the generation process by limiting the pool of LVLMs used for generation to small and efficient models,  (ii) removes the need for external LLMs and LVLMs annotators accessed via paid APIs ( GPT-3.5, GPT-4 or GPT-4V)  by using CLIP as the ranking model, and (iii) removes the need for additional data by reusing the same data used during the SFT step.

The first step of our pipeline consists of generating a set of per-image captions that will be later ranked by a pre-trained CLIP model, along with a subsequent filtering approach to select a set of positive and negative pairs, which are then used for DPO-based training. To this end, we start by selecting the pool of LVLM annotators and the data to be annotated. For the annotators and to reduce the cost of the generation step, we select MobileVLM-v2~ family of models, given their efficiency and performance. As for the data, and to avoid introducing any new sources that might bias the results of , and to further reduce the method's cost,  we opt for the initial pool of data used during the SFT stage of MobileVLM-v2~ models, see Table~ for details. Then, we conduct our data generation pipeline that consists of two steps, the generation of  and .

We start by generating a set of 5 descriptive captions per image. % For each of the MobileVLM-v2 models, we prompt with 5 different prompts ( "identify the setting and note any characters or objects, focusing on visible details.") to increase the diversity of the generated captions. While these captions can be used for CLIP ranking and DPO-based training, they are still produced by generic prompts that are not image-specific. Next, we perform the second step of our data generation pipeline to obtain a set of per-image questions and answers.

To obtain a set of questions per image, we leverage an LLM,  a variant of Mistral 7B Instruct-v0.2~, and feed it with the generated captions and prompt it to generate 2 questions for each image, together with positive and negative answers. The LLM is asked to generate the positive ones based on the fed captions and to generate plausible but incorrect negatives given the image description/caption.

 For an example of the generated captions, questions, and answers, see Fig.~.

Starting with the generated captions, the subsequent data annotation step involves ranking them based on their CLIP image-text similarities and filtering them to obtain a final set of high-quality positive-negative pairs for DPO-based training. Our filtering pipeline includes two stages: , where we eliminate low-quality images and captions, and , where we select the best positive and negative per-image pairs for DPO-based training.

 The initial step of data annotation is CLIP ranking. In this step, we use a pre-trained CLIP model to compute the cosine similarities of the captions with their associated images and then sort them from highest to lowest.

Next, we aim to filter our data and only retain a balanced and higher-quality subset. To achieve this, we start by analyzing the types of images in our data pool (see Tab.~). We define a set of four generic categories: images of text, people, objects, and scenes. Using CLIP and a set of 10 descriptions per category, we create four class prototypes. Each image is then assigned to one of these prototypes based on the highest cosine similarity. As illustrated in Fig.~, we observe that CLIP scores for the text category are the highest, while scores for people, objects, and scenes are relatively similar. Consequently, before applying CLIP-based filtering, we down-sample the portion of text images. We then filter out all generic captions below a given CLIP score ( < 28.0). Additionally, since CLIP is primarily trained on short text, we remove long captions to ensure more precise CLIP scores. For questions, we compute their CLIP scores and remove all questions with low CLIP scores ( < 25.0), thereby eliminating generic questions ( ``what is the main object in the image?'') that are already covered by the generated captions.

Finally, starting from the remaining high-quality image captions and image question-answers, the final step will be selecting a set of positive and negative pairs for DPO-based training. For the questions, since we already have a set of positive and negative responses generated by our LLM, we only need to filter out the low-quality pairs. To achieve this, we use simple regex matching rules to extract an image description from the question, append the positive answer to it to create a synthetic caption, and compute its CLIP image-text scores. We then reject examples where the scores are low. See Fig.~ for an example of the questions filtering process. As for the captions, we consider all possible pairs where the CLIP score difference between two captions is larger than a given threshold (  > 2.0) and where the length of the two captions is similar to avoid introducing false preferences. We then order them based on the CLIP score difference between the positive and negative captions and select the top-ranking pair per image.

 After the data annotation step, the resulting DPO training data consists of 750K pairs, of which 50K are question-answer pairs and the rest are caption pairs. For examples of the final pairs, see Fig.~.

We start by evaluating  in terms of LVLM hallucination reduction, which is our main objective. We use AMBER~, a comprehensive, high-quality, and LLM-free multidimensional benchmark for LVLM hallucination evaluation, which can be used to evaluate both generative and discriminative tasks. As the results from Tab.~ show, MobileVLM-v2 trained with  improves upon MobileVLM-v2 baselines across all model sizes, and sometimes quite significantly, especially for the 1.7B and 7B models. It can also be seen that  significantly improves when applied on top of LLaVA-1.5 7B. Importantly,  significantly outperforms HA-DPO~, the main competing approach, improving the AMBER score 3.2 vs 7.8 when using LLaVA-1.5.  Finally, our LlaVA-1.5 7B+ even outperforms Qwen-VL~, which is trained on significantly larger datasets (1.4B image-text pairs for pre-training and 77M for multitask training while  is fine-tuned on just 0.7M samples), and even matches the performance of GPT-4V without using any GPT-4V model outputs during training.   Overall, these results on a high-quality state-of-the-art benchmark such as AMBER clearly demonstrate the effectiveness of our  approach for reducing hallucinations.

A primary reason for the hallucinatory behavior of LVLMs is the weak alignment between their visual features and the input LLM tokens. A direct way to evaluate this is through simple zero-shot image classification, which is the go-to benchmark for contrastively trained VL models like CLIP~. The typical setup follows a closed-set classification problem, where the names of all possible classes are known a priori and are encoded into class prototypes using CLIP. A given image is then assigned to the class with the highest CLIP image-text scores. Herein, we follow the same protocol with the notable difference that, as our goal is LVLM evaluation, we first prompt the model to generate a free-form image caption describing the main object in the image, then encode the caption using the CLIP text encoder and assign the image to the class with the highest text-text ( caption-class) CLIP score. Here, we opted for a different family of VL embedding model,  SigLIP~, to avoid evaluating using CLIP models similar to those used during the data annotation step. Following~, we evaluate our approach on a suite of 9 diverse datasets: UCF-101~, SUN397~, Stanford Cars~, Oxford Pets~, Oxford flowers~, ImageNet~, Food 101~, Eurosat~ and Caltech-101~. As the results from Tab.~ show, all LVLMs fine-tuned with  significantly outperform their corresponding baselines, showcasing the increased discriminative properties. Note again that these improvements are obtained without affecting the model's performance on other tasks and datasets.

Herein, we evaluate the impact of  training on other vision language tasks and, more specifically, on the popular LLaVA-Bench (GQA~, ScienceQA~, TextVQA~, MME~, MMBench~). As Tab.~ shows, overall,  training does not compromise performance. We further note that accuracy improvements on LLaVA-Bench are heavily tied to the addition of extra training data~ or architectural changes~. As we are not using any additional data  models nor making architectural changes, it is not surprising that the performance after  training remains largely in line with that of the original baseline model.

In Figure~, we show some qualitative examples comparing the predictions of LLaVA-1.5 7B, LLaVA-1.5 7B+HA-DPO, and LLaVA-1.5 7B+. Overall, with , the model is more grounded in the visual content, less prone to hallucinations, and more precise and fine-grained in its descriptions.

Our work primarily follows the original DPO formulation proposed in~. Here we also consider the following recently proposed variations: KTO~, ITO~, SLIC~ and cDPO~. As Tab.~ shows, aggregated, all losses tend to perform similarly, with DPO marginally outperforming the others. 

Herein, we seek to explore alternatives to ViT-H/14 DFN~ CLIP model used in previous experiments, analyzing the impact of the scorer used on the overall performance of the model. We consider a diverse set of alternatives, covering multiple exploratory paths: equally-sized models trained on different data (ViT-H/14 ~ trained on LAION-2B instead of DFN-5B); smaller models (ViT-L/14~) and models trained using different pre-training losses (SigLIP-ViT-L/16~). As the results from Tab.~ show, our approach is generally robust to the exact scorer used, with the notable exception of the ViT-L/14~ model. Notice that this difference is primarily manifesting on the AMBER benchmark. Intuitively, this showcases the importance of a powerful scorer for reducing the amount of hallucinations.

As an LVLM-based approach, our method is subject to the same general consideration ( potential data bias, susceptibility to hallucinations, etc.). Moreover, as the LVLMs are trained on relatively small datasets compared to LLMs or CLIP, gaps within their knowledge domains are possible. This is especially important as neural networks tend to be overconfident outside their seen input distribution. As with all models from this category, we strongly recommend checking the models and the data carefully before deploying them. Despite these general aspects, our approach is shown to significantly reduce the amount of hallucinated content and improve the model's discriminability, hence resulting in more robust and reliable models.