Nowadays, large-scale pre-trained language models have significantly improved the performance of many natural language processing tasks. Pre-trained language models are usually trained on large amounts of text data, and then fine-tuned for specific task. Early research efforts mainly focus on word embedding, such as word2vec  and GloVe . Subsequently, researchers have shifted to contextual word embedding, including BERT , GPT , ENRIE , XLNet  and RoBERTa . However, those PLMs contain millions of parameters and take long inference time, making them inapplicable to resource-limited devices and real-time scenarios. Fortunately, there exist many compression techniques for PLMs, which reduce model size and accelerate model inference while keeping model performance.

Besides quantization  and network pruning , knowledge distillation  has been proven to be an effective technique for PLM compression. As for the most typical PLM BERT, there exist several knowledge distillation methods for model compression. Distilled BiLSTM  tries to distill knowledge from BERT into a simple LSTM. DistilBERT  uses soft target probabilities and embedding outputs to train student model.  BERT-PKD  learns from multiple intermediate layers of teacher model for incremental knowledge extraction.  TinyBERT , MobileBERT , and SID  further improve BERT-PKD by distilling more internal representations, such as embedding layer outputs and self-attention distribution. BERT-EMD  allows each intermediate student layer to learn from any intermediate teacher layer based on Earth Mover's Distance. M LM  uses self-attention distribution and value relation to conduct deep self-attention distillation.  M LMv2  generalizes deep self-attention distillation in M LM , employing self-attention relation.

Although existing knowledge distillation methods perform well in BERT compression, they are limited in two aspects: the relation-level knowledge is not well explored by the student model to enhance performance; and the attention head number of student model is restricted to the same as its teacher, increasing the inference time. Hence, we are motivated to propose a more flexible knowledge distillation method for BERT with improved performance.

% section 4 Since embedding-layer performs feature representation of tokens for each data sample, token similarity relation could be valuable knowledge to enhance embedding-layer distillation. In embedding-layer distillation, token similarity relation is transferred from teacher to student by minimizing the KL-divergence of token embedding similarities between teacher and student, according to the embedding distillation loss function  defined in Eqn.():

where  is the length of input sequence; matrices  and  are token embeddings of teacher and student;  and  are hidden dimension of teacher and student; matrices  and  are token embedding similarity matrices of teacher and student, respectively.

Standard Transformer-layer contains two main sub-layers: Multi-Head Attention (MHA) and Feed Forward Network (FFN). Transformer-layer distillation (illustrated in Figure ) is conducted on MHA and FFN to transfer self-attention relation and feature-level knowledge, respectively.

% figure 2As for MHA sub-layer, the similarities among its output vectors are defined as self-attention relation. At Transformer layer , let  represents the number of attention heads, then output ) of the -th attention head, is computed via:

where  is the input vectors of Transformer layer , with  representing the length of input sequence and  representing the hidden dimension; , ,  are linearly projections of ;  are parameter matrices; and  is attention head size.

In MHA distillation, MHA outputs (i.e.,  defined in Eqn.()) are concatenated together and then split into a certain number of vector groups (named MHA-splits), in both teacher and student models.  We suggest setting the number of MHA-splits as the number of student attention heads. After that, self-attention relation is transferred from teacher to student by minimizing the KL-divergence of MHA output vector similarities between teacher and student. Given that the -th student layer is mapped to the -th teacher layer, the loss function for MHA distillation  is defined in Eqn.():

where  is the length of input sequence;  is the number of MHA-splits;  is the number of student Transformer-layers; matrices  and  are MHA outputs in MHA-split  at teacher's Layer  and student's Layer ;  and  are split-head size of teacher and student MHA-split; matrices  and   are MHA output vector similarities in MHA-split  at teacher's Layer  and student's Layer , respectively.

Note that the teacher and student model should have the same number of MHA-splits. In MLKD-BERT, self-attention relation is transferred by MHA-split rather than MHA attention head. As such, student model does not have to set the same number of attention heads as its teacher. In this way, the number of attention heads in the student model can be set smaller, which could decrease inference time. 

As for FFN sub-layer, the feature-level knowledge is distilled by minimizing the mean squared error of the output hidden states between teacher and student, according to the FFN distillation loss function  in Eqn.():

Given that the -th student layer is mapped to the -th teacher layer, matrices  and  are the hidden states of student's Layer  and teacher's Layer ;  matrix  is a learnable linear transformation, which transforms the student's hidden states into the same space as the teacher's hidden states.

In summary, as the first stage distillation covers both embedding-layer and Transformer-layers, the above distillation loss functions are summed up to the first stage distillation loss function in Eqn.():

As samples with same class label tend to be similar than samples with different class labels, relation among samples would be valuable knowledge for prediction-layer distillation. Here, we will bring in sample similarity relation and sample contrastive relation to enhance prediction-layer distillation.

Sample similarity relation is defined as the sample similarities within a data batch, without considering sample labels. The sample similarity relation is distilled from teacher to student by minimizing the KL-divergence of sample similarities between teacher and student, according to the sample-similarity distillation loss function  defined in Eqn.():

where  is batch size;  and  are hidden dimension of teacher and student; matrices  and  are sample representations in a batch, i.e.,  outputs from the last Transformer-layer of teacher and student;  and  are sample similarity matrices of teacher and student, respectively.

Sample contrastive relation is employed to map samples with same and different class labels (according to ground truth of training samples) into close and distant representation space, respectively.   Sample contrastive relation is distilled by minimizing the sample-contrastive distillation loss function  in Eqn.() :

% %  where  is batch size; , , ;  is class label of -th sample;  is scalar temperature parameter;  is the -th row of ; ;  is linear transformation matrix;  and  are defined in Eqn.() and Eqn.().  

Similar to previous distillation works, we also adopt soft label distillation by minimizing the soft-label distillation loss function  in Eqn.():

where  is scalar temperature parameter;  and  are logits predicted by teacher and student, respectively. 

% table 1% table qa% table 2 In summary, the sample-similarity, sample-contrastive, and soft-label distillation loss functions are summed up to the second stage distillation loss function in Eqn.(): % section 5 Our experiments are conducted on General Language Understanding Evaluation (GLUE)  benchmark and extractive question answering tasks. The former is sentence-level task while the latter is token-level task. GLUE includes 8 tasks:  1) Corpus of Linguistic Acceptability (CoLA) ;  2) Stanford Sentiment Treebank (SST-2) ;  3) Microsoft Research Paraphrase Corpus (MRPC) ;  4) Semantic Textual Similarity Benchmark (STS-B) ; 5) Quora Question Pairs (QQP) ; 6) Question Natural Language Inference (QNLI) ; 7) Recognizing Textual Entailment (RTE) ; 8) Multi-Genre Natural Language Inference (MNLI) , which is further divided into  in-domain (MNLI-m) and cross-domain (MNLI-mm) tasks. The extractive question answering tasks include SQuAD 1.1  and SQuAD 2.0 . The GLUE tasks are evaluated on GLUE test sets and the extractive question answering tasks are evaluated on dev sets.

We take a pre-trained language model BERT-base  with 109M parameters as the teacher model (number of layers , hidden size , intermediate size , and attention head number ), which is fine-tuned for each specific task.  Two student models are instantiated for comparative studies: 1)  () with 14.5M parameters; and 2)  () with 67.0M parameters. The student models are initialized with the general distillation model delivered by TinyBERT. The detailed hyper-parameters are presented in Appendix .

Comparative studies are conducted to evaluate performance of MLKD-BERT against state-of-the-art BERT distillation baselines, including BERT-PKD , DistilBERT , BERT-EMD , TinyBERT , and  M LMv2 .  In addition, to measure the performance improvement delivered by distillation, MLKD-BERT is compared with a same structured pre-trained BERT model  , which is fine-tuned for each specific task without knowledge distillation. Note that all experiments are done without data augmentation.

The comparative results evaluated on the test sets of GLUE official benchmark  are presented in Table .  Here, different evaluation indices are adopted with regarding to the tasks: F1 metric for MRPC and QQP, Spearman correlation for STS-B, Matthew's correlation for CoLA, and Accuracy for the other tasks.

Experimental results in Table  show that, among 4-layer models, our  ranks first on average performance and 5 specific tasks, while ranks second on the rest tasks. Among 6-layer models, our  delivers similar results. Moreover,  performs better than  and , with only  parameters and inference time.  Therefore, our MLKD-BERT has an average improved performance, compared to state-of-the-art knowledge distillation methods on BERT.

To evaluate the effectiveness of MLKD-BERT distillation, MLKD-BERT is compared with  and its teacher BERT-base. Results show that our  is consistently better than  on all tasks with  improvement on average. Comparing with teacher model BERT-base,  is 7.5x smaller and 9.4x faster, while keeping average  performance of its teacher;  keeps average  performance of its teacher, with only  parameters and inference time. As such, the distillation strategies designed for MLKD-BERT are effective to enhance model performance. 

The comparative results evaluated on the dev sets of SQuAD 1.1 and SQuAD 2.0 are presented in Table , with F1 metric for evaluation. As those two tasks are token-level tasks, we remove  and  in Stage 2.  The results in Table  show that our  and  outperform all other methods on SQuAD 1.1 and SQuAD 2.0, which further demonstrates the effectiveness of our method.

As MLKD-BERT can flexibly set attention head number for student model, this group of experiments are to evaluate its effect on model performance and inference time.  4-layer student model  instantiated with varied numbers of attention heads are experimented on GLUE tasks.  with 12 attention heads is taken as baseline as it has the same number of attention heads as its teacher. The inference time is measured by the time ( standard deviation) required for each batch data on a single GeForce RTX 2080Ti GPU. The batch size  is varied to provide more insights. 

As shown in Table , with decrease of attention head number, the inference time decreases very fast with relatively little performance drop.  And such effect is emphasized with the increase of batch size. As for batch size of 64 on MNLI-m task, when attention head number drops from 12 to 3, the inference time of  decreases 14.11\% while keeping over 98\% prediction performance. More experimental results presented in Appendix , deliver similar effect.  Therefore, the flexible setting of student attention head number would allow  substantial inference time decrease at little expense of performance drop.

% table 4% table 3 First we study the effect of MHA-split number  on model performance in MHA distillation.  We instantiate   with 3 attention heads () and 6 attention heads (), respectively.  For the student model with , the number of MHA-splits varies with 3, 6 and 12; and for the model with , the number of MHA-splits varies with 6, 8 and 12.  The training is conducted under the supervision of teacher model BERT-base, which has 12 attention heads.

Table  shows that, the student model performs best, when the MHA-split number  equals to the number of student attention heads .  This might because it could keep the integrity of student model's subspaces. That explains why we suggest setting the number of MHA-splits as the number of student attention heads.

As the number of attention heads in student model could be smaller than that in teacher model, the teacher attention heads are divided into several MHA-splits by the number of student attention heads, so that each student attention head is mapped to several teacher attention heads in a MHA-split. Our method (named Concat-split) concatenates all teacher attention heads in a MHA-split together.  We could have two other methods to map student attention head to its teacher in a MHA-split: 1) Average mapping averages the teacher attention heads in a MHA-split; 2) Random mapping randomly selects one teacher attention head in a MHA-split.

The second group of experiments are conducted to compare the performance of Concat-split against Average and Random mapping.  with 3 attention heads () and 6 attention heads () are instantiated, respectively. As shown in Table , our Concat-split outperforms the two other methods on almost all tasks.  We think it is because our method has less information loss.

% table 6 In Transformer-layer distillation, as FFN distillation and MHA distillation can be applied on both FFN sub-layer and MHA sub-layer, we are to study whether those two distillations could replace each other. As shown in Table , neither MHA distillation  nor  FFN distillation  could perform well on both FFN and MHA sub-layers.  Instead,  on MHA sub-layer jointing  on FFN sub-layer delivers best performance, which is conducted by our method .

Since the outputs of FFN sub-layer contain feature representations of tokens while the outputs of MHA sub-layer encode the relations of different tokens, feature-level distillation (FFN distillation) fits better for FFN sub-layer, and relation-level distillation (MHA distillation) performs better for MHA. That might explain why our method performs best.

The effect of different distillation loss functions (including , , , , and ) on model performance are evaluated by ablation studies on MNLI-m/-mm, CoLA and MRPC tasks.  keeping all distillation loss functions is taken as the baseline.

As shown in Table , greater drop in performance indicates more importance of corresponding distillation loss function.  As such, according to average performance, the distillation loss functions , , , , and  are listed in importance descending order.   As the removal of any distillation loss function leads to average performance drop, we may conclude that the distillation loss functions proposed by MLKD-BERT are all effective to enhance performance.  Therefore, the relation-level knowledge (, , , ) can be complementary to feature-level knowledge () for performance enhancement. That could explain why our MLKD-BERT outperforms state-of-the-art knowledge distillation methods on BERT. 

% table 5% table 7 Here we are to study whether the distillation procedure of MLKD-BERT should be partitioned into two stages. As we have got 6 distillation loss functions for MLKD-BERT, the one-stage procedure is designed to minimize the sum of the 6 distillation loss functions.  Experimental results with  on MNLI-m/-mm, CoLA and MRPC tasks are presented in Table . We find that  with two-stage distillation outperforms one-stage distillation on all tasks.  That might because our procedure partition could emphasize different distillation objectives for Stage 1 and Stage 2. Stage 1 emphasizes distilling feature representation and transformation, while Stage 2 emphasizes distilling sample prediction. 

% section 6