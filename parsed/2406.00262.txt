DDCL  explicitly splits the output representation of the backbone model into distortion-invariant and distortion-variant representations. The contrastive and orthogonal losses are used to supervise the pairwise distortion-invariant and pairwise distortion-variant representations across different views during the contrastive process, respectively. The formula for this process is formulated as follows:

where  is the backbone model of contrastive learning. The subscripts  and  refer to the variables or functions used for distortion-invariant and distortion-variant representations, respectively, and superscripts  and  represent two views of the same sample.  and  refer to the representation in the latent space and the projection head in the pretext task, respectively.

Analyzing the loss function of the distortion-variant representation of DDCL (, Eq. ), we find that DDCL attempts to de-correlate the projected vectors of pairwise distortion-variant representations by making them orthogonal to each other. However, the orthogonality of  and  is not sufficiently necessary for  to reach zero. We find that when training DDCL on a medium and large-scale dataset, the parameter values of the projection head () tend to zero, generating zero values for the projected vectors ( and ). This trivial solution should be considered as a collapsed projection to a null space rather than achieving orthogonality between representations. Consequently, the split distortion-variant representations may not be effectively supervised and disentangled as expected.

To address the trivial solution of , we propose a novel regularization loss, , for the projection head. This loss function ensures that the parameter magnitude of  is aligned with that of , thereby preventing the collapse of . The proposed loss function is formulated as follows:

where  is the L1 loss function,  refers to the L2 norm. As demonstrated in Table ,  effectively stabilize DDCL by preventing training collapse and avoiding trivial solutions.

%and  is a hyperparameter to tune the effect of the regularization

To introduce equivariant representations into contrastive learning and thereby improve the training efficiency, robustness, and generalizability of the backbone model, we rethink the definition of equivariance. Given a transformation group  with group actions  and  in domain  and co-domain , respectively, we consider a function  to be -invariant when it satisfies Eq. . We call it -invariance when  satisfies Eq. . Obviously, -invariance is a trivial case of -equivariance (when ).

Assuming that the group  has another group operation  (identify operation) in the co-domain, Eq.  can be modified as follows:

Up to here, we can consider that when the transformation operation  perturbs the input  according to the group action , we can introduce -equivariance by finding a proper group action  to attribute the effect caused by  to the representation  which is -invariant in the co-domain  (latent space).

As illustrated in Fig. , we refer to the framework design of DDCL to explicitly split the representations extracted from the backbone model into  () and  () according to a predefined separation ratio. Furthermore,  and  are supervised, respectively, using the contrastive loss () and orthogonal loss (), with the help of the projection regularization loss (). The group action  of the group  in the co-domain  is realized as a concatenation operation, and a trainable neural network that is parallel to and shares some parameters with the backbone model . We name the framework CLeVER, an abbreviation for Contrastive Learning Via Equivariant Representation. The formulas are given as follows:

In the training process of CLeVER, we retain the principle of extracting representations invariant to augmentation operations as used in ICL approaches. Furthermore, we incrementally extract representations that can be used to represent the effects of distortion or perturbation for the samples in a learnable manner. This means that CLeVER only provides information about the perturbations without introducing inductive biases or assumptions a priori (, sensitivity or robustness to a specific perturbation). Unlike other CL methods, CLeVER explicitly splits the extracted representations into invariant representations () and equivariant factors (). Thus, during the inference process (, for a classification task), the prediction head of the downstream task performs a joint probabilistic prediction, , , based on  and , as illustrated in Fig. . This joint modeling allows the downstream tasks to leverage both invariant and equivariant information, enhancing the robustness and generalization of the model.

In this paper, to comprehensively validate the generalizability of CLeVER, we select three kinds of representative mainstream backbone models: ResNet , ViT , and VMamba , based on convolutional operators, self-attention operators, and selective state space models, respectively. Additionally, we use different sizes of backbone models, pre-training datasets, and downstream datasets to investigate CLeVER's training efficiency, performance, and robustness. We mainly incorporate DINO  as the basic framework since it supports more mainstream backbone models and is more stable than Simsiam~.

In addition, although this paper does not focus on the effect of the variety of data augmentation strategies on CLeVER, it is noteworthy that CLeVER is fully adaptive and does not require any augmentation-specific design. This suggests that CLeVER can enrich the equivariance of backbone models by increasing the complexity of the augmentation strategy and incorporating more varieties of transformations.

We use ImageNet-1K (IN-1k) and ImageNet-100 (IN-100) for pre-training, with IN-100 serving as our default pre-training dataset over 500 epochs. In addition, we evaluate the performance of CLeVER on both in-domain and out-of-domain downstream tasks. For in-domain downstream tasks (1\% and 10\% semi-supervised learning), we use the same dataset as in pre-training (IN-1k or IN-100). For out-of-domain downstream tasks, we use CUB200 , Flowers102 , Food101 , and OxfordPet  for downstream classification tasks. Additionally, for out-of-domain segmentation downstream tasks, we use DAVIS 2017 , ECSSD , DUTS , and DUT as test sets. All experiments are conducted on four NVIDIA A100 (80G) GPUs, with experimental setups identical to those of DINO~ and DDCL~ (details in Appendix~).

In this paper, we report on three data augmentation strategies: Basic Augmentation (BAug), Complex Augmentation (CAug), and High Complexity Augmentation (CAug+). We refer to the data augmentation strategies (including color jittering, Gaussian blur, solarization, and multi-crop) used by DINO, as BAug. Furthermore, we refer to DDCL using BAug with the addition of rotation as CAug, and CAug with the addition of elastic transformation as CAug+ (details in Appendix~).

To validate the positive impact of equivariance on the robustness of the backbone model, we use perturbed test data in the linear evaluation of the pre-trained backbone model with perturbations. In this experiment, in addition to CLeVER based on DINO, we also include Simsiam , referring to DDCL, as a baseline to validate the effect of our proposed projection regularization. In this paper, Orig. denotes no perturbation (, linear evaluation), CJ represents color jitter, Ro and ET denote rotation and elastic transformations, respectively. `-IR' denotes that only split Invariant Representations of the backbone (, ) are used for evaluation.

The -related experiments in Table  demonstrate that our proposed projection regularization loss improves the robustness of DDCL. This improvement may be attributed to  preventing training collapse and increasing the disentanglement efficiency of DDCL. More importantly, the evaluation results in Table  and  suggest that Equivariant Factors (, ) can effectively improve the robustness of ICL approaches. With Simsiam as the baseline, introducing stable equivariance improves the performance of the backbone under the perturbation of rotation and elastic transformation by about 27\% and 48\%, respectively. Similarly, by incorporating equivariance, CLeVER improves the performance of vanilla DINO under perturbations of rotation and elastic transformation by about 21\% and 31\%, respectively.

We use the pre-trained backbone model for in-domain and out-of-domain downstream tasks to evaluate the generalization ability of CLeVER. The results in Table  show that CLeVER improves the efficiency of in-domain semi-supervised learning compared to DINO. In addition, in the out-of-domain downstream classification tasks, CLeVER provides more significant improvement when pre-trained with complex augmentation strategies.

To validate the impact of introducing equivariance on attention and performance in downstream segmentation tasks, we conduct unsupervised video target segmentation tests referring to DINO. We also perform unsupervised saliency segmentation tests based on TokenCut . The results in Table  indicate that CLeVER significantly improves unsupervised segmentation performance. Moreover, the use of complex augmentation strategies and equivariance notably improve the backbone model's segmentation capabilities.

To explore the role of equivariant factors during inference, we use the CAug-based pre-trained backbone model for rotational invariance and rotational sensitivity testing. In rotational invariance testing, we randomly rotate the OxfordPet data between  and . The rotated data are then used for evaluation in a downstream classification task. In rotational sensitivity testing, we perform a 4-fold rotation prediction (90째, 180째, 270째, and 360째) on OxfordPet data. The accuracy of the predicted rotation degrees is then evaluated as a downstream task to assess the backbone's rotational sensitivity.

The experiments in Tables  and  demonstrate that the equivariant factors extracted by CLeVER do not introduce inductive bias as the vanilla architecture but instead provide perturbation-related information. This information can be utilized in various ways depending on the requirements of downstream tasks, resulting in improved rotational invariance or rotational sensitivity.

We employ several mainstream backbone models to study the generalizability of CLeVER and the impact of equivariance on different backbone models. We also compare CLeVER with some popular ICL approaches. Each linear experiment of DINO and CLeVER is repeated five times. The results in Table  indicate that CLeVER improves the performance of DINO across various types and scales of backbone models. Notably, smaller-scale models gain more significantly from the introduction of equivariance. Interestingly, VMamba , a recently proposed backbone model, can be effectively integrated into the framework to achieve outstanding performance. Fig.  and Table  further demonstrate that VMamba has surprising performance gains from equivariance within the CLeVER framework. This suggests that the integration of equivariant factors not only improves robustness and generalization but also maximizes the potential of innovative backbone architectures like VMamba.

%Each experiment repeats five times and reports the mean.%{ }

We conduct a robustness experiment of CLeVER on the large dataset IN-1k to validate its reliability. In this experiment, we use ViT-Small as the backbone model and pre-train it for 100 epochs. Table  indicates that on the large dataset, CLeVER can still improve the robustness of the backbone model by increasing the complexity of the augmentation strategy, although the performance gain is not as pronounced as on the medium-scale dataset, IN-100. We believe this may be due to the fact that in large datasets, a substantial amount of semantic information exists for the backbone model to learn, making the learning of equivariance more challenging. Furthermore, the ``Trained by CAug+'' part in Table  suggests that the gains from equivariance become progressively more significant as the complexity of the perturbations increases. This highlights the importance of incorporating complex augmentation strategies to maximize the robustness improvements offered by equivariance, even in large and information-rich datasets.

In this subsection, we perform ablation studies on some critical hyperparameters within CLeVER to ensure optimal configurations. Fig.  shows that the optimal separation ratio (, the ratio of the dimensions of  and ) is 0.8. Fig.  demonstrates that the optimal choice of the output dimension for the projection head in CLeVER is the default . Fig.  shows that the optimal weight  for  is 0.001.

Table  quantitatively demonstrates that, compared to DINO, CLeVER significantly improves performance on downstream segmentation tasks, especially when pre-trained with more complex augmentations. Figure  qualitatively illustrates that the attention-based saliency segmentation results of our proposed CLeVER are significantly better than those of DINO.

Referring to DINO , when pretraining the model, we use SGD with base lr = 0.001, initial weight decay = 0.04, momentum = 0.9, and a cosine decay schedule on both IN-1k and IN-100 datasets. We conduct all experiments with a batch size of 128 per GPU on four NVIDIA A100 (80G) GPUs (or a batch size of 256 per GPU on 2 A100 GPUs), following the linear scaling rule~. For linear evaluation, we use a SGD optimizer with 100 epochs, lr = 0.002, weight decay = 0, momentum = 0.9, and batch size per GPU = 128. On the linear evaluation experiments, only the linear layer is trained. In addition, identical to DINO, we use a warm-up strategy for a more stable training process with 10 warm-up epochs. For fine-tune-based downstream experiments (semi-supervised learning with 1\% and 10\% labels and downstream classification tasks on CUB200 , Flowers102 , Food101  and OxfordPet ), we use a SGD optimizer with 200 epochs, lr of backbone and linear layer = 0.001, weight decay = 0.0001, momentum = 0.9, with a batch size of 256 per GPU. If the experiments are conducted with a batch size of 128 per GPU on four NVIDIA GPUs, the memory is less than 40G per GPU and the training time is around 3.5 hours per 100 epochs for ViT-small on IN100 datasets (The training time is also related to the type of hard drive.)

Compared to default augmentation setting used in DINO (, BAug), the CAug has an additional ``transforms.RandomRotation(degrees=(-90, 90))'' for all input images, and the CAug+ has additional ``transforms.RandomRotation(degrees=(-90, 90))'' and ``transforms.RandomApply(, p=0.5)'' for all input images. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   Invariant-based Contrastive Learning (ICL) methods have achieved impressive performance across various domains. However, the absence of latent space representation for distortion (augmentation)-related information in the latent space makes ICL sub-optimal regarding training efficiency and robustness in downstream tasks. Recent studies suggest that introducing equivariance into Contrastive Learning (CL) can improve overall performance. In this paper, we rethink the roles of augmentation strategies and equivariance in improving CL efficacy. We propose a novel Equivariant-based Contrastive Learning (ECL) framework, CLeVER (ontrastive arning ia quivariant epresentation), compatible with augmentation strategies of arbitrary complexity for various mainstream CL methods and model frameworks. Experimental results demonstrate that CLeVER effectively extracts and incorporates equivariant information from data, thereby improving the training efficiency and robustness of baseline models in downstream tasks. CLeVERCode is available at \hrefIntroductiongui2023surveyCaron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021,devlin2018bert,gui2023survey,oquab2024dinovchen2020simplesubfig:Fig1Batzner_Smidt_Sun_Mailoa_Kornbluth_Molinari_Kozinsky_2021,Gerken_Aronsson_Carlsson_Linander_Ohlsson_Petersson_Persson_2023,Xu_Yang_Liu_He_2023,Sabour_Frosst_Hinton_2017,weiler2023EquivariantAndCoordinateIndependentCNNsweiler2023EquivariantAndCoordinateIndependentCNNsi.e.weiler2023EquivariantAndCoordinateIndependentCNNsbai2023robust,Xiao_Wang_Efros_Darrell_2021,dangovski2021equivariant,Devillers_Lefort_2022,wang2024distortionwang2024distortionWe rethink the ECL framework and DDCL, and propose a regularization loss for projection heads to prevent them from collapsing and generating trivial solutions when extracting equivariant representations using orthogonal loss.

    We propose a novel ECL framework, CLeVER, based on our proposed projection head regularization loss and the advanced ICL framework DINO . CLeVER improves the performance of backbones by adaptively introducing equivariance using augmentation strategies of arbitrary complexity.

Caron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021Our proposed CLeVER significantly improves the training efficiency, generalization, and robustness of CL. Additionally, we find that CLeVER is able to boost the performance of small and medium-scale backbones.

    We employ CLeVER for three mainstream backbone models (ResNet , ViT , and VMamba ), as illustrated in Fig. , experimentally demonstrating that various types of backbone models can achieve better performance with CLeVER. Interestingly, we find that VMamba-based contrastive learning has outstanding performance on medium-scale data.

he2016deepDosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_etliu2024vmambasubfig:LineFigRethink the Equivariant Representation in CLe.g.subfig:Fig1bai2023robust, Xiao_Wang_Efros_Darrell_2021, dangovski2021equivariant, Devillers_Lefort_2022, wang2024distortiondangovski2021equivariantwang2024distortionProposed MethodsCaron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021CLeVERMake DDCL Stablewang2024distortion     z_I^{(1, 2)}, z_V^{(1, 2)} = f(t_{1, 2} \circ I)     

    L_I = L_{CL}(h_I(z_I^{(1)}), h_I(z_I^{(2)})) = -Similarity(h_I(z_I^{(1)}), h_I(z_I^{(2)}))     

    L_V = L_{Orth}(h_V(z_V^{(1)}), h_V(z_V^{(2)})) = h_V(z_V^{(1)}) \cdot h_V(z_V^{(2)})     

    L_{DDCL} = \alpha L_I + \beta L_V      i.e.equa:DDCLV     L_{PReg} = L_1(\Vert h_V \Vert_2^2, \Vert h_I \Vert_2^2) = |\Vert h_V \Vert_2^2 - \Vert h_I \Vert_2^2|      tab:StableDDCLCLeVERequa:Invariantequa:Equivariant1     f(t\rhd_Xx) = f(x)  \quad    \forall t \in T, x \in X     

    f(t\rhd_Xx) = t\rhd_Yf(x) \quad    \forall t \in T, x \in X      equa:Equivariant1     f(t\rhd_Xx) = t\rhd_Yf(x) = t\rhd_Y(t\rhd'_{Y}f(x))  \quad  \forall t \in T, x \in X      subfig:overviewInvariant RepresentationsEquivariant Factors      (z_{IR}^{(1,2)}, z_{EF}^{(1,2)}) = t\rhd_Yf(x) = f(t_{1,2}\rhd_Xx) \quad    \forall t \in T, x \in X     

    L_{CL} = CE(Softmax(h_{IR}(z_{IR}^{(1)})), Softmax(h_{IR}(z_{IR}^{(2)})))     

    L_{Orth} = Softmax(h_{EF}(z_{EF}^{(1)})) \cdot Softmax(h_{EF}(z_{EF}^{(2)}))     

    L_{PReg} = |\Vert h_{EF} \Vert_2^2 - \Vert h_{IR} \Vert_2^2|     

    L_{Total} = \alpha L_{CL} + \beta L_{Orth} +\lambda L_{PReg}      e.g.e.g.i.e.subfig:inferenceMake All Backbones CLeVERhe2016deepDosovitskiy_Beyer_Kolesnikov_Weissenborn_Zhai_Unterthiner_Dehghani_Minderer_Heigold_Gelly_etliu2024vmambaCaron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021Chen_He_2021ExperimentsImplementation DetailsWahCUB_200_2011Nilsback08bossard14parkhi12ashi2015hierarchicalWang_Lu_Wang_Feng_Wang_Yin_Ruan_2017Yang_Zhang_Lu_Ruan_Yang_2013wang2022selfCaron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021wang2024distortiondetailsaugRobustness of EquivarianceChen_He_2021i.e.i.e.tab: IN500_1tab: IN500_1tab: IN500_2i.e.tableThe effect of equivariance on the robustness of Simsiamtab: IN500_11.0\columnwidth!

tableThe effect of equivariance on the robustness of DINOtab: IN500_21.0\columnwidth!

Downstream Taskstab: DSwang2022selftab: USSDtab: RO_1tab: RO_2tableExperiments of rotational invariancetab: RO_11.0\columnwidth!

tableRotational sensitivitytab: RO_20.9\columnwidth!

Generalizability of CLeVERtab: backboneliu2024vmambasubfig:LineFigtab: backboneCLeVER in Large-scale Datasettab: imagenettab: imagenetAblation Studysubfig:Fig5ai.e.subfig:Fig5bsubfig:Fig5cConclusionsSummary.Limitations and Future Works.AppendixQualitative Performance of Downstream Segmentation Tasksdsttab: USSDfig: app1Detailed Experimental SetupsdetailsCaron_Touvron_Misra_Jegou_Mairal_Bojanowski_Joulin_2021goyal2017accurateWahCUB_200_2011Nilsback08bossard14parkhi12aAugmentation Settingsaugi.e.