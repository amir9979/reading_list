An overview diagram of BARDIP is given in Fig.   (a). This approach employs the following three major components. : Acts as a DIP prior, working to de-alias the input TSMI obtained from scaled back-projection:

where  is the hermitian transpose of , to output the cleaner version  , with .  (): It is comprised of two fully connected neural networks for quantitative mapping, nonlinear dimensionality-reduction, and outputting Bloch-consistent denoised TSMI . The networks are architecturally similar with two hidden layers of 300 neurons each, however, each aiming to achieve a different objective and processing different inputs/outputs:

Our approach estimates the complex PD analytically~ by computing . And thus,  is obtained from the pixel-wise multiplication of  and PD.

Optimises the  parameters , hence, to reconstruct  (note that the pretrained  is frozen here):

The square root of the density compensation function (DCF) is applied as a preconditioner within  to accelerate optimization. It also utilises the weight term  to prevent either loss dominating the learning. Besides k-space data consistency, eq.  uses two additional priors for reconstruction: 1) DIP spatial image prior via , and 2) Bloch-consistency via  and . DIP-MRF only used the k-space consistency loss for optimizing the .

 optimization was performed using the ADAM optimizer with a learning rate of 1e-4, on the top 5 SVD basis of both approaches over 30k epochs. This selection of hyperparameters aligns with the early stopping strategy reported by DIP-MRF, aimed at preventing overfitting to undesired artifacts. Real and imaginary parts of complex arrays were formatted whenever needed such that they conform two different channels, therefore, instead of using K channels of complex numbers, the networks receive/output 2K channels of real values.

The  module was pretrained in a supervised manner for 1k epochs on an SVD-MRF dictionary, using pairs of T1/T2 values and their Bloch responses (fingerprints) obtained from Extend-Phase-Graph simulations. Training involved multiplying fingerprints by random complex phasors, adding complex Gaussian noise (), and minimising the sum of MSE loss between the denoised and noiseless complex fingerprints, and MAE losses between predicted and true (T1,T2) values, as described in eq. .

The value of the weight term in eq. , , was empirically determined to be . Note, while DIP-MRF has architecturally similar encoder/decoder modules, only the decoder  is pretrained using the available MRF dictionary, whereas the encoder  is self-supervisedly trained in parallel to the .

% ================================================ RESULTS AND DISCUSSION Fig.  shows the decreasing rate of Mean Average Percentage Error (MAPE) of T1/T2 parameters for both techniques during the reconstruction of a single slice from the simulated data for SNR 35 and SNR 40. Note that these metrics are purely for reporting purposes and are not used during training. Table ~ reports the performance metrics on skull-stripped simulated brain maps of the reconstructed quantitative maps at 1k and 30k iterations, averaged across the 17 slices.  Lastly, Figure  showcases the reconstructed q-maps for the real scan slice after iterations 1k and 30k for the two approaches and with two acquisition settings: L=1000 (original scans) and L=500 (retrospectively truncated scans).

The appeal of ground-truth free approaches such as those based on DIP architectures can be counterbalanced by the need of lengthy iterations and the early stopping required to prevent overfitting to the corrupted data. This issue becomes more prevalent when the techniques become unstable, as seen in the baseline approach in Fig. . As such, there is no guarantee that at the stopping point the reconstructed image will lie away from the shown outliers (jumps in T1/T2 MAPEs). Tuning the LR can help to reach steadiness at the expense of longer training. In comparison, BARDIP exhibits a steady and more rapid decrease of MAPE during the entire course of training, reaching an acceptable error level within 1k iterations and plateauing after 10k, which suggests an even earlier stop would be acceptable for this approach. This is shown quantitatively in both in Fig.  and Table , and qualitatively in Fig. . This gain in computation can be attributed to various factors: a) the pretraining of the  module on the available MRF dictionary, as proposed in this work, which differs from DIP-MRF, where the encoder is self-supervisedly trained during reconstruction iterations, b) the choice of , which in this work corresponds to the scaled back projection defined in Eq. . Further, as iterations continue, BARDIP's accuracy improves, without reaching the undesirable overfitting. We attribute this performance to the additional regularisation that BARDIP utilises for solving the problem through its Bloch-consistency enforcing coupled-loss. 

% ================================================ CONCLUSION