% MCMC methods are a class of algorithms used to obtain samples from probability distributions which are otherwise intractable or do not have a full analytical description. %They are most commonly used for probabilistic inference and fitting a model to data . %MCMC algorithms allow us to sample from the posterior distribution once the samples have converged to the target distribution, but they do not scale well to large models such as deep neural networks.  The first application of MCMC to neural networks was proposed by , who introduced Hamiltonian Monte Carlo (HMC) from quantum chromodynamics to the general statistics literature.  %The more widely used No-U-Turn Sampler (NUTS) is an extension of HMC which allows the hyperparameters of the network to be set automatically . HMC requires the entire dataset to be stored in memory during inference which makes it difficult to scale it to large datasets.   However, it wasn't until  introduced Stochastic Gradient Langevin Dynamics (SGLD), that MCMC for neural networks became feasible for large datasets.  %SGLD can be used with mini-batches of data. Other gradient based MCMC algorithms include stochastic gradient Hamiltonian Monte Carlo (SG-HMC)  and  Cyclical Stochastic Gradient MCMC (SG-MCMC) which introduces a cyclical step-size schedule . The smaller steps explore one mode and larger steps sizes are used to discover and jump between multiple modes of the posterior.  More recently,  have revisited HMC and proposed novel data splitting techniques to make it work with large datasets. We use the HMC algorithm in our work.  %We use the HMC algorithm in this work since our dataset is small.%  HMC simulates the path of a particle traversing the negative posterior density space using Hamiltonian dynamics .  %A Hamiltonian function describes the total energy of a system of particles in terms of their kinetic and potential energies.  To apply HMC to deep learning, the neural network parameter space is augmented by specifying an additional momentum variable, , for each parameter, . Therefore, for a -dimensional parameter space, the augmented parameter space contains  dimensions.  We can then  define a log joint density as follows: %%%that is proportional to the Hamiltonian. 

Hamiltonian dynamics allows us to travel on the contours defined by the joint density of the position and momentum variables. % , also known as the phase space.  The Hamiltonian function is given by:  %% where  is the potential energy and  is the kinetic energy. The potential energy is defined to be the negative log posterior probability and the kinetic energy is usually assumed to be quadratic in nature and of the form , %% %     K(m) = (1/2) \, m^{T} M^{-1} m \, ,%     % % where  is a positive-definite mass matrix. This corresponds to the negative probability density of a zero-mean Gaussian, , with covariance matrix, M, which is usually assumed to be the identity matrix.  % The Hamiltonian can thus be written as:% %% %     H(\theta, m) = -  [p(D|\theta) p(\theta)] + (1/2) \, m^{T} M^{-1} m \, .%     % %

The partial derivatives of the Hamiltonian describe how the system evolves with time. % :% %     }{\partial t}  = {\partial m_{i}} = {\partial m_{i}}, %     %     \\ %     }{\partial t} = {\partial \theta_{i}} = {\partial \theta_{i}} . %     % %theoretical acceptance rate should be 1 because the Hamiltonian is conserved. In order to solve the partial differential equations using computers, we need to discretise the time, , of the dynamical simulation using a step-size, . The state of the system can then be computed iteratively at times , , ... and so on, starting at time zero upto a specified number of steps, . % Several existing numerical integrators can be used to solve the system of partial differential equations described by Equations - , including Euler's method and modified Euler's method . These methods produce trajectories that diverge to infinity unless the step size is made smaller, but this makes the algorithm slow. %write about why symplectic integrators are used The leapfrog integrator is used to solve the system of partial differential equations. %described by Equations - . Two hyperparameters, the step-size, , and the number of leapfrog steps, , together determine the trajectory length of the simulation.  % The leapfrog integrator begins by user-specified initial position and momentum values at time . The momentum values are then updated by half a step-size % %as follows:% % % %     m_{i}(t + \epsilon/2)  =  m_{i}(t) - {2} {\partial \theta_{i}} \, \theta(t) \, , % %     % % % for the  leapfrog step. This is followed by a full step-size for the position values using the updated momentum values.% % :% % %% % % %     \theta_{i}(t + \epsilon)  =  \theta_{i}(t) + \epsilon \, m_{i} (t + \epsilon / 2) \, .% %     \\% %     % % % %% The momentum values are then updated by another half step-size using the updated position values.% % %% % % %     m_{i}(t + \epsilon/2)  =  m_{i}(t + \epsilon/2) - {2} {\partial \theta_{i}} \, \theta(t + \epsilon) \, .% %     % % % %% %In order to make sure that the dynamics of the system are conserved, volume in the phase space must be preserved. The momentum variables should transform in an opposite way to the parameters in order to preserve the volume.  The partial derivative of the potential energy with respect to the position, , can be calculated using the automatic differentiation capabilities of most standard neural network libraries. %The use of gradient information reduces random walk behaviour in HMC.

In each iteration of the HMC algorithm, new momentum values are sampled from Gaussian distributions, followed by simulating the trajectory of the particles according to Hamiltonian dynamics for  steps using the leapfrog integrator with step-size . At the end of the trajectory, the final position and momentum variables, ,  are accepted based on a Metropolis-Hastings accept/reject criterion that evaluates the Hamiltonian for the proposed parameters and the previous parameters. %The proposed parameters are accepted with probability given by:% %      [1,(-H(\theta^{*}, m^{*}) + H (\theta, m)] \, . % % 

Variational inference (VI) assumes an approximate posterior from a family of tractable distributions, and converts the inference problem into an optimisation problem .  The model learns the parameters of the distributions by minimising an Evidence Lower Bound (ELBO) objective function, which is composed of a data likelihood cost and a complexity cost which quantifies the difference between the prior and the variational approximation using KL divergence.  %We refer the reader to  for a detailed explanation of VI.% ELBO is a non-convex optimisation function% lower bound on the evidence, connection to Jensen's inquality% considers the region around the MAP solution (mode) where the posterior density might be high but the volume low% LLA depends completely on the MAP estimate found by standard training. How does the MAP estimate compare to the [mode?] of the HMC posterior??% Adv: post-hoc approximation following standard deep learning training. Effecient implementation% https://machinelearningmastery.com/a-gentle-introduction-to-partial-derivatives-and-gradient-vectors/% The LLA is a post-hoc method which allows us to fit Gaussian distributions over the network paarameters after finding MAP estimates

Last-layer Laplace approximation (LLA) constructs Gaussian approximations around the maximum a posteriori (MAP) values learned by standard NN training using the second order partial derivatives of the loss function,  .  %which contain information about the local curvature of the loss function. This method allows one to learn posteriors for the last layer weights of the network, , % \epsilon ^K \Sigma^{(L)}p(\theta) = ( \theta; 0, \gamma^2 I)\gamma^2pR_{{\rm FR}}R_{{\rm FR}} < 0.5R_{{\rm FR}} > 0.5150\times 1501174572107,89310,000104232, 444\sim200,000 \epsilon = 10^{-4}L = 50\sigma = \{ {1, 10^{-1},10^{-2}, 10^{-3} \}}\sigma = {10^{-1} }100020017097.62\% \approx 1 \geq 1100,000^{}}1 \leq 1101010^{-4}10^{-6}10^{-3}10^{-4}\sigma \in [0.03, 0.04]\sigma = 0.01T = 0.015 . 10^{-5}40125 10^{-5}40\sigma = 0.01T \leq 1N = 200NN = 10200^*N=200N=101064\%(x, y) \sim P(X, Y)P(X)P(Y)P(Y)xf_i(x)ixT1NE = -90-900.10.1[-30, 0.1]\sigma = \log (1 + \exp(\rho))\sigma = \exp(\rho)2.5 \%T = 0.010=1+1xy$};%         \fill[gray] (45:1cm) circle[radius=.2cm];%     %     % % % %  % will be removed in pdf for initial submission % 					  % (without 'accepted' option in \documentclass)%                       % so you can already fill it to test with the%                       % 'accepted' class option%     Briefly list author contributions. %     This is a nice way of making clear who did what and to give proper credit.%     This section is optional.%     H.~Q.~Bovik conceived the idea and wrote the paper.%     Coauthor One created the code.%     Coauthor Two created the figures.% % % \maketitle% This Supplementary Material should be submitted together with the main paper.% Table~ lists additional simulation results; see also  for a comparison. % % % % NOTE: necessary when ptmx or no mathfont class option is given% {\Gamma}% {\pi}% How math looks in equations is important:% %     F_{\alpha,\beta}^\eta(z) = \upGamma({2}) \prod_{\ell=1}^\infty\eta {\ell} + {2\uppi}\int_{-\infty}^z\alpha \sum_{k=1}^\infty x^{\beta k}x.% % However, one should not ignore how well math mixes with text:% The frobble function \(f\) transforms zabbies \(z\) into yannies \(y\).% It is a polynomial \(f(z)=\alpha z + \beta z^2\), where \(-n<\alpha<\beta/n\leq\gamma\), with \(\gamma\) a positive real number. The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories. Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs. In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem.

% In this work we use MCMC sampling to recover posterior estimates for classifying radio galaxies with convolutional neural networks. Using samples from MCMC as a benchmark, %We compare the posterior samples ... %MMD results%However ...%problems with BNNs%CPE results - % We show that the "cold posterior effect" previously observed in morphological classification of radio galaxies is due to model misspecification arising from poor variational posterior approximations.Introductionsec:introwilson2022evaluating, vadera2022ursabenchporter2023mirabestAniyan2017ClassifyingNetworkfanaroff1974morphologylukic2019, becker1995, bowles2021attention, scaife2021fanaroffslijepcevic2022radiomingo2019revisitingmohan2022quantifyingmackay1992a,mackay1992bHMC;neal1998view, neal2011mcmcVI;vireview, blundell, practicalviLLA;daxberger2021laplacegal2015bayesianlakshminarayanan2017simplesec:bayesDLsec:datasec:expssec:evalsec:discussApproximate Bayesian Inference for Deep Learningsec:bayesDLHamiltonian Monte Carlosec:hmc_theoryneal1998viewwelling2011bayesiancobb2021scalingneal2011mcmc, betancourt2017conceptual,hogg2018data     [p(\theta, m)] = [p(\theta|D) p(m)] \, .     

    H(\theta, m) = U(\theta) + K(m) = constant ,      Variational Inferencesec:vi_theorypracticalvi, blundell, vireviewLast-layer Laplace approximation sec:lla_theorydaxberger2021laplace