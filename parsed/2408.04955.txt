The SP stage consists in training a neural network , with parameters , via ERM until it reaches a target training accuracy of , where  is a hyper-parameter. When the model reaches the desired accuracy level, the training stops and a forward pass of the entire training set is performed: samples that are correctly predicted are assigned to  while those not correctly predicted are assigned to . This should capture the fact that biased examples are fitted more quickly by the model. More formally:

Using  as a hyper-parameter is convenient for two reasons.  First, the setting of the amount of desired accuracy is dataset agnostic. This is different from prior work  that employs a similar strategy, but with the hyper-parameter controlling the number of epochs to train the model: in such a case, the number of epochs are strictly dependent on the dataset the model is trained on. Second, we can have a precise control of the amount of samples  assigned to the two splits, e.g.  implies that 85\% of training data are assigned to  and 15\% to . In real use cases, we do not know the correct assignments of the samples to the splits, so we have to rely on a a-priori setting of this parameter. When dealing with biased dataset, we consider the case in which high percentages of samples are biased, e.g. 95\% as in previous works : therefore setting  to high values is hardly a problem since the model overfits easily the training set given the multitude of biased samples.

The PH method, instead of looking only at predictions at a specific epoch, considers the history of samples' predictions throughout the entire training stage.  The rationale here is to make the splitting process more robust given that easy (biased) samples are often predicted correctly from the early training stages (often throughout the entire training), whereas hard (unbiased) samples could be difficult to be fit or are even left unlearned by the model. A second reason to prefer a multiple prediction approach is the fact that a prediction from a single epoch may be not representative of the actual difficulty of a specific sample: grouping predictions together is less affected by statistical oscillations.

Hence, the PH procedure consists in ERM training as for the SP method, but at the end of each epoch, we perform a forward pass of the entire training set and check whether the prediction is correct or not, building a binary vector having the dimension of the training set .  We then define the variable , which is equal to  if sample  has been incorrectly classified at epoch , or equal to  vice versa. Hence, we can compute a vector  of dimension , that indicates what samples have been correctly/incorrectly classified at epoch . At the end of training, we will have the  matrix , with  total number of training epochs, which  describes the history of predictions for each sample.

Summing up along the epoch axis produces an  ranking vector  that describes how many epochs each sample has been correctly classified (see Figure , left).  When computing the histogram for  (using  bins - in order to represent all possible values of the entries of ), one observes that most of the samples are correctly predicted throughout the entire training ( rightmost bin in Fig. ), whereas few samples are never or almost never correctly predicted (leftmost bin in Fig. , right). % %% Empirically, we found that most of the samples in the leftmost bins are unbiased. Provided with this insight, we devised a strategy to iteratively split the dataset by identifying the unbiased samples that are assigned to , while assigning the others to .

We train an ERM model with cross entropy loss weighted by , where  is the weight for sample , assuming values in . Values equal or close to  keep the learning rate unaltered or almost unaltered, whereas smaller values impact the learning rate by slowing it down.  In fact, we now would like a model that enhances the differences between bias/unbiased samples, for better discriminating them: the vector  is thus initialized with all entries equal to  and then are progressively modified for this purpose. During training, we leave unaltered the weights for samples that are already well predicted while we decrease them for samples that are difficult to predict, thus implicitly decreasing their learning rate. Every  epochs we compute the ranking vector  and use it to modulate the weights vector  as follows: 

The value of  is clipped in the range , so that the learning rate can only be decreased or left unaltered. In this way, samples that are inherently difficult to be learned are even slowed down by this mechanism. 

When computing the histogram of the ranking vector at the end of training, we select all samples in the most difficult - leftmost - bin (those that have never been correctly predicted - see Fig. , right) and assign them to , while assigning the others to .

Given the two estimated pseudo-labeled subsets  and , we  consider a neural network , with parameters , trained from scratch.

 In this step we seek the best parameters  on the basis of the two subsets  and  via gradient descent.  The 's updating rule is:

 where  is the learning rate. Note that in case the two subsets are obtained by ,  is the same hyper-parameter introduced in the Sect. 4.1. In this step, we scale the two loss functions with two coefficients  and  to deal with data imbalance (). To rebalance the contributions from the two splits, an obvious choice is to set the weights inversely proportional to the cardinality of the two subsets, which is nothing else than the fixed and controllable hyper-parameter  used to estimate the biased and unbiased subgroups. 

 Subsequently, we seek a representation that can conciliate both biased and unbiased samples and at the same time can prevent the model from overfitting the training data (especially ). We take inspiration from Mixup  as a way to combine samples from the two subsets. Mixup consists in using a convex combination of both input samples and labels demonstrating its efficacy as an effective regularizer.  Specifically, we feed the model with synthetic (augmented) samples resulting from the , aiming at breaking the shortcuts present in the data (see Fig. ).

 We construct  by mixing examples of ,  with corresponding labels , as in the Mixup method , by sampling the mixing parameter  from a Beta distribution, i.e.:

Once the augmented samples () are computed, the model is updated combining the weighted ERM and the regularization term: %  where  is a hyper-parameter weighting the regularization term. 

The hyper-parameter  controls the amount of regularization in the final loss: if , the method corresponds to standard (weighted) ERM in which the contributions of the losses on the two subsets are scaled by  and . When  the weighted ERM optimization trajectory is corrected by the regularization term (Fig. ). This corresponds to find parameters  that are good for both  and , but can also eventually reduce the loss value on the newly generated data samples .  We will actually show that performance is not much affected by the choice of : as detailed in Sect. , accuracy increases and reaches a plateau, and only with higher orders of magnitude  (in the order of ) the regularization effect becomes detrimental. We set  for all experiments.  

To conclude this preliminary analysis, we explored how the choice of the two parameters  impacts the final test accuracy on a standard dataset, CIFAR-10.  As shown in Fig.  the convex combination of biased and unbiased samples works reasonably well for a wide range of  and  that control the probability density function of the Beta distribution (in this preliminary investigation only, we used the ground truth annotations for  and  in order to decouple the learning problem from the bias-identification problem). We employed Bayesian optimization to explore the parameters space in order to highlight the region in which the generated samples gives the best accuracy on the test set. Figure  shows that the best combination is attained when the Beta distribution is slightly skewed towards the unbiased samples, namely, they are given more importance than the biased ones. Vice versa, the worst scenario occurs when the distribution puts more emphasis to the biased samples. However, we claim that augmented data can be generated in a more principled way, that is, by   and  as detailed in the next section.

Building on the insights gained from the analysis of Sec. , we designed an end-to-end pipeline that is able to simultaneously learn the classification task and optimize for the best mixing strategy.  Our model (see Fig. ) is composed of 1) a backbone network acting as feature extractor , and 2) a classifier , with parameters  and , respectively, the latter producing logits from the features .We also adopt 3) a module  which outputs two scalar values, namely, the parameters  and  of the Beta distribution. Samples from  and  are fed to  and the corresponding feature vectors  and  are then concatenated. The module  takes as input such concatenation, denoted as  in the equation below, and estimates the parameters  of the Beta distribution, which is subsequently sampled to extract the mixing coefficient :

Specifically, we sample a vector of mixing coefficients 's (one for each pair in the batch) from the Beta distribution.  We rely on the reparametrization trick  in order to have a fully differentiable pipeline, since we need to differentiate through the sampling procedure. To do so, we employed the Pytorch implementation of the reparametrized Beta distribution.  More formally, we sample  for the sample from the Beta distribution parametrized by  and :  Provided with , we compute  (Eq. ), and use it as input for , whose outcome is subsequently fed to . The logits are then passed to the  softmax layer  to infer , which is used as input for the Cross Entropy (CE) loss . \\

Following the paradigm of adversarial training, we want to learn combinations of ,  that generate challenging samples for the network, acting indeed as adversaries, i.e., for which the loss value . Such augmented samples are then used as training samples for effectively minimizing the loss function.

When doing backpropagation we want to seek the parameters  and  that minimize the CE loss, while at the same time looking for the combination -- i.e., the parameter  drawn from the Beta distribution governed by parameters  and  -- of the two inputs  and  that maximize the CE loss. To do so, we apply a Gradient Reversal Layer (GRL)  to change the sign of the gradient when updating the weights  (see Fig. ). \\

Formally, the optimization problem we want to solve is:

In the second equation the dependence on  is implicit in the variables  as defined in equation , since the mixing coefficients  are sampled from , where  and  depend upon  as from equation . Note that we prevent the gradient to flow into  for the two streams (Fig. ). Therefore,  is updated only through . 

 We may optionally introduce a regularizer that acts as a prior knowledge on the objective function to lead to skewed distributions. Since we deal with imbalanced data, i.e. , it is usually beneficial (see Fig. ) to constrain  to learn a family of Beta distributions that are skewed towards the minority group, i.e., the unbiased subset. This can be easily achieved by adding a regularization term  that forces the expected value of the Beta distribution to be close to the (noisy) bias ratio. We chose the simple Mean Square Error as regularizer: 

Thus, the total loss becomes:

where  is a hyper-parameter that controls the amount of the regularization. 

The several stages composing the proposed method and all introduced hyper-parameters will be analyzed in our ablation study in Sect. 7. \\

We consider a benchmark that has been introduced in , namely corrupted CIFAR-10 in which the amount of bias is synthetically injected and  thus controlled.  Corrupted CIFAR-10 is a modification of the original dataset  and has been adopted by Nam et al.  in the context of debiasing. It consists of 50,000 training RGB images and 10 classes. The bias stems from the fact that each image is corrupted with a specific noise/effect (e.g., motion blur, shot noise, contrast enhancement, etc.). In practice, each class has a privileged type of noise under which it is observed during training (e.g., most of truck images are corrupted with Gaussian blur). 

We prove the efficacy of our method on realistic image datasets, namely, Waterbirds, CelebA and Bias Action Recognition (BAR).  Waterbirds has been introduced in  and combines bird photos from the Caltech-UCSD Birds-200-2011 (CUB) dataset  where the background is replaced with images from the Places dataset . It consists of 4,795 training images and the goal is to distinguish two classes, namely  and . The bias is represented by the background of the images: most landbirds are observed on a land background while most waterbirds are observed in a marine environment. 

CelebA  consists in over 200,000 celebrity face photos annotated with 40 binary attributes (e.g. smiling, mustache, brown hair, etc.). We consider a subset of 162,770 photos and solve the task of deciding, whether the image depicts a blonde person or not: this is the same setting considered in past works  that tackle the debiasing problem. The blonde/non blonde binary label is spuriously correlated with the gender label (the minority group blond-male contains only 1387 images). 

BAR has been introduced in  as a realistic benchmark to test model's debiasing capabilities. It is constructed using several data sources and contains 1,941 photos of people performing several actions. The task is to distinguish between 6 categories: Climbing, Diving, Fishing, Racing, Throwing and Vaulting. The bias arises from the context in which action photos are observed at training time: for instance, diving actions are performed in a natural environment employing a diving suit at training time, whereas in the test set, they are set in an artificial environment, e.g. a swimming pool. For details, readers can refer to the original paper .

 We report the performance of our approach on the different benchmarks above mentioned. Accuracy is the metric adopted to evaluate the performance: since we deal with biased training data and balanced data in testing, we are primarily interested in improving accuracy on the unbiased test samples, those under-represented in the training data. However, at the same time, we do not want to lose performance on the biased samples as we learn more general features: in fact, a higher generalization implies that spurious correlations are not used anymore for classification, and this may cause a drop in performance on such samples. For this reason, we report accuracy on the testing subset of unbiased samples only, as well as over the entire test set (biased  unbiased).

All experiments comply the same evaluation protocol used in the competing methods for a fair comparison.  We used ResNet-18  as a backbone for Corrupted CIFAR-10 and BAR, and ResNet-50 as backbone for Waterbirds and CelebA. We remove the last layer from such backbones, adding a 2-layer MLP head on top of it as a classifier. ResNets are pre-trained on ImageNet .  The parameter network  consists of a simple MLP with two hidden layers with  neurons each. We rely on the Pytorch implementation of the Beta distribution for sampling in a reparameterized fashion. We set the learning rate  for all datasets with batch size of  on synthetic biased data and  for realistic bias data. We used Adam  as optimizer.  In all experiments we adopted the prediction history (PH) method to infer the biased/unbiased samples. In our ablation analysis in Sec. , we discuss how different splitting strategies influence the final outcome, as well as further implementation details. 

 Tables  and  present the performances (best and second best) on synthetic biased datasets, reporting the average accuracy (mean  standard deviation) for the whole test set and for unbiased samples only, respectively. 

We compare against a model trained by Empirical Risk Minimization (ERM) as baseline, and different former methods to learn unbiased representations, either using annotation for the bias or not.  For the methods requiring explicit knowledge of the bias (i.e., supervised), we consider REPAIR , which does sample upweighting, and Group-DRO , which tackles the problem using robust optimization. We also compare our performance with that of Learning from Failure (LfF) , BiaSwap  and SelectMix , which learn a debiased model without exploiting the labeling of the bias (i.e., unsupervised).

We consider different ratios of the bias (ranging from  up to ), as in . This ratio indicates the actual percentage of the dataset belonging to  and , i.e., /.  The choice of the method to estimate  and  has a direct impact on the performance. The hyperparameter of the PH method, controlling the way the original dataset is being divided, is  (see Sect. .B), which is set to  throughout all the experiments.  For , we set the hyperparameter  weighting the regularization term to  as a results of a careful ablation (Sec. ).

For Corrupted CIFAR-10, our  method always reaches the best accuracy. We can note here that the gap with former methods, both supervised and unsupervised, is much higher, ranging overall between about 5\% and 20\% regarding the whole test set (min and max difference across bias ratios, Table ), and between about 5\% and 22\%, when testing on the biased samples only (Table ). It is important to highlight that the results of our proposed methods are reported as mean  standard deviation, obtained after 5 run of the algorithms, while the accuracies reported by SelecMix and BiaSwap are the result of a single (presumably the best) run. Further, taking into account the standard deviation, our accuracies are comparable with the one of the winning methods.

Our results show that the obtained performances are rather stable and consistent across datasets and bias ratios, as compared to previous approaches.

In these trials, we still compare against the ERM baseline and Group DRO, as supervised method, and four unsupervised algorithms, LfF , CVaR DRO  and JTT , the same protocol previously reported. Performances are reported in Table . For these datasets, we remind that we do not have the full control of the bias ratios. Specifically, in BAR we do not know exactly the biased/unbiased samples and,   differently from Corrupted CIFAR-10, which have a balanced test set, Waterbirds and CelebA test sets are also imbalanced.  In these cases, it is also important not only to cope with unbiased samples, but also to maintain accuracy on biased data. Consequently, it is paramount here to reach a good trade-off between generalizing to unbiased samples while keeping high performance on biased data as well. Hence, performances in Table  are reported as accuracies over both the entire test set () and the unbiased samples () for Waterbirds and CelebA, and over the whole test set only () for BAR. 

For both Waterbirds and CelebA, we score favorably with respect to other unsupervised methods for the subset of  samples: we outperform the best state-of-the-art (JTT ) of about 1\%, while a larger gap is detected for other competing methods, namely, about 12\% and 10\% in Waterbirds, and 18\% and 5\% in CelebA, for CVar DRO   and LfF , respectively.  

When addressing the entire test set (indicated as  in the table, i.e. biased  unbiased samples), we obtain the best accuracy for CelebA, outperforming JTT, LfF and CVar DRO by 1,1\%, 4\% and 6.6\%, respectively. For Waterbirds, our  is the second best scoring -1.3\% with respect to CVar DRO, followed by the other methods. In this respect, it is important to notice that CVar DRO, while it generally performs well on the entire,  test set, it drastically experiences large drops when considering the  samples only, equal to 20\% points and 18\% for Watebirds and CelebA, respectively. The same behavior is evident for the standard ERM, with even larger gaps, due to the fact that it is not paying attention to any data bias. 

Overall, our proposed method reaches the best trade-off, resulting competitive for both the types of test sets, across the several datasets. In other words, we are able to learn bias invariant representations without giving up accuracy on the biased samples.

Finally, we show also competitive performance against the supervised method Group DRO: without using any bias supervision, on Waterbirds our method surpasses its test accuracy on the entire test set, even if the accuracy on biased data only results lower (owing to the supervision in this case).  On CelebA, the performance are less competitive with Group DRO, yet we still outperform other unsupervised debiasing methods.   Concerning the BAR dataset, since there is no ground-truth for the bias we report only the average accuracy over the whole test set: also inthis case, our method outperforms all other competitors by a considerable margin.

We assessed the quality of the split obtained by the Prediction History (PH) method. We consider F1-score, Precision and Recall as metrics to evaluate the discrepancy between the pseudo-labels and the ground-truth bias/unbias split (see Fig. ).   CIFAR-10 data (bias ratio ) is considered. We evaluated the method over different values of , namely the amount of epochs before refreshing the assignment: we can notice that as long as  increases, recall decreases and precision increases, while F1-score is significantly higher than the random split case (see Table ). We chose  for all our experiments, while different values are not affecting significantly the performance.

We investigate how different amounts of noise in the pseudo-labeling stage impact the final outcome in terms of test accuracy. We employed our  method on CIFAR-10, see Table . We show the test accuracies in the ideal case of perfect subdivision between biased and unbiased samples (Oracle, ), by applying our approach on top of Single Prediction and Prediction History procedures, and in the case of random split. We noted that passing from the oracle conditions (best) to the random split (worst), accuracy drops significantly. Interestingly, the method trained on the random pseudo-labeling always achieves better accuracy than that of the ERM baseline (see Table ), and even higher than those achieved by some methods specifically designed to debias data. Even a coarse split (better than random) considerably increases the final performance with respect to ERM training. Also, our mixing strategy has a strong regularization effect even in suboptimal conditions (i.e., when mixing mostly biased samples). Comparing with the two bias identification strategies SP and PH, we see that the difference is not severe (drop between 1-4\% for Single Prediction). 

In Table , we analyze how  the parameters of the  distribution results in improved performance on both the whole test test and the unbiased subset. The improvement is consistent across all datasets analyzed. We set  for the standard Mixup  strategy we named , and  for  (check the detailed ablation study fr the two parameters in  and  respectively).   We conducted an analysis on the  hyperparameter of   (Eq. ). We investigated  in the range  and evaluated the performance on Corrupted CIFAR-10 on both biased and unbiased test samples, see Fig. . Even in this case, we observe a large range in which the method is benefiting from the regularization effect of Eq. . The case  represents the final loss in which the regularizer contribution is canceled out.  As reported in Sect. , we always used  for evaluating  performances.

 Similarly, we set different values of the parameter  of Eq. , i.e., , and evaluated the performance of  on Corrupted CIFAR-10 (bias ratio ). In Table , we report the accuracy on the unbiased and on the full set of samples. We can note that there is a large range of  values,  , in which the accuracy is reaching high values in both cases (please, note the logarithmic scale in the  axis). This empirically shows that  is not so a sensitive parameter with respect to the proposed strategy, and for this reason, we fix  in all our experiments. 

We report in Table  an ablation study on the augmentation strategies, consisting in sampling ,  in different ways from either  or  or both. The study was carried on as a preliminary investigation on  only, in order to decouple the sample strategy from the learning problem. We consider the Corrupted CIFAR-10 benchmark, and show the final accuracies for the unbiased and full sets, for all the bias ratios considered. Sampling both ,  from  overfits the biased data and results in the worst accuracy, while mixing both samples from  increases the generalization over unbiased samples, but provides suboptimal results, especially for the biased test set. Mixing samples from  and  corresponds to our strategy, which provides the best performance. 

We also report the baseline case in which no augmentation is performed (1st and 6th rows), i.e. ,  in equations  and   are just individual samples randomly drawn from : performances in this case are significantly distant from those obtained by our proposed approaches.  % We also considered the vanilla Mixup data-augmentation as a baseline (2nd and 7th rows). Results are slightly higher than those of mixing  -   (3rd and 8th rows), but lower than the other combinations. This was somehow expected as most of the samples in the dataset are biased, therefore it is very likely that, by selecting pairs to be mixed randomly, biased/biased pairs are picked more often, while biased/unbiased or unbiased/unbiased pairs are less frequently  considered. 

To conclude, this analysis proves that by choosing pairs from  and  is much more effective, even when the two splits are not accurate. 

To conclude, we tested our proposed methods on the standard CIFAR-10 dataset, without any induced bias. With this test, we want to prove that even in the presence of a non-biased dataset, our proposed method is beneficial, showing an improved generalization capacity. In other words, we can tackle a real-world unsupervised debiasing problem, in which we do not actually know a priori whether a dataset is biased or not. We expect that our proposed methods do not underperform with respect to standard ERM training.

Differently from Corrupted CIFAR-10, the standard version of the dataset does not exhibit any known shortcut when it comes to predict the class labels. We compared our proposed method  and the non-learned version  with standard ERM, using the same architecture  as backbone for a fair comparison (ResNet-18). The dataset split is performed with the PH approach (). Results are reported in Table , which reports the test accuracy of a plain ResNet-18 trained via ERM on CIFAR-10, compared with  (regularization parameter  , and , with regularization parameter ). We can observe that in both cases, our results exceed the ERM baseline, thus showing that our approaches are agnostic to the fact that the dataset might be biased or not, making them more amenable to be utilized in real use cases.