This initial step involves enhancing the reasoning ability of the unsupervised base model  by fine-tuning it with a small, high-quality annotated seed data ,  where  is a sampled question labeled with rationale  and answer . % ~% , where  stands for this query's CoT rationale.  Given the complexity inherent to our dataset , each question in the seed data has undergone rigorous examination.  Any uncertain question will be discussed and vote for the final answer to create rationales.  % This exhaustive process ensures the golden references, embodying the pinnacle of accuracy and reliability.  This process is aimed at directly improving the model's basic reasoning ability with the supervised fine-tuning loss function: % The loss function for a sample can be written% as in Equation~. 

where  represents the model parameters,  % \( _{\sim} \) denotes the expectation over the distribution of ,  and  is the probability of taking action  at state , given the policy parameterized by . After supervised fine-tuning, we could get . % ~

To select high-quality examples for the next step, we further prompt  itself to evaluate the response pairs to unlabeled questions generated by itself and .  Then we get  and .  We attach self-filtering prompting we designed in Table~.  We aim to identify instances where  outperforms  based on relevance, coherence, and the presence of detailed rationales. Only responses where  demonstrates superior reasoning are retained. 

This selective approach ensures the inclusion of only high-quality rationale annotations in the training process, thereby improving the overall effectiveness of our methods. % ~ The third step in our methodology, termed "Self-Reinforcement," employs an innovative approach to further enhance the model's performance.  This step is based on the assumption that SFT models will exhibit superior rationale-generating capabilities compared to their unfinetuned counterparts within the same training domain.  This difference in capability is primarily manifested in the quality of rationales produced.

The score  for the output  from  and its reference base model  is derived as in Equation~. 

% We provide new, unlabeled question sets %  % to both  and , allowing each to generate rationales for these questions.  According to our assumptions, more capable models will obtain higher scores in this phase.  This output quality discrepancy can be directly learnt with DPO based on the ranking loss in Equation~.  This enables us to finetune the stronger SFT model  in a way that systematically amplifies its strengths in rationale generation. 

Self-reinforcement provides a reasonable approach to continue to refine its own reasoning ability interactively.  By repeating this process, we enhance the model's understanding and reasoning capabilities to learn from the comparisons between itself and weaker models. 

In the iterative process, we leverage the improved model from the previous iteration, , and compare its output against the base model, , to obtain a new model . % ~  This is formalized as follows:

Where  represents the Differential Performance Optimization function, taking as input the two models to be compared. 

Notably, our experiments in Section~ demonstrate that our approach can continually grow with the improvements in the SFT model's capabilities.  With each iteration of training, the previously "strong" model can serve as the "weaker" model for the next cycle, since the new, stronger model is developed based on the comparison between the two models from the prior round. 

Here,  represents the iterative self-reinforcement learning loss,  and  represent the scores of the rationales produced by  and  respectively. This iterative process allows the model to improve upon itself, leveraging the comparative strengths of each iteration's outcome.

The primary intent of collecting brainteasers in  is to promote LLMs' capabilities in tackling problems that require deep thought and creative solutions.  We systematically collect those questions from a well-designed open-sourced website, Braingle.   Each question is accompanied by a solution that has garnered widespread acceptance among users, along with a difficulty rating and a human rationale reference. 

 A subset of our dataset is distinguished by an additional metric from the website â€“ the success rate of individuals who have attempted. The inclusion of human-assigned difficulty levels and success rates in this subset offers invaluable insights for our subsequent exploration into enhancing the weak-to-strong learning capabilities of LLMs.

The primary intent of collecting riddles in  is to compel LLMs to think beyond the immediate context. A riddle can describe commonsense knowledge in explicit or counterlogical methods~. We collect those well-designed riddles from an open-sourced website famous for stimulating cognitive explosions, ahaPuzzles. 

While ~ initiated the conversation, our dataset goes a step further by incorporating human rationale, vividly showcasing the intricacies of human thought processes.  This addition significantly enhances the potential for LLMs to evolve innovatively and critically weak-to-strong generalizations from human's step-by-step reasoning iterations.

Puzzles are designed to challenge our cognitive faculties, forcing us to tap into both learned knowledge and innate logic in real-world problems.  Unlike riddles, which play on linguistic ambiguities or reconstructing logically coherent narratives, Puzzles hinge on methodical, step-by-step deduction and inference of structured problems. 

We collect puzzles from sawaal, a well-known public website. This aspect is meticulously reviewed and validated by the community, ensuring the dataset serves as a rigorous training ground to promote LLMs from weak and basic capabilities to generalize strong reasoning capabilities.

Parajumbles involve reordering jumbled sentences into a logical sequence, requiring a deep understanding of the relationships within texts.  Including parajumbles in our dataset helps transition LLMs from basic learning to advanced modeling, enabling sophisticated logical reasoning.

The inspiration for this task is drawn from two well-known tests - Common Admission Test(CAT) and Pearson Test of English for Academic(PTE).  Besides CAT and PTE, we also collect and shuffle those paragraphs from~, two open-sourced news datasets collected from various corpora, such as HuffPost, Business Insider, and CNN. 

Critical Reasoning (CR) is essential for evaluating advanced human cognition~.  Inspired by the reasoning questions from GRE and GMAT, our CR dataset tests and enhances LLMs' abilities to handle complex logical tasks such as understanding paradoxes, assumptions, and conclusions. This helps LLMs reflect the complex nature of human logic.

While our CR question format is similar to ReClor~, our dataset includes expert rationale from experienced educators and excludes any identical questions found in ReClor, enhancing our benchmark's distinctiveness and educational value.

In this section, we provide several statistical analyses of our benchmark.  As we can see in Figure~,  distinguishes itself significantly in terms of the average length of questions and rationales when compared to other existing benchmarks.  With questions averaging 348.80 characters and rationales at 396.37 characters, PuzzleBen's content not only exhibits a higher degree of complexity but also provides more elaborate explanations, which further proves 's uniqueness and necessity to the community. 

A distinctive aspect of our PuzzleBen subset lies in its incorporation of difficulty scores for each brainteaser, derived from the pass rates of online users, offering a directional reflection of our collective grasp on reasoning tasks. The outcomes of our experiments, as detailed in Section~, substantiate the effectiveness and necessity of this feature. This subset promises substantial relevance for future reasoning work, ensuring alignment with human cognitive perceptions from a novel direction.

% {%     %     [%         title={Average Length of Questions and Rationales \\ designed in  and the other benchmarks},%         title style={align=center},%         % ylabel={Average Length},%         xmin=0, xmax=6,%         ymin=0, ymax=450,%         xtick={1,2,3,4,5},%         xticklabels={Creak,MathQA,Aqua,GSM8K,PuzzleBen},%         xticklabel style={rotate=45, anchor=east},%         ytick={0,50,100,150,200,250,300,350,400,450},%         legend pos=north west,%         legend entries={Rationale,Question},%         % axis line style={draw=none},%         tick style={draw=none},%     ]%     % Rationale Length with nodes near coords%     \addplot[%         color=blue,%         mark=square,%         nodes near coords,%         point meta=explicit symbolic,%         every node near coord/.append style={font=\small, anchor = south},%         ]%         coordinates {%         (1,57.62) [57.62]%         (2,149.49) [149.49]%         (3,162.30) [162.30]%         (4,240.33) [240.33]%         (5,396.37) [396.37]%         };%     % Question Length with nodes near coords%     \addplot[%         color=red,%         mark=triangle,%         nodes near coords,%         point meta=explicit symbolic,%         every node near coord/.append style={font=\small, anchor=north},%         ]%         coordinates {%         (1,54.46) [54.46]%         (2,130.61) [130.61]%         (3,131.65) [131.65]%         (4,189.80) [189.80]%         (5,348.80) [348.80]%         };%     %     %     % } Table~ shows standard prompting and zero-shot CoT's performance of GPT4 and PaLM2 on five categories of tasks in . 

As we can see, CoT struggles with the parajumble task. The reason might be that parajumble largely tests concurrent reasoning, where one hypothesizes a sequence and then thinks in reverse to verify its correctness. CoT's step-by-step thinking approach can easily introduce errors at the very beginning of the logic. This limitation underpins the necessity for the  dataset, which aims to enrich future research's landscape by focusing on diverse tasks that challenge current models in various novel ways. 

To convince the utility of the human rationales in , we conduct experiments to utilize those collected rationales both in prompting and fine-tuning directions.  Table~ represents the relations between In-Context Learning(ICL) accuracy and k-shot rationale examples.  

As the number of shots of the training examples increases, the performance across most tasks seems to improve.  Specifically, for the Puzzles and Riddles tasks, there's a noticeable increase in performance from the 0-shot to the 8-shot learning.  The Parajumble and Brainteasers task, though starting with a lower performance score, also shows a similar positive trend. 

The evaluation showcases the utility of human reference in . It is evident that increasing the number of shots or examples benefits the model's accuracy, especially in tasks like Puzzles, Riddles, Parajumble and Brainteasers.  This analysis suggests that for tasks demanding a deeper understanding of complex reasoning, a higher number of shots might provide better guidance to the model, leading to improved outcomes.

To further demonstrate the effectiveness of our  dataset, we have conducted a detailed analysis of the effectivenss of collected human rationales in  for SFT. % ~  The results, as shown in Table~, highlights the substantial improvements in LLaMA-13b's performance when finetuned with our dataset. These improvements underscore the quality and relevance of the training data provided in our .  All of those results indicate how well our dataset is suited for enhancing LLMs' complex reasoning capabilities.

% These findings strongly suggest that  serves as a valuable resource for developing and refining AI capabilities in solving puzzles, riddles, and similar tasks that require advanced cognitive skills. The dataset provides a robust foundation for models to learn and adapt to complex problem-solving tasks, thereby proving its effectiveness and utility in the field of artificial intelligence training. Our experiments  Results depicted in Figure~ illustrate a broad trend where Llama2-13b's accuracy on the PuzzleBen subset wanes as difficulty score intervals rise.  This pattern shows that the model's challenges generally match the rising difficulty of tasks as humans perceive them, though not perfectly. Our research points to the possibility of improving model performance by tuning it to align more closely with human perceptions of task difficulty, rather than merely matching answers to questions. This approach could enhance the model's understanding of reasoning tasks.

 We randomly select 6400 questions and its rationales from .  Considering the difficulty of our dataset, each question and answer has all been fully examined and discussed by annotators.  We also randomly select 6400 unanswered questions for each iteration. 

 We choose the pretrained LLaMA2-13b~ as our base model.  Throughout the training, we consistently apply standard hyperparameters: a learning rate of 5e-5, a batch size of 16 instances, and a total of 3 training epochs. Besides, we employ QLoRA~ with a rank of 16, a LoRA alpha set to 32, and a LoRA dropout rate of 0.05.

%   As we discussed in Section~, we introduced a novel method to improve LLM reasoning abilities with minimal human effort.   Self-reinforcement's motivations and settings are different from traditional methods utilizing extensive prompting or heavy fine-tuning.  Hence, we have few comparable baselines.  However, a similar approach, ReFT~, also uses minimal input and RL to enhance LLMs by learning from model-decoded rationales, specifically by sampling reasoning paths and then creating positive and negative pairs based on the final result.  Although this method aligns with ours to some extent, it cannot be applied to unformatted human rationale texts or datasets lacking an exact answer.

Our experimental results on the  dataset using our self-reinforcement approach highlight significant enhancements in model performance. Our method surpassed traditional strategies such as Unfinetuned, SFT, and ReFT, reflecting the efficacy of our iterative, weak-to-strong learning framework. From the base accuracy of 10.38\%, our model's accuracy improved drastically to 37.82\% by the second iteration (), underscoring the potential of self-reinforcement in leveraging weak supervision for substantial gains in reasoning tasks.

These findings support the effectiveness of our self-reinforcement methodology in continuously refining the reasoning capabilities of language models under limited supervision. By iterating through cycles of self-filtering and differential performance optimization, our approach not only enhances the quality of rationale generation but also steadily increases the overall model accuracy.

%  In this ablation study, we further explore self-filtering's potential impacts on our method. The results in Table~ distinctly illustrates the crucial role of self-filtering in enhancing the performance of our self-reinforcement methodology.  By comparing the results of models trained with and without the self-filtering component, it becomes evident that self-filtering significantly boosts accuracy across multiple iterations.

For instance, at iteration , the model incorporating self-filtering achieved an accuracy of 28.11\%, which is a substantial increase compared to the 18.32\% accuracy of the model without self-filtering. Similarly, at iteration , the gap widened even further, with the self-filtering model reaching an accuracy of 37.82\% compared to 18.28\% for the model without this feature. This clear disparity underscores the effectiveness of self-filtering in refining the dataset and improving the model's reasoning capabilities, thus leading to better performance on complex reasoning tasks.