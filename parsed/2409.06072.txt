Amazon Bedrock is a fully-managed service that offers access to foundational models (FMs) from AI21 Labs, Cohere, Anthropic, Mistral AI and so on.  This service provides users with a wide range of FMs, enabling them to choose the model that best suits their specific use case. With Bedrock's serverless experience, users can quickly get started, customize the FMs with their own data, and easily integrate and deploy them into applications using Amazon Web Services (AWS) tools, without the need for infrastructure management. This streamlined approach accelerates the development of generative AI applications. The selected FMs are particularly well-suited for text generation tasks and can be employed as classifiers to detect fraud and abuse by leveraging publicly available datasets.

% Some of the key use cases are as follows: a) Creating new pieces of original content, such as short stories, essays, social media posts, and web page copy.% b)  Build conversational interfaces such as chatbots and virtual assistants to enhance the user experience for your customers.% c)  Search, find, and synthesize information to answer questions from a large corpus of data.% d)  Get a summary of textual content such as articles, blog posts, books, and documents, to get the gist without having to read the full content.% e)  Create realistic and artistic images of various subjects, environments, and scenes from language prompts.% f)  Help customers find what they're looking for with more relevant and contextual product recommendations than word matching. Jurassic models are provided by AI21 Labs which are suitable for various sophisticated language generation tasks such as question answering, text generation, search, and summarization. The models were launched on March 2023 and trained with data updated to mid 2022.

 Jurassic-2 Ultra is a model with 60 billion paratemeters This model has a 8,191 token context window (i.e. the length of the prompt + completion should be at most 8,192 tokens) and supports multiple languages %It is optimized to follow natural language instructions and context, enabling zero-shot prompting. It supports wide range of use cases such as question answering, summarization, draft generation, advanced information extraction, ideation for tasks requiring intricate reasoning and logic. Jurassic-2 Mid is less powerful than Ultra with 17 billion parameters. It also supports 8,191 token context window and multiple languages.  %yet carefully designed to strike the right balance between exceptional quality and affordability. %Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others. Cohere models are text generation models provided by Cohere Inc. The size of the models is not officially disclosed and there are four models available in AWS Bedrock services.

 Command models are Cohere's flagship text generation model. It is trained to follow user commands and can be used for applications like chat and text summarization. Both of the models support 4,000 token context window and only supports English language. Command Light is a smaller and faster version of command.

% Command-Light is a generative model that responds well with instruction-like prompts. This model provides customers with an unbeatable balance of quality, cost-effectiveness, and low-latency inference. Command R and R+ models are a generative language model optimized for long-context tasks and large scale production workloads. Both of them have 128k token context window and support multiple languages.

% Command R+ is a highly performant generative language model optimized for large scale production workloads. We also test Anthropic's Claude family of models which are Claude 2 and Claude 2.1, both of which were launched early 2023 and are able to complete tasks like text generation, conversation, complex reasoning and analysis. Both of the models support multiple languages. %allow customers to choose the exact combination of intelligence, speed, and cost that suits their business needs. All of the latest Claude models have vision capabilities that enable them to process and analyze image data, meeting a growing demand for multimodal AI systems that can handle diverse data formats. While the family offers impressive performance across the board, Claude 3 Haiku is one of the most affordable and fastest options on the market for its intelligence category.  Claude 2 have a context window of 100k tokens and is estimated to have over 130 billion parameters. .  

%Anthropic's highly capable model across a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following.: An update to Claude 2 that features double the context window (200k), plus improvements across reliability, hallucination rates, and evidence-based accuracy in long document and RAG contexts. .%Claude 2.1 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, 2X decrease hallucination rates, system prompts and our new beta feature: tool use  The Mixture of Experts (MoE) models are also within the scope of our testing. Bedrock provides an MoE model of this type from Mistral AI. We will test the Mistral 8x7B, which is an MoE model combining 8 smaller models, each with 7 billion parameters. Concurrently, we will also test their flagship model, Mistral Large. Both of the have a context window of 32k and support mulitple lanuages. %Mistral AI is a small, creative team with high scientific standards. It makes compute efficient, useful and powerful AI models with both a strong research focus and a fast-paced entrepreneurial mindset. It provides open and portable generative AI for devs and businesses.%  Mistral-7B is a decoder-only transformer which leverages Grouped Query Attention (GQA) for faster inference, coupled with Sliding Window Attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost and Byte-Fallback Byte Pair Encoding (BPE) tokenizer to ensure that characters are never mapped to out of vocabulary tokens . Mistral 7B Instruct is an instruction fine tuned model optimized for chat purposes using supervised fine-tuning (SFT) and direct preference optimization (DPO). Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts) . 

 The Mistral Large is Mistral AI's flagship language model, boasting superior benchmark performance compared to the Claude 2 models. Additionally, it features a JSON format mode that ensures the model's output adheres to valid JSON syntax. This functionality is particularly beneficial for our benchmark task result collection, as we require the model to generate output in the JSON format.. 

%  Llama is a family of large language models that uses publicly available data for training. These models are based on the transformer architecture, which allows it to process input sequences of arbitrary length and generate output sequences of variable length. One of the key features of Llama models is its ability to generate coherent and contextually relevant text. This is achieved through the use of attention mechanisms, which allow the model to focus on different parts of the input sequence as it generates output. Additionally, Llama models use a technique called "masked language modeling" to pre-train the model on a large corpus of text, which helps it learn to predict missing words in a sentence.%  Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for limited computational power and resources, edge devices, and faster training times.%  Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for content creation, conversational AI, language understanding, R\&D, and Enterprise applications.%  Stability AI is the world's leading open-source generative artificial intelligence company, collaborating with public and private sector partners to bring next generation infrastructure to a global audience.%  Deep learning, text-to-image model used to generate detailed images conditioned on text descriptions, inpainting, outpainting, and generating image-to-image translations.SDXL generates images of high quality in virtually any art style and is the best open model for photorealism. Distinct images can be prompted without having any particular 'feel' imparted by the model, ensuring absolute freedom of style. SDXL 1.0 is particularly well-tuned for vibrant and accurate colors, with better contrast, lighting, and shadows than its predecessor, all in native 1024x1024 resolution. In addition, SDXL can generate concepts that are notoriously difficult for image models to render, such as hands and text or spatially arranged compositions (e.g., a woman in the background chasing a dog in the foreground).% % Amazon Titan Foundation Models are pre-trained on large datasets, making them powerful, general-purpose models. Use them as is, or customize them by fine tuning the models with your own data for a particular task without annotating large volumes of data.Titan Text models are generative LLMs for tasks such as summarization, text generation (for example, creating a blog post), classification, open-ended Q\&A, and information extraction. They are also trained on many different programming languages as well as rich text format like tables, JSON and csv's among others.%  Amazon Titan Text Lite is a light weight efficient model ideal for fine-tuning for English-language tasks, including like summarization and copywriting, where customers want a smaller, more cost-effective model that is also highly customizable.%  Amazon Titan Text Express has a context length of up to 8,000 tokens, making it well-suited for a wide range of advanced, general language tasks such as open-ended text generation and conversational chat, as well as support within Retrieval Augmented Generation (RAG). At launch, the model is optimized for English, with multilingual support for more than 100 additional languages available in preview. Zero-shot prompting means that the prompt will not contain any examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it. One sample prompt template used in zero-shot prompting:  Few-shot prompting can be used as a technique to enable in-context learning where we provide examples or demonstrations in the prompt to steer the model to better performance.

%  Introduced in Wei et al. (2022),Chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. We can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. The chain-of-thought prompting asks the model to describe the intermediate steps used to reason its way to a final answer within one response. This is useful for tasks that require detailed explanation, planning and reasoning, such as math problems and logic puzzles, where explaining the thought process is essential to fully understanding the solution. Chain-of-thought prompting aims to encapsulate the reasoning process within a single detailed, self-contained response. 

In this work, we experimented with zero-shot, and few-shot prompting. In our future iterations, we will include advanced prompting techniques such as Chain-Of-Thought, Tree of Thoughts, Prompt Chaining,  and Self-Consistency. 

% [colback=white,colframe=black,listing only,listing options={basicstyle=\ttfamily}]%  "Let's think step by step."\\%  "A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?"\\%  "There are 16 balls in total. Half of the balls are golf balls. That means that there are 8 golf balls. Half of the golf balls are blue. That means that there are 4 blue golf balls."% %  The Tree of Thought (ToT) prompting technique proposed by Yao et el. (2023) and Long (2023) is inspired by the concept of considering various alternative solutions or thought processes before converging on the most plausible one. ToT is based on the idea of branching out into multiple "thought trees" where each branch represents a different line of reasoning. This method allows the LLM to explore various possibilities and hypotheses, much like human cognitive processes where multiple scenarios are considered before determining the most likely one. %  The prompt chaining involves an iterative sequence of prompts and responses, in which each subsequent prompt is formulated based on the model's output in response to the previous one. This makes prompt chaining a useful technique for more creative, exploratory tasks that involve gradual refinement, such as generating detailed narratives and brainstorming ideas. Prompt chaining takes a more dynamic approach, with multiple rounds of interaction that allow an idea to develop over time.% [colback=white,colframe=black,listing only,listing options={basicstyle=\ttfamily}]%  "Classify the text into neutral, negative or positive."\\%  "I think the vacation is okay."\\%  "Neutral"\\%  "The food tastes ok to me"\\%  "Neutral"% %  Self-Consistency is one of the more advanced prompting techniques for prompt engineering. Proposed by Wang et al. (2022), self-consistency aims "to replace the naive greedy decoding used in chain-of-thought prompting". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.% [colback=white,colframe=black,listing only,listing options={basicstyle=\ttfamily}]%  "Classify the text into neutral, negative or positive."\\%  "I think the vacation is okay."\\%  "Neutral"\\%  "The food tastes ok to me"\\%  "Neutral"%  Language modeling is a long-standing research topic, dating back to the 1950s with Shannon's application of information theory to human language, where he measured how well simple n-gram language models predict or compress natural language text. Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, machine translation, to information retrieval. The recent advances on large language models (LLMs), pretrained on Web-scale text corpora, significantly extended the capabilities of language models . LLMs are transformer based neural language models that contain tens to hundreds of billions of parameters which are pretrained on massive training data. The most widely used LLM architectures are encoder only, decoder only and encoder-decoder. 

The transformer architecture was originally designed for sequence transduction or neural machine translation to convert an input sequence to an output sequence. It is a simple network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Transformer architecture consists of seven key components. A demonstration of each of the components is shown below.

% %         %      hhh%  Encoder models use only the encoder of a Transformer model . At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having "bi-directional" attention, and are often called auto-encoding models. The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering. The representatives of this family of include: ALBERT, BERT, DistilBERT, ELECTRA , RoBERTa. 

% One of the prominent encoder only model is BERT (Bi-directional Encoder Representations from Transformers). BERT is an encoder-only Transformer that randomly masks certain tokens in the input to avoid seeing other tokens, which would allow it to "cheat". The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. {<<REMOVE>>}% RoBERTa improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective. The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like DistilBERT. {<<REMOVE>>} Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models. The pretraining of decoder models usually revolves around predicting the next word in the sentence. These models are best suited for tasks involving text generation. The representatives of this family of include: CTRL, GPT , GPT-2 , Transformer XL, Llama 2. 

%  Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. The Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging in scale from 7B to 70B parameters (7B, 13B, 70B). The pretrained models come with significant improvements over the Llama 1 models, including being trained on 40\% more tokens, having a much longer context length (4k tokens), and using grouped-query attention for fast inference of the 70B model. {<<REMOVE>>} Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder only access the words positioned before a given word in the input. The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, some models are pretrained by replacing random spans of text that can contain several words with a single mask special word, and the objective is then to predict the text that this mask word replaces. Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering. The representatives of this family of models include: BART, mBART, Marian, T5Instruction tuning is a simple method that combines appealing aspects of both the pretrain–finetune and prompting paradigms by using supervision via finetuning to improve the ability of language models to respond to inference-time text interactions. The supervision teaches the model to perform tasks described via instructions. Recent empirical resultsdemonstrate promising abilities of language models to perform tasks described purely via instructions. Finetuning on groups of language tasks has been shown to significantly boost this zero-shot task generalization of language models .

% Flan-T5 is an instruction fine-tuned version of T5 or Text-to-Text Transfer Transformer Language Model . It is a dense encoder-decoder model based on pretrained T5 and fine-tuned with instructions for better zero-shot and few-shot performance. There are 5 different sizes of this model: (1) Flan-T5-Small (80M Parameters), (2) Flan-T5-Base (250M Parameters), (3) Flan-T5-Large (780M Parameters), (4) Flan-T5-XL (3B Parameters) and (5) Flan-T5-XXL (11B Parameters).The finetuning data comprises 473 datasets, 146 task categories, and 1,836 total tasks which includes toxic language detection. {<<REMOVE>>}%  mT0 is a Multitask prompted finetuning (MTF) variant of mT5. The model is fine-tuned using BigScience's crosslingual task mixture i.e xP3 datasets, creating a variety of new models capable of crosslingual generalization to unseen tasks and languages.There are 5 different sizes of this model: (1) mT0-Small (300M Parameters), (2) mT0-Base (580M Parameters), (3) mT0-Large (1.2B Parameters), (4) mT0-XL (3.7B Parameters) and (5) mT0-XXL (13B Parameters). {<<REMOVE>>} In this paper, we leverage LangChain for in-context learning. LangChain is a robust Python library designed to simplify interactions with various LLM providers. Chains are a vital component of LangChain, allowing multiple elements to seamlessly integrate. In this work, we specifically used the LLMChain.

We have reported precision, recall, and F1 score for various experiments in the Results section above. In Table , and Table , we are showing the 95\% confidence interval along with the F1 score for different tasks and different models. The key observation here is the variance is low that means LLMs are stable in abuse detection. 

DetoxBench: Benchmarking Large Language Models for Multitask Fraud \& Abuse DetectionJoymallya ChakrabortyThese authors have contributed equally to this research.joymally@amazon.com%   \institution   \city   \state   \country Wei Xia1weixxia@amazon.com%   \institution   \city   \countryAnirban Majumder1amajum@amazon.com%   \institution   \city   \countryDan Ma1dangam@amazon.com%   \institution   \city   \countryWalid Chaabenewalidc@amazon.com%   \institution   \city   \countryNaveed Janvekarnjjanvek@amazon.com%   \institution   \city   \countrycodegreenrgb0,0.6,0codegrayrgb0.5,0.5,0.5codepurplergb0.58,0,0.82backcolourrgb0.95,0.95,0.92\shortauthorsChakraborty et al. Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further exploration. The existing applications often narrowly focus on specific tasks like toxicity or hate speech detection. In this paper, we present a comprehensive benchmark suite designed to assess the performance of LLMs in identifying and mitigating fraudulent and abusive language across various real-world scenarios. Our benchmark encompasses a diverse set of tasks, including detecting spam emails, hate speech, misogynistic language, and more. We evaluated several state-of-the-art LLMs, including models from Anthropic, Mistral AI, and the AI21 family, to provide a comprehensive assessment of their capabilities in this critical domain. The results indicate that while LLMs exhibit proficient baseline performance in individual fraud and abuse detection tasks, their performance varies considerably across tasks, particularly struggling with tasks that demand nuanced pragmatic reasoning, such as identifying diverse forms of misogynistic language. These findings have important implications for the responsible development and deployment of LLMs in high-risk applications. Our benchmark suite can serve as a tool for researchers and practitioners to systematically evaluate LLMs for multi-task fraud detection and drive the creation of more robust, trustworthy, and ethically-aligned systems for fraud and abuse detection. Large Language Model (LLM), LLM Benchmark, Fraud \& Abuse Detection, Toxic Language, LangChainIntroductionbrown2020language% Benchmarks help researchers understand what LLMs can and cannot do well. This unfolds the strengths and limitations of the models.% The existence of challenging benchmarks motivates researchers to develop increasingly capable and robust language models.% Some benchmarks provide a way to measure improvements in language understanding and generation over time as new models are developed. - Benchmarks unfold the merits and shortcomings of the different LLMs and allow researchers to quickly understand what LLMs can and cannot do well. 

Evaluating capabilities - Benchmarks allow researchers to compare the performance of different LLMs across different research areas and select the best model for a specific task.

    % Benchmarks give an objective way to compare the performance of LLMs from different research areas. This helps to identify the best model for specific task.

    Enabling comparisons - Benchmarks provide a baseline and motivate researchers to develop increasingly capable and robust language models.

Driving innovation - Benchmarks provide a way to track improvements in understanding natural language over time as new models are developed.

Tracking progresshendrycks2021measuringliang2023holistic\url\url LLMs have the potential to be powerful tools for identifying fraudulent language, patterns, and behaviors. A specialized fraud benchmark would help evaluate and improve the ability of LLMs to detect fraud in text-based data.     Detecting fraudulent activity: Certain groups, such as women, minorities, and LGBTQ+ individuals, often face disproportionate amounts of online abuse. An abuse-focused benchmark would drive the development of LLMs that can better identify and filter this harmful content to safeguard vulnerable users.     Protecting vulnerable users:LGBTQ Fraud costs businesses and consumers billions of dollars each year~. An effective fraud detection LLM could help organizations mitigate these significant financial losses by catching fraudulent activities earlier.     Mitigating financial losses:Financialfraud,FTCData As AI-powered language assistants become more prevalent, it is critical that they do not generate or perpetuate abusive language. An abuse benchmark would help ensure these systems respond in a non-harmful and empathetic  manner.     Enabling more responsible AI assistants: Rigorous benchmarking of LLMs' ability to detect and respond to abuse would provide crucial data on the limitations, biases, and potential harms of these models. This would inform more responsible and accountable AI development practices.     Informing ethical AI development: The challenge of accurately recognizing nuanced forms of abuse, including subtle linguistic cues, would push the boundaries of LLM capabilities in areas like sentiment analysis, contextual awareness, and empathy modeling.     Improving natural language understanding: Unaddressed online abuse can have mental health consequences  and lead to real-world violence . Effective abuse-detection LLMs could help curb these negative impacts on individuals and communities. Mitigating real-world harm:Cyberbullyingmentalhealth} &  &                                                                                                                                                                  &  &                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\                    & 10,944                                                          & Hate, No Hate                                                                                                                                                                   & 1,196                                                               &                                                                                                                                                                                                                \\  Toxic Chat                                                               & 10,165                                                          & Toxic, Non-Toxic                                                                                                                                                                & 746                                                                 &                                                                                                                                                                                                                                                                                                                        \\        & 17,880                                                          & Fake, Real                                                                                                                                                                      & 866                                                                 &                                                                                                                                                                                                                                              \\                      & 16,989                                                          & Fake, Real                                                                                                                                                                      & 9,727                                                               &                                                                                                                                                                                                                                                                                                                                                                                                                                 \\               & 18,650                                                          & Phishing, Safe                                                                                                                                                                  & 7,328                                                               &                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\                   & 11,929                                                          & Fraud, Not Fraud                                                                                                                                                                & 5,187                                                               &                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\                    & 5,573                                                           & Spam, Not Spam                                                                                                                                                                  & 747                                                                 &                                                                                                                                                                                                                                                                                                          \\  Misogyny                                                               & 6,567                                                           &  & 752                                                                 &  \\ Description of the datasets used in this paper.Labels1c|\cellcolor\textbftabularPrior Benchmarkswang2019gluewang2020supergluenie2020adversarialpetroni2019language, petroni2020howlin2022truthfulqawang2020persuasionjiang2024detecting, bhatt2024cyberseceval, guo2024investigation: Fraud and abuse data often contains sensitive personal and financial information about affected individuals and organizations. There are legal and ethical obligations to protect the privacy and confidentiality of this data, which makes public disclosure challenging. That is why there are very few fraud and abuse datasets that are publicly available. Hence, most of the LLMs are not trained on a large fraud and abuse corpus.        Limited Data Availability: Fraud patterns, trends, and correlations are often more readily identifiable in numeric datasets that can be parsed, filtered, and visualized. Among the publicly available fraud and abuse datasets, most datasets are numeric in nature and can not be used for LLM use cases. Limited Textual DataData Details\url\url\url Hate speech refers to communication that expresses  discrimination, prejudice, or hostility towards a person or group based on their race, religion, gender, disability, or other sensitive characteristics. Stormfront is a neo-Nazi Internet forum, and the first major racial hate site focused on propagating white nationalism, Nazism, antisemitism and Islamophobia, as well as anti-feminism, holocaust denial, homophobia, transphobia, and white supremacy . The dataset contains text extracted from this forum. A random set of forum posts have been sampled from several subforums and split into sentences. Those sentences have been manually labelled as containing hate speech or not, according to certain annotation guidelines. 

    hate-speech:\url\url\url Toxic chat refers to online conversations or interactions that are excessively negative, hostile, or abusive in nature. This dataset contains toxicity annotations collected from the Vicuna online demo. The data collection, pre-processing, and annotation details can be found in the paper "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference".  

    toxic-chat:\urlchiang2024chatbot\urllin2023toxicchat Fraud job postings refer to job advertisements that are deceptive or misleading in nature, with the intent to take advantage of job seekers. Some of the key characteristics of fraud job postings include fake or non-existent employers, requests for sensitive information, deceptive job details, upfront fees or payments and urgent or high pressure tactics. The data consists of both textual information and meta-information about the jobs.

fraudulent-job-posting:\url Fake news refers to deliberately fabricated or misleading information presented as if it were true and factual news. Some of the key characteristics of fake news include factual inaccuracy, deceptive intent, undermining public trust. Identifying and combating fake news has become an increasingly important challenge, as the speed and reach of digital media makes it easier for misinformation to spread. 

fake-news:\url Phishing emails have become a significant threat to individuals and organizations worldwide. These deceptive emails aim to trick recipients into divulging sensitive information or performing harmful actions. Detecting and preventing phishing emails is crucial to safeguard personal and financial security. This dataset specifies the email text body and the type of emails which can be used to detect phishing emails by using LLMs. 

phishing-email:\url Fraudulent e-mails contain criminally deceptive information, with the intent of convincing the recipient to give the sender a large amount of money. Perhaps the best known type of fraudulent e-mails is the "Nigerian Letter" or "419" Fraud. This dataset is a collection of 11.9K e-mails with more than 5K "Nigerian" Fraud Letters, dating from 1998 to 2007.

fraud-email:\url Spam emails are unsolicited email messages that are sent out in bulk to a large number of recipients. Spam emails are generally seen as an annoying and intrusive form of unsolicited communication. Most email providers and countries have laws in place to help reduce the impact of spam and protect users from its negative effects. Some of the key characteristics of spam emails include lack of consent and relationship, misleading or deceptive claims, and potential for harm .

spam-email:\url Online misogyny is a pernicious social problem that risks making online platforms unwelcoming and toxic to women. Women have been shown to be twice as likely as men to experience gender-based online harassment . Misogynistic comments can inflict serious psychological harm on women and produce a 'silencing effect', whereby women self-censor or withdraw from online spaces entirely, thus limiting their freedom of expression  .We collected this expert labelled dataset to enable classification of misogynistic content using LLMs . 

misogyny:\urlMantilla2013-MANGMA-4\urlguest-etal-2021-expertLLM Services (Infrastructure)\url\url\url\url\url\urlwidth=0.90\textwidth, height=4cmAMLC_LLM/image/llm.pngThe DetoxBench PipelinelangchaninAmazon Bedrock:AI21 Labs Jurassic:Jurassic-2 Ultra:Jurassic-2 Mid:Cohere:Command and Command Light:Command R and R+:Anthropic:Claude 2:\urlClaude 2.1\urlMistral AI:Mixtral 8x7B Instruct:jiang2024mixtralMistral Large:\urlPrompting-7ptZero-shot prompting:-8.5pt "Please decide if the following text is hate speech or not:"

 "She may or may not be a Jew but she 's certainly stupid , she seems to think the Blacks won't kill her." Text:Output:Few-shot prompting: "Check the following job post: The title is Agile Project Manager. The location is NZ, , Wellington. The requirements is Not Mentioned. The employment type is Full-time. The industry is Not Mentioned. The function is Not Mentioned. "

 "Fake is 1, Real is 0. The job post is  0."

 "Check the following job post: The title is RN PACU . The location is US, GA, . The requirements is Not Mentioned. The employment type is Full-time. The industry is Hospital  Health Care. The function is Not Mentioned."

 " Fake is 1, Real is 0. The job post is  1."

 "Answer the following job post: The title is Credit and Collections Clerk. The location is US, WA, . The requirements is ?. The employment type is Full-time. The industry is Hospital  Health Care. The function is Financial Analyst." Text:Output:Text:Output:Text:width=\textwidth,height=12cmAMLC_LLM/image/zero_shot.pngClassification performance of 8 LLMs for different tasks with zero shot prompting strategy. The results are shown in the form of three metrics - \textcolor, \textcolor, and \textcolor. We referred the LLMs using these abbreviations - AIJ2M - AI21.J2-Mid; AIJU - AI21.J2-Ultra; CCT - Cohere.Command-Text; CCLT - Cohere.Command-Light-Text; ACV2: Anthropic.Claude-V2; ACV2:1 - Anthropic.Claude-V2:1; MM - Mixtral MOE; ML - Mistral Large.fig:zero_shot_figurewidth=\textwidth,height=12cmAMLC_LLM/image/few_shot.pngClassification performance of 8 LLMs for different tasks with few shot prompting strategy. The results are shown in the form of three metrics - \textcolor, \textcolor, and \textcolor. fig:few_shot_figurewidth=\textwidth,height=8cmAMLC_LLM/image/heat_map.pngF1 score of 8 LLMs for different tasks with zero shot prompting and few shot prompting strategy respectively.fig:heat_mapResults\urlprecisionrecallF1 scorefig:zero_shot_figurefig:few_shot_figurefig:heat_map For zero shot and few shot prompting, out of eight datasets, in five cases (spam email, hate speech, fake news, fraud email, misogyny) Mistral family (ML - Mistral Large) performs the best. After Mistral family, the Anthropic Claude models achieve the second best F1 score.

    F1 Score: Anthropic Claude family models are highly precise in most of the cases but the recall is significantly low. For toxic chat  and hate speech detection (few-shot),  the precision is over 90\% but the recall is less than 10\%. Though these models suffer from low recall, but they can be used for proprietary use cases where highly precise results are preferred.

    High Precision but Low Recall: Cohere family models provide high recall i.e. can detect most of the fraud cases. However, the precision is not that great i.e. the false positive rate is high. For fake job detection (few-shot), the recall is 85\% and the precision is 48\%. For fraud email detection (few-shot), the recall is 98\% and the precision is 64\%. 

High Recall but Low Precision: AI21 family models are the fastest for inference (1.5 seconds/instance), while Mistral Large, and Anthropic Claude models are the slowest (10 seconds/instance).

Inference Time: For most of the cases, few-shot prompting does take more time for inference but does not improve the performance of the LLMs significantly. For example, for Mistral Large model, few-shot prompting improves performance in only two tasks - fake job detection, and misogyny detection. 

Effect of Prompts: There are some LLMs (Command R, Command R+ from Cohere family) that could not follow the LangChain JSON parser output formats. Hence, we removed them from our final benchmark results. These models are not the best choice for production use cases as they require additional post processing of outputs which might be expensive. 

Format Compliance: In the misogyny dataset, there are five different categories. Out of five categories, four of them are different kinds of misogynistic categories (Misogynistic, Misogynistic, Treatment, Derogation), and one of them is non-misogynistic category.  We transformed this dataset into a multiple-choice problem, allowing the model to choose one correct option out of four. While reporting the metrics, we take the "weighted" recall, precision, and F1 across all the classes. A121 Jurassic-2 Mid (AIJ2M) model could only classify 2.1\% instances and rest of them resulted as "undecided". Similar to binary classification, here also we got the best results from Mistral, and Anthropic Claude family models. 

Multi-Class Classification:LimitationsThe datasets to the best of our knowledge, are the most representative among publicly available datasets of fraud/abuse detection problems. We do not claim these datasets to be comprehensive, but hopefully with time the collection will grow to cover more business scenarios and dataset variations.

    While these datasets are useful for research and development of fraud detection algorithms, they do not carry any information about real fraud. If someone uses any models trained on these datasets to directly make decisions about fraud, it would cause a negative bias and could lead to false accusations.

    We started experimenting with relatively smaller size LLMs such as FLAN-T5, RoBERTa, mT0, etc. We decided to not include those models in our study as they require an additional  Verbalizer to assign a label (abusive/non-abusive) to every textual input based on higher likelihood. On the other hand, comparatively larger LLMs that we used were capable enough to classify most of the inputs without using Verbalizer.

\urlIn this study, we used zero shot and few shot prompting techniques, and followed the best practices of LLM prompting as guided by huggingface. The reason behind that is we wanted to capture the strength of the models using minimal prompt intelligence. In future version, we will also experiment with other advanced prompting techniques. 

    There are some advanced LLMs that we could not include in our work such as GPT family (not available on Bedrock), Llama family (Data privacy issues). The inclusion of these models could have given different set of results.

    This work only focuses on English language texts. We decided to use English only for two reasons. First, as most of the advanced LLMs support English and majority of prior benchmark works also focused on English only. Second, there are very few non-English textual datasets related to fraud and abuse domain that are publicly available .

Dataset Disclaimers and TermsConclusion \& Future WorkACM-Reference-FormatrefAppendixwidth=0.76\textwidth, height=6.5cmAMLC_LLM/image/Transformer_architecture.pngThe Transformer - model architecture~\citeLLM Architectureminaee2024largedevlin2019bertdai2019transformerxllewis2019bart\textbfvaswani2023attention The ML models use user-entered tokens as training data, while it can only process numeric information. Thus, it is necessary to transform these textual inputs into a numerical format known as "input embeddings". These embeddings function similarly to a dictionary, assisting the model in understanding the meaning of words by arranging them in a mathematical space where comparable phrases are situated close together.     Input Embedding:: The order of words in a sentence is essential in the NLP field for identifying the statement's meaning. The positional encoding is utilized to encode each word's location in the input sequence as a collection of integers which allows the model to grasp sentence word order better and provide grammatically accurate and semantically relevant output. The original transformer architecture uses sine and cosine functions of different frequencies:

    Positional Embedding         PE_{(pos,2i)} = sin(pos/10000^{(2i/d_{model})})

        PE_{(pos,2i+1)} = cos(pos/10000^{(2i/d_{model})})     The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers.The first is a multi-headed self-attention layer and the second is a simple position-wise fully connected feed-forward network. It processes the input text and generates a series of hidden states. This consists of two linear transformations with a ReLU activation in between.

    Encoder:          FFN(x) = max(0, xW_{1} + b1)W_{2} + b2      During the training process, the decoder acquires the ability to predict the next word by analyzing the previous words. In this case, the output sequence is shifted by one position to the right. Consequently, the decoder is able to use the words that came before it.     Outputs (shifted right): The output is converted to a format known as "output embedding." Like input embeddings, output embeddings also undergo positional encoding, enabling the model to understand the order of words in a sentence.     Output Embedding:The decoder is composed of a stack of N = 6 identical transformer layers. In addition to the two sub-layers in each encoder layer, the decoder has a third sub-layer, which performs multi-head attention over the output of the encoder stack. It creates output sequences from positionally encoded input sequences while learns to predict the next word from previous words in positionally encoded output embedding during the training period.     Decoder: The linear layer maps to the higher-dimensional space once the decoder has generated the output embedding. This step is required to convert the output embedding into the original input space. The softmax function generates a probability distribution for each output token in the developed vocabulary, allowing us to generate probabilistic output tokens. Linear Layer and Softmax: F1 score of 8 LLMs with 95\% confidence interval for different tasks with zero shot prompting.zero_shot_confidenceF1 score of 8 LLMs with 95\% confidence interval for different tasks with few shot prompting.few_shot_confidence\textbf  \urllan2020albertdevlin2019bertsanh2020distilbert\urlliu2019roberta\textbf\urlkeskar2019ctrlRadford2018ImprovingLURadford2019LanguageMAdai2019transformerxltouvron2023llama\textbf\urllewis2019bartliu2020multilingual\urlraffel2023exploring\textbfwei2022finetunedwei2022finetuned, chung2022scalingLangChain Framework\urlAdditional Resultszero_shot_confidencefew_shot_confidence