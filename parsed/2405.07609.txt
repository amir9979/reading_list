In the following, we discuss each training split and the type of noise it models.

Our benchmark requires two splits without any label noise: A clean test split to evaluate models trained on noisy training data, and a  training split to measure the upper bound performance. 

Since the original annotations of CoNLL-03 have been shown to be noisy~, we use the labels of ~, a recently released resource in which 7\% of all original annotations were semi-automatically relabeled. In their evaluation,  find their resulting dataset to be of very high quality and largely improved consistency. %They also showed that state-of-the-art NER methods reach significantly higher F1 scores on their cleaned version of CoNLL-03.  The  split in our benchmark is the standard CoNLL-03 test split, with the  labels.

The largest portion of machine learning datasets relies on manual annotation by domain experts to provide high-quality labels. However, errors have been found to occur even in expert annotation, affecting even well-known benchmarks, though usually with relatively low noise shares of under 10\%~.  To represent such noise, our benchmark includes a variant of the train split called , which contains the original CoNLL-03 annotations. As Table~ shows, this split has a noise share of 5.5\% and is thus the split with lowest noise.

In addition to expert annotations, labeled datasets can be obtained via crowdsourcing. These annotations are heavily prone to human annotation errors, which can happen for different reasons %, including accidents, insufficient information and inter-rater variability . In order to create noisy variants of the train set in our benchmark representing real-world human errors, we utilize the crowdsourced labels by . This study involves 47 crowd workers labelling a subset of the CoNLL-03 dataset, of around 400 news articles. %, with each crowd worker labeling a subset of these articles.  They released their dataset and all annotations produced by each crowd worker. We selected only the sentences where the tokenization matched the Clean variant, resulting in 5,885 sentences.  

%Comparing the annotation quality of the crowd workers to our \noiseclean~split, we measure large discrepancies between each worker, from the "best" worker labeling at 0.85 F1 score to the worst labeling only at 0.17 F1. As such discrepancies are expected, much crowdsourcing research focuses on finding efficient ways to filter and aggregate annotations produced by the crowd.

We include two noisy training splits based on crowd annotations into our benchmark: (1)~In the first, , we do a simple majority vote over all annotations provided for each token, i.e.~the baseline method for aggregating crowdsourced annotations. (2)~In the second, , we use an oracle version of the majority vote, selected by either taking the correct label if it is provided by any of the annotators or, in the absence of a correct label, by choosing the label with the majority of votes. This version represents the upper bound of crowdsourced labels given a perfect label aggregation method. As Table~ shows, the noise share of  (36.6\%) is considerably higher than  (15.3\%).

One approach for labeling data without human participation is ~, where entity mentions in target datasets are matched to entity types in knowledge bases (KBs). The matching can be achieved by simple string matching, use of regular expressions or heuristics. % \todo[inline]{A few more details on the process. Longest entity names are matches first? Error sources: Incompleteness in the KB? Wrong matching?}

We include a noisy training variant in our benchmark, adapted from the annotations by  that use the Wikidata corpus and gazetteers collected from multiple online sources as external knowledge bases. After initial POS tagging%with an NLTK tagger  , the unlabeled sentences were matched with the knowledge bases.

This process results in incomplete annotations due to limited coverage over entity types of KBs. %As a result, the KB matching yields a large amount of instances labeled with class "O".  This explains the rather high number of missing entities %total number of entities and the overall noise level (31.3\%) of the  training variant, as shown in Table~. 

Another approach aimed at reducing manual annotation efforts is weak supervision. Here, labels are obtained using a number of 'weak' supervision sources, such as heuristics or expression-based rules. Each weak source is typically specialised to detect only a subset of the correct labels. %Disagreements between the resulting weak labels can be resolved by voting schemata or more complex unsupervised methods.

We use the labels from the approach by  to create our ~label set. This covers 16 weak labeling sources, those used in , including heuristics, gazetteers %(Crunchbase, Geonames, Wikidata) and predictions of NER models trained on other corpora% (Broad Twitter Corpus and Ontonotes5.0) . An example heuristic is detecting PER (person) entities using a pre-defined list of %English  first names.

We aggregate the weak label sets with simple majority voting. We apply majority vote on every token with at least one entity label assigned to it, following . Due to the large number of labelling sources, majority voting yields a large number of entities, as shown in Table , including many false positives. As a result, the label set has a high noise share of 40.4\%.

%Different areas in NLP have witnessed remarkable advances with . 

Our benchmark includes a noisy variant of the train split annotated by an LLM. This follows recent efforts that use LLMs for dataset generation~. Here, the main idea is to pass a natural language description of the annotation task and target classes to an %instruction-tuned  LLM, and provide sentences that the LLM should label. Current works find that LLMs are able to generate high quality labels for some tasks (e.g. sentiment classification) while for others (e.g. NER and question type categorization) the resulting labels are very noisy. 

We created the variant using the Fabricator toolkit  by prompting GPT3.5 for named entities in our training dataset. To use LLM outputs for annotation of NER datasets, a certain output format is required. To achieve this, we provide one example with the correct output format in each prompt. This example is the same for each sentence we wish to annotate, which we refer to as a static one-shot setting. The example sentence was selected from the remainder of the CoNLL-03 training split, which consists of all sentences not included in our benchmark.

As Table~ shows, the ~label set results in the highest noise share of 45.6\%. This is mainly due to the large number of nouns incorrectly identified as entity mentions, which also makes this the label set with the largest number of entity annotations out of the variants in .

An overview of ~is given in Table .  % With the metrics shown, we assess the performance between the \noiseclean~training split and the noisy variants.  The table shows token-level F1 score %(}\normalsize)%(significant if we want to discuss NER as a token classification problem)%, sentence-level %(significant if we want to include only clean sentences in training)   and entity-level F1 score %(}\normalsize), %(metrics of interest in NER).  expressed as percentages. We define the noise level () in terms of the entity-level F1 score, as . The noise levels of the noisy splits range from 5.5 to 45.6 percent.

Furthermore, the table shows the total number of entities, the number of correct entities, as well as the share of different error types. The errors are categorized into 4 main categories:  mentions,  mentions (false positives), incorrect entity  (where the boundary is correct, but type incorrect)  and  matches. Partial matches are special cases where the type is correct, however the mention boundary is only partially correct. Refer to Figure~ for examples.

We observe that the , ~and ~label sets have a lower total number of entity annotations than the ~dataset, and the largest portion of errors are missing mentions. Conversely, the ~and ~label sets have more annotations than the ~dataset, and most of the errors are either an incorrect mention or incorrect type. Most of the errors in the ~label set are due to incorrect type. Regarding the number of partial matches, for almost all noise types, they make up between 10\% and 15\% of all errors. 

We consider two noise simulation methods, namely the simple  used in most prior work and a more involved  method that we design to mirror each noisy variant in .

% more involved method for simulating realistic noise that combines various proposals from prior work. %The most common simulation approaches use either uniform or class-dependent noise.  Uniform noise corrupts samples into any other label with a uniform probability distribution, given a target noise share. Studies investigating simulated noise in the NER task commonly rely on variants of this method . 

Class-dependent noise is based on the knowledge that some pairs of classes are more likely to be mislabeled than others. It is defined by a noise transition matrix, which contains the mislabeling probabilities between all pairs of classes . We design an oracle version of class-dependent noise, where the per-class mislabeling probabilities of real noise are known. This allows us to investigate class-dependent noise in an ideal case, where it is able to mirror real noise closely, even though this is not possible in practice. This method mirrors real noise by utilizing the token-level mislabeling frequencies as probabilities to form a noise transition matrix.

Using each noise simulation method, we created 6 label sets, corresponding to each noise level in . It should be noted that the simulated labels replicate the token-level F1 scores of the real noisy labels, however the entity-level F1 and sentence-level accuracy can deviate. 

In both experiments, we train a baseline approach for NER on each noisy variant of the training split, as well as on the additional simulated noise.  % Experiment 1 compares the impact of different noise types on test set performance, and Experiment 2 evaluates their impact on the training curves and investigates the memorization effect. We evaluate the setting in which all available data to train a model is noisy, including the validation set.

To obtain noisy validation sets for each of our 7 dataset variants, we split the noisy datasets into training and validation sets. All sentences from 66 news documents from 1996-08-24 comprise the validation set, which is around 17\% of all sentences, and are left out from model training and used for hyperparameter tuning. 

 For NER, as a baseline approach, we fine-tune an  transformer using the FLERT approach . It improves upon the regular fine-tuning setup by considering document-level features of a sentence to be tagged. %: for each sentence, 64 subtokens of left and right context (surrounding sentences in the document) are added.  We use a learning rate of 5e-6 and a batch size of 32, for a fixed number of 10 epochs% linear scheduler with warmup. . These parameters were obtained according to the performance on a noisy validation set, keeping in mind that larger batch sizes are more robust to noise .  As an evaluation metric we use entity-level micro F1-score.

In the first experiment, we compare how the clean test set performance is impacted by the 6 types of real label noise when present in the training set. In addition, we provide the same comparison for corresponding simulated noisy label sets.

The results for uniform noise are shown in Appendix . We initially established that uniform noise is significantly less challenging for the model, so in the results from Experiments 1 and 2 we chose to focus our analysis solely on oracle class-dependent noise.

The main results from Experiment 1 for oracle class-dependent noise are shown in Table . Additional analysis of the results can be found in Appendix . Following are our main observations.

 When we compare the test F1 scores of the real noisy variants with the average score of 93.99 achieved when training on the ~variant, we can see that model performance is affected by each noise type. In all cases, the noisy training variants yield lower test scores, even when training with the low-noise ~variant. As the noise levels increase, the impact on model performance becomes more pronounced. This shows that the baseline model lacks robustness to any of the real noise types. When we compare the test F1 scores of the simulated noisy variants, we can see that noise of 5.9 in the training set does not hurt model performance and results in a score comparable to training on the ~variant. However, as simulated noise levels increase, they do degrade the prediction scores on the test set.

 Furthermore, when we compare the real noisy label sets with their equivalent simulated noisy variants, we can observe that the simulated training variants show a score of around 2.5 percentage points higher on average than the real label sets. %This happens in the case of most label sets, excluding the weak supervision labels, where the This shows that for predictive NER models, real noise is more difficult to overcome than simulated noise. In other words, models are more likely to overfit to real noisy labels, rather than simulated ones.

Prior analysis has found that there are distinct phases of learning when training a model on data with label noise~. This has been referred to as a , where models learn patterns that generalize well to clean data, followed by a , where models overfit to the label noise and deteriorate in prediction quality~.  

To investigate this phenomenon for real and simulated noise, we extend the training stage to 100 epochs. At the end of each epoch, we measure the F1 score of the model on both the noisy training split it is being trained on, and separately on the clean training split. The difference between these two scores allows us to measure %generalization and  memorization.

In Table  we show training curves from training with real and simulated variants of ~ for 3 noise types: , ~and . We plot two scores: the F1-score on the respective noisy variant of the training set, and the F1 score on the ~variant of the training set.  %, also highlighting the 10th epoch, where training stops in our baseline approach.  In all training curves, we can observe the memorization effect, with each model perfectly fitting the noisy data by the end of training and reaching an F1 score close to 1.

  However, we note that with simulated noise (see Table , , ) this happens much later in the training process than with real noise. In addition, the training curves of simulated noise show a stage during the early epochs where the score on the clean labels is consistently higher than the score on the noisy labels. This confirms previous findings that the model is able to learn general patterns first, before starting to memorize the noise. 

 With real noise this does not happen and the model starts fitting the noisy labels from the beginning of training (see Table , , ). As a result, the score on the clean labels is consistently lower than the score on the noisy labels, during the entire training run.

Our experiments find that real noise does not display distinct generalization/memorization phases during training, and rather immediately begins with memorization. This makes intuitive sense, as real noise has underlying patterns that may be extracted during learning. This lends further evidence to the increased challenges and the need to evaluate noise-robust learning with real noise. 

We surveyed current state-of-the-art methods for noise-robust NER and found that many approaches rely on the same underlying ideas for handling label noise. In the following, we group approaches by the underlying idea, select a state-of-the-art representative for each group and, if possible, derive an upper bound method for each noise-robust method. For more details about the implementation of compared approaches refer to Appendix .

The first family of approaches relies on utilizing the subset of each noisy dataset in which all labels are correct. They either filter out all likely incorrect annotations and learn only from a clean subset . Or they derive confidence weights for each annotation so that annotations judged to be of higher quality feature more during training, as in the CrossWeigh and COSINE approaches~.  As representative of this class of approaches, we chose confident learning~.

 To obtain an upper bound for this family of approaches, we use an oracle to select the subset of clean sentences from each of the noisy training splits in . We then use a standard fine-tuning approach only on this subset. This setting illustrates the best-case scenario for training on a clean subset only. %This upper bounds represents the best-case

Another family of noise-robust learning approaches seeks to leverage the two phases of learning (generalization and memorization) we discussed in Section~. They seek to either draw our the generalization phase or cease training before memorization begins. While our experiments indicate that these two phases do not exist for real noise, we nevertheless include this family of approaches in our evaluation since they are widely used and to gain additional insights into their behavior on real noise. As representative of this class of approaches, we chose co-regularization .

 To obtain an upper bound for this family of approaches, we use a simple stopping criterion based on the score on %the true generalization using  the clean test set at the end of each epoch. We use the epoch of best generalization to report the final score. This simulates an ideal stopping (albeit at the granularity of full epochs). 

While the approach discussed so far each build on the individual ideas of identifying a clean subset or delaying memorization, many current approaches in fact combine multiple of such ideas in multi-stage pipelines . As representative of such approaches, we evaluate BOND~, which combines pseudo-labeling in a student-teacher setup and sample selection.

 We cannot derive a separate upper bound for pseudo-labeling, as the best case scenario here would mean that all noisy labels are replaced by correct labels. This means that the upper bound for pseudo-labeling is the same as training on fully clean data.

 Table  summarizes the evaluation results. We make the following observations:

 The upper bound of training only on clean examples is the best achievable result for all types of noise. This was to be expected, since in this scenario the labels are not contaminated by any noise. This indicates that noise-robust approaches focusing on identifying the clean subset have high potential. Oracle stopping, on the other hand, does not achieve the same level of performance as the oracle subset, while only slightly outperforming the FLERT baseline. This is in line with our findings in Experiment 2 that the early-learning generalization phase is skipped when training with real noise, indicating that early stopping approaches have less potential.

 Evidently, there is no single best approach for all noise types. While for all noise types some of the noise-robust approaches are able to outperform the baseline, they do so only slightly, with the exception of BOND, on the ~and ~variants. Still, their performance is far below the upper bound. This raises the issue of trade-offs of existing noise-robust learning approaches, since they often require additional hyperparameter tuning or incur computational costs, but only lead to slight improvements over the baseline in the presence of real noise.