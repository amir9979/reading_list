Since the widespread application of LLMs, ensuring their safe and harmless usage has been an important topic. RLHF~ is currently the most widely used method for LLM alignment, which requires training a Reward Model on human-labeled preference datasets and applying it for reinforcement learning to align the target LLM with human values. RLAIF uses a dedicated constitutional model instead of human annotations to determine the priority of preference data, relying on AI to identify preferences. Safe RLHF~ introduces a Cost Model on top of RLHF, emphasizing the safety differences between different responses in preference data annotation and reinforcement learning. To improve the substantial training costs of RLHF, DPO~ simplifies preference training, enabling LLM alignment on preference datasets to only consider the target model and reference model. However, these training-based methods still have high requirements for computational resources and high-quality preference data.

Recent studies have found that guiding LLM decoding by modifying the logits of tokens during the inference process can enhance the capabilities of large models.

Contrastive Decoding~ introduces a relatively weaker amateur model alongside the target base model, treating the logits of the amateur model as noise outside the target model's logits, thus enhancing the performance of the base model by subtracting the confusion noise of the amateur model. In comparison, ARGS~ utilizes the Reward Model in RLHF rather than a generative language model to guide decoding. Specifically, ARGS selects the top-k candidate tokens at each decoding step, evaluates all candidate tokens using the Reward Model's reward value, and selects the token with the highest reward as the final token. SafeDecoding~ applies the original base model as the amateur model and conducts additional safety training on it. The tuned model acts as an expert model for contrastive decoding to defend against LLM jailbreak. This method requires the LLM itself to have a high level of safety performance. Proxy Tuning~ modifies the logits of a larger model by adding the difference between the logits of a smaller model with and without alignment, enabling the larger model without alignment to achieve performance similar to that of an aligned model.

The above methods of guided decoding require additional contrast models outside the original model or obtaining new contrast models through training, which increases memory overhead beyond the original model. Another innovative approach is to conduct guided decoding without introducing new models. RAIN~ utilizes the original large model itself as the evaluator and selects the most suitable output through a tree-based search. However, since each judgment requires to interact with an LLM, RAIN incurs significant time overhead and is challenging to use in practical scenarios. Instructive Decoding~ innovatively adjusts instructions to obtain better and worse responses on the same model for contrast. Compared to introducing additional amateur models, Instructive Decoding is a prompt-based contrastive decoding method that offers advantages in terms of inference overhead. ROSE~ directly applies Instructive Decoding to LLM safety, demonstrating its significant potential for safety alignment. However, the contrast prompts in Instructive Decoding and ROSE need to be manually designed, which limits the universality of this approach.

Generally, our proposed Adversarial Contrastie Decoding can be divided into two stages: Opposite Prompt Optimization (as shown in Fig.~) and Prompt-based Contrastive Decoding (as shown in Fig.~). In Opposite Prompt Optimization, we optimize two opposing soft prompts on a small, generated anchor dataset: the Safeguarding Prompt (SP) and the Adversarial Prompt (AP). The Safeguarding Prompt is designed to enhance the LLM's safety capabilities, encouraging the LLM to refuse to respond to harmful instructions as much as possible. Conversely, the Adversarial Prompt aims to make the LLM produce threatful responses, thereby exposing the model's unsafe aspects.

For each model, prompt optimization needs to be performed only once and requires minimal computational overhead with just several GPU minutes on a single NVIDIA A100. The optimized soft prompts serve as universal system prompts that can be directly concatenated to the text embedding of the user's instruction during interaction. These two opposite prompts finally result in logits for two different outputs during each inference step, which are then used for contrastive decoding.

The anchor dataset is utilized to optimize the two opposing soft prompts. Only a small amount of anchor data is needed for the optimized soft prompts to outperform manually written prompts. We begin by using ChatGPT to randomly generate 100 safe and 100 unsafe instructions following the settings of~, resulting in a total of 200 queries for subsequent data generation. Then, we sample different responses on the Llama-2-uncensored model with three manual prompts: a safe prompt, an opposite prompt, and a null prompt. Through this sampling method, a dataset with 600 instruction-response pairs is obtained, which serves as the anchor data for Opposite Prompt Optimization.

The target Safeguarding Prompt and Adversarial Prompt is initialized with a manual safe and an opposite prompt respectively before optimization. For the safe prompt, we directly apply the system prompt from ~ for Llama-2, which is a widely used prompt for text generation. For the opposite prompt, we partially replace safe words with corresponding antonyms and provide additional prompts to make models always follow instructions no matter what they are. These two types of prompts are demonstrated in Tab.~. The manually initialized prompts are then transferred into embedding for soft prompt optimization as shown in ~.

Where  and  imply manual safe and opposite prompt for initialization and  represents the embedding layer of the target model . The embedded soft Safeguarding Prompt  and Adversarial Prompt  will be optimized in the next stage.

In optimization stage, both soft Safeguarding Prompt  and Adversarial Prompt  are concatenated with embedding of instructions () as in~.

When optimizing the Safeguarding Prompt, we aim to make the target model reject harmful instructions as much as possible when using this prompt. Therefore, we treat data from the anchor dataset where the model rejects unsafe instructions as positive samples and data where it accepts to respond as negative samples. For positive samples, we apply cross-entropy loss  to optimize the soft Safeguarding Prompt. For negative samples, an unlikelihood loss~  is used for optimization. Additionally, we use the data from the safe instructions portion of the anchor dataset to further constrain the prompt optimization, as shown in , to ensure that the model does not mistakenly reject harmless instructions when the Safeguarding Prompt is present.

The loss function of optimizing the Safeguarding Prompt is demonstrated in ~, for which  and  indicate instructions and corresponding responses respectively and  is the -th token of the response. The Safeguarding Prompt is jointly optimized with loss , where  and  represents anchor data with harmful instructions and rejected responses or accepted responses respectively, while  stands for anchor data with safe instructions. 

For Adversarial Prompt Optimization, we use an opposite optimization objective to make the model bypass safety checks and respond to harmful instructions as much as possible. Contrary to Safeguarding Prompt Optimization, we treat the data in the anchor dataset where the model accepts harmful instructions as positive samples and the data where it rejects harmful instructions as negative samples, as demonstrated in opposite losses  and . This encourages the model to respond to all harmful queries when the Adversarial Prompt is applied. Similarly, we constrain this optimization using the safe instructions portion of the anchor dataset to ensure balanced performance.

By optimizing  in Eq., the Adversarial Prompt can better explore the harmful distribution of the model's output space.

Through Opposite Prompt Optimization, we obtain two contrasting soft prompts: the Safeguarding Prompt, which enhances the model's attention to the safety of instructions, and the Adversarial Prompt, which exposes the unsafe aspects of the model's responses. This creates two opposing response distributions at the prompt level.

During inference, the user's instruction is first converted into text embeddings via the model's embedding layer. These text embeddings are then concatenated with the optimized soft prompts separately as~ and fed into the subsequent transformer modules for decoding. After passing through the decoder's head, we obtain the safe response logits from the Safeguarding Prompt and the adversarial response logits from the Adversarial Prompt. Based on these, we perform prompt-based contrastive decoding to derive the final logits used for sampling as shown in ~ and Fig.~.

To validate the effectiveness and generalizability of Adversarial Contrastive Decoding, we conduct experiments on multiple models and safety benchmarks to assess its impact on model safety. We also verified that ACD does not affect the normal usage of models on general tasks. Subsequently, we conduct ablation experiments to investigate the reasons behind ACD's effectiveness. Finally, we discuss the potential of ACD to decrease the risk of jailbreak attacks.

We select seven different models for our main experiment. These include two uncensored models: Llama-2-uncensored-7b (based on Llama-2-7b~) and Llama-3-uncensored-8b (based on Llama3-8b~). These two models were instruction-tuned on datasets without safety examples, helping to demonstrate our method's effectiveness on weakly safety-aligned models. Additionally, we included weakly aligned Bloom-7b~ and Guanaco~ (including 7b and 13b), together with strong-aligned Vicuna-13b~ and Mistral-7b-Instruct~. We select five safety-related benchmarks and sample 100 harmful queries for each benchmark to comprehensively evaluate our method: AdvBench~, Malicious Instruct~, HarmfulQA/DangerousQA~, and Beaver Test~.

Considering that our method does not require additional training or the introduction of extra models, we compare it with other methods that also do not require extra models, specifically Instructive Decoding~, the state-of-the-art model-free guided decoding for general language tasks. Our baselines including: (1) : Regular decoding with a manually designed safe system prompt. (2) Null-prompt Instructive Decoding : Using instructions without a prompt as the contrastive item. (3) Opposite-prompt Instructive Decoding : Using manually designed reverse prompts as the contrastive item.

Similar to other studies on LLM safety~, we apply ChatGPT to evaluate the safety of model outputs and employ the Harmless Rate (HLR) to quantitatively assess model safety. Specifically, for each instruction in the benchmark, we prompt ChatGPT to evaluate whether the target model's response is harmful. HLR is defined as the proportion of harmless responses out of all responses. A higher HLR indicates that the target model has higher safety on that benchmark.

% Please add the following required packages to your document preamble:%  The impact of different decoding methods on the Harmless Rate (HLR) of models is shown in Table 2. The experimental results indicate that ACD significantly enhances safety across almost all models and benchmarks compared to regular base decoding methods. Additionally, ACD generally outperforms the baseline Instructive Decoding in most cases. For several weakly safety-aligned LLMs, such as Llama-2-uncensored-7b and Bloom-7b, where the original model safety is around 50\%, ACD increases the HLR by an average of over 25\% without training the model parameters. Even for models that have undergone safety training, ACD can further enhance their safety performance. Notably, though some models, such as Llama-uncensored and Guanaco, initially less safety-aligned compared to those with safety training, achieve comparable safety performance to these models after applying ACD.

To verify whether the safety enhancements provided by ACD come at the expense of the model's general performance, we evaluate it on two general task datasets: AlpacaEval~ and TruthfulQA~. We sample 100 instructions from these two datasets respectively for helpfulness assessment. For the AlpacaEval dataset, we compare the outputs generated by the model with ACD against the outputs of OpenAI's  and , calculating the win rate using ChatGPT. For the TruthfulQA dataset, we utilize GPT-4 to assess whether the model's outputs are aligned with real-world knowledge and calculate the truthful rate. As shown in Table 3, ACD does not significantly impact the model's performance on general tasks.

The superiority of ACD stems from the strong contrast between the Safeguarding Prompt (SP) and the Adversarial Prompt (AP).

To verify the positive contribution of the SP and AP obtained through Opposite Prompt Optimization to contrastive decoding, we conduct ablation experiments with these two prompts in ACD. As shown in Tab.~, we replace the original SP in ACD with a null prompt and a manual safe prompt, and replaced the AP with a null prompt and a manual opposite prompt respectively. We then test these variations on Llama-2-uncensored and Llama-3-uncensored models across three benchmarks. The results indicate that the safety performance of the decoding methods with these replacements is inferior to that of the original SP and AP contrast.

To more intuitively illustrate the impact of SP and AP, we assess the model's safety performance using only the null-prompt, manual safe-prompt, manual opposite-prompt, and the SP and AP. The results are shown in Fig.~. The SP provides better safety than the manual safe-prompt (higher HLR), and the AP exposes more risks than the manual opposite-prompt (lower HLR). Consequently, the ACD results obtained through the contrast between SP and AP are superior to those obtained through the contrast between safe and opposite prompts in Opposite-prompt Instructive Decoding (oID). These findings reveal the key factor of ACD's effectiveness: building a stronger contrast by optimizing both safer and more harmful prompts, thereby achieving greater benefits in the contrastive decoding process.

A moderate  is more beneficial for ACD performance.

We conduct ablation experiments on the contrastive coefficient  in ~ of Prompt-based Contrastive Decoding with Llama-2-uncensored and Llama-3-uncensored across three benchmarks. Results in Fig.~ show that as  increases, the model's safety initially rises and then falls. The reason is that a too-small  cannot adequately remove negative probabilities from the reverse logits, while a too-large  overly suppresses the probabilities of effective candidate tokens. This result aligns with trends observed in other contrastive decoding studies~. Therefore, we recommend using a moderate  in practical applications, such as 0.4 or 0.5.

 Anchor datasets generated through different models are effective.

We apply various models (Llama-2-uncensored-7b, Llama-3-uncensored-8b, guanaco-7b) to sample responses for the anchor data in Sec.~ and conduct OPO on Llama-2-uncensored and Llama-3-uncensored models. Results in Tab.~ demonstrate that anchor data sampled from different models can enhance model safety through OPO and ACD. This indicates that the SP and AP obtained during the OPO process do not merely learn different response texts but rather capture a general behavior of rejecting or accepting threatful queries, which supports the settings that small-scale anchor data can effectively optimize a universal SP and AP for a model.

Jailbreak attacks~ aim to provoke unsafe responses from aligned LLMs. We find that ACD not only enhances the model's intrinsic safety but also reduces the risk of jailbreak attacks. As shown in Tab.~, we evaluate the Mistral-7b-Instruct against two jailbreak prompts, role-play-based AIM and instruction-following-based Refusal Suppression (RS)~, measuring the Attack Success Rate (ASR) as the proportion of additional harmful responses after the jailbreak. Results indicate that applying ACD significantly improves the model's robustness against jailbreak attacks. This improvement arises because the optimized Adversarial Prompt (AP) itself acts as an embedding-level jailbreak. By subtracting the risks introduced by the AP during contrastive decoding, the probability of harmful outputs can be effectively reduced. This demonstrates ACD's potential to defend against jailbreak attacks.

The source links of LLMs and benchmarks used in the experiments are listed in Tab.~.

Each data of the anchor dataset contains an instruction, three corresponding responses sampled with safe, opposite and null system prompts, and a label for each response to annotate whether the response refuses the instruction. The safe prompt and opposite prompt are the same as the prompts in Tab.~ utilized for Instructive Decoding and initialization of OPO. Generally, we use Llama-2-uncensored-7b to sample all responses for each model because the generated responses for unsafe instructions have a relatively balanced refuse and acceptance ratio. Especially, the responses of the anchor dataset for Llama-3-uncensored-8b are sampled from itself for comparison. After the sampling, we apply ChatGPT to annotate each response with the label ``refused'' or ``accepted'', which represents the attribution for  (harmful  refused)  (harmful  accepted) for optimization of the Safeguarding and Adversarial Prompts in OPO. The detailed prompt to label responses is shown in Tab.~.

The optimization of Safeguarding Prompt and Adversarial Prompt can be seen as a kind of soft Prompt Tuning~. We apply Adam~ with a batch size of 16, a learning rate of  and epochs of 5. To re-balance the weight of losses in  and , we regularize each term with the ratio of data in ,  and .

The ablation study on the contrastive coefficient  in  has shown that a too-small or too-large  is not suitable for the safety of LLMs. We uniformly set  to 0.5 and apply greedy search to generate all responses for better stability.

We use Harmless Rate (HLR) to assess the safety of LLMs' outputs:

Specifically, we prompt ChatGPT to check the safety of each response given the corresponding instruction to count the number of harmless responses. Following the settings of  and , the evaluate prompt is demonstrated in Tab.~. 

For the evaluation of the general ability of LLMs, we apply and adapt the evaluation prompt from ~ to assess the win rate and truthful rate. For the win rate (~) on AlpacaEval, we prompt ChatGPT to compare the output between the target model and  or .

For the truthful rate (~) on TruthfulQA, we prompt GPT-4 to assess whether the output of the target LLM aligns with world knowledge. To make sure that the assessment is accurate and objective enough, we add the ``Best Answer'' term of TruthfulQA dataset into the evaluation prompt as a reference.

Both prompts for the win-rate study and truthful study are shown in Tab.~.

The jailbreak prompts used in Section~ are shown in Tab.~. The metric HLR is the same as safety experiments, while the Attack Success Rate (ASR) means that the ratio of instructions with harmful responses after jailbreak but safe responses before jailbreak, as shown in~.