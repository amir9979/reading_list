%%%%%%%%%%%%%%

In this section, we first motivate the Hessian-based algorithm with a thought experiment. We then describe the algorithm, which produces a matrix  that characterizes the binding subspace. Finally, we describe how the binding subspace is extracted from , and how we use  to construct a binding similarity metric  that measures how bound any two activations  are.

Let us imagine there really is a binding subspace where two activations  and  are bound only if  is large for some low rank . Then, suppose we take two initially unbound activations  and  and perturb the first in direction  and the latter in direction . The binding strength should increase only if directions  and  align under . In fact, if there is a measure of binding strength  that is bilinear in , then  is precisely the second-derivative .

To instantiate this, we need a way of creating unbound activations, and also of measuring binding strength. To create unbound activations, we construct activations that are initially bound and subtract away the binding information so that the binding vectors become indistinguishable. Specifically, we consider a context with two propositions , and let  and  for  be the activations for  and  respectively. Prior work~ showed we can decompose the activation  into a content part  and a binding part , and likewise for  and . We follow the averaging procedure in prior work (explained later) to estimate differences between the binding vectors, . Then, to erase binding information, we add  and  to  and , and subtract the same quantities from  and . This moves the binding vectors to their midpoint and so should cause them to be indistinguishable from each other.

To measure binding strength, we append to the context a query that asks for the attribute bound to either  or . For example, if the attributes are the countries of residence of the entities, we would ask ``Which country does / live in?''. We measure the probability assigned to the correct answer ( and  respectively), and take the average over the two queries as the measure of binding strength , where  and  are the perturbations added to the unbound entity and attribute activations. Therefore, when all activations are unbound (), the model takes a random guess between the two attributes, and so . If  and  are aligned under , we expect .

Thus, in sum, our overall method first estimates the differences in binding vectors , uses this to erase binding information in a two-proposition context, and then looks at which directions would add the binding information back in. Specifically, we measure the binding strength  by computing the average probability of returning the correct attribute after erasing the binding information and perturbing the activations by , i.e. after the interventions

The matrix  is obtained as the second-derivative . For tractability, we parameterize  and  so that they are shared across layers, i.e.  instead of .

To obtain the low-rank binding subspace from , we take its singular value decomposition , where  is a diagonal matrix with decreasing entries. We expect the binding subspace to be the top -dimensional subspaces of  and , , for a relatively small value of .

%%%%%%% To turn the binding subspace  into a similarity metric between two activations  and , we take the activations  at a certain layer , project them into , and compute their inner product under the metric induced by :

We choose  for our models. We discuss this choice and other practical details in Appendix . 

 Following prior work~, to estimate  we take the average difference  across 200 contexts with different values of  and  with the hope that the content dependent vectors  cancel out (likewise for ).

In this section, we first show quantitatively that the Hessian-based algorithm provides a subspace that causally mediates binding, and that this subspace generalizes to contexts with three entities even though the Hessian was taken for two-entity contexts. We then qualitatively evaluate our binding subspace by plotting the binding similarity (Eq. ) for a few input contexts.

To evaluate the claim that the -dimensional subspace s causally mediates binding for entities, we perform an interchange intervention~ on this subspace at every layer. The idea is that if  indeed carries the binding information but not any of the content information, swapping the activations in this subspace between two entities  and  ought to switch the bound pairs. Specifically, we perform the interventions across all layers :

with . If  correctly captures binding information, then we expect the binding information to have swapped for  and .

In greater detail, we consider synthetic contexts with three entities and attributes, describing the propositions . For any pair , , we apply interchange interventions (Eq. , ) to swap binding information between  and . If the intervention succeeded, we expect  to now bind to  and  to bind to . We denote this intervention that swaps the binding between  and  as -.

To measure the success of the intervention -, we append a question to the context that asks which attribute  is bound to, and check if the probability assigned to the expected attribute  is the highest. We do the same for , as well as the last entity that we do not intervene on. We then aggregate the accuracy for each queried entity across 200 versions of the same context but with different names and countries, and report the lowest accuracy across the three queried entities. %

We apply the interchange intervention with different dimensions  for entities using , and for attributes using subspace  with analogous interventions (Eq. , ) on  and  instead (Fig.~). As a random baseline, we take the SVD of a random matrix instead of the Hessian. As an additional baseline, we implement implement Distributed Alignment Search (DAS)~. Specifically, we use gradient descent to find a fixed dimensional subspace that enables interchange interventions between the two entities in two-entity contexts, for various choices of subspace dimension.

As a skyline, we evaluate the two-dimensional subspace spanned by the differences in binding vectors of the three entities. We obtain these difference vectors similarly as  and  used in the computation of the Hessian: we take samples of  and  across 200 contexts with different values of entities, average across the samples, and take differences between the three resultant mean activations. We consider this a skyline because this subspace is obtained from three entities' binding vectors, whereas the Hessian and DAS are computed using contexts with only two entities. 

The results show that the top 50 dimensions of the Hessian (out of ) are enough to capture binding information. Moreover, despite being obtained from contexts with two entities, the subspace correctly identifies the direction of the third binding vector, in that it enables the swaps ``0-2'' and ``1-2''. In contrast, random subspaces of all dimensions fail to switch binding information without also switching content. Further, while DAS finds subspaces that successfully swaps the first two binding vectors, they do not enable swaps between the second and third binding vectors (1-2).  Interestingly, the top 50 dimensions of the Hessian outperforms the skyline for swapping between  and  (and  vs ). This could be due to the fact that the skyline subspace is obtained from differences in binding vectors, which could contain systematic errors that do not contribute to binding, or perhaps due to variability in binding vectors in individual inputs. We discuss details in Appendix~.

 We visualize the binding metric in various contexts by plotting pairwise binding similarities (Eq. ) between token activations. Specifically, on a set of activations , we compute the matrix . We show the similarity matrix for three contexts in Fig.~. We first evaluate an input context in which the entities are bound serially: the first sentence binds  to , and the second binds  to . To ensure that the binding subspace is picking up on binding, and not something spurious such as sentence boundaries, we evaluate an input context in which entities are bound in parallel: there is now only one sentence that binds  and  to  and  respectively. In both the serial context (Fig.~ left) and parallel context (Fig.~ middle), the activations are clustered based on which entity they refer to. In addition, we plot the similarity matrix for a context with three entities (Fig.~ right). Interestingly, the binding metric does not clearly discriminate between the second and third entities even though the interchange interventions showed that the binding subspace captures the difference in binding between them. This suggests that the 50-dimensional binding subspace obtained from the Hessian may either contain spurious non-binding directions or only incompletely captures the binding subspace. Thus, our current methods may be too noisy for contexts with more than two entities. In Appendix~, we use similar plots to show that coreferred entities share the same binding vectors.

% For every domain , we train a probe  that classifies the activation at each position  into either a value  in the domain or the null class . We use the activation at a particular layer  as the input of the probe. We describe later how  is chosen. The probe then has the type . 

We parameterize the probe with  vectors,  as well as a threshold . Each vector can be thought of as a direction in activation space corresponding to a value in the domain. The classification of the probe is simply the value whose vector has the highest dot product with the activation. When all vectors have a dot product smaller than the threshold, the probe classifies  instead. Formally,

To learn the vectors, we generate a dataset of activations and their corresponding values. Then, we set each vector to the mean of the activations with that input. Then, we subtract each vector with the average vector . This can be seen as a multi-class generalization of the so-called difference-in-means probes ~.

To generate this dataset, we use synthetic templates. However, this only provides context-level supervision: we know that the activations in the context collectively represent a certain value in the domain, but we do not know which activations represent the value and which represent . Thus, we have to assign the activations at each token position with a ground-truth label.

To do so, we use a Grad-CAM-style attribution technique ~ similar to that used by . Broadly speaking, we backpropagate through the model to estimate how much the activation at each layer/token position contributes towards the model's knowledge of the content. This identifies both the token position that carries value information and the layers that are the most informative.

The attribution results indicate that the middle layers at last token position are the most informative. We thus choose layer  (out of 40 layers). In Appendix  we discuss the attribution results further. In Sec.~ we evaluate the accuracy of domain probes on the  dataset on which they are trained, as well as the  and  datasets. 

With the binding similarity (Eq. ) obtained from the binding subspace, we can compose domain probes with a simple lookup algorithm. We first identify all the names mentioned in the context with the name domain probe . Then, for every other domain probe , we identify the values it picks up in the context, and for each of these values we look for the best matching name to compose together. The pseudocode is described in Alg. .

In this section we evaluate propositional probes on the three datasets, , , and . The constituents of propositional probes, domain probes and the binding subspace, are obtained from templated data, and we thus view  and  as evaluating out-of-domain generalization. We first evaluate each of the four domain probes in isolation, and then evaluate the propositional probe.

 We evaluate the exact-match accuracy of each of the four domain probes at predicting the correct set of values that are in the context for the three datasets (Fig.~ left). Each context contains  values, and domain probes have to get both right. Despite being trained only on a synthetic dataset, we find that domain probes generalize to the paraphrased dataset , and even the Spanish dataset .

 We evaluate the Jaccard index and exact-match accuracy between the ground-truth propositions and the set of propositions returned by the propositional probe (or prompting skyline). Specifically, for an input context, let the set of propositions returned by the propositional probe (or prompting) be , and the ground-truth set of propositions be . The exact-match accuracy is the fraction of contexts for which , and the Jaccard index is the average value of . Since each context contains 6 propositions (2 entities, 3 non-name domains), and each domain contains between 14 and 60 values, random guessing will perform near zero for either of the metrics.

We compare propositional probes against a prompting skyline that iteratively asks the model questions about the context. First, we ask for the number of names. Then, we ask for the names. For each name, we ask for the associated value for every non-name domain (e.g. ``What is the occupation of John?''), and select the value in the domain with the highest log probability. High performance at the prompting skyline validates the assumption that the model understands the context well enough to answer questions about it. In addition, we evaluate ablations of our propositional probes in which the Hessian binding metric is replaced with the dot-product following a projection into the 50-dimensional and 1-dimensional DAS subspaces (``DAS-50'' and ``DAS-1''), and a 50-dimensional random subspace (``random'') respectively.

We find that propositional probes do comparably with the prompting skyline in terms of Jaccard index for all three datasets (Fig.~ middle), but do significantly worse for terms of exact-match accuracy for . We also note that the accuracies of the domain probes are strict upper bounds on the accuracy of the propositional probe, which suggests a lot of the drop in accuracy in  could be due to the food probe generalizing less well to the Spanish  dataset.

In this section, we provide details of our implementation of Distributed Alignment Search. We base our implementation and hyperparameters on the pyvene~ library. We use the Adam optimizer~, with learning rate 0.001, with a linear schedule over 5 epochs (with the first 0.1 steps as warmup), over a dataset of 128 samples, with batch size 8. We optimize over a subspace parametrized to be orthogonal using Householder reflections as implemented in pytorch~. This subspace is shared across all layers. The loss we use is the log probability of returning the desired attribute after performing the interchange intervention. 

Both the Hessian and DAS are trained on templated datasets that draw from the names and countries domains. We partition each domain into a train and a test split, and construct train/test datasets by randomly populating the template described in Appendix~.