The difficulties of DMs for generating intricate details, especially realistic human faces and hands, are no longer novel~.  As shown in~, images generated by RV5.1~ and SDXL~ usually contain distorted faces.  As previously mentioned, the issue may originate from the scarcity of face data in model training or the fact that the face regions are relatively small compared to the whole image but need to contain intricate details.  In general, to generate images with human faces, it is a common practice to introduce negative prompts based on the classifier-free guidance (CFG) technique~ of DMs to increase the chances of generating high-quality faces.   displays results regarding this, where we see negative prompts indeed contribute to enhancing the face quality but the generated faces are still unsatisfactory.  Practitioners may perform DM-based inpainting~ to specifically re-generate the face regions, but the faces can still be low-quality due to the fundamental issue of the poor face generation capability of existing DMs. 

Next, we conduct a detailed manual evaluation of the face generation quality across three popular DMs: SD1.5~, RV5.1, and SDXL.  Specifically, we leverage the following pipeline for evaluation:

To determine the preference alignment of the five annotators and make the annotation more convincing, we compute the  frequency of more than three annotators among the five as signing the same label to the image to quantify the annotators' agreement and obtain 93.3\%. The integration further helps to mitigate the impact of individual biases and achieve alignment with population preferences.    presents an example of the annotated triplet and  displays the statistics of human preference over the three DMs.  As shown, although the face quality of RV5.1 is not good enough (see~), it still slightly surpasses the larger SDXL, which strengthens the concerns about the bad face issue of existing DMs.  On the other hand, SD1.5 falls significantly behind the other two DMs.

A good metric can enable the automatic, scalable evaluation of the face quality of the generations, avoiding expensive and time-consuming labeling processes by humans and paving the way for the development of new models.  We then investigate this---evaluating how well existing image-wise metrics are aligned with human preference on generated faces, based on the above annotated triplets. 

Concretely, we concern ImageReward (IR)~, Human Preference Score (HPS)~, Aesthetic Score Predictor (ASP)~, and Face Quality Assessment (FQA)~, which are prevalent for evaluating human preference or aesthetic quality in text-to-image generation. % Intuitively, HPS and IR concentrate on the global image instead of the local area, so they are not suitable for evaluating the quality of generated faces.  Thereby, we also develop variants of them, i.e., LocalHPS and LocalIR, where we detect the local face regions with a detector~ and send them into the original scoring pipeline with a default prompt ``A face'' for specific face evaluation.

We are majorly interested in the relative relationships of the metric evaluations on various images instead of the absolute numerical values.  Consequently, we build a small dataset containing roughly 3k annotated triplets, where each triplet forms two pairwise comparisons. For metric evaluation, we calculate the accuracy of the binary ranking based on the metric compared with the human ranking for the data pairs. We list the results in~.  We can observe that the performance of IR and ASP is unsatisfactory, perhaps due to their more attention on global image features, and LocalIR performs slightly better.  FQA is poor as well because it is applied to evaluate the suitability of the face images for recognition and hence can be biased for the assessment of human preference on generated faces. HPS and LocalHPS are the best among the metrics. % Nonetheless, the up to  accuracy still leaves considerable room for further improvement. 

Given the above findings, we aim to develop a new metric to quantify the quality/human preference of synthetic face images.  We dub such a metric as  and expect it to correlate with both the rationality and aesthetic appeal of face generations.  To achieve this, we construct a preference dataset on face images in an automatic and scalable way, based on which we perform model fine-tuning to obtain . 

Though open-source human preference datasets can be applied to training the evaluation models involved in the metric, they are not specifically for faces.  On the other hand, our collected human annotations are limited in amount because the labeling process is both costly and time-consuming, so the resultant data can be primarily used for evaluation instead of model training.  To address such issues, we propose a new collection pipeline for face preference data, based on the inpainting capacity of off-the-shelf pre-trained DMs. Specifically, we

We plot the procedure in the middle column of .  The underlying hypothesis behind this method is that the face quality of an inpainted image  is worse than that of the original one .  This can be easily fulfilled by controlling the noise factors involved in the inpainting pipeline, and we have empirically verified this (see ).  The above pipeline eventually produces a dataset  of 375k  pairs based on 197k natural images. 

We then would like to learn a scorer  to fit the preference dataset .% Drawn inspiration from the modeling of human preference over the aesthetic appeal of generated images~, we utilize a naive ranking loss to tune .  Specifically, given a random mini-batch  from , we minimize the following loss:

where  denotes the sigmoid function.  Other possible learning principles are left as future work. 

Considering the prevalence of IR and the improved capacity of BLIP architecture~ over conventional CLIP~ for modeling human preference~, we adopt IR to initialize our scorer  and then perform fine-tuning to avoid the cold start problem.  Noting that we only care about the quality of faces rather than the properties of the whole image, we detect faces in the image, as done in LocalIR, and tune the model on only the face regions. The prompt is set to ``A face'' by default. We freeze the first 70\% layers of the transformer~ backbone and train with a learning rate of 1e-5 and a batch size of 64 on a single 80G A800 GPU. 

We first report the accuracy of  to rank human-annotated images in~. We see  gets the best accuracy compared to existing metrics, so it can serve as a better metric for evaluating faces in synthetic images.  In~, we illustrate some randomly selected face images and the corresponding s, which implies that rationality and aesthetic appeal of faces are positively correlated with .  To show the generalization of evaluating face quality, we also report  in Tab.~ for different open-source and close-source text-to-image diffusion models, which generate images in the same manner as the test set. We observe PGV2.5 gets the best FS. It makes sense since its authors claim that the image quality is better than MJ's. We also conclude that the face generation quality and the overall generation quality of the models are positively correlated.

We also calculate the detailed statistics of   in BLIP architecture, compared with LocalHPS in CLIP architecture, and FQA, for the images in the human-annotated dataset.  We normalize the output scores to  for each metric and report the statistics corresponding to images from various DMs individually in .  As shown, the median of FQA fails to accurately represent the quality of faces. Conversely, the median of  can better align with the average score given by humans. Moreover, it can showcase the subtle differences between RV5.1 and SDXL, and indicate the much weaker performance of SD1.5. Besides, compared with LocalHPS,  exhibits a larger variance, implying its stronger ability to differentiate the quality of face images. 

We conduct experiments on SD1.5 and RV5.1 and only leverage the guidance loss. Since RV5.1 is a fine-tuned version of SD1.5 and generates better images especially faces than SD1.5, we set the learning rate 1e-5 for SD1.5 and 1e-6 for RV5.1. The models are fine-tuned on 4 80GB NVIDIA A800 GPUs, with batch size of 2 and gradient accumulation of 8. During training, we set  dropping of the text-conditioning.  We leverage two distance metric measures, one of which is  distance, and the other one is DINO~ feature distance.

For the evaluation of general capability, we leverage  MJHQ-30K dataset~ and sample 5k prompts from each category equally and report CLIP-Score, IR and HPS for comparisons. Since there are no fine-tuning methods specifically for faces, we only take the base model and DPO~ as baselines. To evaluate face quality, we sample images given prompts from the evaluation dataset and report the mean of  for each DM. We use PNDM~ noise scheduler and default CFG scale of  and steps of  for inference.

We present the quantitative comparisons in~. As demonstrated, for SD1.5, both ours-  and ours-DINO outperform the base model~ concerning all of the four metrics by a large margin, indicating that the proposed guidance loss is capable of enhancing not only overall image quality but also generated face quality. Compared with DPO methods, we observe slightly lower CLIP-Score and HPS in (ours-, potentially due to the utilization of global optimization loss in DPO. However, in terms of face quality, our methods achieve the higher , which showcases the better generation ability of faces. We emphasize that our work focuses on enhancing the quality of face generation rather than improving the overall quality of the images. For RV5.1, compared to the base model, our methods obtain better IR and HPS and comparable CLIP scores, significantly upgrading . This result again demonstrates our methods can simultaneously improve general generative capability and synthetic face quality. We also visualize some qualitative results to show the superiority of our methods in the improvement of face quality in~. We can observe that compared with baselines, the improvement in face quality is remarkable, in terms of rationality, clearness, and aesthetic appeal.

We discussed before that timesteps and self-attention layers are crucial for detail generation as well as face generation. We conduct the following ablation study with RV5.1 and DINO feature distance metric measure.

To demonstrate the effectiveness of selectively choosing from relatively small timesteps for fine-tuning, we additionally fine-tune DMs under the condition of randomly selecting from all the timesteps while keeping others the same. From~, we can see that in face quality comparison, considering only the relatively small timesteps is more effective. The reason is that as the timestep decreases and approaches zero, the generation process of finer details begins, and focusing on them helps better detail generation as well as face generation. More ablation studies about timesteps will be included in the supplementary materials.

Theoretically, self-attention layers have a greater impact on face generation. To prove it experimentally, we compare Ours-DINO with full fine-tuning. From~, we can see that Ours-DINO with self-attention fine-tuning outperforms full fine-tuning, proving that self-attention takes a significant role in generating details. We believe that self-attention not only enhances image quality but also facilitates the generation of finer details.

We leverage the face pairs for fine-tuning DMs.  A corresponding mask  indicates the location of the face in the image while the prompt  provides a description of the content of the image. The fine-tuning algorithm is presented in the~. We provide more results in~.

In the paper, we conduct ablation studies on timesteps. Here we provide more detailed ablation studies on them. To explore the optimal strategy for achieving better face generation or detail generation, we consider different timestep splitting points  and take 250 as a unit. Specifically, we choose from timestep 0 to , i.e., taking values from , , , and .  The experiments take RV5.1 as the base model and DINO feature distance as the measure. Except for the , we keep other settings unchanged and take the same comparison settings.

As shown in~, as  increases, the detail generation performance, which we focus on and is indicated by , exhibits an initial increase followed by a continuous decrease, and  is optimal, so we adopt it as the default setting in other experiments. The reason why the detail generation is better when  compared to when  may be attributed to the fact that  includes the interval of . As for the other metrics, their focus is primarily on the overall aesthetic quality, which is not within our scope of concern, rather than the specific face details.

To provide a more intuitive understanding of the dataset, we present some details about the process of dataset formation and extract a subset of samples to help readers gain a preliminary understanding of the format and quality of the dataset, and better comprehend the methods and results presented in our paper.

We establish the following annotation rules for human annotators:

We also provide more image triplets in~ to see the correlation between face quality and human preference.

In the inpainting pipeline of DMs, the noise factor is a hyper-parameter, which should be chosen carefully. We intend to degrade the face to resemble the quality of a bad face generated by DMs. Experimentally, we find that an increased noise factor leads to more pronounced degradation, and different sizes of faces exhibit varying levels of resistance to the degradation effects. To this end, we choose various noise factors based on the proportion of the face in the image as shown in~. We go through multiple rounds of data filtering, including the removal of faces that are too small and blurry, as well as other invalid faces. We present more examples in the implicit human preference dataset of faces in~.

We observe a positive correlation between the quality of generated faces and the aesthetic quality of their corresponding images, which is influenced by the model's capabilities. This correlation can be explained by the fact that stronger models exhibit a higher capacity for generating finer details and explains why IR and HPS can perform better than random guesses on predicting human preference on faces, though they are designed and trained on the whole image and focus on the global features.

We provide additional examples in~. It can be seen that when  is higher than about 4, the generated faces are generally plausible. Moreover, the higher the score, the more attractive the faces tend to be, albeit in different ways.  Conversely, when the score falls below 4 and continues to decline, the faces are no longer satisfactory or realistic. At first, the faces may appear slightly blurry and exhibit some distortion in certain details. Eventually, significant blurriness causes the faces to become unrecognizable. Based on the relationship between the  and the quality of face generation, we can take it as a reliable automated evaluation metric for assessing the quality of face generation.

Fine-tuning Diffusion Models     for Enhancing Face Quality in Text-to-image GenerationFine-tuning Diffusion Models for Enhancing Face QualityZhenyi Liao\inst \and Qingsong Xie\inst \and Chen Chen\inst \and Haonan Lu\inst \and Zhijie Deng\inst Liao et al.Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University \and OPPO AI Center\\ \email \thefootnote*Corresponding authors. Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions.  However, they are likely to fall short when it comes to real-life scenarios with intricate details. The low-quality, unrealistic human faces in text-to-image generation are one of the most prominent issues, hindering the wide application of DMs in practice.  Targeting addressing such an issue, we first assess the face quality of generations from popular pre-trained DMs with the aid of human annotators and then evaluate the alignment between existing metrics such as ImageReward~, Human Preference Score~, Aesthetic Score Predictor~, and Face Quality Assessment~, with human judgments.  Observing that existing metrics can be unsatisfactory for quantifying face quality, we develop a novel metric named  by fine-tuning ImageReward on a dataset of (good, bad) face pairs cheaply crafted by an inpainting pipeline of DMs.  Extensive studies reveal that  enjoys a superior alignment with humans.  On the other hand,  opens up the door for refining DMs for better face generation. To achieve this, we incorporate a guidance loss on the denoising trajectories of the aforementioned face pairs for fine-tuning pre-trained DMs such as Stable Diffusion V1.5~ and Realistic Vision V5.1~.  Intuitively, such a loss pushes the trajectory of bad faces toward that of good ones.  Comprehensive experiments  verify the efficacy of our approach for improving face quality while preserving general capability.  We will release source codes and datasets upon acceptance of the paper.

xu2023imagerewardwu2023humanASPfqaFace Score (FS)FSFSrombach2022highrv5.1Diffusion models \and Text-to-image generation \and Face qualityIntroductionsec:introho2020denoising,nichol2021improved,song2020scorekong2020diffwave,chen2020wavegradblattmann2023stable,ho2022imagen,gupta2023photorealisticlugmayr2022repaint,avrahami2022blended,avrahami2023blendedrombach2022high,podell2023sdxlmjnichol2021glide,saharia2022photorealistic,ramesh2022hierarchicalfig:bad facerombach2022highrv5.1podell2023sdxlxu2023imagerewardwu2023humanASPfqaFace Score (FS)FSFSWe perform the first investigation of the bad face issue of DMs and systematically assess a range of image quality metrics for quantifying face quality.      We propose  to better quantify the quality of generated faces, which surpasses existing metrics with a decent margin.      Face ScoreWe propose a guidance loss to fine-tune DMs for generating higher-quality faces and verify its efficacy on both SD1.5 and RV5.1.  Related WorksText-to-image diffusion models.rombach2022high,nichol2021glide,saharia2022photorealistic,ramesh2022hierarchicalDiffusion model fine-tuning and evaluation.zhang2023addingruiz2023dreambooth,hu2021lorabrooks2023instructpix2pixshen2023finetuningkirstain2023pickwu2023humanxu2023imagereward,clark2023directlyblack2023training,fan2023dpokrafailov2023directwallace2023diffusionwu2023humanxu2023imagerewardradford2021learningli2022blipDetail generation.podell2023sdxllu2023handrefinerzhang2023addingzhang2023detectingPreliminaryho2020denoising,song2019generative     q(x_t|x_{t-1}) = (x_t;x_{t-1},\beta_t I),t = 1,\dots,T,

    p_\theta(x_{t-1}|x_t) = (x_{t-1};\mu_\theta(x_t,t),\beta_t I),t = T,\dots,1, ho2020denoisingho2020denoisingronneberger2015usong2020scoresong2020denoising     x_{t-1} = _{t-1}}_{0}+_{t-1}}\epsilon_\theta(x_t,t),t = T,\dots,1,     

     = }\epsilon_\theta(x_t,t)}{}}.     rombach2022highesser2021tamingHuman Preference on Generated Face ImagesFace Score (FS)The Bad Face Issuepodell2023sdxlfig:bad facerv5.1podell2023sdxlho2022classifierfig:negavrahami2023blended,rombach2022highEvaluation of Existing DMsrombach2022highselect 1k prompts related to human subjects in the MS-COCO 2017 5K validation dataset~, which includes descriptions of human-centric in scenes and ,single-person scenarios;     lin2014microsoftfor each prompt, generate a triplet of images (see Fig.~ for an example) with the three DMs (the triplet is discarded if there are no valid faces in any image);     fig:facerankintroduce five human annotators to individually rank the triplet of each prompt based on face quality;     the best image in the triplet receives a score of 3 and the worst receives a score of 1;      integrate the annotation results based on majority voting. fig:faceranktab:annotationfig:negEvaluation of Existing Metricsxu2023imagerewardwu2023humanASPfqadeng2020retinafacetab:AccFace Score: a New Metric for Synthetic Face ImagesFace Score (FS)FSDataset construction.for natural images containing human faces in the LAION dataset~, detect the face regions using detectors~, obtaining face masks ;     schuhmann2022laiondeng2020retinafacemask out and inpaint the face regions with an inpainting pipeline~. rombach2022high,inpaintfig:pipelinefig:facepairRanking loss.We can also input the text prompt corresponding to the image  to the scorer, but omit it here for simplicity. xu2023imagereward     L_{rank}(\phi) = -{||}\sum_{(x^{g},x^{b})\in }[\log(\sigma(s_\phi(x^{g})-s_\phi(x^{b}))], Fine-tuning IR.li2022blipradford2021learningxu2023imagerewardvaswani2017attentionQuantitative comparisons.FStab:AccFStab:scoresFSFSFSmore modelsFStab:globalFSFSImproving Face Quality by Fine-tuning DMssong2020denoisingeq:ddim,eq:predicted x0     _0^{b} = }\epsilon_\theta(z_t^b,t)}{}}, \quad _0^{g} = }\epsilon_{\theta^*}(z_t^g,t)}{}}. 

         L_{guidance}(\theta) = d(m\circ_0^{g},m\circ_0^{b}), caron2021emerging      We visualize the evolvement of face images in the sampling process in , and notice that in the early stage of sampling, the layout and colors are rendered while the details have not yet emerged.      As the sampling process progresses, the details gradually recover and elaborate.      This enlightens us to keep the forward timestep  relatively small to avoid wasting optimization efforts on too noisy states.      From~, we can observe that the emergence of finer details occurs approximately midway through the inference process, so empirically we take  for fine-tuning DMs, where  is the uniform distribution over the interval .     Timesteps.fig:timefig:time Intuitively, the cross-attention layers in DMs capture the correlation between images and texts, while the self-attention ones cope with the interdependencies within the image itself.      With these, we particularly optimize the parameters of the self-attention layers in the U-Net of DMs for face quality enhancement. Self-attention layers.ExperimentsExperiment SettingsTraining setting.caron2021emergingEvaluation Settings.MJHQ-30Kwallace2023diffusionFSliu2021pseudo-10ptResults and Analysistab:mainrombach2022highFace ScoreFSfig:qualitative_comparisonAblationTimesteps.tab:ablationSelf-attention layers.tab:ablationConclusionFace ScoreAppendixAlgorithmalgo:ft_dmfig:more resultsFine-tune DMs by Face Pairsalgo:ft_dm: Dataset , the original DM , \\the target DM , the distance measure function           : The noise scheduler, the number of noise scheduler timestep            // Add noise to images               // Predict                // Predict               Update  based on           The fine-tuned DM      InputInitializationeach  in no grad:with grad:More Ablation Studies on Timestepstab:ablation timeFace ScoreDatasetsRanking Criteria for EvaluationDiscard those triplets if there are no valid faces in any image;     Focus solely on the faces and do not need to consider the alignment between the prompt and the image, the aesthetic aspect of the image itself, or any other irrelevant factors;     Prioritize the rationality of the face before considering its aesthetic aspect.     Select the most frontal and representative face for comparison purposes in multi-person scenes. fig:evaluation datasetFace Pairs Constructiontab:noise factorfig: dsFace Generation QualityEvaluation Discussion\textit Discussionfig:more fsFace Scoresplncs04main