While a variety of MKGs exist in English~, few or none are available in other languages. There are different possibilities for MGK applications; for example, a line of work utilizes graph embeddings for various medical tasks like recommendation systems~, NLI~, and diagnosis prediction~. BioLORD~ uses concepts and relationships from the knowledge graph as part of the LLM pre-training. Another approach for MKG utilization involves enriching the generation process with information extracted from these graphs. This strategy can be viewed as a specialized application of the retrieval-augmented generation framework~, demonstrating the potential to produce more specific, diverse, and factually accurate language. However, applying such techniques in the medical domain is still an area that has not been extensively explored.

%han2023medalpaca LLMs are increasingly utilized in the medical domain; they are primarily implemented for English~ and Chinese~, evaluated for medical QA tasks, and used as medical chatbots. There is also a research direction that focuses on synthetic data generation.  trained the GPT-3 model from scratch using clinical and general English texts, then produced 20B of medical texts utilizing this model and introduced a smaller version of the model on synthetic data only. The resulting model outperforms ClinicalBERT~ and the same model trained on actual data on MedNLI~ and emrQA~ benchmarks. The authors of~ generated clinical texts and manually annotated them for the Named Entity Recognition (NER) task. The evaluation shows that the combination of original and synthetic corpora achieved better performance than using only the initial corpus. In~, the authors improve performance on NER and relation extraction tasks with synthetic data, showing that increasing the number of synthetic sentences can improve model performance up to a certain point, beyond which the improvement becomes marginal. In a recent study~, researchers explored the feasibility of using synthetic text as a training corpus for clinical NER in French. The findings suggest that synthetic clinical notes can be used to train NER models, although applications for other tasks remain to be explored.

The true potential of synthetic data in the medical field remains under active exploration~. However, typical problems related to LLMs, like hallucinations, pose substantial challenges in such a critical field. Ensuring factual accuracy and addressing inconsistencies in medical models remain valuable concerns~. In our research, we strive to bridge the gap in controllable medical data generation, primarily focusing on the Russian language, which is heavily underrepresented in linguistic medical resources.

As mentioned in Section~, Russian-language equivalents of MKG are scarce. For our research, we used the WikiMed database as a foundation to develop the Russian MKG.

The constructed MKG includes the following nodes: diseases (identified by ICD-10 codes), drugs, and symptoms. While diseases and drugs have predefined relations in this database, symptoms and their relations are not specified. The database includes clinical manifestations, which contain potential symptoms in a narrative format. To extract these symptoms, we utilized ChatGPT~, prompting it to identify symptoms from the given text of clinical manifestations. For example, the clinical manifestation of tuberculosis, , should lead to the extraction of symptoms . The extracted data were manually verified by comparing them with the initial text to ensure that only symptoms were included, and no irrelevant information or noise was extracted.

Finally extracted symptoms were then incorporated into the MKG. Its statistical details are presented in Table~.

We collected a dataset of 152k Russian language samples focused on instruction-following for supervised fine-tuning. These samples were derived from various medical benchmarks, databases, and the constructed MKG. Utilizing the MKG, we created questions that require multiple levels of reasoning, ranging from simple 1-hop to complex 3-hop distances. For example, a 1-hop reasoning question like  directly connects diseases to symptoms (Di-S). A 2-hop question, such as , involves linking symptoms to diseases and then to drugs (S-Di-Dr). A more complex 3-hop reasoning question, like , maps diseases to symptoms, then to another disease, and finally to drugs (Di-S-Di-Dr), as shown in Fig.~.  We avoid more than three hops reasoning scenarios as, by our estimate, it produces too vague and error-prone samples. For the clinical notes, we employed two types of tasks: continuation, which extends an existing note from a random point, and generation, where a note is created from prior data like symptoms. We generated at least five different rephrasings for each to ensure instruction diversity.

In addition to real medical data, we also incorporate synthetic data from ChatGPT. Considering that real clinical notes often have many typos and stylistic variations, which may affect model performance, we suggest that adding synthetic notes could improve the model's text generation and be a regularization method. To create this synthetic data, we prompted ChatGPT to generate clinical notes based on patient symptoms, age, and gender. For part of the data, style references with real samples were additionally provided. We also incorporate a medical dataset focused on typo correction to make the model more robust to typos. The structure of the dataset is represented in Fig.~. 

Unlike the English language, to our knowledge, there aren't any open-source generative LLMs tailored for the medical domain in Russian. Thus, we employ GPT-4~ for data generation to establish a strong baseline. 

Our work uses a model based on the LLaMA 2 family~. It is a collection of open generative language models with a parameter range from 7 to 70 billion. We fine-tuned the model with 7 billion parameters using a learning rate  and a cosine learning rate scheduler to fine-tune the model. We utilized a global batch size of 256 and trained the model for three epochs.

To enhance the efficiency and accelerate the training of our model, we employed Low-Rank Adaptation (LoRA)~. This method involves freezing the model's weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture.

The pre-training data for LLaMa-7b consists of 90\% English-language data and only 0.13\% Russian-language data. Therefore, to fine-tune our model, we decided to use the pre-trained checkpoint from Saiga 2 that is fine-tuned on Russian language instructions and dialogues generated by GPT-4.

We prepared a generation task to generate synthetic clinical notes with real data examples and symptoms spanning 105 ICD-10 category codes, as presented in the RuMedTop3 dataset~. We sample symptoms previously extracted from Russian MKG (Section~) according to the approach outlined in Section~.

We aim to achieve a uniform distribution of ICD codes for the generation task, but the lack of data requires inevitable trade-offs. Given the limited set of examples (1,283 samples), and to ensure that the sampling procedure represents the diversity of examples and symptoms, we have adopted a specific approach to determine the frequency of each ICD-10 category code  and computes its weight based on the following rule:

where  denotes the triple application of the function ,  refers to the number of examples corresponding to a given category code , and  represents the number of all unique symptoms within that category. The logarithmic scale used in Eq.~ is implemented to achieve a more uniform distribution of codes.

An exception to this weighting procedure is the category , defined as . As this category does not hold particular interest for downstream tasks, we set the number of generations for this category code to 10, thereby not factoring its weight into the overall distribution. We obtained the final generation task by sampling clinical notes and symptoms for this distribution, containing 2,503 entries. Each entry consists of an ICD-10 code, an example of a real clinical note, and a subset of symptoms.

For the baseline, we generate samples that do not utilize data from MKG in their prompts. The baseline prompt is similar to the original one but contains only the disease name instead of incorporating disease prior information from MKG and a clinical note example. Generated and real clinical notes contain no ICD codes in the text to avoid data leaks.

The actual distribution of symptoms in clinical settings is complex. For example, certain symptoms may not coexist or be specific to a particular age or gender. In this study, however, we assume that symptoms are independently and identically distributed. Consequently, we select multiple symptoms for a disease without considering their inter-relationships. We randomly sample several symptoms from the MKG (Section~) related to a disease, with the count ranging from 1 to 5, which is also chosen randomly.

We have released a dataset of 41,185 synthetic clinical notes in Russian, generated using GPT and fine-tuned LLaMA models spanning 219 ICD-10 codes. The dataset includes all generated samples, regardless of quality, to facilitate various data selection methods. More detailed statistics and descriptions of the data fields are provided in the project dataset repository. According to the provided licenses, all confidential information was anonymized, and researchers can safely use these datasets.

In this research, we utilized the RuMedPrime dataset~, containing 7,625 anonymized entries from outpatient visits to the Siberian State Medical University hospital. This dataset, unique as the only open-source collection of clinical notes in Russian annotated with ICD-10 codes, comprises each patient's clinical note, symptoms, and corresponding ICD code. Based on this dataset the RuMedTop3 task was created, focusing on the ICD code prediction from a free-text clinical note. Given such a task, it is possible to implement an AI service that supports doctors with a second opinion on the diagnosis search.

Our study adopted the same dataset split as RuMedTop3, using 4,690 records for training, 848 for validation, and 822 for testing while incorporating full clinical notes alongside symptoms. Like RuMedTop3, we employ the second ICD-10 classification code hierarchy level. We also evaluated the results on the original RuMedTop3 dataset.

We conducted experiments using both feature-based linear models and transformer models. For the linear model, we employed logistic regression based on term frequency-inverse document frequency (TF-IDF) features. For the transformer models, we run experiments with RuBERT~ and RuBioRoBERTa~ and report the average results from three runs.

ICD code prediction is a multi-class classification task. To evaluate it, we utilize the  score (), defined as follows:

where  is the number of samples and  is 1 if ground truth ICD code  is on a ranked list of  predicted codes  and 0 otherwise. 

We use the BERT-score~ to measure the similarity of synthetic data to the examples and to the provided symptoms (Fig.~).

As can be seen from the higher scores, the GPT-4 model follows instructions more precisely, produces results that are more similar to the example, and makes greater use of the provided symptoms.

While high similarity to the example is desirable, complete replication is unfavorable. To evaluate replication, we calculate the ratio of example N-grams usage, defined as the ratio of unique common N-grams between the generated sample and the example, divided by the number of unique N-grams in the example (Fig.~). For most samples, the N-grams usage ratio is less than 1, suggesting that the examples are far from complete replication in the answer.

One of the most exciting yet practically challenging scenarios involves generating data scarcely present in the original training set or generation of clinically valuable data. We selected two vital ICD codes for the experiment, K81 and I11. The first is cholecystitis, which affects about 20\% of the adult population. The second code denotes a type of heart disease, one of the most common causes of death.

We transferred all real data samples to the test set, making evaluating the experiments with real data in the training set impossible. However, we prioritize a diverse test set in this experiment as it could mitigate the potential poor performance of unrepresented synthetic samples in downstream tasks. We replaced the real data in the training set with 30 synthetic samples for both models and added 59 samples for LLaMA-7b to assess the impact of scaling the number of samples (Fig.~).

Although models trained with such synthetic data still have zero scores in the  metric, they show promising results in less restricted metrics like , demonstrating the potential for further improvement in real data absence scenarios. Thus, synthetic data with specific refinements could increasingly become a viable alternative for training models in data-scarce environments.

Another application for synthetic data is data upsampling. In this experiment, we used the same synthetic data as in the previous section (Section~) and added it to the training set. The results indicate that models can benefit from such synthetic data. For instance, the accuracy of K81 code prediction improves by 17.8\% for the RuBioRoBERTa model (Fig.~). To assess the overall accuracy across all ICD codes, we also evaluate both the baseline and the full prompts (Table~).

For a more detailed analysis, we focused on two codes that were frequently mistaken for each other more than any other pair. This decision was based on the confusion matrix, which measures how often each pair of codes is confused. The analysis revealed that the codes most often confused are M54 and G54.

We selected synthetic data for those codes generated via the same generation task described in Section~ for the GPT-4 and LLaMA-7b models. For the LLaMA, we repeated the generation several times to evaluate the effect of data scaling. Here, we only report on the linear model to depict simultaneous changes for codes not averaged across several models. The experimental results are presented in Table~. While data generated by GPT-4 provides improvements for both codes simultaneously, data generated by LLaMA still offers improvement for one of the codes without a drop for the other.

Although the generated clinical notes contain more information than the data in the RuMedTop3 task, which focuses on symptoms, using the generated data to upsample this dataset is still feasible, as they share the same set of ICD codes. We report results with generated data upsampling in Table~, showing that all models benefit from the synthetic data. %({Russian}Section~)  We performed the human evaluation in a side-by-side scenario to qualitatively assess the synthetic clinical texts. First, we randomly sampled 105 cases from real clinical notes examples according to the general ICD code distribution and paired them with synthetic ones. Second, in each pair, we selected random sentences (with a median number of words of 8) to facilitate labeling and make a fair comparison detached from the notes structure. Such text pairs were presented to a medical intern with the only question --  The assessor was correct in 58.09\% (61 cases). Given that the random guessing is 50\%, we can conclude that our synthetic texts have acceptable quality. In further research, we plan to evaluate the MedSyn framework in more elaborate human assessment scenarios.