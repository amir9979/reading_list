Influnce functions, a subset of Data Attribution methods, seek to measure the effect of a given training point/s on a trained model. For a given training point, they seek to capture the change in behavior of a trained statistical model had that single training point not been part of the training dataset (leave-one-out-retraining). It outputs a value, called the influence value, for each training point in question.  For further discussion on Data Attribution and Influence Functions in machine learning, we refer to .

    % For deep learning models, given the extreme computational cost of leave-one-out-retraining, various gradient-based approaches have been proposed that avoids model retraining. The main challenge in these gradient-based approaches is the calculation of the inverse Hessian matrix.  A large body of work focuses on either estimating this inversion, or approximating the influence value without the Hessian.         The current trend of scaling LLMs to an order of billions of parameters, as well as leveraging huge amounts of training data, pose additional computational challenges for existing influence function methods . Recent works seek to adapt data attribution for these kinds of models: TRAK  uses an "influence function-style" estimation that simplifies the Hessian matrix, while DataInf  leverages LoRA  to calculate influence values for fine-tuned LLMs. These methods make data attribution tractable for modern LLMs. In our study, we use DataInf because our focus is on the fine-tuning phase of training.

        Coreset selection aims to select a subset of the full training data, such that a model well-fitted to the coreset also fits to the full data. One of the first to study it for big data viewed it as a compression method for large datasets.   In machine learning,  introduced a coreset algorithm to boost iterative gradient-based training. Coreset selection has been recently explored for LLMs, given the obvious problem of expensive training  or to understand model properties such as in-context learning . Another line of work such as  and  apply coreset selection for selecting the pretraining data of language models. Our work differs by focusing on the fine-tuning data, which is relevant to practitioners and to the open-source community.

        Our work is closest to  in that they use influence functions to rank data in their selection algorithms.  on the pretraining data, while ours focus on the fine-tuning data.  apply it for vision tasks with clearly-defined evaluation metrics (e.g. image classification) for which they can specify an objective function to achieve that metric, whereas instruction-following does not have a clear evaluation metric and remains an open-problem in the field. We use the perplexity as a coarse proxy to capture what it means to follow instructions effectively.  also uses a definition of influence to select coresets for instruction-tuning. However, we further show that influence functions can be used to measure how much a trained model's coverage on unseen test samples. Additionally, earlier work  used influence values on new, but similar training data to further fine-tune a tuned model.

    Following the formulations of , an influence function measures the impact of a given training point on the change in model parameters for a given validation point. It measures this impact by up-weighing this training point and measuring the rate of change in the model parameters. 

    In the context of machine learning, given a model  parameterized by the empirical risk minimizer , we consider how a given training point changes the validation loss. Given a training point  and a validation point , the influence of  on 's performance on  is defined as

    where  

    and , the Hessian of the empirical loss.

    Equation ~ can be extended to an entire validation set  by taking the average gradient of the validation set as

    The influence value and its sign represents the impact of including a particular point in the training set on a given validation point/set, via the validation loss. For clarity, we follow  in using their terminologies. We define

    For current language models, which rely on transformer-based models with billions of parameters, computing the Hessian term in Equation  is extremely expensive. In practice, we rely on the Hessian's estimation. In this paper, we use the estimation of influence from DataInf by .

.       %Algorithm: %Reducing the number of Layers to calculate the Gradient     While the DataInf algorithm efficiently calculates the gradients of the model parameters for each point, in practice the CPU memory consumption from the gradient collection becomes a bottleneck as the size of the training dataset grows. This is a problem for sizeable fine-tuning datasets such as the Ultrachat subset we use, which contains roughly 50,000 training points. Figure  shows the trend for N=250 points.      % figure of # of layers used x memory % used

    We found that selecting the number of layers is non-trivial. To illustrate this, let  denote the case where we use every layer with a LoRA adapter. First, using large  can result in marginal gains at approximating influence values from all-layer at the expense of the memory budget. Second, different model architectures give different efficiency, implying  is different for each model (and dataset). These two reasons require a hyperparameter search for .

    To enable comparisons across different models given the same dataset, we define the metric Memory Efficiency, , where  is Spearman's rank correlation coefficient of a particular setup with respect to the case where all layers are used to calculate the gradient ().  is a metric of interest because it describes how faithful the ranking of the current setup is to the ranking in the all-layer setup. Note that  layers refer to the first  layers because the first layers capture influence better than the last layers . We then simply set  as the number of layers with the highest , subject to our virtual memory budget. For our experiments, we used  for Gemma-2B and Mistral-7B, respectively. Practitioners should perform this preliminary evaluation of memory efficiency on a small subset, in order to avoid out-of-memory (OOM) errors when calculating the influence values.

Given a large dataset , evaluation set , and reference model  fine-tuned on , to select a coreset , where  

The first step is the most expensive step as the influence function algorithm visits each point. In particular, it is necessary to get the model gradients for each point, which creates a memory bottleneck. In Section , we improved the influence function algorithm we use to mitigate this.

The second step provides an ordered list of the training data with respect to each points' influence value. For the third step, note that  is a hyperparameter and depends on the training budget of how many data points the training can accommodate. Given the definition of proponents, the "top" proponents are the points with most negative influence values. %We decided that the model trainer should specify the choice of dataset size (i.e.  )as opposed to what score they want the model to achieve under a specific metric due to the varied possibilities of evaluating instruction-tuned LLMs, where one metric is usually insufficient to capture its holistic capabilities. For coreset selection, we use Mistral-7B-v0.1  Gemma-2B  as base models and fine-tune them on subsets of a "full" training dataset, which is a random 50k subset of the first round of dialogues from Ultrachat-200k . Note that the models we use are allowed for research. We evaluate the fine-tuned models on a disjoint 250-random subset from Ultrachat-200k, following the same format as the training dataset. For both Sections  and , we use perplexity  on the evaluation set to measure model performance, given that it is directly related to the loss, whose reduction is the goal of fine-tuning (training loss) and is a component to the calculation of  influence values (validation loss).

For simplicity, we refer to mean perplexity of the model on the validation dataset as perplexity and BERT Score between each validation point and the training dataset as similarity.

. Using Equation ~, we calculate the influence values with respect to the entire evaluation dataset. Figure  illustrates our algorithm's performance at coreset selection. The x-axis represents the different selection strategies based on their influence values. Minimum refers to selecting points whose influence values are nearest to zero in absolute terms. Random refers to uniformly-sampled points. In all figures, selecting Proponents consistently leads to the lowest perplexity among the other subset selection strategies. This applies when, with respect to the base models being fine-tuned, the reference model is smaller (as in the case with Gemma-2B or it is of a different architecture with a different pretraining data (as with Phi-2 pretrained on synthetic Textbook-quality data ). Importantly, selecting the best k proponents, which is half of the full dataset, leads to a model that has lower perplexity than a model trained on the full dataset. This shows that it is possible to achieve better model performance with less training data.

Interestingly, the behavior of Minimum and Opponents are similar. In some cases selecting Minimum leads to a model with higher perplexity than Opponents. The similarity in behavior is explained by the distribution of influence values, which is left-skewed as shown in Figure . Thus, there is a significant overlap between the points selected for Minimum and those for Opponents. For how Minimum can lead to higher perplexity than Opponents, it may indicate that points with the least absolute influence changes the model's behavior the least compared to other strategies. In other words, it best retains the model's behavior before fine-tuning, which in our experiments is a base model that is trained to produce verbose output. A high perplexity here is then explained by the Minimum model producing longer text than the other models.

.  We further evaluate the two models of interest: Full (trained on the full dataset) and Proponents-25k (trained on the best 25k proponents) on the Massive Multitask Language Understanding (MMLU). MMLU tests language models for extensive world knowledge and problem solving capabilities in multiple-choice format ~. We use the Gemma-2B models fine-tuned on the Ultrachat subset, which are the same models in the previous experiments. Table  reports the average accuracy across the different subjects as the fine-tuning was intended to improve general instruction-following.

Proponents-25k surprisingly performs similarly to Full for a wide range of tasks, and attains a similar accuracy as Full. There may be large differences in some subjects (around ), but there is similar performance in capabilities which those subjects are components of. For example, for , which is composed of subjects such as \{, Full performs better in the first two subjects, but Proponents-25k performs better for the last subject. We postulate this is because our evaluation set represents samples for general capabilities. We expect In2Core can be used to improve specific capabilities if the evaluation set's distribution matches those capabilities. While it performs slightly worse, Proponents-25k being trained on half the data provides further evidence that additional data used by Full is only marginally useful. 

 For a given test set, some points are  to include in the training data. This goes against the conventional idea that more data is better for transformer-based LLMs, given how their typical training (both pre-training and fine-tuning) is inherently data-intensive. Our work sheds light on the fact that not all points are of equal quality, and we can dramatically reduce the size of the training dataset if we only keep data that works towards our metric of interest. How specifically these Opponents, when added to the training dataset, can undo the influence of Proponents is not well-understood, but this may be related to the phenomenon of catastrophic forgetting , where the abilities learned by a model from a given proponent may be forgotten when learning from an opponent/s.

We expect such harmful points to become increasingly common as synthetic data is increasingly used to fine-tune LLMs, where there is risk of model collapse from using poor-quality training data , or a training dataset that does not accurately reflect the distribution of the validation dataset. As such coreset selection will become an increasingly important method for most types of LLM training. Our algorithm can be incorporated as a pre-processing step for practitioners before fine-tuning.

 Surprisingly, we find that a smaller model (in this case almost 4x smaller than the model to be fine-tuned) can act as a reliable reference model, specifically for selecting what points to avoid adding in the training dataset. We expect that this transferring ability will remain as long as the reference model sufficiently learns the underlying training distribution after training. Importantly, this allows a further cost reduction of the two most expensive steps in our algorithm -- namely the fine-tuning of the reference model on the full training dataset and the subsequent gradient collection.

 We show that our method applies to reference models with distinct model families, even those with completely different pre-training regimes. This implies that a practitioner can simply bypass training a reference model if an open-source version is available.

In Figure , we compare two different notions of measuring the importance of the entire training set to each evaluation point: Semantic similarity via BERT scores  and Influence via influence functions from DataInf. The BERT score of two points is their cosine similarity in the contextual embedding space of BERT . BERT score aims to capture the semantic similarity between two different sequences of texts. In our experiments, we use DistilBERT  for efficiency. For this case, we calculate the BERT score of each training point to each test point. Then for each test point, we take the mean of the scores of all training points on that test point. We define this as the similarity score of the entire training set for that particular test point. Therefore for each test point, we obtain a scalar value as the similarity score. Meanwhile, to calculate the influence of the entire training set on each test point, we take the mean of the training point gradients to reduce it into a single point. We then calculate influence values with respect to individual test points as per normal.

For both types of importance, we sort each test point into ascending order and plot the rank of each point. We compare these ranks to the perplexity of the model on the test point, where the model is the Gemma-2B fine-tuned on the entire training set. The red line for each graph denotes the linear regression line. Coefficient values are within a 95\% confidence interval, but the intervals are too narrow to visualize. For semantic similarity as a metric, we hypothesize perplexity and semantic similarity to have a negative linear correlation (i.e. a downward trend) because test points with lower semantic similarity to the training set signifies that the test points are from a different distribution than the training set. However, while we observe this downward trend, it is a very weak correlation (coefficient = -0.087), suggesting that semantic similarity as measured by BERT scores is not a strong signal to indicate the fit of the training set to the test points.

In contrast, perplexity and influence values exhibit a stronger correlation. For Influence as a metric, we hypothesise perplexity and Influence to have a positive linear correlation (i.e. an upward trend) because test points assigned negative influence values imply that the training set is a proponent of those test points, and the training set is an opponent for the reverse case. We see this relationship between perplexity and influence, and observe that the correlation is stronger (coefficient = 0.56). This suggests that using influence values provide a stronger signal to indicate whether the model can generalize to a particular test point. Furthermore, using influence values is cheaper compared to using BERT scores, because we can reduce the training set to a single point when representing it as a gradient, compared to calculating point-wise similarities with BERT Scores.

We claim that these two measures are complementary because they provide an actionable feedback loop at improving the model. Evaluation metrics measure how close the model is behaving towards a desired behavior, and influence functions verifies if the training data does improve the model towards that desired behavior. Our coreset selection aglorithm is an example of how the two measures synergize instruction-tuning, which in our experiments focus on perplexity and influence. We claim that they are co-dependent because using these measurements on their own provide practitioners inadequate information on (a) whether the training data is sufficient at improving a capability, or (b) whether the additional training data will even contribute towards improving the capability, respectively.

A model's ability to generalize to a test point is dependent on the model's loss on that test point. Semantic similarity, while providing an intuitive explanation for model generalization (a model learning from similar points will perform well on an unseen but similar point), does not completely capture different ways how a model learns from different training points to generalize to a test point. In particular, some points may not be semantically related to the test point, but they still serve to reduce the loss on that point. Since influence values are calculated with respect to the loss value (e.g. how much a given training point reduces the loss on an evaluation point/set), they are a closer metric to capture model generalization, and Section  demonstrates this empirically via the correlation coefficients.

We used NVIDIA A100 Tensor Core GPUs. For influence values computation with the reference models, we used 1 GPU. For fine-tuning of models trained on the full dataset, we use 4 GPUs. For the rest of the fine-tuning setups, we used at most 2 GPUs. We estimate that we used 100 - 200 GPU hours on this project.

We conduct further experiments by using Llama2-7B  as the target models to be fine-tuned. While the proponents version where Gemma-2B is the reference model performs better than the full version, the proponents version where a Llama2-7B is the reference model performs worse than the full version. The latter is likely limited by the number of layers used to calculate the influence values, because the highest spearman correlation we can attain given our hardware constraints was less than 50\% for Llama2-7B.

Despite advancements, fine-tuning Large Language Models (LLMs) remains costly due to the extensive parameter count and substantial data requirements for model generalization. Accessibility to computing resources remains a barrier for the open-source community. To address this challenge, we propose the  algorithm, which selects a coreset by analyzing the correlation between training and evaluation samples with a trained model. Notably, we assess the model's internal gradients to estimate this relationship, aiming to rank the contribution of each training point. To enhance efficiency, we propose an optimization to compute influence functions with a reduced number of layers while achieving similar accuracy. By applying our algorithm to instruction fine-tuning data of LLMs, we can achieve similar performance with just 50\% of the training data. Meantime, using influence functions to analyze model coverage to certain testing samples could provide a reliable and interpretable signal on the training set's coverage of those test points. In2CoreIntroduction2Corresponding author. \\Email: ayrton.sanjoaquin@cnrsatcreate.sgDBLP:journals/corr/abs-1907-10641, DBLP:journals/corr/abs-2110-14168, lin2022truthfulqa, hendrycks2021measuring, zellers2019hellaswag, clark2018think, eval-harness, open-llm-leaderboardalpacaliu2024understandingzhang2023instructionliu2024datasetsjouppi2023tpuhu2021loraliang2023holisticalpaca_evalhampel1974inflkoh2020understanding1) What training points are suitable for the test set? and 2) What test points are suitable given that the model was already trained on a particular training set?he2023simplifyinghu2021lorakwon2023datainfWe developed , an algorithm that significantly reduces the size of the training set while creating a model that outperforms the one trained on the full training set.

        In2CoreWith , we can identify whether a testing point is well covered by the training set, thus providing an interpretable explanation of how a given model reacts to a particular test point.

        In2CoreWe further improve the efficiency of the influence function algorithm by limiting the number of model layers in calculating influence values, and we introduce a method to select the optimal number of layers given a memory budget.

    Related WorkInfluence FunctionsHammoudeh:2022:InfluenceSurveygrosse2023studying, koh2020understandingpark2023trakkwon2023datainfhu2021loraCoreset Selection for LLMsphillips2016coresetsmirzasoleiman2020coresetsli2024shot, chen2024alpagasushan2023understandinghan2023understandingwang2023farewellCoreset Selection with Influence Functionswang2023farewell, yang2023dataset, xia2024lesswang2023farewellyang2023datasetxia2024lessguo2021fastifInfluence Functions for Coreset Selectionkwon2023datainfPreliminarieskoh2020understanding, Cook1980CharacterizationsOA         I(z, z') := (\nabla_{\theta} \loss(z') |_{\theta=\theta*})^T I_{\theta*} (z)              I_{\theta*} (z) := {d\epsilon} |_{\epsilon=0} = -H(\theta*)^{-1} \nabla_{\theta} \loss_k     e:inf         I(z, D(^{val}) := ({m} \sum^m_{i=1} \nabla_{\theta} \loss(z'_{i}) |_{\theta=\theta*})^T I_{\theta*} (z)     pruthi2020estimating - Points with negative influence values. Their addition to the training set reduces the validation loss.         Proponents - Points with positive influence values. Their addition to the training set increases the validation loss.     Opponentse:hessiankwon2023datainfEfficient DataInfsec:improvfig:mem_effall-layerall-layeryeh2022betterAlgorithmwidth=1\textwidthfigs/algo_overview.pngOverview of In2Core for coreset selection. From left to right, we first calculate the influence values of each training point using the validation dataset and a reference model fine-tuned on the full dataset with LoRA. Then, we rank the training points by influence values and select the  highest-scoring proponents as the final training data, where  is a hyperparameter. Finally, we train a base model on this final training data. Both the reference and base model may have distinct architectures from each other.fig:coreset_algoCalculate the influence of each   using .     Rank each  by influence value.     Select  and get the top- proponents as elements of . sec:improvExperimental Setupjiang2023mistralgeminiteam2023geminiding2023enhancingeval_coreseteval_covjelinek1977perplexitywidth=1.0\textwidthfigs/coreset_same.pngPerformance of Models (Ref: Mistral-7B)width=1.0\textwidthfigs/coreset_diff.pngPerformance of Models (Ref: Gemma-2Bwidth=1.0\textwidthfigs/coreset_small.pngPerformance of Models (Ref: Gemma-2BMean Perplexity of models fine-tuned on different coreset selection strategies on the 250-point Ultrachat Evaluation Set. These strategies differ by selecting based on the influence values. 'Full' denotes a model trained on the full training data. Proponents, which is the default strategy in practice, outperform all groups except Full in all cases. Interestingly, some strategies result in a worse model than Random. In Section \ref, we argue that some points inherently degrade model training when included (e.g. Minimum and Opponents).fig:coresetResultseval_coresetModel Perplexitye:inf_allfig:coresetli2023textbooksfig:inf_vals_histMMLU Accuracyhendrycks2021measuringtable:mmlulogical reasoningformal logic, elementary mathematics, logical fallaciesDiscussiondisc_coresetMore data can result in worse models.inherently harmfulgoodfellow2015empiricalshumailov2023curseInfluence values from smaller models transfer to larger models.Influence values transfer across model architectures.In2Core for Model Coverageeval_coreseteval_covwhich test points does the model need more data for?which requests is the model not suited to perform well given its training?Resultseval_covfig:gen_pplzhang2020bertscoredevlin2019bertsanh2020distilbertDiscussiondisc_covInfluence functions and evaluation metrics are complementary and co-dependent at improving model capability.Measuring semantic similarity of training points is insufficient to capture a model's ability to generalize to a test point.eval_covConclusiondegradeLimitationsLLM Evaluationopen-llm-leaderboard, eval-harness, hendrycks2021measuring, liang2023holisticalpaca_eval, zhao2024preliminary, chen2024alpagasus, vicuna2023openai2024gpt4alpaca_evaljacovi2020towardsGroup influenceindividualeval_coresetkoh2019accuracybasu2020secondorderAverage gradient of the tokens as the gradient of the entire sequencexia2024lessfig:seq_lenEthics Statementrillig2023risksAcknowledgementsanthology,customacl_natbibAppendixsec:appendixExperimental Hardware DetailsCoreset selection for Llama2-7Btouvron2023llama