We consider two RE tasks: relation classification (RC) and relational triple extraction (RTE), under two low-shot settings: zero-shot setting and few-shot setting. (1) : Given a sentence  and an entity pair  where  is the subject and  is the object. This task aims to identify a relation  from pre-defined relation set  that satisfy the relationship between subject  and object  expressed by the sentence . (2) : Given a sentence , this task aims to jointly extract the relational triple  from sentence  where  and  are entities and  is the corresponding relation from relation set . (3) : For zero-shot setting, the model are expected to identify novel entities and relations at the inference time without any training examples. For few-shot setting, we formulate it in the typical -way--shot form. Given the training set  and the target sentence set , for  way (relation types), the model utilizes randomly  examples for each relation type from  to form the support set , where  denotes the sentence  and  denotes the relation  in RC or relational triple  in RTE. The model are expected to utilize only  examples from  for prediction and outputs recognized relations in RC and triples in RTE for each target sentence in .

Following previous literature~, the training examples are concatenated and provided as an single input to the model, which is feasible for -shot learning. At test time, the model is evaluated on an unseen target RE task that comes with  training examples (i.e. few-shot learning) or no training examples (i.e. zero-shot learning), and inference directly follows the similar prompting format as in meta-training phases.

Figure~ provides an overview of the meta-training work flow of  which consists of five steps. The model is meta-trained on a collection of RE datasets which we call meta-training datasets. Specifically, for each meta-training iteration  in step 1, we sample a dataset  from  meta-training datasets in step 2. Then  training examples  are sampled from the training examples of the chosen dataset in step 3. To unify different tasks (i.e. RC and RTE) and settings (i.e. zero and few-shot) for convenient transferring, we transform these  samples into the tabular prompting format (introduce it later) in step 4. We then supervise the model (tune the LLMs) by feeding the concatenation of  to the model as an input and train the model to generate  using a negative log likelihood objective in the final step. 

% This simulates ICL at inference where the first  examples serve as training examples and the last example is regarded as the test example. We adopt a tabular prompting for unifying two different tasks and settings that generates organized and concise outputs in ICL. Specifically, a table header "PredicateSubjectObject" is provided to prompt the LLMs to automatically generate a table, where "" is the recognizable delimiter of tables. This strategy is suitable for both zero and few-shot settings compared to solely text-to-text prompting format, as it provides precise instructed signals by table header for zero-shot prompting. During meta-training, we utilize two orders of table header "PredicateSubjectObject" and "SubjectObjectPredicate" to improve the robustness of models and unify the RC and RTE tasks at inference time.

For a new target RE task, the model is given  training examples  for few-shot prompting or no training examples for zero-shot prompting as well as a test input . It is also given a set of candidates  which is either a set of relation labels (RC) or relational triples (RTE). For few-shot setting, as in meta-training, the model takes a concatenation of ,  as the input, and compute the probability of each candidate . The candidate with the maximum conditional probability is returned as a prediction. For zero-shot setting, the model only takes  as the input with no training examples to get the prediction. 

The specific low-shot prompting forms in RC and RTE are illustrated in Figure~. For few-shot prompting, the models easily recover the output of test input  conditioning on  training examples. For zero-shot prompting, the situation becomes a little tricky. Since no training examples are available for in-context learning, the models are unware of relation schema in a specific RE dataset. We transform the multi classification form into multiple binary classification forms. Specifically, we prompt the models with each relation label  to generate the corresponding subject  and object , then we select the correct relation  or triple  from  candidates. In RC task, given relation  and subject , we consider relation  as the zero-shot prediction if its probability of output object  of the original annotation (for RC, we know both annotated subject and object) is the maximum. In RTE task, we prompt the models with each relation  to generate multiple candidate relational triples, then the triple  with the maximum conditional probability is selected as the zero-shot prediction.

%  We use a collection of publicly RE datasets taken from~ and widely considered in RE research community. We have 12 unique RE datasets in total, covering general, news, disease and science domains.  % For the data set with only training set as the original data, we divided it into training set, validation set and test set according to the ratio of 8:1:1. For the data set with only training set and validation set as the original data, we randomly select half of the data in the validation set as the test set and the other half as the new validation set. For other datasets, we adopt the official split.  All these RE datasets are in English and we provide the statistics of these datasets in Table~. Note that in meta-training phases, we only use the training set of 12 RE datasets. To balance the meta-training datasets, we sample 10,000 examples for each training set and include all examples for training sets with fewer than 10,000 samples~.

We experiment on FewRel~ and Wiki-ZSL~ for low-shot experiments. To ensure no overlap between meta-training and target datasets, for each relation label names in meta-training datasets, we discard it if it overlaps with a relation label name in target RE datasets (i.e, two identical phrases appear in two names). 

 We randomly select  relations from FewRel and Wiki-ZSL as zero-shot relations~. We repeat the experiment 5 times for random selection of  relations, and report the average results. We also vary  to examine how performance is affected. We use Precision (P), Recall (R), and Macro-F1 as the evaluation metrics for RC. For RTE, evaluating single triplet extraction involves only one possible triplet for each sentence, hence the metric used is Accuracy (Acc.)~. Note that the randomly sampled zero-shot relations may share similar semantics with some relations appearing in the meta-training datasets. However, since the relation schemas of meta-training and target datasets are different, we can still evaluate the zero-shot transfer learning ability of the models. In other words, the models are expected to understand the zero-shot relation semantics solely based on relation names.

 We conduct experiments on the public benchmark dataset FewRel~, which releases 80 relations and each relation owns 700 triple instances in total. For RC, following the standard configuration of FewRel~, we conducted experiments in these settings: 5-way-1-shot, 5-way-5-shot, 10-way-1-shot and 10-way-5-shot. For RTE, we follow previous work~ to adopt the 5-way-5-shot and 10-way-10-shot settings. Concretely, a relational triple is correct if and only if the spans of the head and tail entity are correctly identified and the associated relation is also predicted correctly. We adopt the standard Micro F1 score to evaluate the results and report the averages over 5 randomly initialized runs. Because the maximum length limitation of LLMs restricts to put too many in-context examples at once, we concatenate the maximum number of training examples satisfying the input length and discard the rest examples specially in 10-way-10-shot setting. And we should point out that this is one of the limitations of . More generally, in-context leaning paradigm is suitable for very few examples, making it difficult to better utilize more training examples compared to traditional methods. 

 We consider both supervised fine-tuned methods and zero/few-shot prompting methods. For , we make comparisons with state-of-the-art matching-based methods ESIM~, ZS-BERT~, PromptMatch~ and RE-Matching~. We also compare a seq2seq-based method RelationPrompt~ and two zero-shot prompting methods Vanilla and SumAsk~ with GPT-3.5~. For , we provide three baseline methods for comparison: TableSequence~, RelationPrompt~ and ZETT~. For , we make comparisons with the following state-of-the-art baselines including traditional few-shot learning methods ProtoNet~ and MAML~, besides the pre-training enhanced methods CP~, HCRP~, LPD~, HDN~ and DeepStruct~. We also compare a recent promising chain of thought and few-shot prompting method CoT-ER~. For , we select supervised learning methods FT-BERT~, FastRE~ and CasRel~, besides the few-shot learning methods MPE~, StructShot~, PA-CRF~, RelATE~ and MG-FTE~. 

% Prior work uses human-authored templates~ or relation descriptions~ to transform the semantic of relation type to a natural language sentence. We eliminate templates and descriptions, using the given input (or a concatenation of inputs if there are multiple) and relation label names provided in the original datasets.  For meta-training, we use a batch size of 4, learning rate of 1e-4 and a block size of 512, training the model for 100,000 max steps with 16-shot learning. To save memory during meta-training, we use deepspeed~ and adopt parameter efficient tuning technique LoRA~ for model training with the rank  to 8 and the merging ratio  to 32. As for the base LLMs, we use popular open-source models such as GPT-2~, T5~ and LLaMA~. Specifically, we adopt GPT-2 (117M), GPT-2-large (770M), GPT-2-XL (1.5B), T5-base (220M), T5-large (770M), T5-3B and LLaMA-7B for experiments. We should note that these LLMs without fine-tuning cannot perform ICL in RE. We empirically discover that they are unable to understand the structural sentences in ICL paradigm and recover the relation labels of test examples in low-shot settings.  % After meta-training, due to ICL is known to have high variance, we compute the average performance over five random seeds. % The results of all other baselines in our experiments are retrieved from the original papers. The main results of zero-shot RC are summarized in Table~, where LLMs with meta in-context training achieve competitive results compared to supervised fine-tuned RC methods and zero-shot prompting RC methods over two datasests when varying numbers of unseen relations. We have two findings about the model parameter scales and overall performances. First, the larger the model scale, the more notable the enhancement in performance. With small model scale,  with GPT-2 even underperforms ESIM. Second, encoder-decoder models seem to achieve better ICL performance than decoder-only models with similar model scales (eg., GPT-2-large and T5-large), which is due to the positive role of encoders in language understanding tasks. For the zero-shot RC task, as  increases, it is straightforward that models are difficult to predict the right relation since the possible choices have increased. Notably, the proposed  with LLaMA delivers superior results compared to state-of-the-art method RE-Matching when dealing with more unseen relations. Such results not only validate the effectiveness of meta in-context training, but indicate  is less sensitive to the number of relations compared to baselines. Another finding is that the recall of  with LLaMA achieves the best or second best results in 5 out of 6 settings, which may be related to our zero-shot prompting strategy. Because we enumerate each relation and subject to prompt  to generate its corresponding object, ensuring the final recall but slightly harming the final precision.

 The main results of zero-shot RTE by varying  unseen relations on FewRel and Wiki-ZSL are summarized in Table~, where  with foundation models that have more than 1B parameters consistently outperforms existing methods across different settings. LLaMA achieves up to 9.45 and 6.87 higher accuracy than the existing state-of-the-art model, ZETT on Wiki-ZSL and FewRel datasets, respectively. Because extracting relational triples from texts is a challenging structure prediction task, the meta-training makes  recover the semantics of the RTE task during zero-shot inference, where the model output is very similar with the meta-training output. However, we note that with the same foundation models,  achieves less satisfied performance compared to existing baselines. Specifically, the average scores of  with GPT-2 and T5-base are 10.22 and 18.55, respectively, where RelationPrompt uses GPT-2 and ZETT uses T5-base but they all delivers better results. This indicates that achieving noticeable ICL results in LLMs requires the relatively large-scale model parameters. As the model scale increases, the advantages of  become more apparent, where T5-3B and LLaMA both show much better performances than previous methods.

 The main results of few-shot RC are summarized in Table~ (left). We observe strong few-shot RC performance of  on FewRel. This suggests that the meta in-context training is beneficial in low-resource regimes via transferring knowledge from similar tasks. First, compared to few-shot and pre-training enhanced RC methods (e.g., LPD and HDN),  with LLaMA achieve more superior performances, and  with smaller base models can show competitive results. Second, compared to CoT-ER which is based on elaborated few-shot prompting and GPT-3,  with LLaMA basically delivers similar average scores, which indicates that meta in-context training successfully boosts the ICL abilities of open-source LLMs in RE. Third,  lags behind DeepStruct which is based on a pre-trained 10B parameter encoder-decoder language model GLM~ and is pre-trained on a collection of large-scale RE corpus. Despite its excellent performance with multi-task fine-tuning, DeepStruct produces unattractive zero-shot transferring ability after pre-training~. In contrast,  appears to be able to adapt to new data, presenting a fair and strong performance with in-context learning. Another explanation is that in-context training examples help the model better understand the new RE tasks, such as the concrete output format of each task. Finally,  with GPT-2 and T5-base still cannot surpass classic few-shot methods ProtoNet and MAML. Compared to pre-train then fine-tune paradigm, showing in-context learning ability tends to require large-scale model parameters. As the exceptional performance of 10B GLM based DeepStruct, meta-training on larger base models may bring better in-context learning results and is worth exploring in the future.

 Table~ (right) reports the few-shot RTE results of  against other baseline models on FewRel. It can be seen that, overall,  with LLMs significantly outperforms all competitive methods and achieves new state-of-the-art in two few-shot settings, which highlights the pivot role of meta in-context training.  Notably, even with GPT-2 and T5-base,  still outperforms most strong baselines such as PA-CRF and RelATE. Similar with zero-shot RTE results, the few-shot RTE results seem to prove that generative methods are more effective in handling the low-shot RTE task. Moreover, current few-shot RTE methods first meta-train on the subset of entire dataset and then are evaluated on the test set. Thus the training and testing data are in the similar distribution and same domain. But  showcases notable distribution and domain adaptation ability.

% We provide ablations and discussions of  w/ LLaMA. We vary the number of training examples  from 0, 4, 8, 16 to 32. In-context learning with  = 0 is equivalent to the zero-shot method. Results are shown in Figure~. Generally, increasing  helps across all tasks and settings. Besides, the few-shot setting seems to have higher variance than zero-shot setting. And increasing  consistently reduces the average performance variance. Especially, for zero-shot cases, performance tends to stabilize when  is greater than 8. This indicates that zero-shot prompting is less sensitive to the number of in-context training examples during meta-training as no training example is provided at inference time. In contrast, meta-training with more examples brings significant improvements for few-shot learning. However, we additionally find that the performance tends to saturate when  is closer to 16~. The saturate phenomenon is likely because the sequence length limit of the language model makes it hard to encode many training examples, which is one of the limitations of the attention technique~. 

 To see the impact of the number of meta-training datasets, we subsample 1, 4, 8 meta-training datasets out of 12 in four experimental settings. For each, we use three different random seeds to additionally see the impact of the choice of meta-training datasets. Figure~ shows the results. On average, low-shot performances generally increase as the number of datasets increase, which is consistent with results in previous work~. Nonetheless, different choices of meta-training datasets brings nonnegligible variance, indicating that a choice of meta-training gives substantial impact in performance. This can be attributed to multiple reasons. On the one hand, the varying amount of training data in different datasets leads to the training effectiveness of the model. On the other hand, the data domains and distributions between meta-training datasets and target datasets also play a key role in model performances.

 Although we ensure that the data distributions and relation schemas during the meta-training and inference phases are different, in fact, there are inevitably similar semantic relations between meta-training and inference relation sets. We use relation label words taken from the original datasets, which contain semantic hints that express what each relation label is supposed to mean. If the model is truly learning the relation in-context, it should generalize when label words are replaced with other English words, e.g.,  is replaced with token , thus not giving any hints about the relation semantics. To this end, we substitute each relation label in meta-training with  where  is the total number of relations in meta-training datasets. At inference time, we also perform similar operations to evaluate the in-context relation learning ability. The results are summarized in Table~. Note that when test labels are replaced,  is unable to perform zero-shot RC and RTE using tabular prompting. First, with test labels be replaced, the overall results suffer grave declines. This indicates that having semantic hints from relation label words is a necessary condition for LLMs to perform low-shot RE tasks. Compared to original LLaMA, meta-training on replaced training labels consistently delivers considerable improvements in few-shot RC and RTE, where  actually benefits from training on the replaced data and improves its in-context learning ability on new RE task. Still, overall performance is relatively poor compared to training on original labels, which implies that learning from relation label words helps the model better capture semantic differences between various relations. And the model can utilize the relation semantic knowledge during inference on target RE datasets.

 We categorize three types of incorrectly predicted unseen relations for analysis and provide an example illustrated in Figure~. (1) The true relation is not appropriate because it comes from distant supervision. It shows the noise originated from distant labeling. That is, we cannot identify the relation between  and  is  in this specific sentence. They just happened to appear together and their relation recorded in Wikidata is . (2) The predicted relation is ambiguous because it is hard to identify the order of subject and object. The golden relation and predicted relation have very similar semantics because they are reciprocal in FewRel. Unfortunately,  frequently reverses the subject and object corresponding to these two relations because it treats the relational triple () as (). This indicates that relying solely on relation label names may bring ambiguity. (3) The predicted relation is not precise for the targeted entity pair but may be suitable for other entities that also appear in the sentence. The targeted entities are  and , and  yields  as the prediction, which is actually correct if the targeted entities are  and . This shows  is able to infer the possible relation for entities in the given sentence. When we prompt the model with relation , the model output the subject  and object  with the max probability. As they are all valid spans in original sentence, we consider  as the true relation. This also hinders the capability of  in extracting overlapping relational triples. More general methods are worth exploring in the future.