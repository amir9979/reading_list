Given an image  of an object ,  is partially labeled by its corresponding attribute vector , where  denotes the total number of attributes and  denotes whether the attribute  is present in the image (`1'), absent (`-1') or unknown (`0'). The positive, negative, and un-annotated attribute sets of  are denoted as , , and , respectively. A model with good attribute recognition capability must correctly predict whether an attribute is present in the input image. 

There are also hierarchical relationships between attributes, which can be considered as a directed acyclic graph (DAG) denoted as  . The adjacency matrix of  is denoted as , where  if and only if attribute  is the parent of attribute . We denote the descendant set of  as , if there is a path from  to  in , we say  is a descendant of , formulated as . A model understanding attribute hierarchical relationship must predict an attribute's all ancestors present when it predicts the attribute present, and an attribute's all descendants absent when it predicts the attribute absent.

We show the evaluation process in Fig.. To perform inference, we first populate the template with the attribute and object names to obtain the prompt. Second, the obtained prompt and the image are fed into the vision-language model to get the inference results. This process is repeated three times to perform different evaluation approaches, including ITC, ITM, and VQA. Through ITM and ITC, we can get the inference scores. And it is anticipated that the scores of positive attributes are higher than those of negative attributes. Through VQA, we obtain the answers of 'Yes' and 'No'. And it is expected that the predicted answers for positive attributes are 'Yes' and 'No' for negative attributes. There are a total of 224,855 positive and negative attributes for 31,819 instances in the test set of VAW. For attribute recognition, we use accuracy to report the metrics for VQA and mean average precision (mAP) to report the metrics for ITM and ITC. 

To evaluate the understanding of hierarchical relationships, we first utilize the attribute hierarchy  to complement the annotations. A complementary example is presented in Fig.. For the positive attribute `navy blue', we add its parent attribute `blue' to the positive attribute annotation. Since 'hooded' has no parent attribute, there is no need to perform a supplement. For the negative attribute `green', we incorporate its descendant attributes into the negative attribute annotation, including `dark green', `pale green', etc. % try to identify its descendant set , and% incorporate each  into the negative attribute annotation.  Similarly, we do not need to operate for `blue plaid' and `sleeveless' since they do not have descendant attributes. After complementation, we have a total of 396,242 positive and negative attributes. Subsequently, the inference operation is conducted on the complementary dataset. We rely on the constraint violation (CV) and mean average precision post coherence correction (CmAP) metrics to report the models' performance in understanding hierarchical relationships. It is hoped that the scores for parent attributes are higher than those of child attributes. In Fig., the score of `navy blue' is higher than `blue', and the model outputs `No' for `blue' while it predicts the presence of `navy blue', which violates hierarchy.

Many large vision-language models  use image-text cosine similarity as one of the pre-training objectives to align image and text representations in the joint embedding space. Given the image embedding  of  and the text embedding  of attribute prompt : `The \{attribute type\} of the \{object\} is \{attribute\}.', we can get the probability of attribute  present in : 

where  is the cosine similarity function and  is the sigmoid function to scale the logits into . This template is shared by ITM and ITC, while we also explore other templates in the ablation study.

It is also a training objective shared by many models, including , which predicts whether a pair of image and text is matched or mismatched. During the training process, hard negative samples from ITC are selected for better fusion between visual and textual information. The ITM head outputs an array with two scores, the first is the mismatch score, and the other is the match score.  The evaluation process of ITM is similar to ITC, however, embeddings will be entered into the ITM head to get scores:

where  is a multimodal encoder,  is the ITM head, and  is the softmax function. Then , the probability of attribute  present in  can be obtained by taking the second element of .

Visual question answering requires the model to answer the question according to the corresponding image. The VQA models used in the experiment are fine-tuned on the VQA dataset , except for MiniGPT-4 , which only provides the pre-trained model. For fairness, we limit the candidate answers to containing only two answers: 'Yes' and 'No'. The question template is `Is the \{object\} \{attribute\}?', while for material attributes it is `Is the \{object\} made of \{attribute\}?'.  Since BLIP2  does not provide an interface to select from candidate answers, we do not perform evaluations on it. 

To evaluate the attribute understanding capability of different models, we utilize four evaluation metrics.

: It serves as an assessment of the overall correctness of a model's responses to questions. 

: Mean average precision is a prominent metric particularly applied to attribute prediction and multi-label classification. Due to the partial attributes annotation of VAW, we only consider annotated attributes.

: It post-processes attribute prediction scores to enforce hierarchical constraints within the model's outputs. Specifically, for a non-leaf attribute, it computes the maximum prediction score among its descendant attributes. Large deviations between mAP and CmAP indicate poorer hierarchical understanding. Given a non-leaf attribute , its post-processing probability:  is a metric utilized to assess the hierarchical constraint violations present in a model's predictions. It measures the extent to which model predictions deviate from hierarchical constraints. For ITM and ITC, we compute the CV based on the models' scores

where  is the number of test images and  is the number of edges in the attribute tree . For VQA, we calculate the CV based on the models' answers. We use  to represent the number of edges present in the image after complementation in the manner described in . While  is to represent the number of edges that violate hierarchy, then we have:

The models compared in our experiments are as follows. We list the corresponding model weights of evaluated large vision-language models in Table .

 provides a baseline for object attribute recognition.  proposes a strong baseline model along with supervised contrastive learning using negative-label expansion.  is trained on 400 million image-text pairs, using a text encoder and a visual encoder to map images and texts to the same feature space.  employs contrastive alignment of image and text representations before fusion, enabling robust vision-language learning without needing bounding box annotations or high-resolution images.   adeptly handles understanding and generation tasks, leveraging noisy web data through caption bootstrapping.   bridges frozen pre-trained image encoders and language models by training a lightweight querying transformer.  proposes a cross-modal skipped-connected network to address the problem of information asymmetry between image and text and improve computational efficiency.  aligns a frozen text encoder and a frozen LLM using one projection layer to achieve advanced multi-modal capabilities.

We demonstrate the attribute understanding ability of different models in terms of attribute recognition and hierarchical relationship understanding, each of which is evaluated in three ways: VQA, ITM, and ITC. For attribute recognition results, Table  presents a comparison between close-set models and the open-set models using ITC and ITM. We find that large vision-language models exhibit substantial attribute comprehension capabilities. For instance, the overall mAP of BLIP2's ITM is very close to the ResNet-Baseline, which is trained on the VAW dataset. Additionally, for tail attributes, the performance of mPLUG, BLIP, and BLIP2 using ITM is even much better than supervised models, indicating that the data imbalance of the training set impairs the performance of supervised models. In contrast, large vision-language models with strong generalization capabilities can mitigate this issue. 

Fig. represents the attribute recognition results of VQA. Among them, ALBEF, BLIP, and mPLUG obtain comparable recognition results, but mPLUG and the previous two models have differences in recognizing different attribute types. Table  shows the results of hierarchical relationship understanding capability, we find large vision-language models can comprehend certain hierarchical relationships. However, there are some deviations between CmAP and mAP. Besides, the CV metrics for ITM and ITC also have a significant gap compared to the supervised model, which demonstrates less favorable hierarchical understanding capability since the scores of some parent attributes are lower than those of their descendent attributes. The CV metrics for VQA seem to be better since it only predicts the presence and absence of an attribute without the limitation of scores. In both attribute recognition and hierarchical relationship understanding evaluations, BLIP2 achieves the best ITM and ITC inference results.

% 层级理解可视化图 According to Table , mPLUG understands hierarchical relationships better than MiniGPT-4. Through the visualization results in Fig., we identify two factors that may influence hierarchical relationship understanding. First, other objects in the image contain the attribute. Like the `olive green planter' in Fig., though both models can correctly determine that green is a negative attribute of the planter, MiniGPT-4 mistakenly identifies it as olive green, which violates hierarchy. The question is about the planter rather than the plant, but it fails to locate the right area when recognizing `olive green'. Second, the attribute phrase is not present on the object, but the attribute contained in the phrase is present on the object. Such as the `wearing white dog' in Fig., both models misidentify when recognizing whether the dog is wearing white or not, while they can distinguish that the dog is not dressed. It is probably because the dog is white and they cannot understand `wearing white'. These visualizations suggest that sometimes models behave like `bags-of-words'  and lack accurate semantic comprehension, thus impairing attribute understanding.

% gradcam可视化图以及柱状图 In Table , we observe that the mAP values of ITM are significantly higher than those of ITC for the same model. This phenomenon can be explained by analyzing the score distribution. Here, we refer to  to perform analysis. Fig. Right shows that ITM has a distinct threshold between positive and negative attributes, with most negative attribute scores concentrated in the low-score region. In contrast, ITC lacks a clear boundary between positive and negative attributes, leading to a lower mAP.  To further analyze the results, we use Grad-CAM  to visualize the interpretable components that contribute to the prediction and the degree of contribution of each feature.  The visualizations in Fig. Left show that both ITM and ITC can correctly locate the target object when provided with a prompt of a positive attribute. However, when recognizing negative attributes that are not present on the object, ITC only concentrates on the entire object while ITM could explore relevant regions of the object that may be associated with that attribute. 

As shown in Fig. left (c), ITC still focuses on the boy when provided with the negative attribute `playing football', whereas ITM attempts to explore the specified attribute and successfully directs attention to the boy's feet. Additionally, we find that the scores for positive and negative attributes vary widely for ITM, but the difference is not obvious for ITC. For example, in Fig. left (c), the difference between positive and negative scores for ITM is 45, while it becomes 2.4 for ITC. The score disparity suggests that ITM is more sensitive to attribute information.  % it can recognize the presence of attributes accurately by exploring relevant regions. In contrast, ITC is limited to object-level recognition and lacks the ability for finer-grained attribute analysis. This may be due to the hard negative sampling strategy in BLIP's ITM training , which enables ITM to capture more nuanced information and achieve better performance in attribute-level analysis.

In previous ITC and ITM experiments, we use the template that includes attribute types, object names, and attribute names. To investigate the impact of different prompting templates, we attempt two additional templates. The first template includes only attributes: `The object is \{attribute\}'; the second template includes both object and attribute names: `The \{object\} is \{attribute\}'. As shown in Table , ITC and ITM respond differently to various templates. For ITC, providing only attribute information often yields the best attribute recognition performance. However, for ITM, solely providing attributes is not sufficient; it requires the addition of attribute-dependent information, i.e., the object, to achieve better attribute recognition. This indicates that ITM possesses better detail-capturing capabilities, and thus, it performs better when more detailed descriptions are provided. In contrast, ITC lacks compositional understanding, so adding additional object information can introduce interference, leading to reduced performance in recognizing attributes. However, ITM requires more time and computational resources for inference compared to ITC. Therefore, enhancing the effectiveness of contrastive learning remains crucial. 

% image resolution对比 We find that during fine-tuning, most models tend to increase image resolution. For instance, ALBEF  takes random image crops of resolution 256  256 as input during pretraining, while during fine-tuning, it gets boosted to 384  384. Similarly, BLIP  grows from 224  224 to 384  384, and for the VQA task, the resolution is 480  480. % Monkey  supports resolution up to 1344  896, which surpasses existing large multi-modal models in many tasks like image captioning and various visual question answering formats.  Considering that higher resolution allows models to capture more subtle visual details, which may be beneficial for attribute understanding, we use ALBEF  to explore the effect of image resolution on attribute comprehension during fine-tuning.

The results are shown in Table . We use the model obtained by fine-tuning on COCO  dataset for the retrieval task. Therefore, we report the text retrieval (TR) and image retrieval (IR) results on the COCO dataset, as well as the attribute recognition results on the VAW test set. To maintain consistency, we reproduce it with the same image resolution as in the original paper (384  384). Then, we decrease or increase the resolution while keeping other parameters constant during the process. From Table , we can see that as the image resolution increases, the performance on the retrieval task becomes better, while the attribute recognition ability initially rises and then falls. These results demonstrate that image resolution is not the main factor contributing to attribute comprehension capabilities since no significant fluctuations are detected when it is increased from 256  256 to 384  384, but it is effective for retrieval tasks.

  Previous work on attribute predictions requires datasets with high-quality attribute annotations, such as VAW . However, the dataset used for finetuning is composed of image-text pairs, which do not provide a detailed description of the attributes in the image. To investigate the effect of captions in the training set on attribute comprehension, we count the overlapping attributes of the COCO training set with the VAW dataset. The VAW dataset has a total of 620 attributes, 605 of which are included in the training set of COCO. After removing the 15 non-overlapping attributes, the mAPs of ALBEF are increased, while for ITC it is 49.0  49.2 and for ITM it is 56.3  56.5.  This preliminary outcome indicates that the diversity of attributes in the training set may influence the ability to understand attributes. To further validate this hypothesis, we conduct fine-tuning experiments on ALBEF by removing attributes of certain types from captions.  Two attempts are made, including `Color Absent' and `Material Absent', which correspond to deleting color information and material information, respectively.

We utilize `bert-base-uncased' for the deletion operation.  For example, to delete color information from COCO, first, the color attributes of VAW and the captions of the COCO training set are encoded to obtain the input ids for attributes and captions. Secondly, those in the input ids for captions that overlap with the input ids for attributes are removed. Thirdly, the input ids for captions are decoded and the first letter of each resulting caption is capitalized to obtain a new caption. After the process, the original caption `Two people are riding a red bike down the street.' is transformed into `Two people are riding a bike down the street.', where `red' is removed. However, it is not possible to remove all the color and material information from the captions, such as `reddish', which is not present in the VAW dataset. Furthermore, the plural form of `fabric', `fabrics', cannot be removed since it has a different input id. Therefore, there is still color and material information after deletion, but the diversity is decreased.

% 去掉color的微调结果,与我们复现的384对比% 

We compare the results of removing color and material attributes with our reproduced results in Table . Significant declines are observed for the AP of color and material type. This evidence supports the hypothesis that during the fine-tuning process, attribute information contained in the captions enhances the model's attribute understanding capabilities, and the diversity of this attribute information plays an important role in the process. It is also notable that the removal of color information results in a notable reduction in other attribute types, such as `material'. This could be attributed to the material-related modifications that occur during the process of removing color information, which in turn leads to a decrease in metrics.  % Overall, experimental results suggest that the diversity % we can enhance the richness and density of attributes in captions to improve the attribute understanding ability of large vision-language models.