Considering failure to be the generation of toxic text, we seek to identify likely failure cases by defining our problem as an instance of Adaptive Stress Testing .%, %which uses reinforcement learning to identify likely failure cases. The Adaptive Stress Testing (AST) framework  uses reinforcement learning (RL) to find  cases of  of a system represented as a Markov decision process (MDP). Failure is defined by some set  that is a subset of the state space . 

An adversary perturbs the state of the underlying MDP (the "defender"). The adversary inputs state  and takes actions  to obtain a new state , which the defender then acts on. The goal of the adversary is to choose actions that maximize:

where  is a reward for achieving failure,  is some inverse distance metric between  and a failure state, and  is the likelihood of taking action  from state . That is, the adversary attempts to identify a sequence of likely actions that the  may take at a state that will lead to a terminal failure condition. 

We define red-teaming a language model as a finite-horizon Markov Decision Process (MDP) . Each action  is a single utterance given by a language model, each  is the text generated so far, and  the conditional probability of some new utterance  has given dialogue  and last turn statement . In this work, we aim to learn a language model  (the "adversary") which, when entailing a selected prompt, produces likely text that elicits toxic sequences from a frozen defender (i.e., untuned) language model .% to elicit toxic sequences from \(\pi_{}\). %%Toxicity detection models exist to formulate a dense reward objective which helps score toxicity of a trajectory automatically . They take a text sequence \(s\) and produce \(R_{\phi}(s) \in [0,1]\), which indicates the degree of toxicity in this sequence.%Given a prompt , suppose we sampled from  a trajectory . This triggers a state transition whereby we sample the output of the defender LM . Finally,  In this task, we define "failure" as the emergence of toxicity. We include two separate failure cases in our reward: first, we score the toxicity of the defender sequence resulting from an adversarial prompt. Second, we consider the form of failure wherein the adversary and defender generations are non-toxic individually but become toxic when taken together. We define "likelihood" as the direct inverse of a sequence's perplexity (i.e. as the perplexity of a particular prompt increases, the likelihood of it emerging from the defender decreases). 

Taken together, this motivates the following reward formulation :

where  is a prompt,  is a trajectory sampled from  given , and  is the output of the defender LM on input .

Then,   is a scaled toxicity score  for the defender's generation,  is a scaled toxicity score for the combined adversary and defender sequence, and  is the negative of a log-scaled perplexity measure  of adversary trajectory.  discusses boundedness of this reward.

% Note that, though only one turn is shown here with , the same interaction can be extended to multiple turns between the adversary and the defender simply by using the entire previous turn as the prompt for the next turn.% Though the reward formulation presented above doesn't naively appear to be corresponding to the AST reward exactly, it fits the formulation when under the context of LM red-teaming. We now provide a sketch of this argument, under the assumption that our defender policy does not experience representation collapse, this reward is sound for generating a red-teaming policy. is the output of a model on input text sequence , which indicates the degree of toxicity in this sequence. "Toxicity" as a numerical property of a sequence refers to this score. We choose Detoxify  ( variant) as the surrogate toxicity model .

We use IPO  to maximize the above reward. IPO is an unsupervised paired-example training scheme that relaxes a key assumption made by the Direct Preference Optimization (DPO)  language model alignment scheme, that paired preference data are rationally ranked according to a single objective. IPO simply requires that paired elements are ranked correctly relative to each other---appropriate for our multi-objective reward (). %This is appropriate for our reward function, which . %: while the DPO objective assumes that the paired preference data are rationally ranked according to a single objective (in our case, this is untrue due to there being multiple, competing terms to \(R\) which brings about a a non-trivial Pareto frontier), IPO relaxes this assumption and simply requires that pairs of entailments are ranked correctly according to their preferability---in our case, ranked by reward score.% Naively, an application of Proximal Policy Optimization  would seem to be appropriate for this case. Yet, as discussed in Section , the multi-modality and unboundedness of the reward signal results in a common failure case of over-optimizing \(R_{\phi}\) with increasingly unlikely or degenerative prompts (similar to ) that initially elicit toxicity, but eventually will lead to representation collapse. Bounding \(R\), a typical strategy to mitigate this, results in the second term being saturated (that is, we obtain likely trajectories that themselves aren't very toxic), achieving a similar effect. These properties results in PPO training, while being theoretically feasible, practically being very unstable for our use.

IPO bounds the amount that  can deviate from its reference  as a linear factor of a hyperparameter  (equation 17 in ). A careful choice of  constrains the  distribution from diverging significantly from baseline, while allowing enough exploration that  can be effectively maximized. In other words, the right  allows  to learn new behavior without forgetting language modeling.

% % Existing direct preference approaches typically use a frozen set of preferences which is not updated throughout training. This type of approach is suitable for optimizing for human preferences, which has ample data and a relatively unchanging preference profile.% However, our task focuses on red-teaming a language model without  knowledge of which toxic prompts would trigger defender toxicity; :collecting the requisite dataset for preference training offline would entail somehow generating, selecting, and iterating a set of likely prompts which results in toxic trajectories from the defender model with which to train our adversary. At that point, there would be no need to use yet another language model to perform automated red-teaming the training data itself would suffice for the red-teaming task. Further, this would result in a model which did not automatically discover possible red-teaming trajectories for it simply learned the likely toxic trajectories given in the preference training data. Hence, we must modify the naive application of IPO to both explore and exploit our defender model in an online learning fashion.% Fortunately, recent work  has demonstrated strong results in using direct preference approaches in an online learning format, whereby preference data is generated on-the-fly through by ranking LM outputs with a surrogate function (in our case, \(R\) from Section ) as the model improves. This is further advantageous because dialogue toxicity may take multiple turns to emerge . As such, each rollout may contain a multi-horizon sequence of completions between the adversary policy and the defender policy.  The original, offline approach to IPO discussed in  collects a dataset for preference training ahead of time by generating a set of trajectories from the defender model with which to train our adversary. Notably, this does not allow training to reflect how the defender responds to an incrementally improving adversary and requires prior knowledge of possible prompts that would elicit toxicity---eliminating the need for red-teaming. % selecting and iterating a set of likely prompts which results in toxic trajectories %from the defender model with which to train our adversary. %At that point, there would be no need to use yet another language model to perform automated red-teaming.  Therefore, we elected to take an online approach to IPO similar to those given in recent work , whereby we generate mini-batches of policy outputs, rank them using  (given in Section ), apply IPO to that mini-batch, and repeat.

 Recall that, in our setting as shown in Figure , each turn consists of a prompt, an adversary output, and a subsequent defender output. We allow our adversary a finite depth of  turns with which to red-team a defender model. To collect the paired outputs needed for IPO, at each epoch we recursively build a finite-depth tree of interactions between a frozen defender model and the policy being trained.   

At each tree depth , we obtain  previous interactions (at , our human-written, non-toxic prompt serves as the only "previous" interaction); using each previous interaction as prompt, we obtain one more turn by first sampling two adversary outputs from the current , followed by sampling the  using the prompt and adversary outputs, and finally rank the two rollouts according to our reward model given in Equation . Figure  illustrates this procedure to a depth of 2, and the procedure is described formally in .

%We therefore obtain  possible next states with which to build the next layer of the tree. After constructing this tree to a certain chosen depth, we flatten it by pairing each prompt's two rollouts to obtain a sequence of  paired outputs for training in that epoch. Our optimization iterates between collecting samples through multi-turn sampling from both the adversary and defender, followed by IPO of the resulting paired samples. After paired preferences are collected using our procedure, standard IPO tuning occurs following ---we solve for  over paired samples collected during that epoch on our policy. Each epoch of the full tuning procedure is outlined in .%---representing  of the procedure that is repeated until convergence. Though directly applying the procedure in Section  would likely bring eventual convergence, the amount of naive occurrence of toxicity would be sparse enough such that the procedure may need to be repeated for a long time.

To address this, we formulate a novel  scheme as a part of our online IPO training procedure outlined in ---using a small amount of known-toxicity-eliciting prompts  during training as occasional supervision to ensure toxicity occurs. Recall that IPO tuning requires two entailments of the same prompt, positive  and negative .

We encourage more rapid convergence by augmenting these samples with a small supervised training set  obtained in the following manner: %for each sample in this set, we obtain two paired samples like IPO . 

First, we sample some , a known prompt to elicit toxicity which may or may not be toxic by itself. Next, we split this prompt around a randomly selected token in the middle. This creates two slices of ------where . Using  (the first half of ) as a prompt, we create a  rollout of the adversary, which we use as the negative entailment: . We use the actual second half of , which we name , as the positive entailment in IPO, assuming that the continuation of prompt  from the original dataset will always be better than our policy's rollout. %and the rollout from our policy  as the negative entailment. 

We include these samples in our dataset as follows: with probability , we swap out a paired sample  of our original data () for a supervised training pair .  compares the results of applying our method with and without weak supervision.%benchmark this choice with both a supervised policy on the toxicity elicitation set only as well as our method without any weak supervision.  To investigate the utility of our RL-driven "alignment" framing, we benchmark this approach against a Supervised Fine-Tuning (SFT) one. In both cases, we use the same base policy as the adversary.  % As perplexity, a key component of our reward, is a direct result of language modeling capability and hence parameter count (CITE), we further report results relating to the effects of scaling the adversary architecture.

Unless otherwise stated, our adversary model is the base GPT-2 architecture . We chose GPT-2  as our primary frozen defender model to study, but further conducted additional experiments using the larger GPT-2 XL to demonstrate scaling effects. Using a GPT-2-based adversary on a larger model allows us to report the robustness of our choice of the efficient GPT-2 adversary architecture even against billion-parameter models such as GPT-2 XL. 

Importantly, whenever the defender model differs from our adversary model, we retrain our adversary from scratch by implementing a reward model that uses the defender's perplexity scores as a part of the reward. For instance, results reported regarding applying our approach on a GPT-2 base policy as an adversary for GPT-2 XL entails training a GPT-2 policy following our procedure, using a reward function  which computes  by scoring the outputs of our adversary GPT-2 using the frozen GPT-2 XL defender.

We use Detoxify , a commonly used toxicity scoring model, as our surrogate sequence toxicity metric  used in our reward; we selected option due to well-reported results across literature in addition to the fact that the model can be called locally. 

%We choose Detoxify  ( variant) as our surrogate toxicity model . We select this option due to the fairly well-reported results across literature for this model, in addition to the fact that the model can be called locally. Subsequent discussion of "toxicity" as a numerical property of a sequence refers to the output of this model when that sequence is input.%---saving a potentially large round-trip time and associated charges during online training with this model to compute the reward function. We use the Detoxify model as the objective source of toxicity in a sequence; the subsequent discussion of "toxicity" as a numerical metric of a particular text sequence refers to the output of this model as a score for that sequence.  One of our primary aims in this study is to tune a model to elicit toxicity in realistic situations. To achieve this, we use a not-necessarily toxic natural textual conversation data as initial "prompts" for training, beginning our roll-out procedure (Section ) for obtaining paired preference data with non-toxic prefixes sampled from this dataset.

The Convokit Reddit (small) corpus  (code lic. MIT, scraped from Reddit by Cornell NLP) has previously been discussed as a genuine source of generally non-toxic prompts that may induce unintended LM toxicity . We split the () data train-dev-test with , and, to ensure that the data used as a prefix (which the adversary entails with likely toxic text) is itself non-toxic, we additionally filter for the prompts' toxicity with . %according to our chosen surrogate toxicity model Detoxify .  Our evaluation results are reported using a randomly held-out test slice of the dataset used for testing which was not used for tuning or parameter selection. 

For our weak supervision procedure, we use the whole  (Lic. Apache 2.0)---a popular set of  English-language prompts known to elicit toxicity.

% We evaluate our proposed policy, our ablations, and each of our baselines using the same procedure. % For each input prompt, the rollout procedure will result in 3 objects that can be scored (one for each turn, whereby the "prompt" is the conversation so far and the "entailment" is the adversarial output). 

During scoring, we compute three key metrics that evaluate both the prompt likelihood and red-teaming ability of our model: (1) the perplexity of the adversarial entailment as measured by the defender model ("") --- to evaluate likelihood of the identified red-team prompt naturally occurring, (2) the toxicity of the resulting defender output (""), and (3) the entire attack/defense turn (""). Toxicity is scored by our chosen toxicity model, Detoxify .

We use a held-out test partition of the ConvoKit Reddit corpus () as the prompt with then a 3-turn entailment attack following the recursive procedure in Algorithm  (but without generating paired positive and negative samples).

Each of our baselines represents one potential trade-off between output prompt likelihood and toxicity elicitation. We adjust each baseline as little as possible subject to fitting our design constraints, i.e., that the adversarial statement entails a prefix that the adversary cannot choose and which is from a non-toxic corpus.

 We perform the evaluation task without any training by using a GPT-2 model for both the adversary and defender. We hypothesize this will result in prompts that are more fluent yet trigger significantly less toxicity.

 We use the train slice of  to tune a copy of GPT-2. We hypothesize that even though our policy is weakly supervised on the same dataset, the RL formulation will result in more fluent prompts and higher degrees of toxicity elicited. For parameters of our SFT baseline model, see Appendix .

 Consistent with previous literature, we further evaluate our work using a set of human-curated, known toxicity-inducing prompts as the adversarial "model". We chose the Bot-Adversarial Dataset  (BAD) as our prompts for this task, and perform an "attack" simply by sampling prompts from this dataset and using the defender model to entail them. Since BAD involves prompts with multi-turn conversations, we benchmark a "multi-turn" attack of our proposed approach against using each accumulated turn of BAD prompts as the prompt; for instance, the benchmark against a three-turn attack using our proposed method involves using a single BAD turn as the first prompt, two BAD turns as the second prompt, and three BAD turns in the third prompt.

In this experiment, we aim to understand the contribution of each term of our reward formulation with respect to our goal of fluent prompts that create toxic defender outputs. %As such, we removed each of the following components of our reward function and tuned our model following the same procedure to analyze its effects. %Furthermore, recent reinforcement-learning driven approaches  often take a reward model similar to parts of our approach given here (though usually use variously different methods to optimize that reward model); the experiments given here seek to further benchmark our model against the usually toxicity-focused reward models given in those approaches and allow us to discuss tradeoffs.

In particular, our ablation study seeks to remove each of the following components in our reward model, train the policy in the same manner as described in Section , and benchmark the results.

 Set , removing the explicit term in our reward function that scores for the toxicity of the defender model. The only term left for toxicity in reward now is , for combined adversary/defender toxicity. % We expect to see a slight drop in the resulting model's ability to actually elicit toxicity within the defender model; because the combined toxicity term  remains, we expect the adversary to maintain a high level of toxicity and therefore should be able to elicit some high toxicity from the defender. We set , removing the term for combined (adversary + defender) toxicity. %toxicity reward is for the defender alone. %within each turn in our model. %We expect our model to experience a drop in its ability to elicit toxicity: most frequency, toxicity emerges when the adversary is toxic as well; removing this term makes the reward signal significantly sparser in terms of when toxicity emerges (even given weak supervision, as the  prompt split may not reliably be both non-toxic and trigger defender toxicity). In this ablation, we set . We only penalize lack of toxicity, and not for likelihood or fluency of prompts. % We expect our model to be able to solve the automated red-teaming task successfully even despite this (i.e., trigger defender toxicity), yet we expect the means by which it does so to deviate from fluent speech. In qualitative analyses of this model's behavior, we expect to see behavior similar to those exhibited by previous RL-driven discrete prompt red-teaming approaches, which is characterized by non-fluent combinations of sensitive words, topics, and assorted symbols. We hypothesize additionally that this will  our model's ability to solve the task given the constrained relaxation. We train our model directly on the RL task without applying any novel weak supervision scheme outlined in Section . % We expect our model to solve the task somewhat successfully, but see a slight drop in performance across the results due to slower convergence.  This ablation removes both the weak supervision and the output prompt likelihood, which results in a reward function similar to previous work in discrete-prompt RL-driven red teaming . We expect this ablation to increase our model's ability to elicit toxicity but at a cost to its generated prompt likelihood.

We find that our approach outperforms baselines at the task of eliciting toxicity (). %As Table  illustrates, our approach is better at eliciting toxicity from a defender, compared to baselines. Compared to the untuned model, our model maintained remarkably low perplexity (within  of sampling from an untuned baseline model) while increasing incidences of toxicity factors of roughly  and  for GPT-2 and GPT-2 XL defenders, respectively. 

 We demonstrate a significantly higher rate of attack success compared to human-written, non-adaptive prompts as well as to prompts generated by supervised fine-tuning a model using the same weak supervision dataset and amount of training steps, indicating strong sample efficiency for our approach.

 It is noteworthy that all of the results within this study were obtained using a GPT-2 adversary---including those that successfully maintained a relatively low perplexity profile as scored by a GPT-2 XL defender. This further validates our method's robustness. Our GPT-2 model is effective as an adversary even when the perplexity signal is given by a larger defender.

 We find that using our training procedure with GPT-2 XL as both adversary and defender yields far lower perplexity scores when evaluated with both GPT-2 and GPT-2 XL as defenders. This indicates that compared to GPT-2, GPT-2 XL is more effective at modeling language not only for itself but also for the smaller GPT2 model. However, we note that this approach results in lower toxicity scores. We hypothesize that these results represent another point in the toxicity-perplexity Pareto front. i.e., because GPT2-XL is more sensitive to fluent text for perplexity due to its larger size, the model converged on a reward maxima at a different trade-off with lower perplexity and also lower toxicity. % We hypothesize that further tuning of reward model hyper-parameters (, , and ) is needed to achieve optimal performance for a large adversary model.% These results stands in contrast of using a supervised fine tuning policy as well as a fix set of human-written prompts which are not learned based on the perplexity of the defender policy, which results in a significantly higher perplexity. Even though the BAD (prompts) baseline contains fluent text , it is generally out-of-distribution from our defender policy (as compared to text directly sampled from it); furthermore, it was collected after the when the GPT-2 model was frozen ---further suggesting that the BAD dataset would be more out-of-distribution as compared to GPT-2.% Further, as our policy is trained in an online fashion, exploring and exploiting the specific defender's incidence of toxicity, % This distinction is important: as we seek to investigate likely trajectories that eventually lead to toxicity, we are keen to analyze not just generally fluent prompts (which is nevertheless an important component), but also  prompts: these results indicate that the adversary prompt trajectories elicited from our approach are  to occur during natural sampling from the defender model, making them especially dangerous in the red-teaming task. summarizes the results of our ablation study. Our approach creates significantly higher-than-baseline rates of toxicity while maintaining near-baseline levels of output likelihood. In other words, although our toxicity elicitations are slightly less successful, they are more likely to emerge naturally through sampling.%: resulting in slightly less successful toxicity elicitation but results which are quite likely to emerge naturally through sampling since they have low perplexity. As hypothesized, removing the explicit reward for defender toxicity %(setting )  decreased its frequency. Intriguingly, it also caused a slight increase in likelihood (i.e., lower perplexity) compared to no intervention. This suggests that while toxic adversarial statements by the adversary may be likely as identified by the defender, their entailments may not.  %Still, as section  describes the learned toxicity triggering prompts are still dramatically more fluently and likely according defender perplexity as compared to existing baseline methods. Removing the reward for "combined" prompt plus defender toxicity () resulted in the model being not much better than no tuning in terms of toxicity. We believe this is due to reward sparsity---neither the weakly supervising  nor natural rollouts create extremely frequent incidences of toxicity. Hence, relying on the ability of the adversary to explore possible trajectories that will elicit defender toxicity without any notion of adversary toxicity results in the model being unable to explore clear exploits suitable for eliciting toxicity.

 As expected, removing the defender perplexity term (setting ) results in an increase in prompt perplexity from the adversary---a rate of increase in perplexity (i.e., decrease in likelihood) of roughly  times higher than our proposed policy. We discuss the drop in likelihood qualitatively as well in Section . Correspondingly, removing the key constraint of likelihood also allowed our approach to elicit toxicity at a significantly higher rate, highlighting the efficacy of our online training procedure to identify more exploits when possible as constraints are removed. 

 Removing the weak supervision procedure resulted in a slight decrease in the toxicity of the resulting policy and the mean perplexity of the resulting prompts. The  of generated prompt perplexity significantly increased, which may indicate that the model is exploiting strategies in eliciting toxicity that would have otherwise been far out-of-distribution.% with both the existing weights as well as the weak supervision dataset. Lastly, removing both weak supervision and the perplexity reward term resulted in the most success in eliciting defender toxicity at the largest cost to output likelihood and fluency. While the policy was able to identify trajectories that easily elicit toxicity, its outputs are almost  times less likely than those generated by our proposed policy.

We now discuss a few qualitative strategies that our model learns as a part of the attack. Generation trajectories are provided in .

We observed that our adversary models discovered several consistent strategies for eliciting toxicity from a defender model. In cases where none of the strategies below were present in the first turn, at least one of them was typically used by the third.

 Political topics including Russia (), Donald Trump, abortion, and gun control, were often evoked to elicit toxicity. Within three turns of the example provided, the policy trajectory had become highly toxic, while the baseline remained non-toxic. 

 Another approach we frequently observed was the introduction of sexual content. Listing  illustrates an example of this behavior. It is important to note that although the example provided is non-violent, sexual violence was a common strategy of our model. Its generations should be labeled with appropriate warnings.

 The last strategy for eliciting toxicity that we discuss is the use of profanity. Listing  shows how a neutral input leads our model (but not the baseline) to generate profanity. %, while the baseline remains non-toxic. %generation using curse words from our model, but not the baseline. % We observed that generating text with curse words was a frequently used and highly effective approach. %Our model's trajectory ultimately led to severe toxicity, in the form of fully-capitalized text primarily containing curse words. % The examples given in Listing  all illustrate cases in which a toxicity elicitation strategy was used within the first turn.  Similarly, Listing  highlights that, when the low perplexity reward is removed, the resulting prompt generations are the least realistic. The model simply produces a generation that rapidly devolves into a list of sexual terms and curse words.

% The exact nature of generations and their fluency varies significantly with defender model size.  As defender model size scales, perplexity scores match human-perceived fluency more closely . Listings  and  show three turns between our model and a defender. Despite both adversary models being GPT-2, using GPT-2 XL as the defender results in a consistent topic (economic privilege and corruption) while using GPT-2 as the defender degenerates into a hateful unstructured utterance. This effect is even more pronounced when GPT-2 XL is both adversary and defender. In these cases, trajectories demonstrate substantially higher fluency (see ).

% Despite the adversarial policy both being GPT-2, a variant trained with perplexity scores from a GPT-2 defender produced a toxic trajectory which moderately adhered to a topic, but was far less cohesive than that produced with the one trained with the GPT2-XL defender. We hypothesize that since the GPT2-XL provides the most meaningful estimate of perplexity, its generation is the most coherent and sensible.