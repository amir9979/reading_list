In DART, the Red LLM  and the Target LLM  interplay with each other iteratively. The process of each iteration is repeated as illustrated in Algorithms~. The main steps can be described as follows:

We initialize parameterized  with our manually constructed instruction dataset  that consists of input prompts and corresponding jailbreaking prompts. In the attacking process of round , the goal of  is to transform  to a jailbreaking   set , then trigger the target LLM  to yield unsafe responses.  is selected by a carefully designed data selection algorithm, then used to train  for the next round.

We also initialize a parameterized  with an instruction dataset  that consists of input prompts and corresponding responses. In the defending process of round ,  strives to resist the jailbreaking set  generated by  and yield responses as . Finally, DART adopts a safe and helpful training dataset  based on active learning to train .

DART adopts two reward models to score a pair (prompt, response) to produce a confidence score. Due to the trade-off relationship between safety and helpfulness ~, we employ a safety reward model  and a helpfulness reward model  to guide the iterative adversarial training process. The termination condition of adversarial training process is when  begins to decay in helpfulness score compared with .

Diversity Evaluator  is to calculate a global diversity score between a candidate pair  and training set . Specially,  computes a BLEU score  between  and the target side of instance in the Red LLM training set , then takes the averaged BLEU score as the diversity score. 

Recent studies  have demonstrated that generating diverse prompts is more likely to effectively attack the Target LLMs. During the adversarial training process, DART not only takes into account the responses from the Target LLM, but also adversarially adjust its attacking directions by monitoring the global diversity of generated attacks across multiple iterations. Compared to MART , we set up an additional barrier for  to accumulate samples using a diversity evaluator  . As illustrated in Algorithms , DART only select a new  whose diversity score  is lower than diversity threshold  and safety score  smaller than safety threshold . In doing so, we can have training set  for next round , where instances are unsafe and diverse.

In DART, inspired by active learning , we assume that a sample is valuable if it is hard to defend. In Algorithms  and Algorithms ,  generates  responses but only selects the pair  defended successfully only once as a new sample to be added. By adopting data selection function  based on active learning,  improves its safety and exposes new dynamically changing safety vulnerabilities to . 

In order to construct an initialized dataset for the Red LLM  to learn the task of prompt rewriting, we designed a pipeline which dynamically detects toxic prompts and transforms them to the corresponding adversarial prompts. First, we built a dataset named  with a high concentration on malicious English data. We adopted HateBERT  as a toxic detector to filter out malicious data from CulturaX  with unsafety scores greater than a threshold . Second, we used Google Translation Engine to translate a part of  to Chinese, and subsequently back-translated to English with Baidu Translation Engine. To eliminate the quality loss in dual MT process, we manually removed sentence pairs which are not semantically equivalent to each other. As a result, we curated  with 960 prompts rewriting pairs to initialize(SFT) the Red LLM. 

The instruction dataset , which is used to initialize the target LLM, consisting of LIMA , Open Assistant  and an in-house instruction dataset. The process of constructing the in-house dataset is as follows: we extract a portion of  and send them to GPT-4 for safety check. Only safety responses are retained.

We evaluated the safety and helpfulness of the target LLM in different iterations. To observe the degree of convergence in the adversarial training process, we extracted 452 samples from  as . We also constructed a dataset with the same distribution as , named  which contains 997 samples. In order to observe whether the universal capabilities of LLMs decreases in the adversarial training process, we uses Anthropic Harmless  and AlpacaEval  as our out-domain datasets. 

We have extracted 225263 samples from  as attacking data  which is used to be transformed to jailbreaking prompts to attack the target LLM .

For the safety reward model, we used the Llama-2-7b-hf  to initialize , then trained it with DeepSpeed-Chat  framework directly on the open-source Anthropic Harmless trainset . However, in our preliminary experiments, we discovered that the reward score of   was not credible due to a large amount of noisy data in the training set. To address this issue, we manually re-annotated a part of training data in Anthropic Harmless. For the helpfulness reward model, we directly adopted UltraLM-13B as our  due to its relatively stable performance compared to other open-source models of the same period.

We used Llama-2-7b-hf  as the starting checkpoint checkpoint to train the Red LLM  and the target LLM   on the instruction datasets  and  respectively. During each iteration  of the adversarial training process,  and  reload checkpoints from the preceding iteration .

Automated red teaming with a dynamical adversarial training process is a novel framework. To the best of our knowledge, our work is the first attempt in this direction. The most related work to DART is MART , which is significantly different from DART in new training data selection. We reimplemented it as our baseline. As illustrated in Algorithms  and , MART selects new training data for adversarial LLMs only through multiple thresholds. Compared with MART, in Algorithms  and Algorithms , additional thresholds are implemented to ensure global diversity and achieve active learning. Table  displays a list of parameters.

The objective of dynamical adversarial red teaming is to improve the safety of the target LLM  without compromising its helpfulness, particularly on out-of-domain data. We provide the trends of reward changes under four different cut-off values(20\%, 40\%, 60\%, 80\%) in Figure . Cut-off 20\% means to calculate the  average score of the samples with the lowest 20\%. As illustrated in , under the premise of stable helpfulness trends, the safety trend increases steadily with the number of adversarial rounds both on the in-domain and out-domain dataset. The safety score of  consistently exceeds that of than the baseline (MART).

To check consistency between human and automatic evaluation, we synchronously conducted human evaluation on Anthropic Harmless testset . As displayed in Table , DART demonstrates a lower violation rate compared to Vanilla and MART in both automatic and human evaluations. For human evaluation on the Anthropic Harmless dataset, compared to Vanilla (instruction-tuning target LLM), MART eliminates the violation risks by 41.7\% While DART eliminates the violation risks by 53.4\%.

DART improves the effectiveness of adversarial training framework by optimizing the process of selecting new training data by two algorithms: global diversity and active learning. In order to analyze the contributions of the two algorithms in DART, we separately added global diversity and active learning to the baseline. As illustrated in Figure , both global diversity and active learning can respectively improve the safety of the target LLMs. DART further improves the safety of LLMs by combining both of them.

We speculate that there are two advantages of the proposed dynamical adversarial framework. In each iteration , the Red LLM  adjusts its attacking direction based on the feed-back from the target LLM . Similarly, the target LLM  also iteratively improves its safety and exposes new dynamically changing safety vulnerabilities to the Red LLM . In order to further explore the necessity of dynamical adversarial training for  and , we only adopted fixed  or  to select new training data in iteration . As illustrated in Figure , it is necessary to adopt the latest states  and  for adversarial training.

 Current researches have demonstrated that diverse attacking prompts are very important to automated red teaming .  We hence designed experiments to compare the effectiveness of two types of Red LLMs: one with diverse attacking prompts and the other with aggressive attacking prompts. The first Red LLM is constructed with global diversity algorithm used in DART. As illustrated in Algorithm , the second Red LLM  multiply attacks the target LLM using attacking prompt  and jailbreaking prompt  respectively, then selects samples  with an averaged safety score difference greater than . We additionally built a dataset  which extracts 452 samples from . The process of evaluation is as follows: 

As displayed in Figure , the safety score of  decreases significantly from iteration , indicating that  has a greater possibility to jailbreak the target LLM.  and  perform consistently on the  dataset. By observing the automatic evaluation results illustrated in Figure , we find that the Red LLM with diverse generation ability tends to make the target LLM safer in the dynamical adversarial framework.

We further carried out experiments to evaluate the performance of the Red LLM in adversarial training from two perspectives: aggressive and diversity. For fair comparison between Baseline and DART, in each iteration , we use  to attack the fixed target LLM . We compared the in number and diversity of successful jailbreaking samples on baseline and DART. Diversity is measured by firstly calculating the mean BLEU  score between any two prompts that successfully jailbreak , then performing a simple linear transformation to convert the numerical distribution. According to Figure , as the adversarial training progresses, DART never decay any performance in both aggressivity and diversity aspect. 

Reward models are widely used in LLM alignment training . The Anthropic Harmless  dataset is open-sourced to train a safety reward model, which is also used in our experiments. But we have found a large amount of noisy data in the dataset. In order to investigate the impact of reward model training data quality on our framework, we manually re-annotated 10k samples from the Anthropic Harmless dataset, creating a high-quality safety reward training corpus with 7837 samples. We denote the reward model trained on the original Anthropic Harmless dataset as . We randomly selected 3000 samples from our re-annotated Anthropic Harmless dataset to train . The method was also used to train  with the entire re-annotated  Anthropic Harmless data. As illustrated in Figure , Figure  and Figure ,  across all safety reward models trained on different reward model training data, DART perform better than Baseline in terms of safety. Table  shows that as the accuracy of the safety reward model increases, the violation rate after adversarial training also gradually decreases. DART has also demonstrated higher safety than Baseline.

\thefootnote\fnsymbol1Corresponding authors.2Equal contribution.\thefootnote\arabic Manual Red teaming is a commonly-used method to identify vulnerabilities in large language models (LLMs), which, is costly and unscalable. In contrast, automated red teaming uses a Red LLM to automatically generate adversarial prompts to the Target LLM, offering a scalable way for safety vulnerability detection. However, the difficulty of building a powerful automated Red LLM lies in the fact that the safety vulnerabilities of the Target LLM are dynamically changing with the evolution of the Target LLM. To mitigate this issue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in which the Red LLM and Target LLM are deeply and dynamically interacting with each other in an iterative manner. In each iteration, in order to generate successful attacks as many as possible, the Red LLM not only takes into account the responses from the Target LLM, but also adversarially adjust its attacking directions by monitoring the global diversity of generated attacks across multiple iterations. Simultaneously, to explore dynamically changing safety vulnerabilities of the Target LLM, we allow the Target LLM to enhance its safety via an active learning based data selection mechanism. Experimential results demonstrate that DART significantly reduces the safety risk of the target LLM. For human evaluation on Anthropic Harmless dataset, compared to the instruction-tuning target LLM, DART eliminates the violation risks by 53.4\%. We will release the datasets and codes of DART soon.

1.0\textwidth!\includegraphicsIllustration of DART. The Red LLM and Target LLM interact with each other in an iterative manner. The goal of the Red LLM is to transform prompts to jailbreaking prompts. The Target LLM dedicates to generating safe and helpful responses to resist the attacks from the Red LLM. Two reward models (RMs, safety reward model and helpfulness reward model) and a diversity evaluator provide a basis to select new training samples for the Red LLM and Target LLM. The Red LLM selects unsafe and globally diverse samples as new training data for the next iteration. simultaneously, the target LLM prioritizes to select difficult samples as new training samples for the next iteration.fig:dart_frameworkIntroductionachiam2023gpt, anthropic2024claudemehrotra2024tree, hong2024curiositydriven, liu2024autodancharan2023text, lin2024mallabai2022training,touvron2023llamatouvron2023llamawallace2021universal,zhu2023autodan,pmlr-v202-jones23awu2023deceptprompt,liu2024autodanchao2023jailbreaking, mehrotra2024treefig:dart_frameworkganguli2022redge2023martWe propose DART, a new and efficient automated red teaming framework in which the Red LLM and target LLM adopt a interact manner to detect the dynamical safety vulnerabilities and fix them.  With multi-round adversarial training, DART significantly reduces the safety risks of the target LLM. We will release the datasets and codes of DART, providing researchers with an open-source framework of iterative automated red teaming after paper reviewing. Initial Red LLM , initial Target LLM , safety reward model , helpfulness reward model , diversity evaluator , attacking prompt set , initial Red LLM training set , initial Target LLM training set , maximum number of newly generated , attack frequency Red LLM , Target LLM        \;      \;     \For

     \;

     \;      \;      \;

DART Training Frameworkalgo:dart_trainRelated Work\bf Manual/Automated Red Teamingtouvron2023llamabai2022trainingchao2023jailbreaking, ge2023mart, mehrotra2024treeshin2020autopromptwallace2021universalzou2023universalpmlr-v202-jones23azhu2023autodanalon2023detectingwu2023deceptpromptliu2024autodanguo2024connectingchao2023jailbreakingmehrotra2024treege2023mart% {\smallattacking prompt set , jailbreaking prompts set , jailbreaking response set , safety reward model , diversity evaluator , Red LLM training set , maximum number of newly generated  \; \textbf safety threshold  and diversity threshold harmful and diverse Red LLM training set        \;      \;     \uIf \textbfalgo:dart_select_red%\smalljailbreaking prompt set , multiple jailbreaking response sets , safety reward model , helpfulness reward model , maximum number of newly generated  \; \textbf safety threshold  and helpfulness threshold safe and helpful target LLM response set  

     \;     \For     \uIf \textbfalgo:dart_select_blueDARTalgo:dart_trainIterative Adversarial Trainingalgo:dart_trainThe Red LLM  transforms attacking dataset  to .  The Target LLM  takes  as input and yield responses as .  The Red LLM  selects new training samples considering feedback from the Target LLM  and the diversity evaluator. The Target LLM  selects new training samples based on active learning.  The Red LLM  and the Target LLM  update themselves independently using new training set for the next round.  Components in DART FrameworkRed LLM Target LLM Reward Modelsbai2022training,touvron2023llamaDiversity Evaluatorpapineni2002bleuAdversarial Attack with Global Diversitymehrabi2023flirt,hong2024curiositydrivenge2023martalgo:dart_select_redTarget LLM Data Selection based on Active LearningSettles2009ActiveLearningLiteratureSurveyalgo:dart_trainalgo:dart_select_blueSelectBluewidth=\textwidthlatex/pics/rm-filter10k-In-domain_Safety.pdf\textbffig:rm-filter10k-In-domain Safetywidth=1.01\textwidthlatex/pics/rm-filter10k-In-domain_Helpfulness.pdf\textbffig:rm-filter10k-In-domain Helpfulnesswidth=0.98\textwidthlatex/pics/rm-filter10k-Out-of-domain_Safety.pdf\textbffig:rm-filter10k-Out-of-domain Safetywidth=1.01\textwidthlatex/pics/rm-filter10k-Out-of-domain_Helpfulness.pdf\textbffig:rm-filter10k-Out-of-domain HelpfulnessAutomatical evaluation by reward models on the in-domain and out-domain datasets. The trend of curves is presented with various percentile thresholds (20\%, 40\%, 60\%, 80\%).fig:rm-filter10kExperimentsData and ModelsSeed Instruction Training DataCulturaXEvilcaselli2021hatebertnguyen2023culturaxCulturaXEvilNEURIPS2023_ac662d74NEURIPS2023_949f0f8fCulturaXEvilIn-Domain and Out-Domain Evaluation SetsCulturaXEvilInDomainSafeInDomainHelpbai2022trainingli2023alpacaevalAttacking DataCultruaXEvilReward Modelstouvron2023llamayao2023dschatbai2022trainingcui2023ultrafeedback-0.45cmAdversarial LLMstouvron2023llamaBaseline and SetupBaselinege2023martalgo:dart_select_red_baselinealgo:dart_select_blue_baselinealgo:dart_select_redalgo:dart_select_bluetab:Hyperparameterwidth=\textwidthlatex/pics/tech-ablation-In-domain_Safety.pdf\textbffig:tech-ablation-In-domain Safetywidth=1.02\textwidthlatex/pics/tech-ablation-In-domain_Helpfulness.pdf\textbffig:tech-ablation-In-domain Helpfulnesswidth=0.98\textwidthlatex/pics/tech-ablation-Out-of-domain_Safety.pdf\textbffig:tech-ablation-Out-of-domain Safetywidth=1.02\textwidthlatex/pics/tech-ablation-Out-of-domain_Helpfulness.pdf\textbffig:tech-ablation-Out-of-domain HelpfulnessIllustrating the contributions of global diversity and active learning in DART and explore whether they can be combined to further enhance effectiveness of adversarial training process.fig:tech-ablationAutomatical Evaluation for the Target LLMsfig:rm-filter10kfig:rm-filter10kHuman Evaluation for the Target LLMganguli2022redtab:human_evaluationwidth=\textwidthlatex/pics/adversarial_necessity-In-domain_Safety.pdf\textbffig:adversarial_necessity-In-domain Safetywidth=1.02\textwidthlatex/pics/adversarial_necessity-In-domain_Helpfulness.pdf\textbffig:adversarial_necessity-In-domain Helpfulnesswidth=0.98\textwidthlatex/pics/adversarial_necessity-Out-of-domain_Safety.pdf\textbffig:adversarial_necessity-Out-of-domain Safetywidth=1.01\textwidthlatex/pics/adversarial_necessity-Out-of-domain_Helpfulness.pdf\textbffig:adversarial_necessity-Out-of-domain HelpfulnessIllustrating the necessity of dynamically adversarial training. Similar to our baseline, we adopt fixed Red LLM  and target LLM  to select new training data for the next iteration.fig:adversarial_necessityAblation Studies and AnalysisOverlap Test on Global Diversity and Active Learningfig:tech-ablationNecessity of Adversarial Training Dynamically-0.2cmfig:adversarial_necessityDiverse vs. Aggressive for Red LLMshong2024curiositydriven,mehrabi2023flirtalgo:dart_select_red_baseline_agressiveIndomainSafeRedCulturaXEvilThe Red LLMs transform the  to target jailbreaking prompts. IndomainSafeRedThe fixed target LLM  is attacked with the jailbreaking prompts. Safety scores are computed by a safety reward model . fig:Red-Importance-Red LM SafetyIndomainSafeRedfig:Red-ImportanceAdversarial Stability of the Red LLM in DART-0.2cmpapineni2002bleufig:number_of_jailbreaking_sampleAnalysis on the Impact of Reward Model Training Data Quality on DARTNEURIPS2022_b1efde53, bai2022trainingganguli2022redfig:rm-filter10kfig:rm-nofilterfig:rm-filter3ktab:reward_model_limitationsConclusionLimitationscustomAppendixAdditional Figures and Algorithmswidth=\textwidthlatex/pics/rm-nofilter-In-domain_Safety.pdf\textbffig:rm-nofilter-In-domain Safetywidth=1.01\textwidthlatex/pics/rm-nofilter-In-domain_Helpfulness.pdf\textbffig:rm-nofilter-In-domain Helpfulnesswidth=0.98\textwidthlatex/pics/rm-nofilter-Out-of-domain_Safety.pdf\textbffig:rm-nofilter-Out-of-domain Safetywidth=1.01\textwidthlatex/pics/rm-nofilter-Out-of-domain_Helpfulness.pdf\textbffig:rm-nofilter-Out-of-domain HelpfulnessBaseline and DART adversarial training with the safety reward model , the training data is original Anthropic Harmless dataset consisted of 42573 samples.fig:rm-nofilterwidth=\textwidthlatex/pics/rm-filter3k-In-domain_Safety.pdf\textbffig:rm-filter3k-In-domain Safetywidth=\textwidthlatex/pics/rm-filter3k-In-domain_Helpfulness.pdf\textbffig:rm-filter3k-In-domain Helpfulnesswidth=\textwidthlatex/pics/rm-filter3k-Out-of-domain_Safety.pdf\textbffig:rm-filter3k-Out-of-domain Safetywidth=\textwidthlatex/pics/rm-filter3k-Out-of-domain_Helpfulness.pdf\textbffig:rm-filter3k-Out-of-domain HelpfulnessBaseline and DART adversarial training with the safety reward model , the training data is manually filtered by human annotators, which consisted of 3000 samples.fig:rm-filter3kwidth=\textwidthlatex/pics/Red-Importance-In-domain_Safety.pdf\textbffig:Red-Importance-In-domain Safetywidth=\textwidthlatex/pics/Red-Importance-In-domain_Helpfulness.pdf\textbffig:Red-Importance-In-domain Helpfulnesswidth=\textwidthlatex/pics/Red-Importance-Out-of-domain_Safety.pdf\textbffig:Red-Importance-Out-of-domain Safetywidth=\textwidthlatex/pics/Red-Importance-Out-of-domain_Helpfulness.pdf\textbffig:Red-Importance-Out-of-domain HelpfulnessAblation experiment compares the effectiveness of aggressive Red LLM and diverse Red LLM in dynamical adversarial training.fig:Red-Importanceattacking prompt set , jailbreaking prompt set , jailbreaking response set , safety reward model , maximum number of newly generated  \; \textbf safety threshold harmful and diverse set        \;     \uIf \textbfalgo:dart_select_red_baselinejailbreaking prompt set , response set , safety reward model , helpfulness reward model , maximum number of newly generated  \; \textbf safety threshold  and helpfulness threshold safe and helpful alignment response set        \;      \;     \uIf \textbfalgo:dart_select_blue_baseline-1cmattacking prompt set ,  jailbreaking prompt set , multiple attacking response sets , multiple jailbreaking response sets , safety reward model , maximum number of newly generated  \; \textbf safety threshold , safety gap threshold , rewrite frequency harmful and diversity set        \;      \;     \For      \;      \;     \uIf \textbfalgo:dart_select_red_baseline_agressiveAdditional Tables