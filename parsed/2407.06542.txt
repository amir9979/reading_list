To ensure the reproducibility, we carefully selected open-source, high-quality datasets for supervised fine-tuning.  These include OpenHermes-2.5 , SlimOrca , MetaMathQA , UltraChat , OrcaMath , Capybara , and Deita-10k . Since OpenHermes-2.5 is a collection of many other smaller open-source datasets, we also implemented a deduplication process to remove duplicate samples across these datasets. We use the latest version of those datasets to ensure that there is no contamination of data for our evaluation benchmarks. We summarize the dataset statistics in . % Table~ presents the distribution statistics for each dataset source, illustrating both the percentage of examples and the percentage of tokens contributed by each dataset. Note the shift in dataset rankings when comparing their contributions in terms of examples versus tokens.% [t]% \centering% {% % }% % % % From Table~, we can observe that Packing + Loss Mask achieves the overall best performance.% Padding + Loss Mask with 10K data already over-fits on the training data.% We suspect that this is because each batch starts with same chat template and the model overfits the chat templates and becomes less robust to the prompt templates used in OpenLLM evaluaiton.% % % % % % % %  For a controlled study, all models are trained on a fixed pool of pairwise preference dataset. We follow  and manually select a mixture of publicly available datasets such as UltraFeedback , HH-RLHF , and TLDR-preferences . These datasets consist of chat responses generated by a variety of LMs, and the winning/losing response is decided by prompting a judge model (e.g., GPT-4) or by asking human raters. We present the dataset statistics in . For more details on these datasets, please refer to . Unless otherwise indicated, all dataset  mentioned in this section are randomly sampled from this 264K mixture.

We follow prior work  and consider using data from UltraFeedback  as prompts. We then sample multiple responses from  and use Pair-RM  as a judge to obtain preference pairs. This results in an online collected dataset of 60k in size. Unless otherwise indicated, all dataset  related to online learning are randomly sampled from this 60k dataset.

We assess our models using OpenLLM  and Arena-Hard-Auto . The HuggingFace OpenLLM leaderboard evaluates an LM across a diverse set of reasoning, math, and knowledge tasks, and the average score is reported.  Arena-Hard-Auto evaluates an LM's instruction-following ability using 500 challenging user queries curated from the live Chatbot Arena leaderboard . To quantify the models' performance, it prompts a judge model (GPT-4-turbo) to compare the generated response against a reference response (by GPT-4), and uses the win rate as the final score. Since evaluation with GPT-4-turbo is expensive and using GPT-4's answers provides reference answers that are too strong, we use GPT-4-Omni as the judge model and answers by GPT-3.5-turbo as references for . We denote this modification as . % % 

Supervised fine-tuning (SFT) plays a critical role in aligning Large Language Models (LLMs), often serving as the first step of alignment. However, different techniques, including sequence packing, padding, and loss masking, have been proposed for SFT . We re-examine the effectiveness of these strategies within the context of alignment. % However, there lacks comparative studies on the most effective SFT strategies. % This work explores and assesses different supervised fine-tuning strategies, which provides better instruction models for the further alignment phases. Packing optimizes the training efficiency by grouping sequences of varying lengths into a single long sequence without requiring any padding. This technique, commonly used in LLM pre-training, is now also utilized in instruction-based supervised fine-tuning, as implemented by models like Zephyr . % We evaluate the performance of packing with and without loss masking in the SFT training process.% This method ensures that each batch contains an equivalent number of tokens, which improves the overall training efficiency and load balance.% It is commonly used in LLM pre-training before, and now is utilized in the instruction supervised fine-tuning for training models like Zephyr }.% The second strategy is padding, which is opposite to "packing". % It involves padding shorter sequences to a predefined maximum length and truncating longer sequences to the maximum length, which ensures that all sequences in a batch have the same length.% It is often combined with loss masking to ignore the loss on the instruction and user tokens. % Padding is used for training Alpaca  and Vicuna , and it is implemented in FastChat% Padding allows the model to focus on the meaningful part of the data and let the model learn the training data more quickly. In contrast to packing, padding extends shorter sequences with padding tokens and truncates longer ones to a fixed maximum length. It is often paired with loss masking, and is implemented in training models like Alpaca  and Vicuna . % We similarly investigate the performance of padding with and without loss masking.% In contrast, padding extends shorter sequences with padding tokens and truncates longer ones to a fixed maximum length.% It is often paired with loss masking to only compute loss on the target output tokens.% This method is implemented in training models like Alpaca  and Vicuna }. % By concentrating on significant data, padding expedites the learning process, allowing the model to assimilate training data more effectively. The standard language model training computes loss across all tokens in a sequence.  Loss masking, however, ignores loss computation on tokens that are not output tokens like user instructions. It prevents the model from learning irrelevant information, alleviating catastrophic forgetting and overfitting.

% % Non-packing achieved the best results in specific scenarios but showed a significant drop in performance when generalized to other input templates, especially in OpenLLM. Meanwhile, Packing with Tokens Masking consistently showed the best overall performance.% SFT refers to the process of updating only a small subset of the model's parameters during the training phase. This approach can lead to more efficient training cycles and reduce the risk of overfitting, especially in large language models. Below is a detailed explanation of the SFT strategies employed in our experiments:% These strategies are tested to determine their efficacy in adapting to different input templates and their overall impact on model robustness and performance in tasks evaluated by the OpenLLM suite.% To test the effectiveness of different supervised fine-tuning strategies, we conduct experiments on a small dataset Deita  with 10 thousand examples and 74 million tokens. % We also tested a larger dataset including Open-Hermes2.5, MetaMathQA and UltraChat with 1.6 million examples and 600 million tokens, to test the effect of scaling.% We use a hyperparameter with batch size 32, sequence length with 8192, learning rate 2e-5, and 3 epochs.% Gemma-2b is used as the base language model here.

Loss masking can be used in conjunction with both packing and padding strategies. Packing without loss masking and padding with loss masking is widely adopted in SFT, but the combination of packing with loss masking is largely unexplored. We evaluate the performance of these strategies on both small and large datasets. For each dataset size , we train all models from Gemma-2b-base over 3 epochs using a batch size of 32, a sequence length of 2,048 tokens, a learning rate of 2e-5. We then repeat this with =10K with DEITA-10k  and =1.6M with Open-Hermes2.5 , MetaMathQA , and UltraChat . % To evaluate the performance of different supervised fine-tuning strategies, we conducted experiments using both small and large datasets. Initially, we used the Deita dataset , including 10k examples with a total of 74 million tokens. % To assess the scalability of these strategies, we also tested a larger dataset that includes Open-Hermes2.5 , MetaMathQA , and UltraChat , which collectively contain 1.6 million examples and around 600 million tokens. % For these experiments, we use a batch size of 32, a sequence length of 8,192 tokens, a learning rate of \(2 \times 10^{-5}\), and a training duration of three epochs. % The Gemma-2b base model served as the starting base language model for training. summarizes our results. We find that combining packing with loss masking consistently yields the best performance across both dataset scales. We believe this is because other strategies may overfit chat templates: the starting tokens in each batch remain unchanged, leading to poor adaptation to unseen templates used in benchmarks such as OpenLLM. Next, we find increasing dataset size widens the performance gap between packing with and without loss masking.  This may be due to the increasing number of user instructions as the dataset size grows, which is unnecessary for the model to learn. Overall, this indicates that  should be trained with packing and loss masking, over a large collection of high-quality datasets as in .

% From Table~, we can observe that combining Packing with Loss Masking consistently yields the highest overall performance across two dataset scales.% In contrast, Padding with or without Loss Masking do not perform well. % We hypothesize that this approach may overfit the chat template, as the starting tokens in each batch remain unchanged, leading to poor adaptation to the unseen templates used in OpenLLM.% Both Packing alone and Packing with Loss Masking exhibit similar performance with only 10K data. % However, as the dataset size increases greatly, the performance of Packing without Loss Masking degrades significantly. % This drop may be due to the Packing-only method learning irrelevant user instructions, potentially including low-quality samples that can hurt performance.% Overall, our proposed Packing with Loss Masking method achieves consistent and better performance for supervised fine-tuning.% We further scale the dataset as outlined in Table~ and train the SFT models for further phases in the alignment pipeline.% % % {2pt}% [!t]%     \centering%     {!}{%         % }%     % % {6pt}% % % % % % % %  Following prior work, we use DPO  and continue training from the last iteration of SFT from . We then compare different training settings such as choosing sequence length/reference model; tuning beta; scaling offline alignment; and filtering preference datasets.

Popular implementations of DPO  use a sequence length of 1024, and a reference model . However, many recent work differs in this setting: using either a longer sequence length , or a different reference model . We compare these configurations by training Gemma-2b on a 10k subset from the mixture dataset and evaluating on OpenLLM and Arena-Hard-Auto*. Specially, we measure the impact of 1) using a longer sequence length of 2048, and 2) using different reference models proposed by prior work. The latter includes using  after additional SFT on the chosen responses (denoted as );  after additional DPO on the 10k subset (denoted as ); and using a stronger model such as LLaMA-3-8b (denoted as ). % Many recent work considers DPO training with a sequence length of 1024 , and using a reference model being the  itself ,  after additional SFT on the chosen responses , or any other model with a stronger capability . We compare these configurations by training Gemma-2b on a 10k subset from the mixture dataset and evaluating on OpenLLM and Arena-Hard-Auto*. We report the results in . shows that training with a longer sequence length of 2048 significantly improves performance on both benchmarks. We believe this is because multi-turn chat data are intrinsically long, and that longer responses may contain more complex reasoning compared to shorter answers . We also find using different reference models such as  or  slightly improves performance. However, as these methods require additional training, for simplicity we use  for the rest of the experiments. % Default configuration: seq len = 2048, bsz = 128, beta=0.1, epoch=6. Beta  is a hyperparameter in DPO that controls the strength of KL-divergence. Besides sequence length and reference model, many prior work  also differs in the choice of . It is unclear whether  can critically affect performance, and how other factors such as training data size can interact with .

To investigate this, we first fix a dataset size , and vary . We then repeat this process for different dataset sizes. % For each training dataset size, we fix the number of training epochs, and vary  from 0.01 to 0.5.  In , we find that 1) using a high KL-divergence  significantly harms performance, and 2) the best  stays relatively consistent across different training data sizes. % This shows that besides hyperparameters such as learning rate,  can also significantly affect performance in DPO. This indicates that  can be tuned using only a small subset of the data, which is much more compute-efficient than sweeping using the full dataset. % % % %  Prior work in SFT shows that scaling high-quality data during pretraining can significantly improve performance . We investigate whether a similar scaling law exists in offline preference alignment. For a given dataset size , we fix all training hyperparameters (e.g.,  and a learning rate of 5e-7) and only vary the number of training steps. We then repeat this process for , all randomly sampled from the dataset in . For other training details, please refer to . We present the results in .

In , we find that 1) under a fixed dataset size, performance quickly saturates/over-optimizes as training step increases ; and 2) increasing dataset size raises the point of saturation. We believe this indicates that similar to SFT, scaling law exists in offline preference learning so that scaling both dataset size and training steps can improve performance.  We note that this finding contrasts many DPO training configurations in prior work, where either a 2-10 times smaller dataset is used , or 2-4 times fewer training steps are performed .

 Besides increasing training data, several prior work  have also explored scaling  training data. These work finds that training with a small selection of ``highest-quality'' data can match or outperform training with the full dataset. To measure the effectiveness of these approaches, we considered training with a 10k data budget obtained from different data selection algorithms: , , , and . These methods select data based on their response length, response quality, prompt diversity, or a mixture of them. We also consider , our simple heuristic that filters data based on a combination of score difference  and high rating alike . For more implementation details on these algorithms, please refer to .

 summarizes the results. We find that 1) simply training on a 10 times larger dataset (100k) outperforms all data filtering methods; and 2)  is the only method that is competitive with random sampling in  benchmarks. We believe the former result strengthens the importance of data quantity and diversity in offline preference learning. The latter indicates that filtering methods based on attributes about the data itself may be insufficient for DPO, and that ``better'' data may be model dependent . % % %  Finetuning  with preference data obtained online has proven highly effective in further enhancing model performance . Given the high computational complexity of online training , we investigate whether it remains ``essential'' compared to the much more efficient offline alternative (). % Given the high computational complexity of online training , we use a fixed reward model and investigate the impact of different training dataset sizes on the final performance.

We measure the effect of various online training data sizes on the final performance, and compare it against offline DPO. Specifically, we follow  and first sample  responses with a temperature of  for each prompt from the UltraFeedback dataset. We then use Pair-RM as a judge and use the best and worst response as  and , respectively. Similar to , we perform one iteration of online training using DPO. We train all models from the DPO checkpoint trained with 10k randomly sampled data (denoted as ).

In , we first find that online training mainly benefits chat benchmarks (Arena Hard Auto*) but not core capability/knowledgege benchmarks (OpenLLM). We believe this is because online preference pairs are derived from  itself, making it unlikely for  to acquire  knowledge or skills. Next, we find that increasing the number of online training samples to 10k reaches comparable performance to offline DPO with 100k data. This indicates that online training remains competitive, and can be much more sample efficient than offline training for chat benchmarks.

% it seems that training with half as many epochs as the offline DPO training.%  Following prior work  and , we manually pick a mixture of high-quality SFT and DPO datasets. % Given the scaling trends in , we additionally included large-scale datasets such as OpenHermes-2.5 , SlimOrca , MetaMathQA , UltraChat , OrcaMath , Magicoder-Evol-Instruct , Capybara , and Deita-10k  for SFT and Nectar  for offline preference learning. This results in 1.95m data for SFT and 180k data for DPO. For online DPO, we follow  and use prompts from UltraFeedback . We summarize the datasets used in . We aggregate our findings from  into a single training recipe. During the SFT stage, we use the packing with loss masking strategy. During the DPO stage, we 1) use a sequence length of 2048 and , 2) sweep for the optimal  using a small (10k) subset, and 3) train our models over a large dataset with a compute budget equivalent to the best model in . For online DPO, we follow  and use Pair-RM  as a judge. We perform one iteration of online DPO and train on the full 60k online preference data. % analysis gemma best data=100k, 8 epoch, batch size=128% equivalent compute gemma2b with data=180k: 4 epoch bsz=128 or (equiv steps) 2 epoch bsz=64% equivalent compute llama8b with data=180k: 1 epoch bsz=128 We use the same datasets from  for SFT. Given the scaling trends for offline preference learning (), we additionally add Nectar , HelpSteer , and PKU-SafeRLHF . For online learning, we follow  and use prompts from UltraFeedback. We summarize the datasets used in .

 To holistically evaluate our models performance, we follow prior work and consider in total four benchmarks: Arena-Hard-Auto , AlpacaEval-2 , MT-Bench , and OpenLLM . In addition to the evaluation methods used in , AlpacaEval-2 uses a length-controlled (LC) metric to evaluate the model's instruction-following ability; and MT-Bench uses GPT-4 as a judge to score the model's response on a diverse set of QA tasks. We use the standard evaluation setting for all benchmarks, such as using GPT-4-turbo as the judge model and GPT-4 as the reference for Arena-Hard-Auto (c.f. ). % %  We train all of our models from Gemma-2b-base  and LLaMa-3-8B-base . These models are pre-trained with trillions of tokens from the web, and highly performant for a wide range of text generation tasks . We denote our models trained after each phase as , , and , representing the SFT, offline DPO, and online DPO stages, respectively.

We mainly compare our method against the officially released instruct models, Gemma-2b-it and Llama-3-8b-it. % We compare against the offically released instruct models Gemma-2b-it and Llama-3-8b-it. From the base model, Gemma-2b-it is first trained using SFT, and further finetuned using a novel, close-sourced RLHF algorithm . LLaMA-3-8b-it is trained using a combination of SFT, rejection sampling, PPO, and DPO . Both models are trained using close-sourced data. % % % 

We present the main results in . We find that after the SFT and offline DPO training phase, our Gemma-2b model already outperforms the Gemma-2b-it on all benchmarks.  It also matches or surpasses various popular 7b models, such as LLaMA-2-7b-chat  and Vicuna-7b . Similarly, our LLaMA-3-8b-lion-dpo shows competitive performance against the LLaMA-3-8b-it, despite only being trained with SFT and DPO. Finally, after online DPO training, our models further improve, surpassing the officially released instruct models in all benchmarks. We believe this result indicates the effectiveness of our training recipe throughout the three stages of alignment training.

% We present the main results in .% The Lion series models showcase a significant enhancement in the capabilities of language models across various benchmarks when compared to both baseline models and earlier iterations of the same architectures. Our results, as depicted in Table , clearly illustrate the benefits of our iterative training approach, incorporating supervised fine-tuning (SFT), direct preference optimization (DPO), and online direct preference optimization (ODPO).% % The Gemma-2b-lion-dpo model, which employs both SFT and DPO, achieved the highest scores among our 2B models across all metrics, scoring 55.93 in OpenLLM and achieving impressive improvements in AlpacaEval-2 (10.5) and MT-bench (6.68). This performance is indicative of the model's robustness and its ability to generalize across different types of language understanding tasks.% Comparatively, the Gemma-2b-lion-odpo model did not show results, indicating a potential overfit or inefficacy in the online DPO phase. This underscores the complex nature of online learning phases and their dependency on the quality and type of online feedback integrated during the training process.% For the larger scale models, the LLaMA-3-8b-lion-dpo significantly outperformed its counterparts, reaching a groundbreaking score of 71.28 in OpenLLM, demonstrating the effectiveness of our DPO training at scale. The inclusion of ODPO further enhanced its performance, particularly in Arena-Hard-Auto and AlpacaEval-2, where it reached scores of 20.7 and 23.1 respectively, setting new state-of-the-art results.% % Our training strategies have proven particularly effective. Packing with loss masking, as applied during the SFT phase, has shown to efficiently handle large datasets by optimizing the training load and focusing the model's learning on relevant features. This was further complemented by our DPO phase, where the optimal beta value was calibrated to enhance the preference learning without leading to over-optimization, as visualized in Figure .% % The sequence probability changes depicted in Figure  provide insightful visual evidence into the model training dynamics. Models that performed best (Figure ) exhibited a parabolic improvement pattern, optimizing the preference distinction between more and less preferred responses (denoted as  and ). This pattern was not as pronounced in undertrained models, which showed smaller changes in probability margins, indicating insufficient learning of user preferences.% % These results underscore the importance of careful calibration of training parameters and strategies in developing language models that are not only effective in understanding and generating human-like responses but also in aligning closely with human preferences and instructions. The Lion series, through its comprehensive and empirically driven training approach, sets a new benchmark for the development of sophisticated, instruction-following language models. To provide insights into the opaque training process during preference learning, we additionally measure how the sequence probability for  and  change after DPO training. We use a fixed unseen test set of  obtained from public datasets (see ), and compare  for each preference pair before and after training. We find that many of the best-performing models have a parabolic shape as shown in . This shows that well-trained models learns to improve confidence not only in pairs they could already distinguish correctly before training (i.e., ) but also equally in pairs they previously could not (i.e., ). Please refer to  for more details. % To provide insights into the opaque training process during preference learning, we additionally record how the sequence probability for  and  change after DPO training.% We use a fixed unseen test set of  obtained from public datasets (see ), and compute  for each preference pair before and after training.% We then qualitatively compare between various models from  and from . We present the visualizations in .% In , we find that many of the best-performing models have a parabolic shape as shown in .% This indicates that well-trained models learn to not only increase their confidence in pairs they could already distinguish correctly before training (i.e., ), but also improve on pairs where they previously could not distinguish (i.e., ).% Undertrained models () achieves a similar shape but with a much smaller magnitude.% While overtrained models () show change in probability at an even greater scale (e.g., for ), it is often achieved by sacrifising performance on the other side of the plot (e.g., ). In our analysis, we investigated the effect of various training strategies during each stage , and aggregate a training recipe using the best results from each stage. However, it is possible that there are combined effects between two or more stages (e.g., modify SFT and offline DPO simultaneously), which could lead to different or better results. Since this would result in an exponentially larger search space for training strategies, we chose to conduct our experiments in a sequential manner. We leave this exploration for future work. % We focus on fixing model from each stage, and investigate other stages independently. It may be possible that there are combined effects. Unlike SFT, in our prior experiments we find that offline preference datasets can vary significantly in quality and quantity. We therefore manually selected a mixture of high-quality datasets for our experiments in  based on some empirical heuristics (). Since this choice is empirical, we believe results may vary when, in the future, datasets of higher quality and larger sizes become available. We believe creating new, higher-quality datasets is perpendicular to our work, and we leave this for future work.

Our analysis primarily focuses on the Gemma-2b-base model. This is because 1) Gemma-2b is a light-weight yet performant model used widely in the community, and 2) it requires significantly less compute to conduct analysis as in  compared to using larger models such as LLaMA-3-8B-base. However, we believe it would be beneficial to extend our analysis to other model architectures such as LLaMA-3-8b  and Mistral-7b . We plan to extend our experiments with models of different sizes and architectures in future work. % Could have trained on more models such as Mistral, Phi, etc. Gathering a high-quality dataset of sufficient scale is imperative to study various properties of current offline preference learning algorithms. % To study various properties of current offline preference learning algorithms, gathering a high-quality dataset of sufficiently large scale is imperative.  There are many open-source preference labeled datasets available online, including Ultrafeedback , TLDR-Preferences , and Nectar . However, they show significant differences in 1) the quality and diversity of the prompts, 2) models used to generate the responses, 3) judge models, and 4) the number of preference pairs. It is therefore unclear which dataset is of sufficient quality to train a model on, and how the model's performance changes on downstream benchmarks. % At the time of writing, there are many open-source perference labeled datasets available online, including Ultrafeedback being ... as well as OpenHermes Preference, PRM pairs, etc being created by individuals. These datasets altogether amounts to over 1 million trainable preference pairs, however it is unclear if many of them are of sufficient quality.

To this end, we first selected a collection of 12 datasets, and empirically measure each dataset's quality by 1) sample upto 10k samples from each dataset, 2) train SFT-finetuned Gemma-2b and record its performance on MT-bench. We present the results in  and . We then used the top-six datasets according to their overall score in our offline preference learning experiments in . The baseline is our  model. % To this end, we first selected a collection of 12 datasets, and measure each dataset's quality by training a language model on it and recording its performance change on MT-bench, and then construct a mixture that is based on the performance improvements. Specifically, we first measure each dataset's quality by training Gemma-2b on a 10k subset (for dataset less than 10k in size, we use the full dataset) and measure its performance on MT-bench. We then included the top-six datasets according their overall score, and any dataset that achieves top-one score in any individual category. Conveniently, we find that the top-six datasets not only significantly outperform baseline, they also cover top-one dataset for any category. For all runs, we finetune from the best  obtained from . We use a sequence length of 2048, , batch size of 128, learning rate of 5e-7, and vary training steps for each run. For =100k, we continue training from previous runs instead of starting from scratch to save compute.

In addition to our result in  measuring performance in OpenLLM and Arena-Hard-Auto*, we also present other metrics such as evaluation loss, reward margin, and reward accuracy in . We note that the evaluation loss and reward margin are  with the performance in Arena-Hard-Auto* (or OpenLLM). This indicates that simple metrics such as evaluation loss and reward margin may not be good indicators of model final performance. %% 

While datasets such as UltraFeedback  provide ratings in  for chosen/rejected responses, other datasets such as TLDR  and HH-RLHF  does not. This makes methods such as filtering based on score difference (e.g., ) not applicable.

To this end, we consider a simple approach to use Nexusflow/Starling-RM-34B , the best reward model according reward-bench , to provide a score prediction to all of our training data. Specifically, we first used the reward model to compute a real value score for the prompt + chosen response and prompt + rejected response separately. Next, since the predicted score is a real value, we 1) rescale it to  to obtain , and then 2) consider a least square solution to find  under the function:

where the least square error between the true score  for data that contains a GPT-4 annotated score and the rescaled  is minimized. This results in . The average least square error and absolute error is  and , respectively. Finally, we augment the entire 264K training data with this score , and present the score distribution in . % % % % % %% % % 

Note that this reward model achieves 67.72\% accuracy over the entire 264K dataset (both with and without our score transformation). This indicates that  30\% of the predicted score might not be accurate. Therefore, we use the original score annotation when available, and use the predicted and rescaled score only when necessary.

We consider data filtering algorithms both from the instruction-tuning domain and from the preference learning domain. This include algorithms such as , , , and . We also consider , which can be seen as a combination of  and : first removing pairs that has a score difference of less than two, and then sampling 10k data that has the highest chosen score in each bin. We apply each of the algorithms above to select 10k data from the 264K shown in . We present the selected data distributions for each algorithm (except for ) in , and for  in .

% %  In this work, we introduced an efficient DPO implementation for Transformers. The motivation is to eliminate the computation overhead caused by padding tokens, as in DPO, chosen and rejected samples normally have varied lengths. Our approach involves removing all padding tokens within a batch and concatenating the remaining sequences into a single, continuous sequence. To handle sequence boundaries effectively in the self-attention layers, we utilize FlashAttention . This ensures that the removed padding tokens do not interfere with the processing of the valid tokens. An illustration of this process can be found in Figure~.

We evaluated the training times for LLAMA3-DPO with and without the fast DPO model implementation.  The experiments were conducted using the specified offline preference dataset, running on a setup of four A100 80GB GPUs. As shown in Table~, our fast DPO model implementation achieves a 27.48\% speed improvement.

All the training experiments in this paper were conducted on 4×A100 80GB GPUs. We used Deepspeed  for all our experiments as we find that storingin model weights in fp32 is essential for DPO's performance as learning rate is small. For other training details, please see Table~ and Table~.

% 

Following our findings in , we train the  series using a combination of datasets from the instruction-tuning domain and the preference learning domain. For SFT, we use the same dataset collection as in . Given the scaling trends of offline DPO, we add in more preference datasets such as Nectar  in addition to . We summarize the datasets for training the  below:

We used data from UltraFeedback  as prompts.  We sample multiple responses from  and use Pair-RM  as a judge to obtain preference pairs.  This results in an online collected dataset of 60k in size.

% Table~ shows the detailed evaluation results for OpenLLM tasks.% The performance of our aligned Gemma-2b models and LLaMA-3-8b models show significant improvements compared to the official instruct models. For the Gemma-2b model, the lion-sft, lion-dpo, and lion-odpo alignment training improved the average scores from 46.51 for the base model to 55.98 for the lion-odpo model. Similarly, for the LLaMA-3-8b model, the performance increased from 63.05 for the base model to 71.28 for the lion-dpo model and 71.22 for the lion-odpo model. These enhancements demonstrate the effectiveness of our alignment pipeline in surpassing the officially tuned instruct models which rely on closed-source data and algorithms.

Table~ presents a comprehensive breakdown of the performance of our Gemma-2b and LLaMA-3-8b models across various OpenLLM tasks, highlighting the improvements brought by the lion-sft, lion-dpo, and lion-odpo alignment training methods. The lion-sft model showed substantial improvements across all tasks, with significant gains in GSM8k (53.37) and TruthfulQA (43.80). Building on these improvements, the lion-dpo model particularly enhanced ARC (52.30) and HellaSwag (72.47), while maintaining strong performance in other tasks. The lion-odpo model achieved the highest scores overall, excelling in ARC (53.75) and HellaSwag (73.04), and maintaining superior performance in GSM8k (53.53).

For the LLaMA-3-8b model, the lion-sft variant displayed robust performance across all tasks, with notable scores in GSM8k (76.72) and TruthfulQA (54.26). The lion-dpo model further improved performance, achieving higher scores in ARC (63.91), HellaSwag (82.95), and significantly in GSM8k (80.59) and TruthfulQA (60.01). The lion-odpo model marginally outperformed the lion-dpo model, attaining the highest scores in ARC (63.99), HellaSwag (83.18), and TruthfulQA (61.12), while maintaining exceptional performance across all tasks.

Both the Gemma-2b and LLaMA-3-8b models benefit significantly from the alignment training, with each subsequent model (sft, dpo, odpo) showing progressive improvements. The lion-odpo models generally achieve the highest scores, demonstrating the effectiveness of this alignment method in enhancing model performance across diverse tasks. % [!t]%     \centering%     {%       %     }%     %     % %  To provide insights into the opaque training process during preference learning, we additionally record how the sequence probability for  and  change after DPO training. We use a fixed unseen test set of  obtained from public datasets (see ), and compute  for each preference pair before and after training. We then qualitatively compare various models from  and from . We present the visualizations in .

In , we find that many of the best-performing models have a parabolic shape as shown in . This indicates that well-trained models learn to not only increase confidence in pairs they could distinguish correctly before training (i.e., ), but also improve on pairs where they previously could not (i.e., ). Undertrained models () achieve a similar shape but with a much smaller magnitude. While overtrained models () show a change in probability at an even greater scale (e.g., for ), it is often achieved by sacrifising performance on the other side of the plot (e.g., ).

% Background Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. % However,.. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. % Despite many papers and libraries providing analysis and training platform for alignment, there is a lack of comprehensive studies covering the entire language model alignment pipeline.% What we do We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. % We conduct a rigorous analysis of the best strategies for supervised fine-tuning, offline, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at . % We will make our models, training datasets, and code publicly available. % Our best models, trained from Gemma-2b-base and LLama-3-8b-base, exceed the performance of officially tuned instruct counterparts while only using publicly available datasets and open-source algorithms.% We provide an easily reproducible training recipe for future alignment research.% We find that our model, tuned from the base models, can exceed the performance of official tuned instruct models while only using publicly available datasets.% We find that our model can exceed the performance of official tuned instruct models while only using publicly available datasets.% 1) replicating the complete alignment pipeline% 2) exceed officially released instruct models% We present a fully reproducible training pipeline and trained our models using publicly available datasets.% Our models, trained from Gemma-2b and LLaMA-3 8b base models, have achieved comparable performance to the state-of-the art models.% Our code and models will be made publicly available.https://github.com/Columbia-NLP-Lab/LionAlignmentIntroductionbrown2020language,openai2024gpt4,touvron2023llama,llama3modelcardwei2023jailbroken,deshpande2023toxicityouyang2022training,rafailov2023direct,meng2024simpo,guo2024direct-ipocui2023ultrafeedback,starling2023tunstall2023zephyr,snorkel2023iterative-dpo,dong2024rlhf-workflowgemmateam2024gemma,llama3modelcardtunstall2023zephyr,xu2024cringe-iterative-dpogemmateam2024gemmallama3modelcardzheng2023judgingalpaca_evalzheng2023judgingopen-llm-leaderboard-1mmWe present a rigorous analysis of modern alignment training pipelines, and identify a set of design choices that significantly impact the performance of language models.     We aggregate our empirical findings into a step-by-step recipe, and show that our models outperform the officially released instruct models, which relies on closed-source datasets and algorithms.     We make our model, training dataset, and code publicly available for future research in language model alignment.     % \item We aggregate our empirical findings into a step-by-step recipe, and release the  series of models trained from Gemma-2b-base and LLaMA-3-8b-base, the datasets, and the code.     % \item We show that using only publicly available datasets and a simple training pipeline, our models exceed the performance of the officially released instruct models as well as models of larger sizes. \piref\pi_\text\pisft\pi^\textPreliminariessec:Preliminariesziegler2020finetuning,bai2022training,ouyang2022trainingsnorkel2023iterative-dpo,xu2024cringe-iterative-dporafailov2023directSupervised Fine-tuning Stage     p(y|x) = {|y|} \sum_{i=1}^{|y|} \log \pi_\theta (y_i | x, y_{<i}). subsec:Supervied FinetuningOffline Preference Learning StageUnder the formulation of reward maximization under a KL-divergence constraint r(x, y) = \beta \log {\piref(y|x)} + \beta \log Z(x)


    _ = -_{(x, y_w, y_l)\sim }\left[\log \sigma \left( r_w-r_l \right)\right]. subsec:Offline Preference LearningOnline Preference Learning Stagexu2024cringe-iterative-dpo,guo2024onlinedpo,snorkel2023iterative-dposubsec:Online Preference LearningAlignment Procedure Analysissec:Training Procedure Analysissubsec:Experiment Setupsubsec:Supervied Finetuningsubsec:Offline Preference Learningsubsec:Online Preference Learninggemmateam2024gemmaExperiment Setupsubsec:Experiment SetupSFT Training DataOpenHermes2.5SlimOrca,longpre2023flan,mukherjee2023orcayu2023metamathding2023enhancingmitra2024orcamathdaniele2023amplify-instructliu2024whattab:sft_data_statsOffline Preference Learning Datadong2024rlhf-workflow,tunstall2023zephyrcui2023ultrafeedbackbai2022trainingstienon2020learningtab:dpo_data_statssubsec:DPO Analysis Dataset SelectionsubsetsOnline Preference Learning Datameng2024simpo,xu2024cringe-iterative-dpocui2023ultrafeedbackjiang2023llmblendersubsetsEvaluation Benchmarksopen-llm-leaderboardarenahard2024zheng2023judgingWe use version GPT-4o-2024-05-13We use version GPT-3.5-turbo-0125subsec:Supervied Finetuning,subsec:Offline Preference Learning,subsec:Online Preference LearningArena-Hard-Auto*Supervised Fine-tuningsubsec:Supervied Finetuningvicuna2023,tunstall2023zephyr,shi2024instructionPackingtunstall2023zephyr\urlPaddingalpacavicuna2023\urlLoss Maskingliu2024makes-deitaOpenHermes2.5yu2023metamathding2023enhancingtab:sft_resultstab:sft_data_statsOpenLLM         \includegraphics     Arena Hard Auto*         \includegraphics

Measuring the effect of dataset size () and training steps (FLOPs) on final performance. While performance can quickly saturate given a fixed , increasing the dataset size increases the point of saturation. Dotted lines are our interpolation using a degree 2 polynomial. % Varying the number of training steps with DPO. For each dataset size , we vary the number of training steps and record their performance changes. Dotted lines are our interpolation using an degree 2 polynomial. fig:scaling-dpoOffline Preference Learningsubsec:Offline Preference Learningrafailov2023directsubsec:Supervied FinetuningChoosing Sequence Length/Reference Modelalignment_handbook2023,tunstall2023zephyrmeng2024simporafailov2023direct,gorbatovski2024trdpo=SFT chosen=DPO=LLaMA-3-8btbl:seq-len-n-ref-modelzhao2024longismore=SFT chosen=DPOTuning Betatunstall2023zephyr,gorbatovski2024trdpofig:dpo-hyperparam-betaHowever, we note that using a subset too small (e.g., ) do not yield meaningful variations across runs. OpenLLM         \includegraphics     Arena Hard Auto*         \includegraphics     Effect of training on 10k data selected using different filtering algorithms. We find that simply training on a larger dataset (100k) outperforms all methods.fig:dpo-dset-algoScaling Offline Alignmenthoffmann2022compute-optimal,kaplan2020scaling-llmtab:dpo_data_statssubsec:Training Hyperparams for Scaling DPOfig:scaling-dpofig:scaling-dporafailov2024scaling-dpo,gao2022scaling-reward-modeltunstall2023zephyr,ivison2023camels-dpomeng2024simpo,gorbatovski2024trdpoFiltering Preference Datasetsliu2024makes-deita,zhou2023lima,zhao2024longismoredownDeitaliu2024makes-deitaLongestzhao2024longismoreAlpagasuschen2024alpagasusArgillaargilla-dpo-mix Some algorithms such as \framework are originally designed for SFT datasets and uses a single prompt-response pair . In these cases, we use  from DPO datasets as . A2argilla-dpo-mix,wang2024secretsAlpaGasussubsec:Dataset Filtering Algorithmsfig:dpo-dset-algoA2bothxia2024less,yu2024teachingOnline Preference Learningsubsec:Online Preference Learningdong2024rlhf-workflow,guo2024onlinedposchulman2017proximal,snorkel2023iterative-dposubsec:Offline Preference Learningmeng2024simpoWe note that results may vary with different reward models \cite, which we leave for future work.jiang2023llmblendermeng2024simpooffline DPO (10k)tbl:online-dponewThe \framework SeriesLion% \tabcolsep4pt0.86

  Evaluating the \framework series across multiple chat and core knowledge benchmarks. We report the win rate and length-controlled win rate for Arena-Hard-Auto and AlpacaEval-2, respectively.tbl:main_exp\tabcolsep6ptExperiment SetupTraining Recipesec:Training Procedure Analysisfig:scaling-dpomeng2024simpojiang2023llmblenderTraining Datasetssubsec:Supervied Finetuningsubsec:Offline Preference Learningstarling2023wang2023helpsteersafe-rlhfmeng2024simposubsec:LION Training DatasetsEvaluation Benchmarksarenahard2024dubois2024length,alpaca_evalzheng2023judgingopen-llm-leaderboardsec:Training Procedure Analysissubsec:Experiment SetupModels and Baselinessubsec:models and baselinesModelsgemmateam2024gemmallama3modelcardopen-llm-leaderboard-lion-sft-lion-dpo-lion-odpoBaselinesgemmateam2024gemmallama3modelcardundertrained     \includegraphics     \label best performing     \includegraphics     \label overtrained     \includegraphics     \label

We track the changes in the probability margin  under various training configurations, and find that the best-performing models exhibit a parabolic pattern. Arena-Hard-Auto* results from left to right are 10.7, 14.8, and 13.2. Test loss from left to right is 0.63, 0.65, and 1.33. % Visualizing change in sequence probability for  and  after DPO training. We track the changes in the probability margin  under various training configurations, and find that the best-performing models exhibit a parabolic pattern, where preference pairs with the largest margin show the most improvement. % Arena-Hard-Auto* result from left to right is 10.7, 14.8, and 13.2. Test loss from left to right is 0.63, 0.65, and 1.33. fig:dpo-seqprob-change-full-12ptMain Resultssubsec:Main Resultstbl:main_exptouvron2023llamavicuna2023Qualitative Analysissubsec:Qualitative Analysissubsec:DPO Analysis Dataset Selectionfig:dpo-seqprob-bestsubsec:Details on Qualitative AnalysisRelated WorkSanh2021MultitaskPT,wei2022finetuned,chung2022scaling,Mishra2021CrossTaskGVkaplan2020scaling-llm,hoffmann2022compute-optimalSchulman2017ProximalPO,Stiennon2020LearningTS,Ouyang2022TrainingLMschulman2017proximalXu2024IsDSrafailov2023directethayarajh2024ktoxu2024contrastiveguo2024direct-ipo,hong2024orpocui2023ultrafeedback,safe-rlhf,starling2023guo2024onlinedpo,xu2024cringe-iterative-dpo,snorkel2023iterative-dpo,Xu2024IsDSConclusionLionLimitationsSensitivity of Model BackboneindependentlySensitivity of Training Datasubsec:Offline Preference Learningsubsec:DPO Analysis Dataset SelectionMore Model Architecturessubsec:More Model Architecturessec:Training Procedure Analysisllama3modelcardjiang2023mistralEthical Considerationsiclr2024_conference,customtable0\thetableA\arabicfigure0\thefigureA\arabicMore Details on Alignment Procedure Analysissec:More Details on Analyzing Offline Preference LearningOffline Preference Dataset Curationsubsec:DPO Analysis Dataset Selectioncui2023ultrafeedbackstienon2020learningstarling2023fig:data-eval-mt-benchfig:data-eval-mt-bench-distsubsec:Offline Preference LearningTraining Hyperparams for Scaling DPOsubsec:Training Hyperparams for Scaling DPOsubsec:Supervied Finetuningfig:scaling-dpotbl:eval-inconsistentinconsistentReward Annotation for Data Filteringsubsec:reward_annotationcui2023ultrafeedbackstienon2020learningbai2022trainingArgillastarling2023lambert2024rewardbenchfig:rm-labeled-distscale=0.75images/fast_dpo_implementation.pdfIllustration of efficient DPO implementation. Traditional DPO training requires adding padding tokens to the batch. Our implementation can remove the need of paddding tokens, and thus improving the training efficiency.fig:fast_dpo_implementationCoding     \includegraphics Extraction     \includegraphics Humanities     \includegraphics Math     \includegraphics Reasoning     \includegraphics Roleplay     \includegraphics Stem     \includegraphics Writing     \includegraphics MT-bench performance after training Gemma-2b on upto 10k samples from each dataset. Datasets we used in \Cref are colored in purple.fig:data-eval-mt-bench-dist0.82

    Automatic evaluation metrics such as loss, reward margin, and reward accuracies on test set is inconsistent with final performance on benchmarks such as Arena-Hard-Auto*. All runs used .tbl:eval-inconsistent-5ptOpenLLM Performance     \includegraphics Arena Hard Auto* Performance     \includegraphics Effect of  on model performance across datasets of different sizes.fig:dpo-hyperparam-beta-full         \raisebox         \includegraphics     Data distribution for \framework, which 1) removes DPO pairs with score difference less than two, and 2) sample 10k data that has the highest chosen score in each score difference bin.fig:data-selection-distOriginal     \includegraphics     \includegraphics Predicted     \includegraphics     \includegraphics Score distribution for the dataset we used in \Cref. (a) Responses from Ultrafeedback, Orca-DPO-pairs, and Capybara-DPO already contain scores annotated by GPT-4/GPT-4-turbo. (b) We rescaled the score prediction produced by Nexusflow/Starling-RM-34B.fig:rm-labeled-distMore Details on Dataset Filtering Algorithmssubsec:Dataset Filtering AlgorithmsDeitaliu2024makes-deitaLongestzhao2024longismoreAlpagasuschen2024alpagasusArgillaargilla-dpo-mixA2ArgillaAlpagasustab:dpo_data_statsA2fig:data-selection-dist-fullA2fig:data-selection-distAlpagasus-10k     \raisebox     \includegraphics Longest-10k     \raisebox     \includegraphics DEITA-10k     \raisebox     \includegraphics Argilla-10k     \raisebox     \includegraphics Data distribution after applying the respective filtering algorithms.fig:data-selection-dist-fullMore Details on Training \framework SeriesEfficient DPO Implementationdao2023flashattention2fig:fast_dpo_implementationtab:fast_dpo_training_timesTraining DetailsRasley2020DeepSpeedSOtab:sft_training_detailstab:dpo_training_detailsTraining Datasetssubsec:LION Training Datasetssec:Training Procedure AnalysisLiontab:sft_data_statsstarling2023tab:dpo_data_statsLionDetails of Supervised Fine-Tuning Data  :     The OpenHermes-2.5 dataset contains 1 million diverse, synthetic samples. It includes data from various sources like , CamelAI , , and several others, each contributing to fields ranging from physics and mathematics to code assistance and medical tasks. Please check the  for details.

OpenHermes-2.5OpenHermes2.5https://huggingface.co/datasets/jondurbin/airoboros-3.2?not-for-all-audiences=trueAiroborosli2023camelhttps://huggingface.co/datasets/lmsys/lmsys-chat-1mChatBot Arenahttps://huggingface.co/datasets/teknium/OpenHermes-2.5repo :     MetaMathQA is created using question bootstrapping, where mathematical questions are rewritten from GSM  and Math  dataset. The dataset is further enriched by rephrasing questions and using rejection sampling to select only correctly answered paths, enhancing diversity and reasoning capabilities.

    MetaMathQAyu2023metamathcobbe2021gsm8khendrycksmath2021 :     The SlimOrca dataset is a curated subset of the OpenOrca  data, containing about 500,000 GPT-4 completions refined using human annotations from the FLAN  dataset to remove incorrect answers. 

SlimOrcaSlimOrcamukherjee2023orcalongpre2023flan :     UltraChat is large-scale, informative, and diverse multi-round dialogue dataset aimed at improving language model conversational skills.      It contains 1.5M samples with  a wide range of topics and instructions.

UltraChatding2023enhancing :     OrcaMath comprises 200,000 synthetic mathematical problems created using a collaborative multi-agent setup with GPT-4. 

    OrcaMathmitra2024orcamath :     Capybara uses the Amplify-Instruct method to create synthetic multi-turn conversations from quality single-turn seeds.      It focuses on diverse, logical reasoning across domains, with each conversation exploring deep, diverse topics. 

Capybaradaniele2023amplify-instruct :     Deita is an open-source dataset aimed at enhancing instruction tuning for Large Language Models (LLMs) through Automatic Data Selection.      It incorporates a dataset of 10,000 high-quality, alignment-specific Supervised Fine-Tuning (SFT) data points. This data is primarily selected from larger datasets including 58K entries from ShareGPT , 105K from UltraChat , and a 143K mixture from WizardLM  data .  Deita-10kliu2024whatvicuna2023ding2023enhancingXu2023WizardLMELluo2023wizardcoder0.85

Detailed task evaluation results on OpenLLM.tab:openllm_detailsDetails of Offline Preference Data :      This data is used to train a reward model for summarization. Summaries for the reward model came from the TL;DR dataset.     We use the comparisons data, where annotators chose the better of two summaries.

    TLDRstienon2020learning :     This dataset contains 83.4K preference entries annotated for harmlessness and helpfulness.      Each entry includes two responses to a question, with safety meta-labels and preferences. The responses came from Alpaca-7B, Alpaca2-7B, and Alpaca3-8B models, following SFT performed on Llama2-7B and Llama3-8B with the Alpaca 52K dataset.

    PKU-SafeRLHFsafe-rlhf :     It is open-source Helpfulness Dataset designed by NVIDIA to improve models' helpfulness, factual accuracy, and coherence, with adjustable response complexity and verbosity. It contains 37,120 samples, each including a prompt, a response, and five human-annotated attributes of the response, rated from 0 to 4: Helpfulness (overall helpfulness), Correctness (pertinence and accuracy of facts), Coherence (consistency and clarity), Complexity (intellectual depth), and Verbosity (amount of detail).

    HelpSteerwang2023helpsteer :     UltraFeedback is a diverse preference dataset with 64k prompts and 256k responses from various sources, annotated by GPT-4 for instruction-following, truthfulness, honesty, and helpfulness.      It includes 380k high-quality feedback entries, allowing the creation of 1 million comparison pairs.      Prompts are sourced from datasets like UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN, ensuring broad representation and diversity.

    UltraFeedbackcui2023ultrafeedback :     Nectar is a high-quality 7-wise comparison dataset.      It features diverse chat prompts from sources like , , , , , and .      Responses from models such as GPT-4, GPT-3.5-turbo, LLama-2-7B-chat, and Mistral-7B-Instruct are ranked by GPT-4, resulting in 3.8M pairwise comparisons.

    Nectarstarling2023https://huggingface.co/datasets/lmsys/lmsys-chat-1mlmsys-chat-1Mhttps://sharegpt.com/ShareGPThttps://huggingface.co/datasets/Anthropic/hh-rlhfAntropic/hh-rlhfhttps://huggingface.co/datasets/openbmb/UltraFeedbackUltraFeedbackhttps://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196kEvol-Instructhttps://huggingface.co/datasets/SirNeural/flan_v2Flan:     The DPO dataset enhances Python coding abilities using the validated  dataset for "chosen" responses. "Rejected" values, generated with a mix of  and , are assumed to be of lower quality.

    Py-DPO\urlhttps://huggingface.co/datasets/Vezora/Tested-22k-Python-AlpacaPython-Alpacahttps://huggingface.co/jondurbin/airoboros-l2-13b-3.1.1airoboros-l2-13b-3.1.1https://huggingface.co/jondurbin/bagel-7b-v0.1bagel-7b-v0.1:     The Distilabel-Capybara dataset, created by  addresses the lack of multi-turn open datasets for DPO/RLHF by providing multi-turn dialogue preferences on top of .

Distilabel-Capybara\urlhttps://github.com/argilla-io/distilabeldistilabelhttps://huggingface.co/datasets/LDJnr/CapybaraCapybara:     Similar to Distilabel-Capybara, this dataset is created by       to generate preference labels on top of .

Distilabel-Orca\urlhttps://github.com/argilla-io/distilabeldistilabelhttps://huggingface.co/datasets/Intel/orca_dpo_pairsOrcaDetails of Online Preference Data1mmcui2023ultrafeedbackjiang2023llmblenderPerformance Detailstab:openllm_detailsMore Details on Qualitative Analysissubsec:Details on Qualitative Analysissubsec:DPO Analysis Dataset Selectiontbl:main_expsec:Training Procedure Analysisfig:dpo-seqprob-change-fullfig:dpo-seqprob-change-fullfig:dpo-seqprob-bestfig:dpo-seqprob-undertrainedfig:dpo-seqprob-overtrained