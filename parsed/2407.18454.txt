% Language Models (LMs)~ are advanced language models (LMs)~ with massive parameter sizes and exceptional learning capabilities~.  LMs are computational models designed to understand, generate, and predict human language. These models have played a crucial role in various NLP tasks, such as text generation~, translation~, and sentiment analysis~. The development of LMs has undergone significant changes, from statistical language models (SLMs) to neural language models (NLMs), then to pre-trained language models (PLMs), and finally to large language models (LLMs)~. The Transformer architecture~, particularly its self-attention module, has been instrumental in driving this progress. This module has enabled efficient handling of sequential data, parallelization, and effective capture of long-range dependencies in text. This has led to significant advancements in NLP, enabling LMs to process large amounts of data and generate responses that are more logical and contextually relevant. A notable characteristic of contemporary LMs is their ability to engage in in-context learning~, which involves training the model to produce text that is influenced by a specific context or prompt. This allows LMs to generate responses that are more logical and contextually relevant, making them well-suited for interactive and conversational applications. However, recent studies have highlighted that LMs frequently incorporate unintended social biases and prejudices, reflecting the biases present in their training data and amplifying them in generated content~. These biases can have harmful consequences when LMs are deployed in real-world applications, emphasizing the need for ongoing research and development of methods to identify, evaluate, and mitigate bias in LMs. Addressing these issues is essential for ensuring the ethical and equitable use of LMs in society. % The core component in several LMs is the self-attention module in Transformer architecture~. This module acts as the primary building block for language modeling tasks. Transformers have significantly transformed the field of NLP by efficiently handling sequential data, enabling parallelization, and effectively capturing long-range dependence in text. An important characteristic of current-state LMs is their ability to engage in in-context learning~, which involves training the model to produce text that is influenced by a specific context or prompt. Thus, LMs can produce responses that are more logical and appropriate to the situation, which makes them well-suited for interactive and conversational uses. However, several studies have pointed out that LMs frequently embed unintended social assumptions and prejudices, reflecting biases present in the training data and amplifying them in generated content~. These biases can lead to harmful consequences when LMs are deployed in real-world applications, necessitating ongoing research and development of methods to identify, evaluate, and mitigate bias in language models. Addressing these issues is crucial for ensuring the ethical and equitable use of LMs in society.% {The paragraphs in this section are too many, which makes the structure unclear. My suggestion is to add some markers to clearly distinguish which part discusses intrinsic bias, extrinsic bias, and the prompting paradigm.}

The study of fairness in LMs has garnered substantial attention, revealing that social biases within the models are the primary cause of unfairness. This finding underscores the urgent need to detect and address these biases to guarantee reliable and equitable model performance across various applications. Unchecked biases in LMs can perpetuate harmful stereotypes, marginalize minority groups, and lead to discriminatory outcomes, emphasizing the importance of developing fair and unbiased LMs as a fundamental objective in AI research.

% Research on fairness in LMs has received significant attention. It has been found that the main factor contributing to unfairness in LMs is the presence of social biases within the models. This focus highlights the critical need to identify and mitigate biases to ensure equitable and reliable model performance in various applications. Biases in LMs can perpetuate stereotypes, marginalize groups, and lead to discriminatory outcomes, making the development of fair and unbiased LMs an essential goal in AI research.% {There is a lack of connection between these two sentences. }In cognitive science, social bias refers to the realization of actions and judgments based on prior knowledge, which may be incorrect, incomplete, or obtained from other people.% {Make sure you call large-sized LMs consistently, not larger models or large models or LLMs} Our survey categorizes LMs into two groups based on their training strategies: (1) ; and (2) . The emergence of GPT-3 marked a significant shift in the status of both paradigms with the proposal of various large-scale in-context LMs~ thereafter. Before GPT-3, the pre-training and fine-tuning paradigm was the traditional training strategy. Then, the advent of GPT-3 led to the discovery of large-sized LMs with extraordinary emergent abilities, such as few-shot learning with GPT-4~, LLaMA-1~, or LLaMA-2~. The prompting paradigm replaces the pre-training and fine-tuning paradigm as a more suitable learning strategy for large-sized LMs. Furthermore, there are notable differences in both approach and definition of fairness between these two groups of models.

. In the medium-sized LMs, biases are typically divided into two types: intrinsic bias~ and extrinsic bias~. Intrinsic bias refers to the bias in the representation output by the pre-trained model, which is task-independent since it does not involve downstream tasks. It is also known as upstream bias or representational bias. In contrast, extrinsic bias refers to the bias in the model output in downstream tasks, also known as downstream bias or prediction bias. The performance of extrinsic bias depends on specific downstream tasks, such as predicted labels for classification tasks and generated text for generative tasks.

The effort to evaluate intrinsic bias in these models begins with the Word Embedding Association Test (WEAT)~ and the methodology proposed by Tolga Bolukbasi et al.~. In these approaches, intrinsic evaluations refer strictly to those computed using only the internal state of a modelâ€”essentially metrics over the embedding space~. However, with the advancement of LMs, the notion of  has undergone a significant transformation. The emergence of dynamic embeddings that adapt to context has enabled a more precise and context-sensitive evaluation of prejudice. By learning within sentence contexts and being designed for use with embedding metrics for sentence-level encoders, these models can now be evaluated more effectively. Specifically, they can be used to assess differences in predicted token probabilities or distributions across various social groups, providing a more nuanced understanding of intrinsic bias. This evolution aligns with the capabilities of modern LMs, which are equipped to handle complex linguistic contexts and capture subtle biases in language. % the concept of ``intrinsic'' undergoes a transformation with the use of dynamic embeddings that adjust based on the context. These embeddings are learned within the context of a sentence, making them more suitable for use with embedding metrics for sentence-level encoders. Additionally, employing complete sentences facilitates a more focused assessment of different aspects of prejudice, utilizing sentence structures that inquire about certain stereotypical connections. These models are primarily designed to predict the following word, which can be leveraged for various specific purposes. For instance, the definition of intrinsic bias evolves to reflect differences in predicted token probabilities or distributions across different social groups. This shift allows for a more nuanced and context-sensitive evaluation of bias, aligning the concept of intrinsic bias with the capabilities of modern, context-aware LMs.% In addition to intrinsic bias, extrinsic biases in medium-sized models are classified into different definitions depending on the downstream tasks: Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks.  Beyond intrinsic bias, medium-sized LMs also exhibit extrinsic biases that manifest in different ways depending on the downstream task: natural language understanding (NLU) and natural language generation (NLG). NLU tasks, such as classification~ and natural language inference~, can reveal biases in predicted labels; whereas NLG tasks, such as recommender system~ and question-answering ~, can reveal biases in the generated content. Evaluating and mitigating these biases requires task-specific strategies, including diverse and representative training datasets.

 The traditional concept of intrinsic and extrinsic bias cannot be simply applied to measure bias in the prompting paradigm, particularly for large-sized LMs. This is because the internal representations of most large-scale LMs, especially closed-source models, are not readily available. Therefore, evaluating fairness in these models requires analyzing the model's output in response to different input prompts. This involves examining the model's output for signs of bias based on various prompts. These tasks can be approached from different viewpoints and accomplished through various generative tasks, such as completing prompts, engaging in conversations, and reasoning through analogies. Additionally, different evaluation metrics can be employed, including demographic representation~, stereotypical association~, counterfactual fairness~, and performance disparities~. In this survey, we will explore the concept of fairness in large-sized LMs through these evaluation strategies.

% Overall, our survey will explore the concept of fairness definitions in LMs, examining how these definitions apply to medium-sized and large-sized LMs. By understanding and addressing the unique challenges associated with models, we aim to contribute to a deeper understanding of fairness in LMs. An overview of the fairness definitions in LMs mentioned in this paper is presented in Figure .

We categorize fairness definitions in LMs into two branches based on the LMs that they are applied to, including: (1)  and (2)  as presented in figure . These types of LMs are distinguished by their training strategies: medium-sized LMs typically follow the pre-training and fine-tuning paradigm, while large-sized language models operate under the prompting paradigm. 

In medium-sized LMs, biases are further categorized into two types based on their manifestation: intrinsic bias~ and extrinsic bias~. There are two types of intrinsic bias that will be presented including similarity-based bias and probability-based bias~. Extrinsic bias, on the other hand, refers to biases that manifest in the model's outputs during downstream tasks. The extrinsic bias definitions are further summarized into two categories: natural language understanding (NLU) tasks with text classification~ and natural language inference~; and natural language generation (NLG) tasks with recommender system~  and question-answering ~. In large-sized LMs, further categorizations are based on evaluation strategies designed to quantify fairness in these models, including demographic representation~, stereotypical association~, counterfactual fairness~, and performance disparities~. Overall, this survey explores fairness definitions in LMs according to the proposed taxonomy, examining their application to various concepts, and aims to deepen the understanding of fairness in LMs by addressing the unique challenges associated with these definitions.

% Overall, this survey explores the concept of fairness definitions in LMs per the proposed taxonomy, examining how these definitions apply to different concepts. By understanding and addressing the unique challenges associated with models, we aim to contribute to a deeper understanding of fairness in LMs.% To comprehensively define fairness in LMs, we provide some general notations that are used in our survey. These notations are specifically described in Table . In particular, the socially sensitive topic \(T\), including aspects such as gender, race, religion, age, nationality, etc., encompasses several  (aka ) presented by a set \(G=(g_1, g_2, ..., g_n)\) such as  for gender topic or  for religion topic. A demographic group \(G_i\) can be defined by a collection of sensitive attributes \(A_i=[a_, a_, a_, ..., a_]\), which are also referred to as protected attributes. For instance, the sensitive attributes associated with the demographic group ``Female'' may include \([woman, girl, female, mom, grandma, Kelly]\). Similarly, the sensitive attributes associated with the demographic group ``Male'' may include \([man, boy, male, dad, grandfather, Joseph]\). In the context of LMs, a demographic group can be presented in a sentence that includes its sensitive features.

To establish a comprehensive understanding of fairness in LMs, we introduce a set of general notations that will be used throughout this survey, as outlined in Table . Specifically, we define the concept of a socially sensitive topic , encompassing aspects such as gender, race, religion, age, nationality, and so on. This topic is represented by a set of demographic groups (aka social groups), denoted by , which includes specific groups like  for the gender topic or  for the religion topic. Each group is characterized by a set of sensitive attributes: . For instance, the demographic group ``Female'' might be characterized by the attributes , while the group ``Male'' might be defined by . In the context of LMs, these demographic groups can be depicted as features within sentences.

% In this section, we present the experimental setup used in this paper. We list the models and datasets for the experiment corresponding to each definition used in our survey in Table . The detailed results are presented in their respective sections. By exploring these models and datasets, we aim to understand the manifestations of bias in various contexts and across different training paradigms. To aid others in the community and encourage further development of fairness in LMs, we have made our implementation publicly available online at {https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions}.

This section presents the experimental setup used in this paper, listing the models and datasets for the experiment corresponding to each definition used in the survey in Table~. Detailed results are presented in their respective sections. Exploring these definitions and datasets helps practitioners understand the manifestations of bias in various contexts and across different training paradigms. To aid the community and encourage further development of fairness in LMs, the implementation has been made publicly available online at .

%{I'm not sure whether medium-sized LMs or large-sized LMs should be used consistently throughout the text, and the use of pre-trained LMs and LMs in this paragraph seems odd, since the title of the section indicates that it's an introduction to medium-sized LMs. Decide whether it needs to be changed}

Intrinsic bias, also known as upstream bias or representational bias~, refers to the inherent biases present in the output representation generated by a medium-sized LM under the pre-training and fine-tuning approach. These biases are independent of specific downstream tasks and arise from the vast corpus of data used during the initial pre-training phase. They may manifest by favoring certain words, phrases, or concepts over others, deeply ingrained in the model's parameters as a reflection of the training data and processes.

This section provides an overview of the definitions of intrinsic bias for medium-sized LMs, which are categorized into two main types: similarity-based bias and probability-based bias. These definitions are primarily based on metrics used to evaluate intrinsic bias, which may include statistical measures of distributional similarity, co-occurrence patterns, and other quantitative assessments of the model's output.

Similarity-based metrics refer to biases that arise from the way different words or phrases are clustered or related in the embedding space. For example, if the model consistently groups words related to one gender or ethnicity more closely than others, this indicates a similarity-based bias.  We illustrate an example of similarity-based bias in LM in Figure . In this example, the model is considered biased because its embedding space shows differences in the associations between European American and African American names with the attributes pleasant and unpleasant.

The experimental evaluation of similarity-based metrics is summarized in Table . In this table, we report the overall magnitude of bias in the BERT model~ with the effect size (). All the tests are derived from Caliskan et al.~. %and statistical significance with a combined  (). %  Probability-based bias refers to biases that are evident in the likelihood distributions generated by the model. This can include the likelihood of generating certain words or phrases over others, which can reflect underlying prejudices present in the training data. There are two classes of metrics to quantify this bias: masked token metrics and pseudo-log-likelihood metrics~.

 Masked token metrics compare the distributions of predicted masked words in two sentences that involve different social groups.  An instance of probability-based bias in a medium-sized LM, as depicted in Figure , shows a disparity where  and  are predominantly associated with the male group, while  and  are more frequently linked to the female group. Such outcomes reveal a gender prejudice in these LMs when forecasting the  token for the two groups.

  Pseudo-log-likelihood metrics assess the likelihood of a sentence being a stereotype or anti-stereotype by estimating the conditional probability of the sentence given each word in the sentence. According to Gallegos et al.~,  An example of probability-based bias in a medium-sized LM, as shown in Figure , is evident when the stereotypical sentence  is deemed more probable than the anti-stereotypical sentence .

Our experimental evaluation of pseudo-log-likelihood metrics is presented in Table . The table provides insights into the percentage of examples where the BERT model~ assigns a higher likelihood (pseudo-likelihood) according to each metric to stereotypical sentences compared to less stereotypical sentences. This analysis utilizes two widely used datasets: the CrowS-Pairs dataset~ and the StereoSet dataset~.

Extrinsic bias refers to the disparity in a LM's performance across different downstream tasks, also known as downstream bias or prediction bias~. This type of bias occurs when a model's effectiveness varies for different types of tasks or different demographic groups, potentially leading to unequal outcomes in real-world applications. This section provides an overview of the definitions of extrinsic bias for medium-sized LMs. In these models, extrinsic bias is typically evaluated using benchmark datasets and task-specific metrics~. According to the downstream tasks for pre-training and fine-tuning paradigms, we summarize the extrinsic bias definitions into two categories: natural language understanding (NLU) tasks and natural language generation (NLG) tasks. For NLU tasks, such as text classification or natural language inference, extrinsic bias refers to the influence of biased data on model performance. On the other hand, for NLG tasks like question-answering and recommender systems, extrinsic bias can manifest in the generation of biased language or stereotypes in text generation tasks.

NLU represents a wide spectrum of tasks that aim to improve comprehension of input sequences~. We provide an overview of fairness definitions in LMs across various NLU tasks, highlighting the distinct nuances and perspectives.

 is an important task commonly used to assess bias in LMs . Numerous studies investigate the disparities in text generation accuracy between different racial or gender groups in text classifiers~. Although the evaluation metrics used in these works are largely identical to those applied in traditional ML classification, the consistency in metrics allows for a direct comparison of bias and fairness issues between LMs and traditional ML models in the classification tasks. This highlights that while the methodologies may differ, the underlying challenges of achieving fairness remain similar. 

For instance, Chhikara et al.~ evaluate bias in LMs by adopting seven prominent fairness definitions from ML classification tasks. These notions include Statistical Parity~, Equal Opportunity~, Equalized Odds~, Overall Accuracy Equality~, Treatment Equality~, Causal Discrimination~, and Fairness through Unawareness~.  % Their observations reveal that these notions of fairness perform comparably to specific prompts at an abstract level. {For those who haven't read the paper, they may not understand "abstract level".}  From the experiment result, it is crucial to acknowledge that although LMs guarantee accuracy among different demographic groups, specific measurements such as Disparate Impact, True Positive Rate, and False Positive Rate still reveal a bias towards the female group.

The Bias-in-Bios dataset~ is another significant resource in this field. It consists of third-person biographies that assess the correlation between gender and occupation. The goal of this dataset is to study gender bias in occupation classification. Each biography in the dataset includes explicit gender indicators (such as names and pronouns) and annotations for occupation. Using this dataset, the model is fine-tuned on samples without occupation information. Then, the gender bias in occupation classification is measured by comparing the classification results for different gender groups, as calculated by the equation presented in Equation~. This approach proposes fairness metrics to quantify the difference in True Positive Rates () between binary genders  and  for each occupation .

 where  and  are random variables representing the predicted and target labels (i.e., occupations) for a biography, and  is a random variable representing the binary gender of the biography's subject. 

The idea behind this metric is that the fair LM classifier should have similar performance in terms of  across demographic groups. This means that the classifier should be equally accurate for different gender groups when predicting occupations. If the  gap is close to 0, it indicates that the model does not favor one gender over another in terms of classification performance, thereby achieving fairness in occupation classification. The closer the  score is to 0, the better. In our experiment, we analyzed gender bias using the Bias-in-Bios dataset with the BERT model. The model achieves an accuracy of 0.30 and a  score of 0.00, where  and  represent the male and female groups, respectively. This result indicates that while the model's overall accuracy is moderate, it successfully maintains an equal  for both genders, demonstrating no bias in occupation classification and achieving fairness as per the defined metric.

%{This paragraph has nothing to do with fairness and seems unnecessary to write} Agree%However, one challenge in using LMs for classification tasks is that existing LMs are typically trained and evaluated on large text corpora, whereas many classification tasks rely on structured tabular datasets. This type of data finds extensive use in high-stakes domains~ where information is typically structured in tabular formats as a natural byproduct of relational databases~. However, tabular datasets cannot be directly applied to LMs, necessitating methods to transform tabular datasets into linguistic datasets. This transformation is crucial to ensure the models can effectively process and learn from the structured data, allowing for more accurate and fair predictions. To address this issue, Hegselmann et al. ~ introduces TabLLM, a general framework to leverage LMs for few-shot or zero-shot classification of tabular data. For few-shot classification, TabLLM creates a labeled dataset for fine-tuning procedure by serializing tabular feature names and values into natural-language strings using various methods: manual template, table-to-text, and LLM.%  (CR) Coreference resolution is a task aimed at identifying phrases (mentions) referring to the same entity~. To better identify gender bias in coreference resolution systems, Zhao et al. proposed a benchmark named WinoBias for the intra-clause coreference resolution task, based on the Winograd format. This benchmark evaluates a model's ability to associate gender pronouns and occupations in contexts that reflect both stereotypes and anti-stereotypes. % The WinoBias benchmark includes two types of sentence templates (as shown in figure ...): Type 1 lacks semantics and follows the typical Winograd style syntax, requiring coreference decisions to be made using world knowledge about given circumstances; Type 2 provides some semantics and syntax, expecting the model to perform better. To evaluate bias, the data is divided into two sections: one where correct coreference decisions require linking a gendered pronoun to an occupation stereotypically associated with that gender, and another where linking is required to the anti-stereotypical occupation. A LM passes the WinoBias test if, for both Type 1 and Type 2 examples, pro-stereotyped and anti-stereotyped coreference decisions are made with the same accuracy. (NLI) is the task of determining whether a given hypothesis can reasonably be inferred from a premise in natural language~. The goal is to assess how well LMs perform in analyzing the relationship between premises and hypotheses. For example, this involves evaluating how LMs associate occupations with gender. This evaluation entails analyzing entailment relations in pairs of sentences, where the premise () includes occupation terms and the hypothesis () includes gender-specific terms.  For instance, Bias-NLI~ evaluates the associations between gender and occupation by inferring entailment relations between pairs of sentences. The construction of these entailment pairs follows a specific template: . In this construction, the premise's subject is filled with an occupation word, while the hypothesis's subject is filled with a pair of gender words. Bias-NLI introduces three distinct metrics to access bias, which are: 1) Net Neutral () calculates the average probability of the predicted neutral label across all pairs of entailments; 2) Fraction Neutral () calculates the proportion of sentence pairs that are predicted as neutral labels; and 3) Threshold  is a hyperparameter that indicates the proportion of entailment pairs whose probability of being predicted as neutral is higher than it. In the paper, it is set to 0.5 and 0.7.  and  are defined as the following:

% {In the formula for FN, why is there a 1 after ni} where  is the number of pairs;  are the probability for the entail, neutral, and contradiction labels, respectively;  is the indicator function.

% The primary aim of these metrics is to assess gender bias in NLI models. This assessment is achieved by analyzing how these models associate occupations with gender. This analysis is conducted through entailment relations in pairs of sentences. The premise of each pair contains occupation terms, while the hypothesis contains gender-specific terms. This setup allows for evaluating the model's tendency to predict neutrality. These metrics aim to evaluate gender bias in NLI models by examining how they link occupations and gender through entailment relationships in pairs of sentences. These pairs are constructed with occupation terms in the premise and gender-specific terms in the hypothesis. This approach enables an assessment of the model's inclination toward predicting neutral outcomes. A fair model would exhibit high  and  values, signifying a high likelihood and proportion of neutral predictions. This approach ensures that models handle gender and occupation as separate entities, promoting fairness and independence in their associations.  In our experiment, we analyzed gender-occupation bias using the BERT model. For , , , and , we obtained results of 0.421, 0.397, 0.374, and 0.209, respectively. These results indicate that the BERT model shows a moderate ability to predict neutral outcomes. At  = 0.5, a value of 0.374 reveals a further decrease in neutrality, implying increased bias as the threshold for entailment tightens. Finally, the sharp drop to 0.209 at  =0.7 highlights significant bias, as the model becomes less likely to predict neutral outcomes under stricter conditions. This diminishing trend as the thresholds get higher shows that the model is biased when it comes to how it handles associations between gender and occupation. This shows that even though BERT can often tell the difference between gender and occupation, it still has significant bias, especially when the criteria are very strict. %{In addition to the ability to predict neutral outcomes, also introduce the extent of bias}%{There are many instances of inconsistency between abbreviations and their full forms in the content. For example, NLI and natural language inference are used interchangeably, and other abbreviations exhibit similar issues.}% {This content about BBQ-NLI is too short.}% Furthermore, Akyurek et al. introduce BBQ-NLI, a benchmark for natural language inference aimed at quantifying human cognitive biases across three domains and identifying 16 harmful stereotypes.  Furthermore, Akyurek et al.~ introduce BB-NLI, a benchmark specifically designed to measure and identify human cognitive biases in NLI. This bias benchmark is built upon the BBQ dataset~, which detects biases in QA systems, BB-NLI repurposes the same contexts as premises and converts questions into statements, facilitating bias evaluation in an NLI framework. For instance, a scenario like  is adapted into two potential hypotheses:  and . BB-NLI measures bias by analyzing a model's responses to these hypotheses and assessing how it associates negative behaviors with specific groups. The bias score is defined as follows: 

where  represents the number of  answers for pro-stereotypical statements and  represents the number of  answers for anti-stereotypical statements, and  represents the total number of responses for both types of statements. 

 score ranges from -1 to 1. A score of 1 indicates that the model consistently answers  to pro-stereotypical and  to anti-stereotypical statements, while -1 indicates the opposite response pattern. A bias score of 0 suggests that the model's predictions align equally across all scenarios, implying fairness in its judgments. In our experiment, we analyzed gender, race, and religion bias using the T0 model~ with BBNLI bias scores of 4.49, 12.77, and 13.98, respectively. This result indicates that the T0 model exhibits a notable degree of bias, with the highest bias detected in the category of religion, followed by race and gender.

NLG tasks refer to LMs' downstream tasks in generating specific texts~, which consist of several tasks, including question answering, recommendation, and other open-ended generation tasks.

%  is a task that aims to condense a document while preserving its core information~. There are two primary approaches to text summarization: extractive summarization and abstractive summarization. Extractive summarization involves selecting and copying relevant fragments from the input text, whereas abstractive summarization generates original sentences that may not be present in the original text. In contrast, recent advances in language modeling research have focused on abstract generalization, aiming to generate coherent and informative summaries that go beyond mere extraction of existing text segments~.  % % For abstractive summarization, fairness can be quantified as a distribution alignment between the generated summary and the source text~. The distribution is calculated by the ratios of semantic units of different social attribute values. In this paper, the authors define two types of fairness: (1) Ratio Fairness: the target value distribution \(p_y\) shall follow the same value distribution as source \(p_x\) (\(p_x=p_y\)) and (2) Equal Fairness: the target value distribution \(p_y\) shall follow the uniform value distribution \(p_g = [{r}, {r}, ...,{r}]\), regardless of source.% By adopting this approach, a fair abstractive language summarizer should give the same distribution of semantic units related to social attributes between the generated summary and the source text. This ensures that the summary reflects the proportional representation of these units as they appear in the source (Ratio fairness) or distributes them uniformly regardless of their source distribution (Equal Fairness). {Experimental results}% When the experiment is done, using our own experimental data% {The first sentence lacks citation} (QA) is an essential task in the domain of human-computer interaction, and it has been extensively used in various situations such as search engines~ and chat bot~. To evaluate the bias in these systems, researchers have developed benchmark datasets like BBQ~, which assesses bias in QA models. This dataset includes nine areas of social bias and consists of 58,492 context samples that were manually created to test ambiguity and disambiguation. Each example appears with two questions that reflect a harmful bias: one asks for the target of a harmful stereotype (), and the other asks for the non-targeted entity (). To measure when biased model outputs are likely to manifest, we assess both cases where there is not enough information in the context to answer the question (leading to the correct answer being an expression of uncertainty, such as ) and cases where the correct answer is present, allowing us to test when the biases that we already know are present in LMs override the correct answer. % {I'm having some difficulty understanding this paragraph, so see if you can improve the presentation of this paragraph}. I just added some explanation to the above introduction, you can read it. The authors propose a bias score to measure the degree of bias in a model's responses across two different contexts: ambiguous contexts, which lack the necessary information to answer the questions, and disambiguated contexts, which provide the required information. This score indicates the percentage of non-UNKNOWN answers that align with a social bias, ranging from -100\% to 100\%. Positive bias scores occur when the model outputs the bias target negatively (e.g.,  for ) or the non-target positively (e.g.,  for ). The bias scores for disambiguated () and ambiguous () contexts are defined as follows: 

%{non-UNKNOWN is capitalized, but non-target is not, right?} yes, I get it from the paper where  represents the number of model outputs that reflect the targeted social bias ( the bias target in negative contexts and the non-target in non-negative contexts) and  is the total number of model outputs that are not UNKNOWN ( all target and non-target outputs).

The core concept behind this metric is that a model should be evaluated not only for its accuracy but also for its alignment with societal norms and prejudices. The bias score measures how much the model's responses reflect societal biases by indicating the frequency with which they conform to or oppose these biases.A score of 0\% indicates that the model's responses are completely unbiased, meaning they neither conform to nor oppose societal prejudices. A score of 100\% indicates that the model's responses fully align with societal prejudices, consistently producing biased outputs. Conversely, a score of -100\% indicates that the model's responses consistently oppose societal prejudices. In our experiment, we examined gender bias in the RoBERTa model~ using the BBQ benchmark. The bias scores for disambiguated and ambiguous contexts are 10\% and 14\% respectively. These scores suggest that the RoBERTa model's responses have almost no social bias regarding this metric.

%{I think with the results of this experiment (10\% and 14\%), it can be stated that this model has almost no social bias at this metric. Because I think almost all of the experimental results show that the mod has bias, and none of the experimental results show that the mod does not have bias}%{The object described in the section on recommender systems is ChatGPT, which is a large-scale LLM and should not be in a section on medium-scale LLMs} I moved it to Performance disparity. %  % M. Smith et al.~ propose a more inclusive bias measure dataset HOLISTICBIAS, which contains nearly 600 descriptors with respect to 13 different demographic axes. For example, descriptors for the demographic axis ability are  and  for auditory as well as  and  for mobility. These descriptors are inserted into 26 sentence templates that measure bias to generate 459,758 sentence prompts. % The authors classify the model's responses to the conversation into 217 conversational styles and then come up with two metrics to calculate the bias score: 1) Full Gen Bias () computes the variance between the distributions of conversational styles of responses across descriptors; 2) Partial Gen Bias () computes the contribution of a certain style cluster to the whole. They are formalized as follows: (RS) are algorithms designed to personalize contents or items for individual users based on their preferences~. However, there is increasing concern regarding the negative social consequences of recommendation systems~. Researchers have recently focused on both item-side~ and user-side~ unfairness issues in recommendation systems. Specifically, the fairness analysis of RS can be conceptually classified as user-side fairness, as discussed in~. 

Hua et al.~ introduced the Unbiased P5 (UP5) model to enhance user-side individual fairness in RS by either removing or preserving sensitive user attributes, such as gender, age, and occupation, based on user preference. They proposed the Counterfactually Fair Prompt (CFP) method. For encoder-decoder large-sized LMs, an encoder prompt removes sensitive attributes, and a decoder prompt maintains model performance. According to this paper,  An RS can be considered individually/counterfactually fair if the user's sensitive information is eliminated during the recommendation process, ensuring that the recommendation output stays consistent across different counterfactual scenarios~. We demonstrated an example of the extrinsic bias of LMs in the recommendation downstream task in Figure , where a LM gives dissimilar recommendations for different users. 

They assessed bias in UP5 using the AUC (Area under the ROC Curve) metric to evaluate the involvement of sensitive attributes in recommendations. By using AUC, the authors measure fairness in LM for recommender systems through adversarial learning~. Specifically, they employ a discriminator module to predict sensitive attributes from prompt embeddings. A lower AUC in predicting sensitive attributes from embeddings indicates that the embeddings do not contain information about these attributes, suggesting the model is fairer. Since the AUC ranges between 0 and 1, an AUC score of 0 signifies that the UP5 model has no bias and preserves individual fairness by reducing the influence of sensitive attributes in its recommendations. In our experiment, we analyzed age, gender, and occupation bias in the BERT model using MovieLens-1M dataset~. AUC scores for these biases are 73.35, 78.52, and 64.79, respectively. These relatively high AUC scores indicate that the BERT model retains significant information regarding sensitive attributes such as age, gender, and occupation, suggesting the presence of bias in its recommendations. 

% To assess bias in RecLLMs, the authors use ChatGPT in a greedy-search manner and calculate the similarities between the reference status (which refers to recommendation results without any sensitive attributes in the user instruction) and the recommendation results obtained when specific values of the sensitive attribute are used. By comparing these similarities, the authors can measure the extent of fairness. The researchers came up with two metrics, Sensitive-to-Neutral Similarity Range (SNSR) and Sensitive-to-Neutral Similarity Variance (SNSV), that measure how unfair something is by figuring out how different different aspects of sensitive attribute values are from each other. For the top  recommendation: % %     SNSR(K) = \max_{a\in A} (a)-\min_{a\in A} (a) % % %     SNSV(K) = {|A|}\sum_{a\in A}^{}((a)-{|A|}\sum_{a'\in A}^{}(a'))^2}% % {Whether need to add a' introduction?}% where \(A=\{a\}\) denote a sensitive attribute where \(a\) is a specific value of the attribute.% The main idea behind these metrics is that the model should give recommendation results that are not significantly influenced by sensitive attributes, ensuring that all users receive fair and unbiased recommendations. A model that consistently yields low SNSR and SNSV scores is considered fair, as it demonstrates minimal deviation in recommendations when sensitive attributes are varied. A smaller SNSR indicates that the recommendations are consistent regardless of the sensitive attribute, implying fairness. SNSV, on the other hand, captures the overall variability in these similarity scores. Lower SNSV values suggest that the recommendations are uniformly distributed across all sensitive attribute values, indicating a lack of bias. {Experimental results}% Demographic representation~ evaluation methods assess social bias by quantifying the frequency of demographic word references in the text produced by the model in response to a given prompt~.  uniform distribution), indicating balanced representation across groups.}

Demographic representation~ evaluation method assesses social bias by analyzing the frequency of demographic word references in the text generated by a model in response to a given prompt~.  In Figure , we illustrate an example of the extrinsic bias of large-sized LMs based on the demographic representation evaluation method, where a LM gives different probabilities for a male and female term with the prompt .

Stereotypical association~ method assesses social bias by measuring the disparity in the rates at which different demographic groups are linked to stereotyped terms ( occupations)  in the text generated by the model in response to a given prompt~.  We demonstrate a case of intrinsic bias of large-sized LMs based on this evaluation method in Figure . In this example, the LM tends to link the attribute  for  more than , indicating the presence of gender bias within the model. % % {This paper you also introduced in "Demographic Representation". Here you need to describe why he presented it here again.}

Counterfactual fairness~ evaluates bias by replacing terms characterizing demographic identity in the prompts and then observing whether the model's responses remain invariant~.  A case of extrinsic bias of large-sized LMs based on the Counterfactual Fairness evaluation method is depicted in Figure . In this example, the LM produces different responses to the original and altered prompts, showing the impact of demographic information on the text generated by the LM.

Performance disparities~ method assesses bias by measuring the differences in model performance across various demographic groups on downstream tasks.