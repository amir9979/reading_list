% Being able to reason over multi-turn conversations (i.e. interactions going beyond a single turn) is a critical ability of modern assistants. However, as described above, conversational abilities typically sharply decline at smaller model sizes . As a result, to  Enabling conversational abilities in SLMs through supervised fine-tuning is a data intensive process requiring careful data curation at scale.  % However we do observe that LLMs naturally posses this conversationality that has been historically difficult to enrich in smaller models .  Multiple lines of research, e.g.  recently found that data quality, besides scale, is imperative when aiming to effectively distill language models. % from large LLM teacher models into smaller SLM students.  Building on top of these insights, we present a distillation method to explicitly enrich SLMs with conversational reasoning abilities by introducing three new concepts to generate diverse, engaging and fluent conversations: conversational graphs, turn based prompt augmentations, and explicit linguistic features.  % We separate our contributions to better distill conversational abilities into 3 dimensions, each of which will be elaborated individually in the following sections.% Old version: % One of the main directions of this paper is to provide a better way to synthesize and utilize conversational data for any domain, independent of available multi turn corpora. This is curently a major restriction in the modeling landscape, espeically so for small models, which (as shown in section ) are less general agents with emerging capacities , and hence require to be trained on data as close as possible in the final domain. For conversational data,  where truly conversational datasets are rare and difficult to obtain , being able to train a small language model for conversational scenarios on conversational data is arguably an important prerequisite for successful model training. As such, we present a novel approach to distil conversational SLMs in any domain with single-turn or general language data available. Our approach can be separated into N components, each of which plays a crucial role in the overall dataset synthesis, and will be elaborated individually in the following parts. Synthesizing diverse, yet naturally flowing conversations is imperative when imitating human interactions. Generating valid, diverse and coherent conversation is hence a key challenge for our multi-turn synthesis framework. To to able to make some guarantees regarding conversation validity, we propose a conversational graph generation approach inspired by Markov Chains. An example conversation graph is shown in Figure~. While specific instances of the conversational graph vary based on the synthesis task at hand, the general structure defines a set of ``conversation links'' (visualized as vertices), representing blueprints for prompting conversational turns, connected by transition probabilities (here: edges) used to sample a ``conversational chain'', a sequence of conversation links representing a valid and natural multi-turn conversation from the graph. 

In general, for a specific task (e.g. grounded question answering), a conversational graph  with vertices  (defining ``conversation links'') and edges  (representing transition probabilities) is defined. To generate a question-answering conversation using , we sample from the set of valid edges (e.g. \{,  and \} if ) and instantiate the sampled target link (e.g.  in Figure~). In the next step, we now sample from \{, \}, since ).  Once we reach the defined conversation length  (e.g. 4 in Figure~), we end the graph traversal and return the valid and diverse conversational blueprint for teacher model synthesis.

% Please note that while Figure~ shows a complete graph with connection weights between any two conversational links, specific weights can also be set to , not allowing unnatural transitions.% [og version] With the conversational graph ensuring dialogue consistency and diversity at the interaction level, we next introduce one of the key concepts of conversationality of individual turns: Conversational phenomena, tying together subsequent user-agent turns in a conversational manner.% Utilizing the conversational graph, we ensure dialogue consistency and diversity at the interaction level, an important component to create high-quality conversational generations. % Next, we dive deeper into the ``turn-by-turn'' conversational generation process, where we explore the role of intra-turn link generations and the concept of conversational phenomena application to enhance the overall synthesis quality.  Once a valid conversational chain is sampled from the graph according to the transition probabilities, the links are executed in-order (e.g., in Figure~: ), with an optional context prepended to the sequence for grounded tasks (e.g. grounded reasoning). While the conversational graph defines the ``macro'' level conversation structure, the individual links define the conversational turns themselves. Each link  % does thereby not just represent a shallow conversational turn, but contains link-specific information. These include,  thereby contains: (1) A link-specific prompt to steer the conversation and (2) potential seed data for the current step (see Figure ). The prompt is mandatory in every link and should ideally utilize methods to enhance the distillation quality, such as ``Chain-of-Thought'' reasoning steps (CoT, , not shown in Figure~). The prompt can optionally (depending on the link itself) utilize additional seed data samples to support diversity in the conversational chain or, in cases where auxiliary data is required, directly use external information (e.g., context). The role of the prompt and seed are configurable depending on the link and present the primary means to enable explicit conversational phenomena in the conversation. % , which will be described in the next section.

Figure  shows the flow of data in a single generation step, where the prompt template, originating in the Link, is filled in by the conversational context and diversified by an optional data point. It is then used to generate a new conversational turn through the teacher ``DataGen LLM''. The newly generated conversation turn is then added to the stored conversation and the process is started over with the next link in the conversational chain.

% Looking back at the example graph we generated in section , we now instantiate the links, fill in the prompt with the current document  and generate a user-agent turn. E.g.: % % NewConverationLink(prompt=``Look at this document: {DOCUMENT}. % Generate a question-answer pair a layman user would ask and an AI system would answer''% .format(DOCUMENT=get_document())% % After the LLM has generated a first set of user-agent conversational turns, the second sampled Link  can now use this prior context.% As discussed in sections~ and , we use conversational graphs to ensure high-level consistency of conversational traces and per-link prompts and seeds for intra-turn diversity. However,  Up to this point, our generations could still result in a sequence of independent, single turn utterances. Inspired by everyday conversations between humans, we use explicit linguistic phenomena to naturally tie turns together. Figure~ visualizes this core principle behind the approach for our running example using coreference to refer back to a prior conversational turn. This way, we explicitly tie together entity mentions in the context and synthesize semantic follow-ups. Figure~ shows an example of a synthesized multi-turn conversation based on a CoQA context document.

% In our example, this means that using the conversational phenomena, we are now replacing the sampled link  with a semantic follow-up, for example:% % FollowUpLink(prompt=``Look at the last conversation turn: {CONVERSATION}. % Generate a followup question-answer pair a layman user would ask and an AI system would answer % about one of the main entities.''% .format(CONVERSATION=get_conversation().get_last_turn())% 

This results in our final, steerable and diverse conversation synthesis framework proposed in this paper.

% [h]%     \centering%     \includegraphics[width=\linewidth]{img/cria_paper_fig_conv_phenom.png}%     %     % % Given our conversational graph generation, the intra-turn variance and the conversational phenomena application, we sample data for the LLM distillation synthesis at different scales and from different seed data. % Our \methodname~framework aims to improve small-scale task specialist models through conversational distillation, supporting assistant-style on-device use-cases on wearable and mobile devices. To enable Small Language Models (SLMs) to acquire conversational abilities, we introduce a new data format, which adds additional flexibility to the otherwise limited set of roles in common chat templates, such as the Llama format . Specifically, we allow for an arbitrary number of conversational roles. In the most basic case, a conversation contains two roles, a  and an  (where even this naming convention is flexible) as shown below: % The  role thereby traditionally represents the current and/or previous queries or instructions from a user towards the model, while the  represents the user facing output of the model interaction. This basic ,  interaction is shown below:

Besides the core roles of  and , the  role is a common third role, sometimes also referred to as the  role. The  represents any contextual information the model requires to reason over, which can range anywhere from an instruction to external knowledge in the form of documents, (external) conversations, structured representations, or additional modalities (e.g., images). The  generally plays a crucial role as the interface between on-device information and the model, as shown below: % A simple , ,  interaction is shown below:% Further common roles are  or .

The second advantage to our approach is Role Weighting, which refers to the ability to adjust the training loss depending on the role. For example, the primary responsibility of an assistant-style model is to generate meaningful  turns based on a context. To emphasize this behavior, learning the  role should be the primary objective during model training.  % As a result, we want to focus on the agent-loss in most cases. However, introducing additional (intermediate) roles, we might want to revisit these weights.  In general, having the ability to control the loss based on the underlying role allows us to gain better control over the learning process, leading to more well defined models.

% % As a result, obtaining the maximal possible benefit from the automated \methodname~approach, the scaling factor plays a crucial role. Generating a few tens of samples might also be possible through human annotations, however, when aiming for thousands or million of datapoints, this becomes infeasible, and automated methods, such as our new approach here, become imperative. To evaluate our distillation approach for small-scale conversational agents we choose the task of grounded reasoning for a variety of reasons. First, targeting smaller, specialist language models at on-device size, a natural focus is put on the models interaction with on-device context (e.g. summarize notes on-device). Thus, a knowledge grounded task is a natural choice. Second, given on-device limitations (which restrict usage to SLMs),  % (especially so compared to full-sized LLMs with tens to hundred billion parameters) we can not expect the model to reliably retain large amounts of information. This is evident when evaluating SLMs on knowledge intensive tasks, e.g., MMLU . To this end, a grounded agent offers a more attainable goal. We therefore focus on question answering centered scenarios, where users interact with the system and expect truthful (i.e. grounded) responses to their requests. 

% MM:% A natural extension of the grounded reasoning task described above is the augmentation of the on-device assistant to additional modalities. This allows the model to not only ground the conversation in the textual context and conversation history, but further utilize available multi-modal information, such as images  % Our \methodname~framework enables us to leverage LLMs to generate conversational multi-turn data, a traditionally rare data type, due to the resource-intensive generation procedure using human annotations. Specifically,  In this paper, we use two main models:\\  is the 70B Llama3 instruction tuned checkpoint, one of the most capable open-source, instruction tuned large language models to date. \\   is a pre-trained 1.4B Llama2-style model. % with 24 layers, each containing 16 self-attention heads. %  in this paper is the original CLIP ViT-L model %  used to align the vision encoder outputs with the backbone SLM is a 6-layer MLP module, inspired by . \\

We use additional model sizes and architectures for ablation experiments and comparisons. Specifically, we further employ the 70B Llama2 instruction tuned checkpoint as an alternative teacher model . As an alternative student model, we use a pre-trained 500M Llama2-style model. As instruction-tuned baselines, we explore 7B , 1.4B and 500M Llama2-style models as well as Phi-3 . Further baselines are taken from the literature and directly cited in the relevant sections.

We utilize two sets of datasets in this work: First, to generate diverse data for our ~distillation approach, we present our synthesis datasets in section~. Subsequently, we describe our evaluation datasets to compare our distilled models against supervised and zero-shot baselines in section~. Lastly, section  describes the dataset metrics used for our comparisons.

To evaluate the grounded reasoning abilities of our ~distillation approach, we explore two synthesis scenarios: intra-domain and zero-shot.

We aim to compare our distilled models (based on purely synthetic conversations) against gold datasets, using the same set of available documents during training (see in-domain contexts (blue) in Figure ). Specifically, we use two common multi-turn grounded question answering tasks: % , sometimes also referred to as ``Machine Reading Comprehension'' (MRC):  Conversational Question Answering (CoQA)  and Question Answering in Context (QuAC) . Both tasks test the ability of a system to generate responses to a query based on a given context and conversational history. The dataset statistics are shown in Table~. While both datasets target conversational question-answering, CoQA answers are generally in a factoid-style (i.e. short and precise), while QuAC responses are more elaborate and, as a result, longer. We call our intra-domain distilled model .

As shown in the bottom right corner in Figure , in this setting, we use web documents as the context to reason over in question answering style.  % In comparison to the previously described intra-domain synthesis setting, this approach does not introduce any new ``domain knowledge''. With the automatic nature of our approach to constructing conversations, we are only limited by the amount of available seed data to ensure data diversity. As a result, we can leverage large-scale data collections to effectively scale our ~approach across several orders of magnitude. To show the effect of scaling up our data distillation approach, we use readily available web data, hence calling this version of our models % Figure ~ shows a schematic on the two-stage process of first synthesizing the data (as described in section ~) and the student model (SLM) training, described in section ~ % % MM:% % While we do not synthesize additional data for the visual domain and its alignment, we use popular visual captioning datasets to align the vision encoder (CLIP ViT-L ) with the fully \methodname~trained SLM. Specifically, we align the model using a mix of the Coco , Flickr , TextCaps , ConceptualCaptions , as well as internal datasets. We evaluate all models on two human test sets provided as part of the CoQA  and QuAC  corpora. We utilize the original context as well as the complete grounded question-answering conversation, akin to the human multi-turn training set shown in the top left in Figure . % Figure  shows an overview of the synthesis and evaluation combinations of our distilled models. % MM:% In the multi-modal scenario, we evaluate the visually grounded question answering performance on the VQA , VizWiz , and VisDial  datasets.  As a held out, zero-shot evaluation scenario, we show results on the (grounded) abstract summarization task based on two popular summarization datasets: CNN/DM  and XSum .

Due to the nature of LLM synthesized conversations, our generations are more aligned with the long-form responses in the QuAC dataset, than the factoid style CoQA answers. For this reason, we decide to  % pick a different target metrics for the two evaluation corpora. Specifically, we  opt for the less strict recall metric for the CoQA dataset to not overly penalize results based on their output length.  % For example, a common style discrepancy between LLM synthesized text and the CoQA response style we don't want to punish is a short elaboration/explanation on the decision, e.g., a model response of ``yes, he did regret it'' instead of a simple ``yes''. This is especially in line with our aim to generate natural and human-like responses, as compared to short factoid style answers. To ensure that model candidates don't exploit the recall metric by generating excessively long responses, we supplement our results with additional response length statistics. Given the similar human-like style of QuAC gold answers and our generations, we present the common F1-scores here.

Given our focus on conversational grounded abilities, we further evaluate all models in two distinct scenarios: (1) The standard approach, assuming a gold conversation history for every turn in the conversation and  % This way, we evaluate the ability of the model to correctly predict a single agent response under the assumption that all previous conversation turns have been correctly answered and  (2) a more realistic scenario, in which we evaluate the ability of the model to predict the conversation as a whole by using prior SLM predictions as the conversation history. % , meaning that instead of using the gold conversation history to predict each turn by itself, we use prior turn predictions in the history. % This setup defines a more difficult task, given that prediction errors in early turns are propagated throughout the conversation. For a more complete picture of the conversational performance of our models and baselines, we present both evaluations with our main results. % Besides the traditional metrics to evaluate CoQA and QuAC performance, we further employ an LLM-based evaluation to abstract from syntactic similarities inherent to the recall and F1 metrics. Specifically, we compare our generations on the CoQA and QuAC datasets against gold labels using the 70B Llama3 teacher model.% We evaluate our \methodname~distilled models against a variety of baselines: This baseline uses the in-domain, single-turn, human-annotated question answering dataset. To obtain this baseline, we use the original multi-turn human-annotated dataset and remove all turns past the first interaction (see top right in Figure ).  % Comparing our synthesis approaches against this baseline, we aim to gain intuition about the role that conversational generations play to learn meaningful responses.  We expect multi-turn datasets to exceed the performance of this baseline on conversational tasks. 

 Similar to Single-Turn Human annotations, this baseline consists of the original, human multi-turn conversations provided as part of the CoQa and QuAC datasets (see top left in Figure ). This baseline is a super set of the setup described above and predicted to   % Compared to the single-turn baseline, we expect the performance of in-domain multi-turn trained models to  be significantly better for tasks that required conversational reasoning. % , given the data is (1) human curated and (2) stylistically aligned with the evaluation datasets. %  A common alternative to task specialist models are instruction-tuned checkpoints. Given that these models are solely prompted to solve a specific task (e.g., as done in ), instruction-tuned models are an important alternative to task specialists in domains where training data is sparse or non-existent. We use a range of instruction-tuned baselines at different parameter sizes to verify the benefit of our ~framework.

% Please note, that all baselines and datasets we synthesize are trained on the same set of student models using a homogeneous set of hyper parameters.% MM:% % % After the student model is trained using our \methodname~distillation approach, we further align the frozen text-only model with general vision abilities  in order to show that the conversational features learned in the text space also transfer to the vision modality. Given the natural structure of the conversational grounded reasoning task, consisting of a textual context and a conversation around it, the structure can be naturally extended beyond the text modality. Along this line, we augment our model with additional, non-textual modalities (specifically: images) by adding a modality encoder and an aligner module to our language model. The aligner module thereby projects the encoded images into the input space of the SLM during alignment and inference. Focusing on the image modality, the architecture is heavily influenced by the AnyMAL architecture , designed at the 70B parameter range. Similarly, we use a frozen pretrained modality encoder and our previously trained, conversationally grounded small language model. We then train an aligner module on the captioning task in order to learn the mapping between the image modality and text embeddings. To mimic the zero-shot settings, we do not use any visual QA tasks in the alignment step, but rely on our language model to impart the conversational abilities. This experiment targets one of the most important properties of our ~distillation framework: the ability to synthesize data at scale. While small-scale, human-annotated datasets exist, producing them is resource-intensive. Using , we can synthesize large amounts of diverse conversational data across multiple orders of magnitude. In the ablation experiment in Table  we show the influence of the number of synthesized conversation on the model performance. The trend across orders of magnitude is clear: larger synthesis scales improve the performance near linearly up to 1 million samples.

This ablation explores the impact of different student model sizes on the conversational question answering performance. We compare the Llama2-like 1.4B student checkpoint used in the main results (using the full dataset) with a Llama2-like 500M student checkpoint. In Table  we observe (as expected) a clear quality regression when moving from 1.4B parameters to the 500M size. However, compared to the human-annotated training datasets (see ``Human multi turn''), our distilled zero-shot model still only slightly under performs the model trained on human multi-turn data.

This experiment explores the impact of the teacher model synthesis quality on the final distillation performance. We compare the 70B instruction-tuned Llama2 and Llama3 models. In Table  we see a clear quality improvement from using the Llama3 teacher model, showing a 2\%+ absolute performance improvement of the distilled student models when trained on Llama3 synthesized conversations.

% % As presented in our main evaluations as well as the ablations, we observe a consistent performance gap between the gold and predicted conversation history. Despite the gap being present in all evaluations, we observe that the gap is generally smaller in our conversationally distilled models, compared to both, the single and multi-turn approaches, pointing towards a more consistent conversation trajectory across turns of our model. In this exploration, we cover an important dimension to better understand our conversational distillation system for grounded reasoning: The per-turn performance on the CoQA evaluation dataset. % to gain a better understanding on how our model performs on different parts of the conversation.  Specifically, we compare the per-turn test set performance of the gold single-turn, gold multi-turn and our full (1M samples) trained models. In Figure  we find that our ~Web model is on par with the gold multi-turn trained baseline for short conversations (three or less turns). For longer conversations, our approach near-consistently outperforms the multi-turn baseline. The single-turn baseline performs consistently worse, with the performance gap generally increasing in later turns.

% %  We further evaluate our final model (based on the full 1M training process) on a commonly used subset of language model evaluations (LM evals) for small language models (e.g. used in ). Namely, we evalute the performance for ARC-easy and -challenge , BoolQ , PIQA , SIQA , Hellaswag , OBQA  and Winogrande . The goal of this evaluation is to show the performance of our grounded reasoning based distillation approach on this diverse set of language understanding tasks compared to pre-trained and instruction-tuned model alternatives. As shown in Table , the ~Web trained model on average still under performs instruction-tuned versions of the same base model, however, can improve over the pre-trained checkpoint, without any prompt adjustments during the evaluation. We believe that not regressing, yet even slightly improving the language model evaluation, shows promise of grounded reasoning tasks to act as rather generalist models.

In this section, we present a small-scale human evaluation to confirm the validity of our evaluation metrics. We ask the human reviewers to rank three responses in the context of a textual document and the prior conversation (randomly taken from the CoQA test set). From the response ranking, we retrieve three binary win rates: (1) between our ~Web predictions and gold labels, (2) comparing the human multi-turn trained baseline and gold labels, and (3) selecting our ~Web predictions or the human multi-turn trained baseline. Table  shows the results of the human evaluation on 20 distinct documents containing 284 question answer rankings. We can see that there is a large overlap of equally good responses between our ~distilled models and both, the multi-turn baseline and the gold answers.  %In addition, our method shows a slightly increased win-rate compared to the gold label than the multi-turn baseline.  The results of the small-scale human evaluations  % shows promising performance of our novel approach,  further validate our findings using traditional metrics. % on the CoQA and QuAC datasets. Lastly, we explore another  zero-shot scenario in the realm of grounded reasoning tasks: abstractive summarization on CNN/DM  and XSum .  % Given the similarity in the task of abstractive summarization and grounded question answering, we believe that this is a reasonable zero-shot task to explore for sub-ten-billion parameter models. % Along those lines,  Table  shows the zero-shot results obtained from instruction-tuned 1.4B and 7B Llama2-style baseline models, as well as the 3.8B Phi-3 checkpoint compared to our full ~Web distilled model. We further show two supervised fine-tuned model comparisons to put the zero-shot performance into perspective, a fully fine-tuned 1.4B Llama2-style model and a competitive BART-Large model . Looking at the results, we find that ~Web outperforms both Llama2-style instruction tuned baselines, at the 1.4B and 7B scale. While the Phi-3 checkpoint outperforms our model on the CNN-DM dataset, the delta is small given the significant size difference between the models. Comparing the models on the XSum dataset, ~Web outperforms all other models, despite their significant size advantage.

% % % % How does the scale of the vision encoder impact our tasks