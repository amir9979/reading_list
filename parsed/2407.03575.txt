MIL has been widely applied in many fields, e.g., pathology~, video analysis~, time series~. In particular, the applications of the MIL in Whole Slide Image classification can be roughly summarized into two sub-categories: i) instance-based MIL~ and ii) bag embedding-based MIL. Instance-based methods typically require the propagation of the bag-level label to each of its instances to train the model. Consequently, the final bag-level prediction is obtained by aggregating instance-level predictions. However, empirical studies have proven its performance inferior to the embedding-based competitors because of the noisy instance-level supervision~. In contrast, bag-embedding-based methods start by projecting instances into feature embeddings and subsequently aggregate the information of these embeddings to obtain the bag-level prediction. Since the introduction of attention-based MIL (AB-MIL)~, the prevailing applications of bag embedding-based MIL in WSI analysis have revolved around this framework. However, AB-MIL operates under the assumption that all instances within a bag are independent and identically distributed while failing to uncover inter-instance correlations. Therefore, numerous of its follow-up works centered around mitigating this limitation by taking advantage of non-local attention mechanism~, transformer~, pseudo bags~, sparse coding~, and low-rank constraints~. 

Most existing mainstream MIL methods have modeled correlations mainly through similarity between instances. However, they did not consider the variability of instances between and within bags. Conversely, clustering/prototype-based MIL employs attention scores for selecting prototypes~, potentially introducing noise and misleading model decisions~. Unlike attention-guided methods, PMIL  suggests a two-stage framework that first leverages clustering to identify reference prototypes and capture the sub-cluster representation among patch instances and prototypes. However, unrestricted optimization in prototype selection can easily lead to suboptimal outcomes, and a limited number of prototypes can result in a loss of diversity (limited by computational resources). In this paper, we explicitly model the diversity among instances in bag-embedding-based MIL through a learnable global representation. Although the proposed method falls into the category of transformer-based MILs, it differs from the previous transformer-based MILs~ in two main aspects. First, we model the diversity between instances by comparing instances to the proposed global vectors via a cross-attention mechanism. Second, we propose a tokenized global vector to summarize the context information of positive instances. 

 The transformer~ has been widely applied in computer vision~, time series modeling~, and the natural language processing fields~. Standard transformers discover contextually relevant information by modeling the correlation between elements within a sequence through the self-attention mechanism. However, the traditional self-attention operation has quadratic time and space complexity , with respect to a sequence containing  elements. In the context of MIL, sequence length typically becomes quite large since one bag often approximately comprises ten thousand instances. This extremely long sequence poses significant computational intractability. Although  demonstrate that proper approximation of standard self-attention can reduce its quadratic complexity to linear, it still struggles to capture extremely long-term dependencies of context . In contrast, the cross-attention mechanism~, which was originally proposed to relate positions from one sequence to another, allows models to consider cross-sequence information. Inspired by this, we  propose to model the diversity between and among instances through a cross-attention between instances and the proposed global vectors (see details in Section~). This dramatically reduces the complexity compared to the self-attention mechanism (see Appendix C for details of model complexity) since the number of global vectors is significantly less than the sequence length.

To accommodate the variability of the target lesions within and between bags, we develop a diverse global representation in the MIL pooling stage. Specifically, we define the global representation of the target (positive) instances as a set of learnable vectors given by  % 

with  where  is the number of global vectors. It is worth noting that a feed-forward network (FFN) is used to embed further both the input instance vectors  and the global vectors  (see Fig. ). However, we keep using  to denote global vectors for notation brevity.   

% To capture the global representation of positive instances, we defined a learnable parameter  as the global representation. The  is the number of global representation instances, and  is the embedding feature dimension. The  extracted patches project to instance features  after through the instance projector. We feed  and  to a multilayer perceptron layer (MLP) for reducing dimension to , which obtained ,  respectively.  The standard AB-MIL framework assumes the instances are independent and identically distributed while overlooking the correlation effect between instances. Hence, the self-attention mechanism becomes a natural choice for modeling the inter-instance correlation. However, due to the large number of instances within a bag in MIL, the quadratic time and space complexity  of standard self-attention poses a significant challenge in computation. Alternatively, the previous transformer-based MIL~ mitigates this problem by employing Nystrom-Attention~, approximating the standard self-attention with linear complexity, which has proved effective of modeling correlation between positive and negative instances. It could be used to gather similar instances together by attention, benefiting from filtering background information. However, self-attention usage only guarantees the general separation of the positive and negative instances in a bag, which overlooks the diversity between instances and between bags. 

Here, we implicitly model the diversity between instances by comparing the similarity between each instance vector and the proposed diverse global vectors. Specifically, this is achieved through a cross-attention mechanism where the global vector  serves as queries, and a bag of instance vectors  is used as key-value pairs. Formally, the -th head of the proposed cross attention is given by

where , ,  are learnable parameters for linear projections, where  is number of heads. For the derivation purposes, we follow the traditional definition of the attention mechanism in the transformer (i.e., ). The output of the yielding multi-head cross attention (MHCA) is the concatenation of the outputs from all heads through a linear projection: 

where  is a trainable parameter. The proposed cross-attention mechanism reduces the quadratic time and space complexity  in the standard self-attention mechanism to linear  where . In practice, we applied the Nystrom-Attention to the instance vectors and global vectors before performing the cross-attention (see Fig. ) for two main reasons. First, applying self-attention to input instance vectors can facilitate filtering out the background. Second, applying self-attention to the global vectors can increase their discrepancies. 

% In WSI, notwithstanding the application of preprocessing to extract patches~, the presence of instances with background information persists. Moreover, medical experts typically took into consideration contiguous regions to diagnose, primarily due to the positive patches having shared tumor morphologies~. The self-attention is a good choice to build the correlation between instances by querying the similarity. Similar to the , we utilized the multi-head self-attention (MSA) to filter the background and enhance the lesion instances. It is worth noting that we did not use the position encoding. The MSA can be represented as % % %     MSA(Q,K,V)= Concat(head_{1},...,head_{h})W^{O}\\%     where \ \ head_{h} = Attention(QW_{h}^{Q},KW_{h}^{K},VW_{h}^{V})% % % % here  was -th head output, and the linear projections are parameter matrices  and . The attention applied to the scaled dot-product to obtain, present as:% % Attention(Q,K,V) = softmax(}{}})V% % where  is the scaling factor, which equals the key dimension. The vanilla self-attention required huge memory and calculation resources. To overcome this, we employed the linear time complexity self-attention~. We fed the instances embedding feature  as ,  and  to , and the enhanced instance feature can denote that:% % I' = MSA(I,I,I)% % After going through the MSA layer, we obtained the enhanced feature , which given more attention weight for similar instances. It also means irrelavent negative instances would separate from positive instances(e.g., normal and background patches).     The vision transformer includes a class token to encode the globally discriminative representation associated with certain labels in image classification tasks. This token is typically added to the input token embedding by serving as a summary of the entire image. Building upon this inspiration, we propose to add a tokenized global vector  as a summary of all the other global vectors. Now, the yielding global vectors can be denoted as  . The output of the tokenized global vectors after the cross-attention layer (Eq.()) is then used for bag-level classification. Following the convention in AB-MIL, the yielded importance score of each instance can be computed as 

At first glance, adding the token to the global vectors instead of the input instance embedding appears counterintuitive. However, an in-depth analysis reveals its favorable properties. The proposed global vectors are learned in an unsupervised way (see details in Section~), which poses a significant challenge in perfectly eliminating information from negative instances in the global vectors.  This may be attributed to the similarity between positive instances and their adjacent negative instances, as tumor-adjacent regions typically exhibit high-density, quantitative expression in the spatial relationships of cells~. Each diverse global vector encapsulates a collection of analogous tissue features. As a result, certain global vectors emphasize certain types of positive instances. Accordingly, adding tokenized global vectors facilitates the model to capture the most discriminative global representation while suppressing the information from the negative instances (as evident in Fig. (b)).  

% The global representation  aims to learn global positive instances, which remains a challenging task in the absence of instance-level labels. % Despite the imposition of various constraints, as discussed in Section~ diversity loss and positive instances alignment, G is still treated as an approximate global positive instances representation.% The main reason is the inherent disparities among positive instances (patches), and positive instances are similar to corresponding neighboring negative instances. Additionally, distinctions manifest within negative instances and instances present in normal WSIs. The diversity property of the global instance representation resulted in simpler  % `Here, we concatenated a learnable token   to global representation  , and obtained the global representation with token . We enable the token to learn and capture the most discriminative feature representation through the self-attention mechanism. Same with Equation~, we adopt the MSA to implement this: % % GT' = MSA(GT,GT,GT)% % Here, we employed the global representation with token  as the , , and . Tokens employ self-attention to distill representative positive representations while filtering out background and false positive instances of information within the global representation. Another advantage is that the attention weight enabled the gathering of context information from other representations, which enhances the lesion information within the global representation. The MSA output the enhanced the global representation  included distilled Token  .% %  To build the correlation between the instance feature  and global representation with the distilled token . Similarly to previous work~, the MSA could apply   and  from different sources (e.g. source encoder and target decoder), which aim to learn different relationships between the target decoder and various parts of the input features. Here, we treated the global representation with the distilled token  as the query and the enhanced instance feature  as . Formally,%  %     P'= MSA(GT',I',I')% % % where the MSA output was a global representation aware feature , it is worth noting that the attention map can be denoted: % % A = softmax(^{Q})(W_{h}^{K}K)^{T}}{}})% % where  , N was number of instances,  was the number of global representation and token. The k+1 attention maps correspond individually to  instances with respect to distinct global positive representation-aware activation maps. As we discussed before, the token distilled the most discriminative feature from global representations. So, we extracted the enhanced token  from the aware feature , which would be used to predict  through classifier . The cross-entropy gave the classification loss% % _{ce}=-y \cdot \log \sigma\left(y'\right)+\left(1-y\right) \cdot \log \left(1-\sigma\left(y'\right)\right)%  Due to the weakly-supervised nature of MIL, how to learn the global representation of the target of interest remains an open problem. In this section, we introduce two strategies that can be used to learn a reliable and diverse global representation in MIL, respectively: i) positive instance alignment and ii) diversity learning via utilizing the linear algebra property of the DPP. % Under the context of lacking instance-level labels, it is challenging to learn the global positive (tumor) instance representations. To overcome this, our proposed positive instance alignment and diversity learning constrain the global representation. To enforce that the global representation aligns with the instances of interest (i.e., positive instances), we push the global vectors toward the positive bag centers but away from the negative bag centers. To do so, we first define the center of the positive and negative bags as  and , respectively. Similar to~, the positive and negative centers are then updated in a momentum fashion at each training iteration:

where  denotes the momentum update rate, which is set empirically to .  and  are the index sets of positive bags and negative bags, respectively. This indicates that the update of the positive instance center occurs only if a positive bag is fed into the network. The same strategy is applied to the negative center update (i.e., updated if and only if a negative bag is encountered). Up to now, we can formulate a set of triplet .  The triplet loss~ is then adopted to enforce the global representation  being close to the positive bag center while away from the negative bag center:

where  is the margin parameter, and  denotes the distance measure. We use cosine similarity as the distance measure.

Although the positive instance alignment mechanism pushes the global representation to be aligned with the positive bag center, it is likely to result in a trivial solution where all the global vectors are identical. However, a diverse global representation is desired to capture the variability of positive instances. Hence, we propose our unique diversity loss inspired by DPP for data selection to maximize the diversity among global vectors and hence better summarize the instances.  %known for promoting the diversity of points within a subset, DPP is a well-known diversification tool  and is often used to select diverse subsets . Inspired so, rather than use it for selection, we utilize it as a diversity measurement. % We propose our unique DPP loss to maximize the diversity among global vectors and hence better summarize the instances. % we propose a diversity loss to diversify the global vectors.% Hence, leveraging the DPP, a stochastic point process,% %known for promoting the diversity of points within a subset,% we propose a diversity loss to diversify the global vectors.% To address this challenge, we leverage the DPP, which is a stochastic point process for facilitating the diversity of points within a subset~, to propose a diversity loss. 

Mathematically,  is an L-ensemble DPP if the likelihood of an arbitrary subset  drawn from the entire set  satisfies:

where  denotes a submatrix of the similarity   indexed by . In the case of prompting diversity of global vectors , the similarity matrix is given as , we simply set  and each global vector  is treated as a data point, and the total number of subsets can be calculated as . It is worth noting that the matrix  is positive semi-definite. 

Lemma~ immediately implies that a diverse subset is more likely to span larger volumes. This is because as the similarity between two data points (i.e., ) increases, they will span fewer areas (see Fig. (a) and (b)), hence decreasing the probabilities of sets containing both of them (see Eq.()). Accordingly, feature vectors that are more orthogonal to each other span the largest volumes  (see Fig.(a)), hence resulting in the most diverse subsets. 

According to Theorem~, we propose a diversity loss  to diversify the proposed global vectors by minimizing the negative logarithm of :  % For arbitrary vector ,% %     x^\top^{\top}x%     = (x^\top)(^{\top}x)%     =(x^\top)^2\geq 0% % Therefore,  is P.S.D.% Remark~ is also important to stabilize the training because the diversity loss  can be arbitrarily small (up to ) without the constraint  (see the proof of Theorem~). We also add a small value  to prevent the logarithm of the determinant from being negative infinity (i.e. any two global vectors become collinear). The final diversity loss is given as 

where  denotes the identity matrix. It is noteworthy that the complexity to compute the loss is approximate , which is negligible (see Appendix D).

% i) low complexity: for a global vector , , where the main overhead is an SVD decomposition of  to get , resulting in a complexity of  due to  is often set a small number (e.g., 5)  The proposed MIL model is trained in an end-to-end fashion by jointly optimizing the weighted combination of cross-entropy (ce) loss that corresponds to the bag-level classification, triplet loss, and the proposed diversity loss:

where  and  are balance parameters.

% [!t]% \centering% \includegraphics[width=1.0\textwidth]{Figure/params.jpg} % Reduce the figure size to slightly narrower than the column.% % %  The proposed method outperforms the other state-of-the-art MIL aggregation models by a large margin in both the CAMELYON16 and TCGA-NSCLC datasets using features extracted by three different means (see Table~). We also show the statistical superiority of our method in Appendix E. Specifically, the proposed model outperforms the second-best models in terms of accuracy (1.7\%; 1.3\%), F1 score (3.1\%; 1.5\%), and AUC (1.1\%; 1.7\%) when using features extracted from ResNet-50 in CAMELYON16 and TCGA-NSCLC, respectively.  A similar performance gain is observed on features extracted from ResNet-18 including accuracy (3.4\%; 1.1\%), F1 score (3.5\%; 1.0\%), and AUC (4.4\%; 1.1\%). We also observe an improvement in accuracy (3.4\%; 1.1\%), F1 score (3.5\%; 1.0\%), and AUC (4.4\%; 1.1\%) when using features extracted from the vision transformer. In general, the proposed model shows a greater performance improvement in the CAMELYON16 dataset compared to the TCGA-NSCLC dataset. This might be attributed to the fact that CAMELYON16 consists of more diverse instances than TCGA-NSCLC. 

We also observe the performance of the three sets of feature embeddings varied: the ViT feature embeddings outperform the ResNet-18 features but show inferior performance compared to the ResNet-50 features. This is mainly attributed to the fact that a greater number of positive instances is extracted by the ResNet-50 (provided by DTFD-MIL) as shown in Fig. (d). In contrast, a smaller portion of positive instances in the extracted patches may accompany a drop in performances~. This phenomenon benefits the pseudo-bag partitions in DTFD-MIL, as more positive instances within a bag are prone to result in less noisy pseudo-bag labels. This accounts for the drop in DTFD-MIL performance when applied to feature embeddings that contain a lower proportion of positive instances.

We conduct ablation studies on model design variants in the CAMELYON16 dataset with features extracted by a ResNet-50, unless specified otherwise.  % The Figure~, we present how % Please add the following required packages to your document preamble:% % Please add the following required packages to your document preamble:% % %   We ablate different components of the proposed model, i.e., the positive instance alignment module and the diversity loss. While the model without these two components serves as the baseline in Table~. We first observe that incorporating the proposed global vectors described in Section~ (without employing any of the learning strategies in Section~) yielded an AUC of 0.922 and 0.928. This AUC exceeds that of most existing MIL models, except for DTFD-MIL (MaxMinS  AFS) (see Table~ and~).  Subsequently, by including the proposed positive instance alignment module, we observe a performance gain of (2.2\%, 2.8\%) in accuracy, (2.3\%, 2.9\%) in F1 score, and (2.2\%, 2.8\%) in AUC. Up to now, we outperform the DTFD-MIL in terms of accuracy and F1 score (see Table~ and~), and achieve a similar AUC (AUC = 0.944,0.956) compare to the DTFD-MIL(AFS) (AUC = 0.946,0.951). Further incorporating the proposed diversity loss into the objective function yields a performance gain of (1.3\%,0.7\%) in AUC, which outperforms DTFD-MIL (AFS) by (1.1\%,1.2\%). 

 As shown in Table , including the tokenized global vector  yields a remarkable performance gain by improving accuracy by (1.0\%, 0.5\%), F1 score by (1.3\%, 0.6\%), and AUC by (2.2\%, 0.6\%). As consistent with the pathological findings that instances are diverse, we observe that different global vectors indeed corresponded to different instance representations, which can be depicted by the attention map produced by different global vectors in Fig. .  However, we also observe that the learned global vectors still include non-tumor related representation, particularly around tumor boundaries, as positive instances around tumor boundaries have a similar appearance to surrounding negative instances (see Fig. .(c) and (d)). As a result, incorporating tokenized global vectors can mitigate this problem by capturing the most discriminative positive (tumor) regions (see Fig. .(b)). 

 We find that the optimal number of global vectors  in different data sets may vary due to dataset intrinsic properties.  Specifically, the optimal  for the CAMELYON16 and TCGA-NSCLC dataset are  and , respectively (Fig. .(a)).  We observe that an overly large  is likely to decrease performance as it will harden the learning task (see Fig. .(a)). 

By conducting a grid search, we find that the optimal setting of the balance parameters is  and  (see Fig. .(b) and (c)). An overly small  and   (e.g., ) is likely to enforce inadequate constraints on the learned global representation by deviating it from learning meaningful information of instance of interest. While larger balance parameters (e.g., ) distract the model from the main classification task, leading to a drop in classification performance.