In recent years, image generation technology has attracted considerable attention in computer vision and machine learning. Image generation models aim to capture underlying patterns from real image data distributions and synthesize realistic images. With the development of deep learning-based generative models, significant progress has been made in recent years. Existing image generation models fall into four main categories: Generative Adversarial Networks (GANs)~, Variational Autoencoders (VAEs)~, Flow-based models~, and Diffusion models~.

GANs, consisting of a generator and a discriminator trained simultaneously, produce high-quality synthetic data resembling real data but struggle with diversity and stable training due to mode collapse. VAEs, inspired by variational Bayes inference, map input data to latent space, generating diverse samples and providing uncertainty estimates, though often producing blurry reconstructions. Flow-based Models, which learn invertible transformations for exact likelihood computation and efficient sampling, face challenges in modeling complex data distributions, making them unsuitable for global image generation. Diffusion Models, using a noise-corruption and reverse process, have shown remarkable performance in generating high-quality images despite slow generation speeds. In this paper, we leverage diffusion models to generate global remote sensing images.

Diffusion-based image generation can be divided into two categories~: unconditional generation and conditional generation.

An unconditional generation model refers to a model that generates images without depending on any form of control, operating entirely in a random sampling manner. The most representative work is the Denoising Diffusion Probability Model (DDPM) proposed by Ho et al.~. The DDPM builds upon the theoretical foundation established by Sohl-Dickstein et al.~, operating by reversing a diffusion process. This method disrupts real data during the forward process, adding noise to the data in small steps until it becomes Gaussian noise, then, learns to reverse these steps to generate data from noises. Based on DDPM, Nichol et al.~ propose learning variance to improve model performance. This design not only enhances the quality of generated images but also accelerates sampling speed. To further accelerate sampling, Song et al. propose the Denoising Diffusion Implicit Model (DDIM)~, modeling the diffusion process as a non-Markovian process. Without requiring adjustments to the training process of DDPM, the DDIM generation framework significantly speeds up the sampling procedure with only a small impact on quality. Additionally, the DDIM framework enables a deterministic generation process, ensuring that generated images depend solely on the initial noise of the reverse process.

The conditional diffusion model refers to a class of models that generate images based on additional signals, building upon the unconditional diffusion model. According to the type of input conditions, they can be divided into class-to-image generation~, image-to-image synthesis~, text-to-image generation~, image editing~ etc. Since DDPM's reverse process directly operates in pixel space, more computation and memory are required to predict high-dimension noise, making it challenging for the model to generate high-resolution images. There are two main approaches to addressing this issue. The first is cascaded generation~, where low-resolution images are generated first and then used as conditional inputs to generate higher-resolution images sequentially. The second approach is latent space generation~, using an encoder-decoder network structure to compress the image into a lower-resolution latent space, followed by a denoising diffusion process within the latent space.

In this paper, we adopt a cascaded generation strategy, but with a novel approach that differs from previous methods. Firstly, previous models require designing and training multiple models separately for each resolution stage, which inevitably increases training costs. In contrast, our model shares the same network weights across different stages, using resolution embedding encoding to guide the learning of variable-resolution information. Additionally, while previous methods typically impose size constraints on input and output images at each stage, our approach does not have such restrictions, offering greater flexibility.

In the field of remote sensing, image generation models have recently garnered increasing attention. Previous research on this topic mainly focuses on text-to-image and image-to-image tasks. For the text-to-image generation task, one common approach is to fine-tune latent diffusion models~ directly based on remote sensing text-image datasets~. The image-text data pairs are usually obtained by a pre-trained image captioning model and then are refined manually assisted by GPT-4. In addition to the direct fintuning, Sebaq et al.~ propose a generation framework containing two individual diffusion models, where the first model generates images from prompts and the second one upscales the generated images. For the image-to-image generation task, the input conditional images usually include maps~ and semantic layouts~. 

Despite some attempts at image generation in the field of remote sensing, previous methods have been typically trained on small-scale or limited datasets, lacking diversity in land cover types and resolutions. Additionally, these methods have only been capable of generating remote sensing images up to 512512 pixels in size, with few attempts made at generating images for larger-scale scenes. In this paper, we thoroughly explore these issues by designing and training a foundational generative model for remote sensing, achieving promising results.

% 预训练解译大模型、图文多模态大模型;Foundation models refer to large pre-trained deep learning neural networks trained in a task-agnostic manner on massive amounts of data. These models can be applied to various downstream tasks using fine-tuning, few-shot, or zero-shot learning. The foundation models have recently been preliminarily studied in the field of remote sensing, emerging as a new research direction. These models are per-trained on large amounts of data and can be adapted to various downstream tasks through fine-tuning, few-shot learning, and zero-shot learning. Current research on foundation models in remote sensing mainly focuses on two aspects: vision-centric foundation models and vision-language foundation models. % % The vision-centric foundation models in remote sensing are designed to learn robust feature representations from raw remote sensing data without the need for manual labeling. The learned representations can then be transferred to various downstream tasks, such as image classification, object detection, and segmentation, resulting in improved performance. These models predominantly rely on the Masked Autoencoders (MAE) architecture for training on vast datasets. They are tailored to address the distinctive features inherent in remote sensing imagery, including image content, multi-scale properties, temporal dynamics, and multispectral attributes. Enhancements are made either in network structural refinement or training paradigm optimization to augment the model's capacity for the representation of remote sensing data.In earlier years, vision-centric foundation models in remote sensing are typically trained using supervised learning. However, these models are constrained by the limited availability of supervised training data. Recently, self-supervised techniques have become mainstream, as they can leverage large amounts of unlabeled data. The self-supervised pre-trained foundation models are designed to learn robust feature representations from unlabeled remote sensing images. They can then be transferred to various downstream tasks, such as image classification, object detection, and segmentation, resulting in improved performance. These models predominantly rely on the Masked Autoencoders (MAE) architecture~ and are tailored to exploit the distinctive features inherent in remote sensing imagery, including image content~, multi-scale properties~, temporal dynamics~, and multispectral attributes~. Besides, the improvement of the model's structure and training paradigm~ is used to enhance the model's capacity for the representation of remote sensing data.

Remote sensing vision-language foundation models are trained on a large amount of image-text paired data. Some methods~ adopt contrastive learning, which aims to train an image encoder and a text encoder to align the visual and linguistic domains. The learned aligned vision-language representations can be used for many downstream tasks, such as zero-shot classification~, image-text retrieval~, and image captioning~. For example, Zhang ~ filtered remote sensing image-text pairs from publicly available large datasets to build the RS5M dataset and fine-tuned the CLIP~ model to obtain a GeoRSCLIP model. Recently, autoregressive pre-training has been widely used for training vision-language models, such as~. Most methods utilize CLIP to extract visual features and design connectors to map these visual features into large language models for predicting corresponding text. They typically start with image-text pre-training and then further refine the model through instruction fine-tuning.

% 尽管遥感领域已有一些基础模型,但是在遥感图像生成领域还没有 Although the aforementioned work has made preliminary explorations in foundational models for remote sensing, research on generative foundational models in this field is still relatively scarce. In this paper, we propose MetaEarth, a novel generative foundation model for global-scale remote sensing image generation, which expands the research boundaries of current remote sensing foundation models. % , exploring the creation of worldwide, multi-resolution, unbounded, and virtually limitless remote sensing images.

The Denoising Diffusion Probabilistic Model (DDPM)~ is a type of generative model that leverages a diffusion process for image generation. It involves a , where the image data is gradually corrupted with noise, and a , where a neural network learns to denoise the noisy image step-by-step to recover its original content. Through the , high-quality images can be generated by iteratively refining noisy samples, leading to impressive results in various image synthesis tasks.

Suppose  denotes a clean image. During the , a set of Gaussian noises are added to  gradually according to a variance schedule  over  time-steps:

where  represents the joint probability distribution of  conditioned by .  represents the conditional distribution from  to  in a one-step diffusion processing. Since the  of DDPM operates as a Markov chain, according to Eq.~(), by setting , the conditional distribution of  can be directly represented as a multivariate Gaussian distribution:

By utilizing the reparameterization trick and introducing a noisy image layer  sampled from a Gaussian distribution, , the relationship between  and  can be written as a one-step blending operation:

Given the  described by Eq.~(), one can easily prove that if the scheduling parameters  are properly set, when  is sufficiently large, the distribution of  can be approximated to a standard multivariate Gaussian distribution, i.e., .

In the , starting with , we aim at recovering the  step-by-step through a denoising processing. In each denoising step, a trainable network with parameters  is trained to estimate the distribution :

According to~, the model can be trained by optimizing the Variational Lower Bound(VLB):

where  represent the Kullback–Leibler divergence between two distributions  and . In Eq.~(),  is tractable:

Assume that the variance  in the above denoising diffusion process is a constant-controlled diagonal matrix , in this way, the training can be re-formulated solely to estimate :

Then,  can be re-written as: 

where  is a constant that does not participate in the optimization process and can be omitted.

By introducing a noise prediction network , combined with Eq.~() and Eq.~(),  can be expressed as:

Therefore, by setting , based on Eq.~() and Eq.~(), we can finally obtain the training objective function as:

where  is a time-variant scaling factor on different loss terms through different denoising steps.

We propose a resolution-guided self-cascading framework for generating various scenes and resolutions. Fig.~ shows the overall structure of the proposed MetaEarth. This framework enables recursive enhancement of image resolution using a unified generation model. 

The entire generation process consists of a series of stages, progressing from low-resolution images to high-resolution images. In each generation stage, the model is conditioned on the previously generated low-resolution images and their corresponding spatial resolutions. The low-resolution images provide scene categories and semantic information, while spatial resolution information enables the model to perceive and represent features of images at different scales. 

Suppose  and  represent the clean image and the spatial resolution (m/pixel) at the -th generation stage.  and  are embedded as conditional variables when generating images at -th stage. Assuming that at each generation stage, the model increases the resolution of the input image by a factor of , e.g. , if the image generated at stage  has a size of  pixels, then at stage , we will obtain a high-resolution image with a size of .

Assuming there are no memory constraints, by repeating the aforementioned operation  times based on stage , we will ultimately obtain a series of images with varying resolutions and pixel sizes: 

where the image  generated from -th stage will eventually has a size of  pixels. To achieve the above process with reasonable memory and computational costs, we designed a sliding window generation process and a noise sampling strategy that enables the generation of continuous, unbounded scenes in a memory-efficient manner. These approaches will be detailed in Subsection~.

In each generation stage, both the previously generated image and the time-resolution embeddings are utilized as conditional input variables. For the conditional input , whose dimensions mismatched with , inspired by~ and~, we redesigned their network architecture. We begin by encoding the low-resolution image  with an encoder . Subsequently, we use a set of upsampling and convolution layers  to align the dimensions of the feature maps with . Lastly, we merge the features by concatenating  with the feature maps along the channel dimension.

where  represents the concatenation operation between two tensors along their channel dimension.

For the spatial resolution , we firstly encode it through a frequency encoding transformation:

where the frequency factors are defined as:

To let the model perceive more subtle resolution changes, we set  as a large number, . Then, the encoded resolution is further projected by a trainable Multi-Layer Perception network .The complete encoding process for  can be represented as follows:

where  are the learnable parameters of the .

Through the above process, we obtain the spatial resolution embedding at -th generation stage . Similarly, the time-step variables  during the denoising process can be also encoded through the frequency encoding transformation  and then projected into a -dimensional embedding vector with an additional MLP:

where  are the learnable parameters. We finally add  and  together to produce the final conditional embedding vector at the -th denoising step from the -th generation stage:

Given the conditional embedding vector  all together with the conditional image features , we uniformly express them as the condition variables of the -th generation stage and -th denoising step:

Based on Eq.~() while maintaining the forward process unchanged, we incorporate the condition  as input to build a conditional denoising model:

Through the aforementioned design, we can finally generate images of different resolutions in a self-cascading manner driven by resolution and time-step encodings.

To achieve the generation of large-scale remote sensing images of arbitrary sizes, we propose an unbounded image generation method including a memory-efficient sliding window generation pipeline and a noise sampling strategy. Figure~ illustrates the proposed method.

In the self-cascading generation framework, we crop the conditional input images into a set of image blocks to control the memory overhead within an acceptable range. However, simple cropping and stitching will result in noticeable seams at junctions. We analyze the occurrence of seams from two factors in generative diffusion models: the conditions and the noise sampling. 

From the perspective of generation conditions, when the receptive field of the model extends beyond the image boundaries, the generated content may differ semantically from adjacent images. To address this issue, we consider blocks as sliding windows with overlapping of 1/2 window area so that pixels originally located at the boundaries are repositioned as central areas. The overlapped regions serve as semantic transition blocks between adjacent images so that the semantic discontinuity between image blocks during stitching can be solved.

Additionally, the initial noise sampling is another crucial factor affecting the continuity between tiles. Due to the randomness of the noise sampling, the generated content within the overlapping regions may vary, resulting in miss-alignment of between adjacent blocks. To mitigate pixel-level discrepancies, we propose a noise sampling strategy that, without modifying the model training process, generates identical or near-identical content in the overlapping regions, ensuring pixel-level continuity. 

During the reverse process of conditional diffusion, we adopt the sampling method of DDIM~. The denoising sampling process from  to  is represented as follows:

where , and .  is a predetermined factor ranging from 0 to 1.  When  the DDIM sampling process is identical to DDPM. When , the denoising process is fully determined, and the generated image is solely determined by the initial noise and the condition . In our method, we set , that is, model it as a completely deterministic sampling process. In this setting, since  is solely determined by the initial noise  and the condition , we represent the generation process using an injective function:

Therefore, if the receptive field of the denoising network is smaller than the overlapped region, with the same condition  and the same initial noise at the overlapping regions, we can easily prove that the generated images at the center of the overlapped region will be strictly same.

Based on the above analysis, as shown in Fig.~, to ensure that vertically adjacent image blocks generate similar content in their overlapping regions, we need their initial Gaussian noise to satisfy the conditions  and . Similarly, to ensure that horizontally adjacent image blocks generate similar content in their overlapping regions, we need  and . To make sure that all the above conditions are always true during the generation process, we can first sample for one block, and then make copies for the corresponding neighboring blocks.  To further simplify the sampling process, in our implementation, we set all blocks with the same initial noise, i.e., , where .

For the dataset constructed above, we set the model to increase the input image resolution by a factor of . For the noise schedule design, we adopt a linear schedule ranging from a minimum of 0.0015 to a maximum of 0.0155. During training, the number of sampling steps is set to 1000, while during inference, we utilize the DDIM acceleration strategy with the number of sampling steps set to 50.

We design a U-Net-like architecture to predict noise, with a total number of about 600 million parameters. The encoder and decoder of the network are composed of 5 blocks each.  undergoes 4 rounds of 2 downsampling/upsampling. The number of channels in each block is multiplied by a factor of  based on the base number of channels. Each block contains 3 ResBlocks, with AttentionBlocks included specifically for blocks with an 8-fold channel multiplier. We utilized RRDBNet~ as the encoder for conditional images. 

For each feature map  within every ResBlock, we divide the embedding  into two parts using the chunk function in PyTorch and then scale the feature map  with the chunked embeddings:

With the above design, we can integrate both low-resolution input and spatial resolution to predict the noise  for each timestep.

When training a single model for cascaded generation, two main challenges arise: Firstly, high-resolution and low-resolution images are from different sensors and are captured under varying spatiotemporal conditions. This results in significant differences in style, content, and details between pairs of high and low-resolution images, leading to potential mismatches between image pairs. Secondly, during the iterative inference process of self-cascading generation, each input image to the generation model in the cascading steps originates from the model's previous output. However, the input data utilized during model training may differ in distribution from the input data during inference. This distributional shift may result in distorted or unreasonable image generation results. To address the aforementioned issues, inspired by~, we introduce high-order degradation to simulate the above factors. We artificially degrade high-resolution images into low-resolution ones, thus constructing training image pairs. The high-order degradation process can be represented as follows:

where  represents a simple degradation model typically comprised of blur, scaling, noise addition, and JPEG compression, which can be expressed as:

where , , ,  represent the blur kernel, the scaling factor, noise, JPEG compression, respectively. During training, we utilize high-order degradation to downsample real images into corresponding low-resolution ones, following the parameter settings as described in~. During inference, we do not apply any degradation - the low-resolution conditional input can be either real images or images generated from the previous cascading steps.

When constructing the final loss function for denoising model training, we introduced the Perception Prioritized (P2) weighting and imposed a weighting factor on different denoising steps~. This design forces the model to focus more on restoring perceptually rich content during training, resulting in higher-quality image generation. Based on Eq.~(), the final loss function for training the denoising network is defined as follows:

where the weighting factors  are defined as:

where  and  are hyperparameters,  is the signal-to-noise ratio of the noisy image .

Our model is implemented using PyTorch. We utilize the AdamW optimizer with an initial learning rate of 2e-6 to optimize parameters and set the total number of epochs to 30. To increase the training batch size, we employ gradient accumulation and multi-GPU parallelism. The batch size per GPU is set to 1, and gradient accumulation is set to 8. The training process takes in a total of 2,000 GPU hours on a set of NVIDIA RTX4090 GPUs. The training is initialized from scratch without loading pre-trained parameters.

To generate multi-resolution remote sensing images for any region globally, we collected a large dataset, consisting of three levels of remote sensing images from Google Earth, corresponding to resolutions of 64m/pixel, 16m/pixel, and 4m/pixel, covering the vast majority of the Earth's landmass. A total of about 731,000 non-overlapping images, each sized 256×256 pixels, were used to train and evaluate the model. For each image in the dataset, the spatial resolution and the latitude and longitude are recorded. We partition the dataset into training, validation, and testing sets in an 8:1:1 ratio.

We use Fréchet Inception Distance (FID)~ to evaluate the quality and diversity of the generated images. A lower FID indicates higher fidelity and diversity in the generated images. FID quantifies the similarity between the feature representations of generated and real images using a pre-trained deep convolutional neural network. The FID score is defined as follows:

where  and  are the mean feature vector of the real and generated images, respectively.  and  are the covariance matrices of the feature representations of the real and generated images, respectively. The features used for computing the FID are extracted from the final average pooling layer of a pre-trained Inception v3 network~.

Our experiments show that the proposed MetaEarth can generate a variety of remote sensing scenes worldwide, including glaciers, snowfields, deserts, forests, beaches, farmlands, industrial areas, residential areas, etc. Fig.~ and Fig.~ showcase some results of our model. Owing to the guidance from the conditional input, the model is capable of generating vivid and diverse images with distinct regional characteristics at different resolutions. We also evaluated the semantical guidance provided by the image conditional input. In Fig.~, we use large-scale low-resolution images of 256m/pixel in the Americas, Europe, and Asia as image condition guidance, and we can see the model can generate urban images consistent with the regional style.

%  

The self-cascading generation framework in our method enables the model to generate images with both spatial resolution diversity and content diversity. Our experiments show that using high-order degradation models to downsample high-resolution images for training pairs improved the model's generalization ability. Consequently, the model can take generated images as input to produce higher-resolution images with richer details. Fig.~ illustrates the generated results of multiple resolutions. At the very beginning stage, the model is conditioned on images with a resolution of 256 meters, finally resulting in generated images with a spatial resolution of 4 meters. The results also suggest the model's strong diversity-generating capabilities.  We can see that as the number of generation stages increases, the content diversity between images gradually increases due to the variability introduced by the generation model.

Fig.~ shows the unbounded generation results with the proposed sliding window generation. Although we applied block-by-block generation, it can be observed that the proposed method generates continuous land features across image blocks. This indicates that our window overlapping and noise sampling strategy greatly alleviate visual discontinuities caused by image block stitching, thereby achieving boundless and arbitrary-sized image generation. Fig.~ shows an example of a high-resolution large-scale image generated by our model.

Thanks to being trained on large-scale data, our model possesses strong generalization capabilities and performs well even on unseen scenes. To verify this, we designed an interesting experiment. In the experiment, we aimed to create a ``parallel world'' and used a low-resolution map of ``Pandora Planet'' (generated by GPT4-V) as the initial condition for our model and then generated higher-resolution images sequentially. Fig.~ shows the generated results. It can be observed that despite our training data not covering such scenes, MetaEarth is still able to generate images with reasonable land cover distribution and realistic details.

Despite the rapid advancements in generative models represented by Stable Diffusion and DALLE, their capabilities in generating overhead remote sensing scenes are limited. In Fig.~, we compare MetaEarth with other state-of-the-art text-to-image models, including GPT-4 V (DALLE), the latest version of Stable Diffusion, and Ernie. We can see that MetaEarth significantly outperforms these models in terms of image realism and the rationality of scene layouts. The primary reason behind this difference is the lack of relevant remote sensing training data during the training phase of previous models. In addition, the three contrasting models lack sensitivity to resolution control and are difficult to accurately capture the correspondence between resolution and image content, making it difficult to generate multi-resolution images or unbounded scenes. MetaEarth, on the other hand, excels in these aspects, demonstrating its superior capability in generating high-quality, realistic remote sensing images across various resolutions and scene layouts.

In this section, we conduct experiments to validate the effectiveness of the proposed methods, including the resolution-guided generation and unbounded generation.

In our method, we introduce spatial resolution as a condition to guide the model in perceiving features and details of images at different scales, achieving the generation of images at multiple resolutions using a single model. To evaluate the impact of spatial resolution guidance on the quality of generated images, we design a set of ablation experiments where we solely use the images generated from the previous stage as conditional inputs and compare their image generation quality with the full-implemented model under various scenarios. Specifically, we remove the branches that take resolution as input (frequency encoding  and MLP network ) while keeping the other structures and parameter settings unchanged. During the inference, both models adopt the DDIM acceleration strategy, setting .

We conducted quantitative analysis in different regions around the world. The regions are divided by using different criteria, including the division of regions at different longitudes, or the division of northern and southern hemispheres. The experimental results are shown in Table~. The ``w/o sr'' rows represent the network without using the resolution guidance. The first column of the table gives the resolution of the generated images. Except for the 4m*, the model inputs are low-resolution images obtained by downsampling the corresponding resolution real images through bicubic interpolation. The inputs for 4m* are randomly cropped from the real 16m resolution images.  Due to the different characteristics of sensors of different resolution levels, the FID value of 4m* is significantly higher than that of other rows, but our resolution guidance strategy is still effective.

We can see that compared to the model w/o using resolution guidance, our model achieves better FID scores in all testing scenarios except for generating images of the southern hemisphere. In our design, the resolution embedding scales the features of the denoising network, allowing the model to generate images with unique features specific to different spatial resolutions and produce higher-quality outputs. Our experimental results also confirmed this. Such performance improvement becomes more pronounced when the input consists of real images with more complex distributions. The decline in image generation quality for the southern hemisphere is primarily due to the limited variability of land features at different resolutions in this region. The southern hemisphere encompasses vast areas of deserts (in Africa and Oceania) and forests (in South America), where the differences in features across different resolutions are relatively minor compared to the northern hemisphere. Therefore, even without resolution guidance, the model may also generate images of better consistency with the true distribution.

To measure the continuity of the different generated blocks in the seam area, in Fig.~, we compare the generated results w/ and w/o using the proposed noise sampling strategy. We can see that with the proposed noise sampling strategy, we obtained smoother results, and the traces of patchwork are difficult to detect with the naked eye. We also designed a gradient-based method to quantitatively evaluation the transition smoothness of different approaches. We randomly sample 750 generated images and calculate the absolute directional gradient values of adjacent pixels at the seams and then take the average. Experimental results are shown in Table~. We evaluate the proposed two sub-strategies respectively, one is overlapped sliding window (marked as ``Overlap" in Table~) and another is the constraints on initial noise sampling (marked as ``Noise Constraint" in Table~).  Smaller gradient values indicate better results.   The experimental results demonstrate that the combined use of the overlapping cropping strategy and our proposed noise sampling method can effectively alleviate the seams appearing during image block stitching, thereby enhancing the visual quality of the generated arbitrary-sized images.

The proposed MetaEarth can be used as a powerful data engine to generate high-quality training data that can benefit downstream tasks. In this experiment, we choose remote sensing image classification, a fundamental task in remote sensing image interpretation, as our test bed for downstream evaluation.

To quantitatively demonstrate the application of our model in downstream tasks and further evaluate the fidelity and diversity of the generated images, we design a remote sensing image classification experiment, using our model as a data engine to provide training data support. Our classification dataset contains seven categories of remote sensing scenes: beach, desert, farmland, forest, industrial area, mountain, and residential area. Each class contains approximately 150 images, with image sizes of 256256 pixels. The dataset is re-collected from our global 4m resolution images. The training and testing sets are divided by a 3:1 ratio.

%  

We choose four different models as classifiers: VGG19~, ResNet34~, ViT-B/32, and ViT-B/16~. We first train four baseline models using the original dataset. Then, we use the generation model to augment the training dataset to five times its original size. After mixing the augmented data with the original one, we retrain the four models from scratch using the combined dataset. For the augmentation process, we downsample the images in the training dataset and utilize the model to generate diverse results.  % Some of the augmented results are shown in Fig~.

Table~ shows the classification accuracy on the test set for different models using different training data. Compared with the original models, the models trained with data augmentation show consistently improved classification accuracy. This indicates that MetaEarth provides informative data for training, which also reflects the high quality and diversity of the images generated. This indicates that when faced with tasks with insufficient or sparse data, MetaEarth has the potential to serve as a data engine to provide usable data at lower costs.

In addition to serving as a data engine for data augmentation, MetaEarth also holds immense potential in the construction of generative world models. World models are a crucial pathway to achieving general artificial intelligence, providing essential training and testing environments for an agent's perception, decision-making, and evolution. Recently, the emergence of OpenAI's Sora model has brought significant attention to research in the field of world models. Generative world models seek to understand the world through generative processes, typically integrating closely with technologies such as large language models, visual foundation models, and video/image generation models.

As a generative foundation model, MetaEarth opens up new possibilities for constructing generative world models by simulating Earth's visuals from an innovative overhead perspective. This unique approach allows for the creation of high-resolution, multi-resolution, and unbounded remote sensing images that cover diverse geographical features and ecological systems. Given a virtual viewpoint's altitude and movement path, MetaEarth can generate and predict content beyond the immediate scene, thus creating a dynamic environment with intelligent interactive behaviors. By leveraging MetaEarth, researchers may generate a realistic and dynamic visual environment that can interact with intelligent agents such as drones and remote sensing satellites in various scenarios, ranging from urban planning and environmental monitoring to disaster management and agricultural optimization, facilitating their training, testing, and validation. 

Constructing generative world models in the aerospace intelligence domain is an important research trend for the future. In this regard, MetaEarth offers valuable insights and methodologies. We believe that as a starting point for this line of research, MetaEarth will significantly contribute to the development of generative world models for aerospace remote sensing, providing new possibilities for its future advancements.