% [!b]% % % % % \Require Memory , Library , Demos , Hints , wake iterations M% \State \# % \State  \Comment Initialize replay buffer% \For {} \quad %     \State  (,,) %     \For {} %         \State Sample random initial simulator state %         \State %         \State Execute program  and evaluate success %     \EndFor%         \State  %         \State  % \EndFor% \State % \State  \# % \State  % \For {}%     \State  = () \Comment {Propose skills}% \EndFor% \State  % % 

Our framework (see Fig.~) receives at each cycle  a set of  language demonstrations  that contain instruction-policy-success tuples for specific tasks: . The demos are appended to the agent's memory . The policy code  is factorized based on the current library skills .  The success code  describes how the agent can use privileged simulation data to verify the success of a given instruction (e.g. check contact, object poses etc.). The output is a set of formal skills, expressed as Python functions, which can be directly used for real robot deployment. This process is repeated for an open-ended number of cycles, enabling the agent to expand its library in lifelong fashion. Each cycle consists of two phases:

 During the wake phase, the agent interacts with its environment in order to grow its proficiency in solving more tasks.  We use an LLM to iteratively propose new task instructions  based on the input demos , the current memory state  and some general hints .  The proposed tasks are executed and verified in the simulator using another LLM's generated policy and success code respectively: , where  the initial simulator state of proposal . Successful  tuples are appended to the experience memory , which helps the agent to recover more relevant context throughout exploration. The process is repeated until an iteration threshold is met, or the LLM decides that it has completed all objectives denoted in the hints. The acquired experiences are stored in a replay buffer .

 During sleep, the agent reflects on its acquired experiences to compose new skills. To achieve this, we first represent each experience as an abstract syntax tree of its policy code. Experiences are clustered such that codes that have the same tree structure modulo variable and constant names are grouped together for a total of M clusters .  We then feed each cluster into an LLM that uses the experiences as examples to define new functions that will update the library .  The demo policies are refactored based on the proposed functions , and the wake phase is replayed from scratch, starting only with the refactored demos.  When the agent fails in a previously succeeded task, its policy-success is also refactored and appended to the memory:

This process ensures that by the end of the cycle, the memory will contain the minimum number of experiences needed to replicate the performance of the wake phase.

 Following previous works , we employ frozen pretrained vision-language models for zero-shot vision-language grounding In particular, we use MDETR  for referring expression grounding and CLIP  for open-vocabulary classification. For control, as we wish to demonstrate the capability of LRLL to progressively build complex skills from simpler ones, we start with the most basic primitives: moving the arm to a certain pose and opening / closing the gripper.  Motion planning is performed via inverse kinematics from end-effector space.

% \noindent  We use LLMs for: a) , i.e. producing action code, b) , i.e. producing a success function, c) , i.e. interacting with a simulator to propose and complete new tasks based on human guidance, and c) , i.e. compressing explored task experiences into new skills. Agent experiences are formalized as tuples of task instruction, action and success code, either provided by the human teacher or ``imagined" by the LLM exploration module . When appended to the memory, each experience is indexed by its instruction embedding, provided by an encoder-based LM  .  In order to retrieve experiences to serve as prompts, the query instruction  is embedded by the same model  and the experiences of the top- most similar instructions based on maximum marginal relevance search  are returned:

where  the set of already selected retrievals,  the cosine distance metric and  a diversification hyper-parameter. This rule is applied  times to retrieve diverse experiences.

 Each agent skill corresponds to a function, implemented as a Python API. % The library can be queried for skill information, such as the skill name, description (docstring), code, as well as a dependency tree of required skills. This allows the library to render prompts with skill information as context to the LLMs. The library maintains skill information such as their names and descriptions, and is able to trace skill dependencies from a given code snippet. Before learning begins, the library is initialized with the initial primitives. A wrapper around the library and the agent converts the newly acquired skills into API modules that are executable in a robot simulator.

 The replay buffer is a replica of the experience memory but only for the explored experiences of the current cycle. Additionally, for each experience, the simulator states are saved. The replay buffer is reset at the beginning of each new cycle.

% % % In this work, we move from static prompts fed to LLMs to sequences of dynamic prompt templates and LLM calls chained together, enabling LLM outputs to behave as prompt fields in the input of another LLM call.  We implement three LLM modules:  % Three modules are implemented: 1) policy and success code generation , 2) exploration of new task variations and compositions , 3) Python function definitions from a set of example code snippets ().  The actor-critic comprises of two parallel LLM calls, one for policy and one for success code generation. Both prompt templates are instantiated after retrieving experiences from the memory for a given query, and follow the same general structure:

For inference, the query is appended to the prompt as a comment and the LLM fills the corresponding policy or success code. We find that this code-based completion format works robustly also for chat-based LLMs .

%  %  The exploration module proposes the next tasks to complete in the simulator. The goal is to use the demos as guidance and introduce both task , i.e. alter concepts present in the instruction (e.g. color, spatial direction etc.), as well as task , i.e. combinations of concepts present in the demos (e.g. desired destinations for placing objects). The prompt contains:

Before proposing tasks, we ask the LLM to reason about its proposals , which significantly helps in responding better to the provided hints.  % Proposed tasks are executed and evaluated in the simulator (using the actor-critic) and the result updates the prompt.% Exploration continues until the LLM decides that it has addressed all hints, the teacher intervenes, or a fixed iteration threshold is met. We find that decomposing exploration to a chain of two LLM calls, prompted separately for compositions and variations, leads to faster completion. Task variations, proposed by the second LLM call, are not included in the progress prompt field. A temperature parameter of  is set at successive iterations to encourage diverse responses.

% %  This module leverages LLMs' capabilities to define Python functions out of examples. The goal is dual: a) maintain the same code logic, but abstract code variations such as target objects and destination regions as arguments to the new function, and b) extract boilerplate code snippets and abstract them to new functions. This is achieved by prompting the LLM in two rounds. The prompt contains:

We also ask the LLM to provide a docstring, which is used as the skill description, as well as to re-write the given examples based on the generated function, which is used to refactor the memory and move to the replay stage of the sleep phase.

 We leverage OpenAI's   engine for all LLM generations, and  as the memory embedding model. Our system is built using the LangChain library . Our simulator environment is built on Pybullet  and it is based on the Ravens  manipulation suite, with the blocks-and-bowls setup replicated from previous works .  We introduce more tasks and language variations, for a total of 41 task templates, organized in a curriculum of 4 cycles: a) , i.e. precise motions relative to objects/regions, b) , i.e. determining attributes, resolving spatial relations and counting/enumerating objects, c) , i.e. single picking, releasing and placing tasks, and d) , i.e. long-horizon tasks that involve multiple objects and destinations.

 For conducting generalization experiments, we generate task instances in three splits : seen instructions with either seen (SA) or unseen (UA) attributes, and unseen instructions with unseen attributes (UI).  % We evaluate the agents in all three splits after each cycle and report averaged success rates.  For studying learning in the lifelong setup, we also propose two more splits: a) a  split, which contains unseen compositions of tasks from the present cycle with all previous tasks (with seen attributes), and b) , which contains the FT tasks from the previous cycle. These splits are meant to study whether the agent can learn to transfer knowledge between tasks (FT), and to what extent it ``forgets" or improves on previous tasks (BT).

 We consider four baselines: a) learning end-to-end multi-task policies with , adapted as in  (not applicable in all tasks), b) prompting LLMs for primitive-based policy code using a static prompt, as in  (without hand-crafted routing between LLM sub-systems), c) , where we remove the sleep phases from our  and only retrieve examples from the memory without abstraction, and d) , where we attempt to synthesize new skills directly from the human demonstrations, without the exploration of the wake phase. 

We first wish to evaluate the performance of  compared to established baselines in our simulated tabletop domain.  To that end, we developed a simulated teacher that samples tasks from a set of predefined templates. The teacher generates up to 5 demonstration (1 per SA template) and multiple test (10 per UA, UI template) tasks in the beginning and end of each of our 4 cycles.  For end-to-end learning with CLIPort , we sample  trajectories per task template using a scripted expect for each cycle and train the model incrementally. For our LLM-static baseline , we append demos from each new cycle in the LLM's prompt.  The same demos are provided to  at the beginning of each cycle's wake phase.  Agents are tested at the end of each cycle. We repeat our experiments three times with different teacher seeds and report averaged success rates in Table~.

We observe that CLIPort struggles with unseen attributes and its performance degrades drastically with unseen instructions. % Due to its pick-and-place based nature, CLIPort is also not applicable in simple coordination or visual reasoning tasks. LLM-static is robust to unseen attributes ( average drop) and can generalize significantly better in unseen task instructions, with an average success rate of \% in all cycles.  We find that this baseline's main limitation is producing non-executable code in cases of unseen instructions at later cycles, which we attribute to its inability to interpret and compose multiple skills from a limited demonstration context.  % (tends to repeat existing code patterns from the demos or hallucinate unknown functions).%  phenomena  Such skill compositions are (partially) already explored during the wake phase of our , and abstracted to functions during the sleep phase, resulting in policy code that is much shorter and functional in style.  This robustifies 's generated policies, which translates to an average increase of  in unseen attribute and  in unseen instructions compared to LLM-static.

Our ablations focus on exploring the effect of each of our proposed components and discussing options for implementation.

%  We compare the averaged success rates of all baselines in FT/BT instructions. Results are reported in Table~. First, we assess that all baselines are robust in tasks from previous cycles, showcasing the immunity of LLM's in-context learning to forgetting. However, no actual increase in backward task's performance is reported in any baseline. For forward transfer, we see a large increase in averaged success between LLM-static and . Even without refactoring code  ( baseline), retrieving explored experiences leads to better compositional abilities, with a  delta from static.   

 When prompted with a few examples, the difference between a static and a retrieved prompt is marginal. In the late cycles of the curriculum, we observe the effect of  kicking in the static baseline, leading to several instabilities in the LLM responses, such as ignoring the first examples in favour of more recent ones or referring to variable names outside the current scope. Retrieval-based baselines tackle such issues by ensuring a fixed context length for the LLM actor.

 The contribution of the exploration module is vital, as the performance of  is consistently much lower across cycles. This is due to the high difficulty of abstracting skills from one-shot demos, which usually leads to a one-to-one mapping of instructions to functions, without adding any actual refactoring. When adding exploration, the abstractor has significantly more examples to define new skills. To evaluate the breadth of variance in the explored tasks, we visualize the  projections of their instruction embeddings  compared to demonstration and test tasks within a cycle (see Fig.~). 

 never abstracts the explored tasks into new skills, and so needs to retrieve a lot of examples in order to obtain sufficient context. This effect bottlenecks the agent to the quality of the retriever. % With the sleep phase, experiences are refactored to be just a call to the new skill, which allows providing sufficient context with few examples.  Besides performance gains, sleep leads to other practical benefits (see Fig.~). First, the amount of experiences required to maintain the same success within each cycle is drastically reduced, leading to a  decrease in RAM required to store experience embeddings.  Second,  with refactored memory requires much less retrieved experiences to maintain high performance in UI tasks.  Additionally, as the experiences are refactored to be simple function calls (to the abstracted skills), the retrieved code is itself smaller, which leads to smaller prompt lengths and hence cost gains for using GPT.

 We find no significant difference between  and  in the quality of generated policy code or function abstraction. For exploration, we find that both models provide rich variance in the proposed tasks, but the chat model tends to be less responsive to the hints signal. This effect can be ameliorated by running more exploration iterations. The choice of the embedding model is more important, as experiments with smaller encoder LMs such as Sentence-BERT  showed a tendency to retrieve instructions that are similar lexically (e.g. same object noun appears), but not necessarily convey the same task.  % Barplot with 5 cycles as x-axis and gen/on results in y-axis for three bars: lrll, lrll-no-sleep, lrll-no-wake.% %     \item effect of sleep-phase. Point to barplot %     \item efficiency of wake-phase exploration. Point to barplot. (+tSNE of actual and "imagined" tasks for some selected cycles).%     \item (if time: effect of LLM size and type: Compare GPT with Llama-2-7b/13b/70b-quant)%  We repeat our curriculum with  in a dual-arm robot setup with two UR5e arms and a Kinect sensor. We provide vision APIs for open-vocabulary detection with MDETR  and attribute recognition with CLIP . To assist in articulated grasping, we also integrate GR-ConvNet  for 4-DoF grasp synthesis as a vision API. The motion primitives for moving the arm and opening/closing the fingers are parameterized by the left or right arm.  We include a catalog of 12 household objects, including fruits, soda cans, juice boxes etc.  We first train LRLL using our default 4-cycle curriculum in the Gazebo simulator  and then test the agent in the real robot. We demonstrate that the robot is able to perform long-horizon rearrangement tasks that combine precise spatial positioning with reasoning about object attributes, without any further adaptation from simulation. % % A video of transfer demonstrations is included as supplementary material.  Errors were observed mostly at motion execution due to collisions, as well as perception errors due to CLIP misclassifications.

%