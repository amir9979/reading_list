Few-shot learning aims to recognize new categories with limited labeled data. This research area can be further divided into visual-based methods and semantic-based methods. Visual-Based methods focus on extracting category-related features from images for classification. These methods can be roughly divided into two categories: optimization-based methods and metric-based methods. The former aims to learn a set of initial model parameters that can quickly adapt to new categories~. The latter aims to learn a metric space where inter-class distances are maximized, while intra-class distances are minimized~. Semantic-Based methods attempt to enhance visual recognition performance by fusing the complementary information of visual and textual modalities~. These methods usually introduce complex network frameworks to effectively utilize textual information. For example, Xing et al.~ proposed an adaptive fusion mechanism to combine visual prototypes with semantic prototypes obtained through word embeddings of class labels. Peng et al.~ utilized graph convolutional networks to incorporate additional knowledge from knowledge graphs. Yan et al.~ proposed a word embedding-guided attention mechanism to obtain label prototypes for multi-label few-shot learning problems. Different from these, our method does not require manual annotation to collect complex textual knowledge or design a complex network architecture, but instead makes full use of the rich knowledge of LVLM and its good alignment between image and text to perform classification on new classes.

% % % FS-FGIC~ is a challenging problem that requires more effective feature learning than traditional FSL tasks. ~ first defined the few-shot fine-grained recognition task and used bilinear features to learn a segmentation mapping classifier. Subsequently, PoseNorm~ explored the effect of part annotations and showed that learning part features can significantly improve the performance of few-shot learning methods on fine-grained datasets. MattML~ uses a task embedding network to automatically learn a task-specific initialization through an attention mechanism. ~ uses spatial attention to capture fine-grained details of objects and channel attention to capture the global context of the image. HelixFormer~ is a Transformer-based dual helix model that solves the FS-FGIC task by learning cross-image object semantic relations in local regions of the image. BiFRN~ exploits a bidirectional reconstruction process to increase inter-class variations and reduce intra-class variations. And our method explicitly exploits the image description capability of LVLM to generate local and global attribute descriptions for fine-grained categories to assist classification.%LVLM marks a major leap forward in visual language modeling, greatly enhancing reasoning and comprehension capabilities  LVLM marks a major leap forward in vision-language modeling, which is designed to process and interpret cross-modal  information, capable of proficiently handling complex tasks that require deep understanding of context. Inspired by the remarkable success of Large Language Model (LLM) instruction fine-tuning~, the LVLM community has increasingly focused on incorporating instruction-following data to further enhance the model's understanding of downstream tasks. Recently, LLaVA-1.5~ improves its instructions based on LLaVA's framework and obtains performance on a wider range of VQA tasks. InstructBLIP~ enhances zero-shot capabilities by performing instruction fine-tuning on multiple datasets. Shikra~ and Kosmos-2~ extend LVLM to visual ground truth tasks using instructions with bounding box coordinates. Qwen-VL-Chat~ improves the model's multi-round dialogue interaction capabilities through instruction fine-tuning. %Therefore, we design instruction-following datasets according to the meta-learning paradigm and fine-tune LVLM to fully exploit its few-shot classification ability.   Therefore, in this context, we introduce the meta-learning paradigm, focusing on organizing Meta-task instruction-following datasets and fine-tuning MLLM. The purpose of this approach is to fully tap the potential of MLLM in few-shot classification tasks. By leveraging meta-learning, we expect the model to better adapt to downstream tasks.

%The FSL problem is usually defined as a -way -shot classification problem, in which the model needs to classify the sample  in the query set  into one of  unknown  based on a small number of labeled examples . Since it is very challenging to directly train the model using such a small support set , a large labeled dataset  is first used for pre-training to initialize the model. Previous research works often use meta-learning strategy to divide this base  into many -way -shot subsets, each of which contains a support set and a query set, to simulate the situation of few-shot learning in practical applications. It should be noted that there is no overlap between these base categories  and the novel category , ,    = . The purpose of  is to enable the model to generalize to unseen categories 

The dataset of FSL is generally divided into two parts: a base set  for pre-training to initialize the model and a novel set  for testing, where  denotes the image and  represents the label. The label space for both sets are disjoint, meaning that . During testing, the support set  is randomly selected from , which includes  classes, each containing  samples. The model must then accurately classify the images in the query set  into one of the  classes present in the support set , where  is the number of query samples per class. This classification task is generally referred to as an -way -shot task.

% % % In FSL, the common setup follows the meta-learning% paradigm. Meta-learning, or ``learn to learn" is a methodology aimed at enabling models to quickly adapt to new tasks by improving the learning process across multiple stages.% Unlike traditional machine learning, which involves training a model  on a large dataset  to fit the data, meta-learning divides the training process into two levels.% 1) Inner-level: This level focuses on training a base learner  on individual datasets. Each dataset  is a subset of  that contains few data, consisting of a train set  and a test set . The base learner is trained on  to learn the task-specific characteristics and find the optimal state for the current task:% % %    f_{i}^{*} \leftarrow \nabla l(f_i(D_i^{tr})).% % % Then, the performance of  is validated on the query set  and the loss is recorded as: .% 2) Outter-level: This level involves training a meta learner  which synthesizes the training experiences from all tasks:% % %     L=\sum_{n=1}^N l(f_{n}^{*},D_n^{te})),% % % where  is the sum of tasks. The meta learner  uses accumulated knowledge to guide the  towards optimal behavior. This learning approach enables the model to learn how to find optimal solutions across different tasks, thereby achieving good generalization when confronted with unseen tasks.

To explore the direct application of LVLM on FSL tasks, we first organized the commonly used FSL evaluation datasets into -way -shot format. To be detailed, we design Meta-Task Instructions to prompt LVLM to generate responses, as illustrated Figure~. However, we found that straightforwardly applying LVLM on -way -shot FSL did not yield satisfactory performance. To enhance LVLM's performance, we adopt an instruction tuning method in a meta-learning manner. Specifically, we collects various datasets from areas such as scene recognition, general object recognition, sentiment analysis, fine-grained recognition, and remote sensing. These fine-tuning datasets are then organized into meta-task instructions.

Following the meta-task instruction tuning, LVLM evaluates the similarity between query and support samples, and aligns query samples with candidate answers. Consequently, the fine-tuned model can make more accurate predictions from the limited examples provided by the meta-task instructions. However, LVLM can sometimes be overly confident, relying on categories seen in the pre-training data and overlooking the support information during query sample classification. To avoid this problem, we propose a label augmentation (LA) method via character perturbation strategy to enhance the model's focus on the support data.

Before introducing the character perturbation strategy, we should first take an inside look at the model's output process. The token embeddings  of the LVLM are trained to represent the entire textual space. When given an image embedding , the LVLM identifies the image and outputs the predicted token in the following manner~:

where  is the softmax function,  is to transform  for aligning with , and  represents the most probable single token for .

Based on our meta-task instructions, we use interleaved  image-text pairs where image features are represented as  and the prompts ``What is this? \{classname\}." are tokenized as . Query image feature is , the query prompt ``So what is this? Output is one of  " is .  Then we can derive the complete formal expression of the input:

where  is the concatenation operation and  is a special token to indicate the boundary. Assuming a class  has  tokens. Now predicting  is equivalent to auto-regressively predicting its tokens:

where  is the -th token of , and  is the sequence of tokens before the -th token.

Through the above analysis, to prevent LVLM from becoming overly confident, a straightforward approach is to disrupt common token sequences. For instance, during pre-training, the token sequence `yel'-'low' for the word `yellow' is typical. However, by perturbing the original word `yellow' to `yelowla' during fine-tuning, it results in a new token sequence `yel'-`ow'-`la'—which is strange to the LVLM. Therefore, LVLM is enforced to focus on support data instructions to learn the task paradigm. Here is how we implemented the perturbation: 

    split the class name according to a specific symbol such as `', and then re-combine the class name after splitting.  , `A330-300' to `300-A330'.

    take the last few characters of the class name and place them at the beginning of the other perturbation methods.  , `elephant' to `anteleph'.

    randomly select a number from 1 to 10 as the number of characters to randomly sample from  to , and insert these sampled characters into random positions within the class name.  ,`streetcar' to `sttrKeeutcEayrU'.

    shuffle all characters in the class name. , `shrew' to `hsewr'.

% 

In the process of LVLM performing FSC, it implicitly leverages its internalized knowledge. % (&i.e&., we do not provide it with additional knowledge or require it to output extra information).  Since LVLM's knowledge is beneficial to FSC, we attempt to explicitly utilize it. Considering that LVLM performs well in image captioning, we let LVLM generate image-related descriptions to assist the model inference process in subsequent sections.

% Different from previous studies that only use class names or generate a single global image text description or manually select relevant local attributes as semantics, we design an uto-Adaptive ulti-View ulti-Layer escription eneration Framework  using LVLM to generate high-quality descriptions of local and global attributes for fine-grained categories, as shown in Figure 4. Different from previous studies that only use class names or generate a single global image text description or manually select relevant attributes as additional information, we design an adaptive attribute description generation framework using LVLM to generate high-quality attributes and global descriptions for images in each category. The detailed steps are as follows:

%In this step,  specifies the type of data set they wish to analyze (\eg, bird species) and the number of attributes desired (). The LVLM responds by suggesting  relevant attributes, along with brief explanations of their importance in describing the images within the specified data set. In this step, the type of dataset to be analyzed (, bird species) and the number of attributes desired () are specified. The LVLM then suggests  relevant attributes, along with brief explanations of their importance in describing the images within the specified dataset.

%Given the selected attributes from Step 1, the user requests prompt suggestions for each attribute. These prompts serve as guidelines for the LVLM when generating descriptions based on the chosen attributes. The LVLM provides concise and tailored prompts for all  attributes, ensuring that the generated responses remain focused and informative.% Given the selected attributes from Step 1, prompt suggestions are requested for each attribute. These prompts serve as guidelines for the LVLM when generating descriptions based on the chosen attributes. The LVLM provides concise and tailored prompts for all  attributes, ensuring that the generated responses remain focused and informative. After obtaining attributes in step 1, LVLM is required to generate prompts for each attribute. These prompts serve as guidelines for LVLM to generate descriptions in subsequent steps. LVLM provides concise and tailored prompts for all  attributes, ensuring that the generated descriptions remain focused and informative.

% For each of the  attributes identified earlier, the corresponding prompt is presented to the LVLM. In response, the model generates detailed descriptions of the image's characteristics based on the specific attribute being considered. For each of the  attributes identified previously, the LVLM is provided with a corresponding attribute prompt from Step 2. In response, the model generates a specific detailed description of the attribute for the image.

Finally, the specific attribute descriptions from Step 3 are combined into a single, comprehensive description sentence and fed to the LVLM. The LVLM responds with its overview of the image. This attribute-global description not only captures the essence of the image, but also highlights its unique features at multiple respectives of detail.

Through our adaptive attribute description generation framework, for each support or query image in the meta-task instruction, we can obtain  attribute descriptions regarding each image to assist the subsequent model inference process. See the appendix for more details on the attribute descriptions generation process.

% % However, adding the generated descriptions to the prompt fed to the LVLM did not yield good results. This approach increased the context length, adding to the model's burden and task complexity. Directly using these descriptions for classification and applying adaptive weighting to the results obtained from LVLM introduces long processing times. Additionally, due to the redundancy and lack of discriminative power in the descriptions, the results are often unreliable. Therefore, we designed a simple yet effective semantic-based method to leverage these descriptions. This approach not only reduces the model's burden but also enhances the LVLM's self-consistency.

To leverage the generated attribute descriptions, we initially integrated these descriptions with meta-task instructions and then prompted LVLM. However, this method did not yield better results, as it increased the context length and introduced additional complexity. % Furthermore, due to redundancy and lack of discriminative power in the descriptions, the resulting inferences were often unreliable. Instead, we designed a simple yet effective semantic-based method for candidate selection (CS) using these descriptions as illustrated in Figure~. This approach not only reduces the task complexity but also enhances the self-consistency of LVLM.

For each of the  samples in the meta-task instruction, there are  attribute descriptions. The  description for the  sample is denoted as , where . Additionally,  represents the  description for the query sample.

% For each image in the meta-task instructions, we have  descriptions. In a meta-task instruction with  samples, the  description for the  sample is denoted as , where . Additionally,  represents the description for the query sample.

We compute the text similarity  for description  between the query sample and support samples to obtain  text similarity matrices. We then aggregated these similarities to obtain an overall text similarity :

Then we utilize  to identify the top  classes as candidate categories , while the rest are considered unreliable. % Based on , we can rerank the candidate categories. We roughly identify the top  candidates as potential classes , while the rest are considered unreliable. We then compare  with the LVLM's initial inference result . If candidate categories contains the initial inference result ( ), % If  is in , we consider the result to be validated. Otherwise, we reorganize the  ways ( ways from  and  way from ) meta-task insturction to prompt LVLM again for a final inference. Final inference reduces the classification difficulty as it generates answers from fewer categories. Furthermore, this approach leverages self-consistency to enhance the reliability of the model's output .

For instruction tuning, we selected 13 datasets from ELEVATER~. These datasets span various domains such as remote sensing, scene recognition, stripe recognition, and fine-grained classification. For these datasets, we randomly split the classes into base and novel sets with a 7:3 ratio, using the base sets for fine-tuning. Note that we carefully select datasets to avoid data leakage and make fine-tuning and inference categories have no overlap.

For inference, we evaluates the proposed method on eight established FSL datasets: MiniImageNet (MINI)~, CIFAR-FS (CIFAR)~, TieredImageNet (TIERED)~, CUB~, Stanford Dogs (Dogs)~, FGVC-Aircraft (FGVC)~, Oxford 102 Flower (Flowers)~, and Stanford Cars (Cars)~. % The first three datasets are coarse-grained, while the remaining five focus on fine-grained categories including birds, dogs, aircraft, flowers, and cars respectively. Typical FSC methods are typically tested on the first three datasets as well as CUB, while fine-grained FSC methods are evaluated on the latter five datasets. We followed the standard base-novel split~. % For the remaining datasets, we randomly divided the classes into base and novel sets with a 7:3 ratio, using the novel sets for evaluation.% For more details about the datasets, please refer to the appendix.% , while fine-grained FSC methods are evaluated on the latter five datasets.% For the first four datasets, we used the standard base-novel split~, and for the remaining four, we randomly divided the classes into base and novel sets with a 7:3 ratio, using the novel sets for evaluation.% For more details about the datasets, please refer to the appendix. We utilized the interactive Qwen-VL-Chat model as our LVLM. Its large language model was initialized with the pre-trained weights of Qwen-7B~, the visual encoder adopted the ViT architecture and was initialized with the pre-trained weights of Openclip's ViT-bigG~, and the visual-language adapter consisted of a single-layer cross-attention module with random initialization.

To improve training efficiency and reduce training costs, we chose the quantized version Qwen-VL-Chat-Int4 (Qwen-VL for simplicity), froze the LLM and visual encoder, and used Q-LoRA~ to fine-tune the model's adapter. Specifically, the learning process utilized a cosine learning rate scheduler with a base learning rate of  and a warm-up ratio of 0.01. Optimization was performed using the Adam optimizer, with a weight decay of 0.1 and a  parameter set to 0.95, which ensured stability in convergence. The maximum sequence length of the model was set to 2048 tokens to effectively handle long sequences. Additionally, we directly used the frozen SBERT (all-MiniLM-L6-v2)~ as the text encoder used in the semantic aided inference step to measure the similarity between sentences, which had been trained on a 1B sentence pair data set and could effectively capture the semantic information of sentence vectors.

Due to LVLMs' tendency to generate lengthy content and complex class names, we employ three metrics for flexible and comprehensive evaluation:

Since models that are not fine-tuned often produce unreliable outputs, we use  as the default metric. In contrast, fine-tuned models provide more stable results, for which we use  as the metric. The results for all three evaluation protocols will be detailed in the appendix.

% [h]%     \centering%     %     %     % % [h]%     \centering%     %     %     % % 86.9452 86.5125 88.7075 90.44 89.80 88.55%83.534 83.55 81.808 82.486 83.57 84.56 To evaluate the effectiveness of our approach, we conduct extensive experiments on eight datasets in a 5-way 1-shot setting. Table~ compares our results with state-of-the-art (SOTA) methods for general FSC, while Table~ contrasts our approach with methods specialized for fine-grained FSC.

As shown in Table~, our approach outperforms existing SOTA techniques with improvements of 2.02\%, 5.08\%, 2.66\%, and 0.60\% on the MINI, CIFAR, TIERED, and CUB datasets, respectively. Moreover, our method achieves an average accuracy of 96.93\% across these four datasets, surpassing the highest average accuracy of 90.44\% achieved by PTMAP-SF-SOT by 6.49\%. It also outperforms the vision transformer-based methods PMF and CAML, which have average accuracies of 89.80\% and 88.55\%, respectively. As shown in Table~, in the fine-grained domain, our method improves upon SOTA techniques with gains of 10.56\%, 17.87\%, 6.50\%, 16.33\%, and 11.02\% on the CUB, Dogs, FGVC, Flowers, and Cars datasets, respectively. Our method reaches an average accuracy of 97.60\% across these five fine-grained datasets, significantly surpassing the highest average accuracy of 84.56\% achieved by the SRM method by 13.04\%.

It is evident that Qwen-VL performs poorly on both tasks. We will provide detailed analysis of this phenomenon in the Analysis Studies section. % Additionaly, most methods we compare, except for some using Vision Transformer backbones and additional training data, employ ResNet or Wide ResNet (WRN), which are smaller than our ViT-big G backbone. Direct comparisons with other methods may not be entirely fair, as our approach leverages an advanced visual encoder and a larger pre-training dataset.% But it offers benifits that our LVLM to achieve SOTA performance across diverse downstream datasets with just one fine-tuning, without needing additional adjustments on the base set. Nevertheless, our approach can improve LVLM to achieve SOTA performance across diverse downstream datasets with just one fine-tuning, without requiring additional adjustments on the base set. This advantage indicates the superiority of applying LVLM in FSC tasks.

% However, LVLMs do have limitations, including input token constraints, hallucination issues, and slow response generation. Therefore, LVLMs are recommended for scenarios requiring single fine-tuning, long-term benefits, and explainability. For immediate responses or constrained deployment environments, smaller end-to-end models are more suitable.% % \iffalse% [h]%     \centering%     %     %     % % \fi To validate the effectiveness of our proposed methods, we conducted ablation studies on meta-learning, label augmentation (LA), and candidate selection (CS), as detailed in Table~.  Across eight datasets, we observed the following average improvements: 1) Meta-learning fine-tuning alone led to a substantial 53.27\% improvement in model performance; 2) Adding LA, CS, and both LA and CS together resulted in performance gains of 54.17\%, 53.59\%, and 54.33\%, respectively; 3) Incorporating CS provided significant benefits to training-free methods, achieving 25.69\% accuracy gains. These results demonstrate that each component in our proposed methods contributes to enhancing the model's few-shot classification capabilities.

We observed severe position bias in Qwen-VL across all eight datasets, with specific illustrations for CUB and Flowers shown in Figure~. We compare Gold Balanced and Gold Fixed setting for detailed investigation. In the Gold Balanced setting, gold answers are evenly distributed across the 5-way candidates, meaning they appear 1000 times at each position out of 5000 test instances. However, LVLM's actual outputs are concentrated in the early candidate positions. For example, in the Flowers dataset, more than half of LVLM's responses selected the first candidate position.

Regarding Gold Fixed experiment setting, gold answers are in the same position among the all candidates. For example, Gold Concentrated 3rd Position means that all gold answers appear only in the third position of the candidates. When gold is concentrated in the first position, the model shows higher accuracy. However, as gold is moved to later positions, model accuracy significantly drops, indicating that the model has difficulty accessing candidates positioned farther away. 

Both experimental settings confirm that a significant reason for LVLMs' poor performance in FSC is position bias, manifested as a tendency to favor the first few candidates while having limited access to more distant ones.

Additionally, when gold is fixed in the second or third position, making it easy for the model to access, performance still remains poor. This phenomenon indicates another potential reason for LVLMs' subpar performance in FSC: their inability to effectively extract and utilize information from support samples to guide classification.

We extend our experiments from the 5-way setting to the 3-way and 7-way settings, where the gold distribution is balanced. We calculate the normalized standard deviation (NSD) of the model's actual answer positions compared to the balanced gold distribution. A higher NSD indicates a worse position bias problem where model and gold answers have greater position differences. % deviation from the balanced gold positions. Figure~ shows the NSD values for our method and the untuned LVLM across eight datasets and three -way -shot settings. The unfine-tuned LVLM exhibits severe positional bias even in the 3-way setting, which becomes more pronounced in the 7-way setting. In contrast, our model maintains balanced output distributions across 3-way, 5-way, and 7-way settings, without difficulty in accessing answers at the end of the candidate list. Notably, although our instruction-following dataset is organized for 5-way 1-shot tasks, the fine-tuned model performs well in the 7-way setting, where the candidate list is longer than that in pre-training. These results demonstrate that our method effectively mitigates the positional bias problem.

% 为了进一步验证模型在推理数据集上的性能是更依赖于预训练知识,还是所提供的支持样例,我们将exchange和pertubation的方法应用于推理数据集上。dataset列中的exchange OoO ^0^ what are you doing 和perturbation指对下游任务的标签分别执行交换类名和字符扰动的操作,model列指的是SFT过程中对数据集采用了何种增强技术。结果表明,在下游任务发生了类名交换、字符扰动等情况后,使用vanilla数据进行微调的模型并不能很好地适应这种情况,代表模型还是更加依赖于预训练知识,而不会从所提供的支持样例中吸收知识。使用添加了标签增强的数据进行微调,模型则学会使用所提供的支持样例中的知识,特别是交换类名的技术增强的数据。 To further verify whether the model's performance on the inference dataset relies more on pre-trained knowledge or the provided support examples, we exchange and perturbation labels in the inference dataset. Specifically, label exchanging makes inference data conflict with the pre-trained knowledge in LVLM, while label perturbation makes labels unseen in LVLM pre-training.

As demonstrated in Table~, the model without applying Label Augmentation (LA) does not adapt well to these conflicting or unseen scenarios. In contrast, fine-tuning with LA allows the model to better integrate information from the support examples. While the character perturbation strategy offers only slight improvements in standard FSC tasks, its effectiveness is clearly demonstrated in this experiment, enabling the LVLM to focus more on the information provided by the support examples.

We computed the similarity between support samples and query samples for each individual attribute and used the maximum value as the classification result. Results for CUB and CIFAR are reported in Figure~ (left). These results show that a single attribute alone lacks sufficient intra-class similarity and inter-class discriminability to effectively aid LVLM classification. The performance on the CUB dataset is notably worse than on CIFAR, likely due to the finer-grained nature of the CUB dataset requiring more discriminative attribute descriptions.

As shown in Figure~ (right), the accuracy of aggregated attributes surpasses that of any single attribute at the top-1 level and shows significant improvement at the top-2 level. At the top-3 level, the accuracy consistently exceeds 90\%. % This outcome supports our hypothesis that while a single attribute may not exhibit the highest similarity with the support samples in every attribute, the aggregated performance across all attributes is likely to be more effective. According to the experimental results, we utilize aggregated attributes to select  candidates for the final reference. It is less likely to omit the gold answer and can effectively simplify the classification task in the final reference.

We also illustrate the Candidate Selection (CS) process in Figure~. We compare LVLM's output with the top-2 attribute-based candidates. If the LVLM's output is not among these top-2 candidates, it is considered a mismatch, and we reorganize the candidates. We count how often the gold label appears in these reorganized candidates, as this reflects the potential classification performance. The results show that the gold label appears in up to 90\% of the new candidates. We also report the accuracy of the initial inference and the accuracy after applying CS. The experiments demonstrate the effectiveness of our candidate selection method, which helps retain the correct answer and simplifies the task for better performance, especially benefiting the unfine-tuned Qwen-VL.

% % % We fine-tune the LVLM with FSC centered task. To assess whether our method affects the model's performance on other tasks, we test it on zero-shot captioning and general VQA tasks. The results, shown in Table~, indicate that our method not only maintains but actually improves performance on these tasks. We attribute this improvement to the meta-learning instruction fine-tuning paradigm and the label augmentation strategy, which enhance the model's visual perception and its ability to extract useful information from provided instructions.