Natural Language Generation is the process of producing a natural language text in order to meet specified communicative goals. The texts that are generated may range from a single phrase given in answer to a question, through multi-sentence remarks and questions within a dialog, to full-page explanations. In our evaluation, we mainly focus on text-to-text styles.  In general, the task of NLG targets at finding an optimal sequence  that satisfies:

where  represents the number of tokens of the generated sequence,  represents a set containing all possible sequences, and  is the conditional probability of the next token  based on its previous tokens  and the source sequence  with model parameters .

Next, we will introduce some classic and widely-researched sub-tasks of NLG, with several corresponding datasets.

Dialogue generation refers to the process of automatically generating coherent and contextually appropriate responses in a conversational setting . The ultimate goal of dialogue generation task is to create responses that are relevant, informative, and engaging to the user.We utilize two English dialogue datasets characterized by clear emotional flow and topic constraints, as well as one English dataset that incorporates speakers' personalities. Furthermore, we employ a Chinese open-domain dialogue dataset for evaluation purposes.

Text summarization is the process of condensing a piece of text, such as an article, document, or news story, into a shorter version while preserving its key information and main ideas . Text summarization can be performed through two main approaches:   and . In our evaluation, we utilize multiple abstractive summarization datasets, specifically choosing two renowned datasets for the English and Chinese languages. % % Story generation aims at automatically creating coherent and engaging stories . The input of story generation task can take various forms, including beginning , outline , prompt , or abstract , etc. Advanced methods or models of this task typically involve defining the story structure, characters, settings, and desired narrative elements .% We employ two datasets in Chinese and English, where story beginnings serve as inputs. Additionally, we utilize an English dataset in which story outlines are provided for evaluation purposes.% %     \item    is a compilation of 100,000 short stories, each consisting of five sentences, that display a general sense of understanding. These stories adhere to a daily theme and incorporate a variety of common-sense causal and temporal relationships found in everyday occurrences..%     \item   is a large English dataset of 300K human-written stories paired with writing prompts from an online forum. %     \item   is a benchmark dataset for evaluating Chinese long text understanding and generation.%  Typically, large language models (LLMs) refer to Transformer-based models containing tens or hundreds of billions of parameters and trained on extensive corpora of texts . These LLMs demonstrate significant capabilities in understanding natural language and solving complex tasks. Furthermore, they have showcased their ability to perform new tasks based on textual instructions or with just a few examples . The emergence of these few-shot properties is a result of scaling models to a sufficient size, leading to a line of research that focuses on further scaling these models .

Previous LLMs, such as T5 , GPT-3 , OPT , and PaLM , primarily emphasized scaling model size rather than considering the quality and quantity of data. However, recent studies have demonstrated that, given a fixed compute budget, the best performance is achieved by smaller models trained on larger datasets . Additionally, most of these models are not open-source and can only be accessed through APIs for inference, which poses inconveniences for model evaluation and usage. In order to address this issue, numerous researchers have proposed excellent open-source architectures and trained models, including GLM-130B , ChatGLM , LLaMA , and Pythia . Furthermore, advancements in fine-tuning techniques have contributed to the success of deploying these models with limited resources, such as Lora  and P-Tuning . Therefore, this paper aims to conduct systematic evaluations of these models and their fine-tuned versions, categorized into four groups: .

ChatGPT is a large language model based on OpenAI's GPT-3.5 architecture . It is designed specifically for generating conversations and answering user queries. ChatGPT employs large-scale pretraining and fine-tuning methodologies, utilizing vast amounts of textual data to learn statistical patterns and semantic knowledge of language, and perform well in zero-shot and few-shot settings, and can understand the input instructions.

ChatGLM is a freely available dialogue language model that operates in both Chinese and English languages. It follows the GLM architecture and boasts an impressive parameter count of 6.2 billion. ChatGLM-6B incorporates similar technology as ChatGPT, with a specific focus on Chinese question answering and dialogue. The model undergoes extensive training on a dataset containing approximately 1 trillion tokens in Chinese and English. The training process includes supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. Despite having only 6.2 billion parameters, the model demonstrates the ability to generate responses that align with human preferences.

T5 , which stands for Text-To-Text Transfer Transformer, is a transformer-based language model developed by Google Research. Instead of training separate models for different tasks, T5 is trained in a text-to-text pattern. This means that it is trained to perform a wide range of NLP tasks by transforming the input text into a standardized format that specifies the task to be performed. In our evaluation, we select two new fine-tuned versions of T5, namely: Flan-T5-XXL and FastChat-T5.   Flan-T5  is a fine-tuned version model class of T5 that has been trained on a variety of datasets phrased as instructions. It has shown impressive performance on several benchmarks, demonstrating strong zero-shot, few-shot, and Chain-of-Thought (CoT)  abilities. Flan-T5-XXL is the largest released checkpoint of this model, boasting a parameter volume of 13B. It inherits the extensive knowledge base of T5 while also being capable of understanding natural language instructions and performing the corresponding tasks.  FastChat  is an open platform for training, serving, and evaluating large language model based chatbots. And FastChat-T5 is an open-source chatbot trained on this platform by fine-tuning Flan-T5-XL (3B parameters) on user-shared conversations collected from ShareGPT.

LLaMA  is a collection of foundation language models ranging from 7B to 65B parameters proposed by Meta AI. Unlike other famous LLMs, LLaMA is only trained on publicly avaiable data, making it compatible with open-sourcing. Numerous remarkable and impressive models have emerged as a result, built upon the LLaMA framework and trained using diverse datasets. Among these models, we have chosen a few prominent ones for evaluation: Open-LLaMA, Vicuna, Alpaca, and GPT4ALL.  Open-LLaMA  is an open reproduction of LLaMA trained on the RedPajama dataset . We leverage the 7B version of this model  for evaluation.

 is fine-tuned based on a 7B LLaMA model using a dataset consisting of 52,000 instances of instruction-following data. This dataset is generated using the techniques outlined in the Self-Instruct paper , which aims to address the limited instruction-following capabilities of LLaMA models. To create the training data, the authors initially generate the data using OpenAI's GPT-3 and subsequently convert it into 52,000 instances of instruction-following conversational data using the Self-Instruct pipeline. This dataset is referred to as the Alpaca dataset. The Alpaca model is then fine-tuned to generate responses in conversations similar to ChatGPT.

In our evaluation, we utilize Alpaca-Lora-7B, a low-rank adapter for LLaMA-7b fit on the Stanford Alpaca dataset, and Chinese-Alpaca-13b, a Chinese model version of Alpaca.

 is fine-tuned based on LLaMA models using user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture. So it is basically fine-tuned with ChatGPT conversations. We utilize the 13B version of Vicuna, which is Vicuna-13B. % and Chinese-Vicuna-13B. is a fine-tuned LLaMA 13B model and the GPT4All community has built the GPT4All Open Source datalake as a staging ground for contributing instruction and assistant tuning data for future GPT4All model trains.

Pythia  is a project by EleutherAI that combines interpret-ability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive Transformers. We utilize two versions of Pythia which are Oasst-Pythia and Dolly.

 is an open assistant model developed by the Open-Assistant project. It is based on a Pythia 12B model that was fine-tuned on human demonstrations of assistant conversations collected through the Open-Assistant human feedback web app.

 is a Language Model (LLM) with 12B parameters, designed to follow instructions accurately. It has been trained on approximately 15,000 instruction/response fine-tuning records known as databricks-dolly-15k. These records were created by Databricks employees and cover various capability domains sourced from InstructGPT . These domains include brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.

% % Please add the following required packages to your document preamble:% % \usepackage[normalem]{ulem}% {\ul}{} In our evaluation, we aim to showcase the generation capabilities of LLMs in zero-shot scenarios. Therefore, we refrain from providing any additional information to the model for each of the aforementioned datasets. Specifically:

We defer the evaluation of LLMs on Chinese datasets and other NLG tasks such as story generation, along with results of manual and GPT-4 rating, to future research endeavors.

Because LLMs that we evaluate possess the ability to comprehend instructions and perform corresponding tasks, so in order to ensure fairness, we develop an input template that is applied to every dataset for each task, serving as the input for every large language model. This template consists of two components: the instruction and the input. Figure  illustrates the templates designed for both the Chinese and English datasets, and Table  shows the content of  and  for each dataset.

Although each LLM may have its own optimal decoding strategy, for the sake of fairness, we have standardized these hyperparameters across all LLMs. We employ the Top-k and Top-p sampling, with  and . Additionally, a temperature value of  and a repetition penalty factor of  are imposed. Furthermore, we specify a maximum token length of  and a minimum token length of  for the generated content.

Through case study, we observe that despite emphasizing the exclusion of any additional output in the input, regrettably, most LLMs still generate redundant information in their output. Therefore, we find it necessary to apply post-processing to the outputs of these models. To ensure fairness, we adopt the same post-processing strategy for all LLMs. Specifically, we utilize the keywords ``'' or ``'' for segmentation. If the segmented content consists of a single line, we consider it as the final result. If the segmented content spans multiple lines, we use ``||'' as segmentation keywords and select the first sentence with a length not less than 16 as the final result.

There have been numerous previous works on datasets we used, and these works have achieved good results. Therefore, despite the fact that most of these works have proposed models much smaller than LLMs and have predominantly utilized supervised fine-tuning methods, we still compare them with LLMs to highlight some characteristics of LLMs. For each dataset, we select several recent works with better performance and report their results.

We utilize several common automatic metrics for NLG tasks.  is used to assess the difficulty or confusion of a language model in predicting a sequence of words.  (B-1, B-2, B-3, B-4)  is used to assess the quality of machine-generated translations by comparing them to human reference translations.  (MT)  considers the accuracy and recall based on the entire corpus, and get the final measure.  (R-L)  calculates the overlap between the generated output and the reference summaries or translations using various techniques such as N-gram matching.  (D-1, D-2)  quantifies how many distinct or different N-grams are present in the generated text, providing an indication of the model's ability to produce varied and non-repetitive output.

Besides these widely-used metrics, we also develop a new metric called  (PPR), which means the proportion of samples that need to be post-processed to the total number of samples. % % We conduct a human evaluation on open-domain dialogue generation. We recruit university students to evaluate the quality of conversations.% We follow up previous dialogue generation efforts  and employ several metrics to evaluate the dialogue quality :  measures relevance to the dialogue context,  evaluates information provided, and  checks grammatical accuracy. We also check for  and factual errors.% Note that the Coherence, Informativeness, and Fluency scale is , whose higher score indicates a better performance. Moreover, the scale of Hallucination is , whose lower score indicates a better performance. The automatic metrics results of LLMs on the three datasets are shown in Table , ,  and . Although automatic metrics cannot fully reflect the performance of the models, we can still draw the following conclusions from them.

First, apart from ChatGPT that has the largest scale of 175B, the two T5-based models consistently outperform others in terms of the  metric. This indicates that the generated content of Flan-T5-XXL and FastChat-T5 largely aligns with the instruction requirements stated in the input template: "" Interestingly, both of these models follow an encoder-decoder architecture, while all other models follow a decoder-only architecture. This suggests that encoder-decoder models demonstrate superior understanding of input instructions under the same model scale. We speculate that having an encoder allows the model to comprehend the input content effectively, thereby executing the corresponding task more successfully.

Second, Alpaca-Lora consistently ranks either first or second in the richness of output content. Moreover, the models using the same architecture as Alpaca-Lora also achieve higher scores in terms of D-1 and D-2. This indicates that LLAMA-based models are capable of producing more diverse and less repetitive content.

Last, ChatGPT, the model with the largest parameter scale, performs the best overall on all four datasets, securing the first or second position most frequently. This suggests that increasing the parameter size and training data volume of LLMs is consistently one of the most important methods for improving model performance.

The automatic metrics results of LLMs on the three datasets are shown in Table ,  and . Our observations from the two datasets can be summarized as follows:

The Flan-T5 and FastChat-T5 models employ an encoder-decoder architecture, exhibiting remarkable proficiency in instruction comprehension, as evident by their minimal requirement for post-processing. This finding is corroborated by the analysis of dialogue generation. Moreover, our investigation on the XSum dataset reveals that both models surpass other LLMs, consistently attaining top positions across various metrics such as BLEU and ROUGE scores. These impressive results are likely attributed to the inherent strengths embedded within their model structures.

The automatic metrics results of LLMs on the three datasets are shown in Table  and . These tables provide further analysis of the performance of current LLMs on NLG tasks: when the required generated text is excessively long, the models struggle to follow instructions effectively. This is evidenced by the WritingPrompts dataset from Table~, where many models have BLEU scores that are close to or equal to zero.

To illustrate the enhancement in the performance of LLMs using parameter-efficient fine-Tuning methods, we employ LoRA~ and P-Tuning V2~ to fine-tune ChatGLM-6B, LLaMA2-7B, and LLaMA2-7B-Chat. The results on the Empathetic Dialogues, LCCC, ROCStories, LOT, and LCSTS are presented in Table~ and~. As shown in the tables, the scores of various metrics significantly improve after fine-tuning the models compared to the non-fine-tuned results. Furthermore, the results in N-grams Matching metrics (BLEU and Rouge) far surpass the previous SOTA results. This demonstrates that LoRA and P-Tuning V2 can substantially enhance the fitting capability of LLMs to datasets without incurring excessive computational resources.