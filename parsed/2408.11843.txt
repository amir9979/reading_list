The main idea of dataset construction is two-fold. First, to measure the ability of knowledge retention, we propose to create a commonsense knowledge dataset. Second, to prevent excessive knowledge retention and to measure generalization ability, we propose to construct a paraphrased social bias dataset. The process of dataset construction is illustrated in Figure~. To ensure the quality of the generated data, we propose to collect real-world social biases  from existing datasets, which will serve as the basis for data generation. Social biases are gathered from three domains (gender, race, and religion) across six datasets, including StereoSet~, Crows-Pairs~, WEAT~, WinoBias~, Winogender~, and BEC-Pro~. Each dataset comprises sentences or words demonstrating biases, with details provided in Appendix~.

 To better distinguish the boundary between out-of-scope knowledge and in-scope biases, we propose creating commonsense knowledge about sensitive groups. First, we extract sensitive subjects (e.g., , ) from . Then, we generate commonsense knowledge  about these subjects by prompting GPT-4. Finally, we manually validate the usability of . Knowledge in  does not constitute bias and should be retained after debiasing. However, it tends to be distorted by group-invariant debiasing methods.

To prevent excessive knowledge retention, we propose to evaluate the generalization ability on further social biases. For social biases in , we propose generating semantically similar expressions . To ensure the quality of the generated data, we have conducted meticulous human validation, with details provided in Appendix~. We also analyze the diversity and challenge of BiaScope using case examples in Appendix~.

In this part, we introduce the corresponding evaluation metrics for the constructed datasets. 

 assesses the percentage of commonsense knowledge in  retained after debiasing. The evaluation of  is conducted according to the following criteria:

where  represents commonsense knowledge in .  and  represent the prediction of the original and debiased model.  is the indicator function.

 evaluates the  generalization ability on paraphrased biases in . As a complement to RS, it aims to prevent the model from over-retaining knowledge and thereby losing its generalization ability. It computes the percentage of data that a model gives a biased prediction as opposed to an unbiased prediction:

where  and  denotes the probability of the biased prediction and unbiased prediction.

In this step, we propose to investigate the storage of social biases in LLMs. Typically, a social bias consists of a certain social group and the biased description, which amplifies social inequalities~. For instance, in the statement ,'' the phrase '' is the biased description associated with the social group ``.'' In light of this, we formalize the social bias as a knowledge triplet , where  is the subject (i.e., ),  is the object (i.e., ), and  is the relation between them (i.e., ), inspired by .

To investigate how social bias  is stored as association between the social group and biased description, we propose to use its counterfactual knowledge  for contrast. This involves altering the social group or biased description (e.g., changing  to ) to better probe these biased associations. Our contrastive bias localization is performed in three runs, with a complete illustration in Figure~ in the Appendix~:

: We pass the biased prompt  into the model and collect all hidden states   , in a forward run towards biased prediction, where  is number of layers.

: We pass the counterfactual prompt  to the model to modify the biased prediction. Hidden states will also change due to the alteration of the input subject.

: To measure the effect of certain layer  in the model to the biased prediction, we restore the biased states  of  and perform the forward run. Then we calculate the recovery degree of biased prediction, which indicates the effect of layer .

 Denote the prediction probability on the object of the biased run as , and the probability of the counterfactual run as . In this way, the total biased effect (TE) can be defined as: TE . In the restoration run, the probability will recover from  to  due to the restoration of certain biased states , which reflects the contribution of these states to the biased prediction. Denote the probability of restoring layer  as . The indirect biased effect (i.e., recovery degree) IE of layer  can be calculated by IE . The layer with the largest IE will be selected as the decisive layer. 

Following Step 1, we propose to select the layer that contributes most significantly to the bias as the decisive layer. Assuming the input hidden states to be , the decisive layer (i.e., feed-forward network, FFN) in the original LLM can be formulated as follows:

where  and  denote the parameters (i.e., keys and values matrices) of the first and second linear layers in the FFN, respectively. We propose to envelop the decisive layer with a fairness stamp. The fairness stamp is a 2-layer Feed-Forward Network (FFN) layer, which helps modify the output of the decisive layer with a few external parameters to achieve the goal of fairness. The output of the enveloped FFN layer is given by:

where ,    are the parameters of the fairness stamp. Then, the stamp is optimized with the objectives of bias mitigation and knowledge retention, while other parameters are frozen.

 With a social bias  and its counterfactual knowledge , we propose to mitigate the gap between their probabilities of prediction on the objects:

where  follows the definition in Section~.  denotes the probability of predicting the object  given the prompt .  

We propose to retain knowledge in two ways. First, we retain the probability distribution for the input prompts  to control the deviation from the original model. Second, we retain the probability distribution on the prompt  that combines pre-defined template (e.g., ``'') and the input subject (e.g., ), which helps retain the perception of different social groups and prevent the model from degradation of knowledge. The two loss functions are as follows:

where  is the predicted probability vector of all objects.  and  represent the origin and debiased model.  represents the Kullback-Leibler Divergence.

To prevent the model from overfitting to particular inputs, we utilize prefix texts  to enhance generalization ability across various contexts. These prefix texts are randomly generated by the model, for instance, "", and are concatenated to the front of the prompts.

The overall objective can be formulated as follows with hyper-parameters  and :

We employ the representative masked language model  ()~ and generative language model  ()~ as our backbones. Extended experiments are conducted on , ~ and ~. 

We consider the following debiasing techniques as baselines. The techniques can be grouped into two categories. :  Counterfactual Data Augmentation ~, ~, ~ and Iterative Nullspace Projection ~  pre-train, perform dropout on the rebalanced corpus, or remove sensitive attributes from the representations. ~ mitigates gender bias utilizing a contrastive learning objective on entailment labels. : ~ proposes to directly probe the encoded biases through prompts, then mitigate biases via distribution alignment loss. : ~ proposes to leverage internal knowledge to discourage it from generating biased text.  ~ proposes a machine unlearning-based strategy to efficiently remove bias. : ~ and ~ are proposed to effectively and efficiently modify the knowledge in a language model. We adapt their knowledge updating objectives to align with our fairness objectives for comparison, while all other settings remain unchanged.

We evaluate the debiasing performance on ~, ~, as well as our proposed . Stereotype Score , Language Modeling Score , and Ideal Context Association Test Score  correspond to StereoSet to evaluate the extent of bias, the ability to predict reasonable association, and the combined score of the former two, with details in Appendix~.  is also employed by Crows-Pairs to measure bias. As for BiaScope, we utilize   and , as introduced in Section~. 

Bias mitigation is conducted over the collected biased knowledge in Section~.  We utilize two-layer fully connected neural networks with the ReLU activation function as the fairness stamp, with a hidden dimension of 1024. Additional details are in Appendix~.

The debiasing results are delineated in Table~ and Table~. It is observed that all debiasing baselines fail to yield satisfactory results in knowledge retention (i.e., RS), which proves our claim that group-invariant methods compromise the individual knowledge to distinguish between different social groups.

As shown in Table~ and Table~, our proposed FAST is the first to achieve near-perfect bias mitigation (i.e., SS lower than 52 for BERT) on the two evaluating datasets, while SS of existing approaches, in terms of gender, are still higher than 56. Further, FAST can also largely retain a high RS, and achieve the highest LMS and ICAT.  This demonstrates the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs.  In addition, we report the performance of knowledge-editing approaches ROME and MEMIT. It can be discerned that neither ROME nor MEMIT significantly improves SS over vanilla BERT. Overall, comparing results demonstrate the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs.  Supplemented debiasing results and qualitative studies are in Appendix~.

In order to further validate the scalability of FAST, we conduct additional experiments on larger models, i.e., GPT2-XL, GPT-Neo-2.7B, and Llama-2-7B, with results reported in Table~.  After debiasing, FAST induces a significant reduction (9.4 in average) in SS, and a great improvement in ICAT. Meanwhile, FAST can also retain the Retention Score for larger language models. These demonstrate the consistent effectiveness and scalability of FAST.

 is the most straightforward measure for the  within the debiased model~. It computes the percentage of knowledge for which a model assigns the biased object as opposed to the unbiased object. The evaluation of  is conducted according to the following criteria:

where  is the debiased model.

, employed in StereoSet~, has been utilized. Based on the knowledge pairs in , we select an irrelevant  to form . LMS represents the percentage that a model that prefers a relevant association (either the stereotypical association or the anti-stereotypical association) as opposed to an irrelevant association. The evaluation of  is conducted according to the following criteria:

 is proposed by  combine both LMS and SS by . It represents the language modeling ability of a model while behaving in an unbiased manner.

To ensure the quality and diversity of the generated data, meticulous human validation was performed. We employed four human evaluators, all of whom possess good English proficiency, to annotate and verify the feasibility of the datasets generated by GPT-4. 

 For each knowledge pair within , we paraphrase the prompts combining  with the same semantic expression.  We hired 2 undergraduate students, all with good English proficiency. We first asked the students to manually paraphrase ten pieces of biased knowledge into semantically similar ones. Then, the manually paraphrased samples were combined with the prompt as context for GPT-4 generation. After generation, we performed sample checks on 10\% of the data for each dataset. In these samples, the agreement on successful generation reached 100\%. 

We construct  by collecting commonsense facts related to the sensitive attributes, such as ``'' We initially created alternative facts by prompting the GPT-4 API. We then asked the students to manually validate every generated fact, ensuring that each fact in the retention dataset constitutes reasonable commonsense knowledge rather than bias.

Our retention dataset collect data that contrasts existing debiasing evaluation datasets by incorporating both stereotypical and natural gender differences. Traditional stereotype data (e.g., "In common sense, the mom brings up the child.") often reflects the stereotypes of gender roles in human society. In contrast, our retention dataset includes examples like "In common sense, the mom gives birth to the child." that emphasize biological gender distinctions. This approach expands the scope of debiasing evaluation to include both gender biases that need to be addressed and natural gender differences that should be acknowledged. Furthermore, regarding the diversity of the retention dataset, various perspectives of commonsense differentiating knowledge are taken into account during the generation process. In Table~ and Table~, we showcase data cases generated by GPT-4 from different perspectives, highlighting the dataset's diversity. For the paraphrased dataset, our primary goal is to generate data that retain the original sentence's meaning while avoiding the introduction of new biases. Consequently, the diversity of the paraphrased dataset is dependent on the diversity of the original biased data. To achieve greater diversity than existing benchmarks, we create paraphrases from biased expressions in various formats from six distinct sources, as illustrated in Table~.

As shown in Table~, the ideal RS score is 100\%, while the average RS score for all debiasing baselines is only 70.57\%, indicating a significant 29.43\% shortfall from the optimal value. This substantial discrepancy underscores the difficulty of our benchmark. In contrast, our method exhibits a deviation of only 4.17\% from the optimal RS, which is approximately one-sixth of the gap observed in the baseline method. These results highlight the limitations of existing group-equalizing methods and the superiority of our approach. PS is designed to complement RS by preventing excessive preservation of knowledge. The optimal PS value is 50\%,  and certain baselines, such as INLP and MABEL, are in close proximity to this optimal value, reflecting their debiasing efficacy. However, many baseline PS scores significantly deviate from the ideal value (e.g., 58.65\% for Dropout and 57.64\% for AutoDebias), which emphasizes the challenge posed by our benchmark.

Considering a transformer-based Large Language Model (take a decoder-based transformer as an example). The model takes into the input sequence  and predicts the probability of the next token (). The updates in a transformer block can be formulated as follows:

where  and  represents the output of the  feed-forward network layer and self-attention layer.

In this section, we provide a complete illustration of our Step~1 in Figure~. 

Denote  as a biased knowledge such as ().  is the counterfactual knowledge (i.e.,  is ). Our biased knowledge localization is performed in three steps, with a complete illustration in Figure~ in the Appendix:

: We pass the prompt  into the model and collect all hidden states   ,  where  is number of tokens and  is number of layers.

: We replace the subject with  and pass the new prompt  to the model to corrupt the biased prediction. Hidden states corresponding to the subject token(s)  will be updated with . 

: Towards certain layer  in the model, we hook the biased states  at subject token(s)  and perform the counterfactual run. Then we calculate the recovery degree of biased prediction, which indicates the causal effect of  to biased prediction. The layer with highest causal effect will be selected as the decisive layer.

 Denote ,  as the probability of biased prediction and counterfactual prediction. Let  denotes the probability of counterfactual prediction with restoration of the biased states . The indirect causal effect (IE) of a certain layer can be calculated by .

We collect biased knowledge related to three domains (gender, race, and religion) from six existing datasets (StereoSet~, Crows-Pairs~, WEAT~, WinoBias~, Winogender~ and BEC-Pro~). These datasets have been benchmarked to detect biases within Language Models (LLMs). The statistics of our constructed knowledge base can be referred to Table~, with a detailed description referred to in the following.

~ employs a methodology to evaluate a language model's propensity for stereotypical associations. The procedure is essentially a fill-in-the-blank challenge, where the model is given a sentence with a missing word and must select from a stereotypical word, an anti-stereotypical word, or an irrelevant word. 

~ constitutes a dataset featuring intrasentential minimal pairs. Each pair comprises one sentence depicting a socially disadvantaged group in a manner that either conforms to or contradicts a stereotype, and another sentence that is slightly altered to reference a contrasting, advantaged group. The language model's task involves assessing the probability of masked tokens that are exclusive to each sentence within these pairs.

~ is comprised of word sets that pertain to either attributes or targets. It evaluates the associations between concepts of social groups (for instance, masculine and feminine terms) and neutral attributes (such as terms related to family and occupation).

~ and ~ are designed to assess gender-based stereotypical associations with various occupations. In some instances, these evaluations involve associating gender-specific pronouns with occupations that are stereotypically linked to that gender. In other cases, the task is to associate pronouns with occupations that are typically considered non-stereotypical for that gender.

 (The Bias Evaluation Corpus with Professions)~ is a tool for assessing gender biases in the context of occupations. It comprises 5,400 sentences, each generated from a template that includes a term denoting a person and one of 60 professional terms. During the evaluation process, both the person-related and professional words in these sentences are masked for analysis.

We consider the following debiasing techniques as baselines. The techniques can be grouped into two categories. : ~ involves re-balancing a corpus by swapping bias attribute words (e.g., he/she) in a dataset. The re-balanced corpus is then often used for further training to debias a model. ~ proposes to increase the dropout parameters and perform an additional phase of pre-training to debias. ~ proposes to obtain debiased representation by subtracting biased projection on the estimated bias subspace from the original sentence representation. ~ is also a projection-based debiasing technique to remove protected property from the representations. ~ mitigates Gender Bias using Entailment Labels. : ~ proposes to directly probe the biases encoded in pre-trained models through prompts, then mitigate biases via distribution alignment loss. : ~ proposes to leverage a model's internal knowledge to discourage it from generating biased text.  ~ proposes a machine unlearning-based strategy to efficiently remove the bias in a trained model. We also include  the original model on the same data and with the same objectives as our proposed .

Bias mitigation is conducted over the collected biased knowledge in Section~.  We utilize two-layer fully connected neural networks with the ReLU activation function as the fairness stamp. The hidden dimension is 1024. The batch size is set to 4. We use Adam optimizer with a learning rate of 0.1. We train each batch for 20 iterations.  is set to be 40 and  is 0.1. The model is trained on 8 RTX 3090 GPUs with 24G memory. We utilize pre-trained backbone models in the Huggingface Transformers library~.

we present the results of knowledge locating on other backbones, as illustrated in Figure~ and Figure~. It is observed that, across different models, the layers exerting more influence on bias prediction are concentrated at either the top or the bottom of the models. Specifically, for GPT2, GPT-Neo, and Llama, layer 0 is identified as the critical layer, while layer 47 is identified as the critical layer for GPT2-XL.

Furthermore, we have conducted experiments on the average indirect effect of different positions~(tokens) in the prompts of biased knowledge, as shown in Figure~. Results indicate that the subject in the prompt exerts the most substantial influence on the model's bias prediction, while other tokens also affect bias prediction to varying degrees. 

We provide some examples of our FAST in Table~. It can be observed that in terms of biased knowledge and paraphrased biased knowledge, FAST can calibrate the tendency to predict a biased object. On the other hand, for commonsense knowledge, the debiased model still outputs the correct object. These demonstrate the effectiveness of bias mitigation and knowledge retention of our FAST.

in terms of religion are supplemented in Table~. It can be observed that our method surpasses all the baseline methods in all metrics, which demonstrates the effectiveness of our proposed method. 

 in terms of race and religion are presented in Table~, which also demonstrates the consistent performance of our method in different debiasing tasks.

We also report the debiasing performance on the test sets BEC-Pro and Winogender in Table.~. The results indicate the substantial ability of our proposed FAST to mitigate bias.

We average the causal tracing results across all training samples and localize only one layer for parameter efficiency. The distribution across layers exhibits a clear pattern where the indirect effect of the last layer is more than twice that of the others (Figure~). We analyze statistics on the bias layers across different datasets. and quantify the number of individual data instances in each dataset that result in the same bias layer, as shown in Table~. Different datasets tend to result in similar bias layer location, and within each dataset, most samples lead to the same layer. Additionally, we report the distribution of bias layer by the number of samples in Table~. Bias layers span all layers, with layer 11 accounting for a large proportion of samples.  % The objective of locating bias layers is to understand knowledge storage mechanisms (i.e., to determine if specific layers play a more crucial role than others in recalling biased knowledge), thereby guiding more precise and efficient bias calibration. The effectiveness of our knowledge location strategy is further confirmed through empirical experiments in Section~ of our manuscript.  While our statistical conclusions are consistent across bias layers, it must be acknowledged that the bias layer does not represent the vast majority of data (for example, 90\%). Thus, the bias layer may vary with different datasets. Using multiple layers, as in MEMIT, represents a potential improvement strategy.

To validate the effectiveness of our proposed fairness stamp (Section~), we compare our proposed  with directly  the original model on the same data and with the same objectives. We report the performance of fine-tuning on all layers () and on the located layer (), with results provided in Table~. It can be discerned that there is a significant decline in RS and LMS, while FT can achieve comparable SS scores with our method.  This suggests that direct fine-tuning of model parameters can lead to overfitting to the new data, thereby affecting existing knowledge.

We investigate the effectiveness of our proposed method under continual debiasing settings. We perform FAST on different knowledge sets in sequence and evaluate their performance. Results are reported in Table~. It can be observed that SS obtained in the front stages is steady across the following stages, which indicates that after calibration on other knowledge, existing stored knowledge is still retained. Besides, LMS and ICAT even increase slightly in the process. These results prove the feasibility of continually updating the perception within large language models.

In this section, we discuss the effectiveness of inserting our fairness stamp into multiple layers. We conduct experiments on the located layer and its adjacent one and two layers, with results presented in Table~ andTable~. It can be observed that debiasing results vary slightly with the number of layers, illustrating the robustness of one-layer calibration.

In Table~, we report the number of external parameters and time consumed by our proposed method in two stages. The time is counted on a single RTX 3090. The debiasing process can be finished in several seconds, indicating the feasibility of timely LLM calibration.

We assess the sensitivity of batch size in the debiasing process. We alter the batch size from 1 to 128 and evaluate the debiasing performance, with results presented in Table~. It can be observed that the performance is consistent across different batches of calibrated knowledge, which proves the robustness of our proposed method in practical usage.

We supplemented the results of verifying the robustness of our method under varying memory sizes in Table~. We alter the dimension of hidden states (dim) in our FAST, thereby changing the number of external parameters. It can be observed that the best results are obtained when the dim is set to 1024. Further increases in dim do not yield better debiasing results.  On the other hand, as the dim continues to decrease, the decline in SS is not significant, indicating the robustness of our approach with limited parameters. However, LMS and RS deteriorate with the continuous decrease in dim, suggesting that a reduced number of parameters cannot embed specific knowledge calibration. Therefore, we ultimately select 1024 as the dimension of the hidden states.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs. Code will be publicly available.

redWarning: this paper contains content that may be offensive or upsetting.IntroductionIntroductiondevlin2018bert, floridi2020gpt, brown2020languagezhao2019gender, navigli2023biasessheng2021societalmay2019measuring, gehman2020realtoxicityprompts, kaneko2021debiasinggallegos2023bias, garrido2021surveyzmigrod2019counterfactualliang2020towards, ravfogel2020nullhe2022mabel, cheng2021fairfilguo2022auto, yang2023adept, li2023prompt, dong2023co%图片全局居中-0.1inwidth=1.0\linewidthfigures/fig1v4.drawio.pdf\small (a) Expression towards different groups (e.g., mom/dad) does not necessarily constitute a bias. (b) Existing debiasing approaches indiscriminately neutralize different social groups, resulting in unreasonable predictions. (c) Our approach performs fine-grained calibration on biases, while retaining other knowledge.fig:Illustrationgallegos2023biasfig:IllustrationMy  gives birth to me.hanna2020towards, gallegos2023bias, kumar2022language, devinney2022theoriesliang2020towards, he2022mabel, dong2023cogallegos2023biasBiaScopeMy mom gives birth to me.Fairness-Stamp (FAST)sinitsin2020editable, de2021editingfig:IllustrationWe identify that existing debiasing benchmarks lack sufficient evaluation regarding the effects of debiasing techniques, and introduce a new benchmark  to evaluate the ability to retain individual commonsense facts and generalize to other social biases.   BiaScopeWe propose a novel bias mitigation framework , which calibrates fine-grained individual biases, enabling nuanced model debiasing.   FASTComprehensive experiments and superior performance demonstrate the significant potential of our fine-grained debiasing strategy in achieving fairness for LLMs.  Related WorksRelated WorksBias Mitigation in Large Language Models.(1) Fine-tuningzmigrod2019counterfactual, webster2020measuringhe2022mabel, cheng2021fairfilliang2020towards, ravfogel2020null, kaneko2021debiasing, dev2020measuringhan2021diverse, he2022mabellauscher2021sustainable, xie2023empirical(2) Prompt-tuningguo2022auto, yang2023adept, li2023prompt, dong2023copost-hocschick2021self,chen2023fastgallegos2023biashanna2020towardsKnowledge Editing.Knowledge LocatingPreliminariessinitsin2020editable, de2021editing, dai2021knowledgemitchell2022memory, murty2022fixing, dong2022calibrating, hartvigsen2022aging, huang2023transformer, zheng2023canmitchell2021fast, gupta2023editing, hase2021language, meng2022locatingmeng2022locating,meng2022mass,dai2021knowledge, li2023pmet, chen2024largevig2020investigating, finlayson2021causal, chen2024learnable%图片全局居中width=1.0\linewidthfigures/BiasKEv6.drawio.pdfAn illustration depicting the construction process of the BiaScope benchmark.illustration of construction of BiasKEBiaScope BenchmarkBiasKE Benchmark Constructiongallegos2023biasBiaScopeBiased Knowledge DatasetEvaluating MetricsDataset ConstructionBiased Knowledge Datasetillustration of construction of BiasKEnadeem2020stereosetnangia2020crowscaliskan2017semanticszhao2018genderrudinger2018genderbartl2020unmaskingBiased Knowledge Benchmark datasetCreate commonsense knowledge dataset.man/womanChristians/JewsCreate paraphrased social bias dataset.Dataset Construction DetailsDiversity and Challenge AnalysisEvaluation MetricsEvaluating MetricsRetention Score (RS)

(, {^*},\Omega_{R}) =  _{k_R \in \Omega_{R}}\{{}[k_R] = {^*}[k_R]\}, Paraphrase Stereotype Score (PS)

(^*, \Omega_{P}) =  _{k_p \in \Omega_{P}}\{_{^*}[k_p] > _{^*}[k_p^{'}]\}, %图片全局居中width=1.0\linewidthfigures/ICML_24_pipelinev15.pdfAn illustration of our FAST framework. (a) We first locate the decisive layer towards biased predictions. (b) A fairness stamp is inserted at the decisive layer, which is optimized with the objective of bias mitigation and knowledge retention.fig:pipelineMethodMethodTask Formulation.Overview.FASTfig:pipelineStep~1: Locate the Decisive Layer for Biasesstep1Definition of Social Bias.Biased Knowledgewang2023decodingtrust, bommasani2022trustworthy, allport1954natureBlack people are more likely to commit a crimeare more likely to commit a crimeBlack peopleBlack peoplecommit a crimeare more likely topetroni2019language, jiang2020canContrastive Social Biases Localization.Black peopleWhite peoplefig:Locate Biased KnowledgeLocate Biased Knowledge app(1) Biased run(2) Counterfactual run(3) Restore biased statesDetermine the decisive layer.Step~2: Insert Fairness Stamp at the Decisive LayerStep2 () = (^\top) ,

'() = () + (^\top) , Bias Mitigation.

    _e = {\lvert \Omega \rvert}\sum_{(k_1, k_2)\in \Omega} \lvert  _{}[k_1]- _{}[k_2]\rvert, step1Knowledge Retention.\{subject\} is \_Black people     &_{s1} = {\lvert \Omega \rvert}\sum_{p_i\in \Omega}_{KL} (_{}[\star|p_i], _{^{*}}[\star|p_i]),     &_{s2} = {\lvert \Omega \rvert}\sum_{s_i\in \Omega}_{KL} (_{}[\star|p'(s_i)], _{^{*}}[\star|p'(s_i)]),  x_j My father told me that

     = _e + \alpha _{s1} + \beta _{s2}. ExperimentExperimentExperiment detailsExperiment detailsModels.BERTbert-base-uncaseddevlin2018bertGPT2GPT2-smallradford2019gpt2GPT2-XLGPT-Neo-2.7bgpt-neoLlama-2-7btouvron2023llamaBaselines.(1) Fine-tuning(CDA)zmigrod2019counterfactualDropoutwebster2020measuringSentenceDebiasliang2020towards(INLP)ravfogel2020nullMABELhe2022mabel(2) Prompt-tuningAuto-debiasguo2022auto(3) Post-hocSelf-Debiasschick2021selfFMDchen2023fast(4) Knowledge EditingROMEmeng2022locatingMEMITmeng2022massDatasets and Evaluating Metrics.StereoSetstereoset2020Crows-Pairsnangia2020crowsBiaScope(SS)(LMS)(ICAT)Biased Knowledge BenchmarkSSRSPSEvaluating MetricsImplementation details.Biased Knowledgemore Experiment details\small Debiasing results on BERT. The best result is indicated in \textbf. : the closer to 50, the better. ``-'': results are not reported. Reported results are means over three training runs. Due to space limitations, results with statistical significance analysis, as well as results in terms of religion are provided in the Appendix~\ref.0.7 \renewcommand

Debiasing Results on BERTDebiasing PerformanceExisting debiasing methods cannot retain individual commonsense knowledge.Debiasing Results on BERTdebiasing gender GPT2Our approach surpasses existing debiasing and knowledge-editing baselines in both bias mitigation and knowledge retention.Debiasing Results on BERTdebiasing gender GPT2More exp% 中间不能有空格,否则不会并排显示table\small Debiasing Results on GPT-2 in terms of gender. : the closer to 50, the better.0.74         \renewcommand

        debiasing gender GPT210pttable\small Debiasing Results on larger models. : the closer to 50, the better.0.73         \renewcommand         Debiasing Results on larger modelsOur approach scales to larger models.Scalibility to Larger ModelsDebiasing Results on larger modelsAnalysis and DiscussionAnalysis and DiscussionLanguage Modeling Capability Analysis.wang2018glueExperimental results of GLUE tasks on BERT. \small Experimental results of GLUE tasks on BERT. We report Matthew's correlation for CoLA, the Spearman correlation for STS-B, and the F1 score for MRPC and QQP. For all other tasks, we report the accuracy. ``-'' means not reported. The best result is indicated in \textbf and the second best in \underline.0.82 \renewcommand

Experimental results of GLUE tasks on BERT.Effectiveness Analysis of Bias Locating.Effectiveness of Knowledge Locating.Knowledge Locating results of BERT.step1Effectiveness of Knowledge Locatingmore Knowledge Locating ResultsRobustness of Knowledge LocatingStep2secFine-Tuning vs. Our FAST\includegraphics\label\includegraphics\label\includegraphics\label\small (a) The average indirect effects of every layer in BERT. (b) Debiasing Performance on different layers in BERT. (c) Ablation on the Number of External Parameters. Experiments are conducted on BERT in terms of gender. SS is transformed by  so that it is also higher is better.-3pt\small Ablation Study on  and . Experiments are conducted on BERT in terms of gender. : the closer to 50, the better. The best result is in \textbf. 0.77 \renewcommand

Ablation Study on Alpha and Beta-10ptSensitivity Analysis on Hyper-parameters and Fairness-Utility Trade-off Analysis.Ablation Study on Alpha and Betakim2020fact, liu2022accuracyAblation Study on the Number of External Parameters.dimAblation on the Number of External ParametersdimdimdimdimAblation Study on Batch SizeComputational Complexity Analysis.Computational Complexity Analysis% 中间不能有空格,否则不会并排显示table\small Computational complexity analysis on \\BERT and Llama-2. ``B'' denotes billion.-5pt0.85

        Computational Complexity Analysistable\small Results on knowledge-editing task. The best result is in \textbf and the second best in \underline.-5pt0.82         \renewcommand

        Comparison on the Knowledge-editing Task.Effectiveness on the Knowledge-editing Task.levy2017zerogpt-jmeng2022locatingmitchell2021fastmeng2022locatingmeng2022massComparison on the Knowledge-editing Task.-3pt\small Ablation Study on the losses. : the closer to 50, the better. The best result is in \textbf0.8

Ablation Study on losses-10ptAblation Study on the Losses.Ablation Study on the LossesAblation Study on lossesConclusion and LimitationLimitationBroader Impactgeva2020transformer, meng2022locating, geva2022transformer, chen2024learnableDebiasing Results on larger modelswan2023biasasker \small \bibliography BiaScope Benchmark ConstructionMetricsBiased Knowledge BenchmarkStereotype Score (SS)biasnadeem2020stereoset, nangia2020crowsSS

 ({^*},\Omega_{S}) = _{(k_1,k_2) \in \Omega_{S}}\{_{^*}[k_1] > _{^*}[k_2]\}, Language Modeling Score (LMS)nadeem2020stereoset\Omega_{S}o_{ir}k_{ir} = (s, r, o_{ir})

({},\Omega_{S}) &=  _{(k_1, k_2) \in \Omega_{S}}\{_{}[k_1] > _{}[k_{ir}]\} \\ &+\{_{}[k_2] > _{}[k_{ir}]\}. Ideal Context Association Test Score (ICAT)stereoset2020Dataset Construction DetailsDataset Construction DetailsParaphrased dataset.\Omega_{S}(s, r)Retention dataset.\Omega_{R}Jesus' resurrection is commemorated by  when they celebrate Easter.Diversity and Challenge Analysis of BiaScopeDiversity and Challenge AnalysisDiversity of the Benchmark.Commonsense Knowledge Differentiating Male and Female People.Commonsense Knowledge Differentiating Black and White People.constructing biased knowledge pair from different datasets.Challenge of the Benchmark.Challenge of the Benchmark.MethodPreliminaries

h_t^{(l)} = h_t^{(l-1)} &+ a_t^{(l)} + m_t^{(l)},\\        a_t^{(l)} &= (h_1^{(l-1)}, h_2^{(l-1)}, ..., h_t^{(l-1)}),\\       m_t^{(l)} &= W_2 \sigma (W_1 \gamma (a_{t}^{(l)}+h_t^{(l-1)})), Locate Biased KnowledgeLocate Biased Knowledge appfig:Locate Biased KnowledgeThe doctor, performing surgery is a, manThe nursefig:Locate Biased KnowledgeBiased runCounterfactual inputRestoration runCausal effect.%图片全局居中width=1.0\linewidthfigures/ICML_24_pipelinev5.drawio.pdfIllustration of our debiasing framework.fig:Locate Biased KnowledgeExperimentMore expExperiment detailsmore Experiment detailsDataset.Biased Knowledge Benchmark datasetnadeem2020stereosetnangia2020crowscaliskan2017semanticszhao2018genderrudinger2018genderbartl2020unmaskingThe statistics of collected biased knowledgesStereoSetnadeem2020stereosetCrowS-Pairsnangia2020crowsWEATcaliskan2017semanticsWinogenderrudinger2018genderWinobiaszhao2019genderBEC-Probartl2020unmaskingBaselines.(1) Fine-tuningCounterfactual Data Augmentation (CDA)We use the reproduction of CDA, Dropout, SentenceDebias, INLP and Self-Debias provided by \urlzmigrod2019counterfactualDropoutwebster2020measuringSentenceDebiasliang2020towardsIterative Nullspace Projection (INLP)ravfogel2020nullMABELWe use the debiased models provided in \urlhe2022mabel(2) Prompt-tuningAuto-debiasWe use the debiased models provided in \urlguo2022auto(3) Post-hocSelf-Debiasschick2021selfFMDchen2023fastFine-tuning (FT)FASTImplementation details.Biased Knowledgewolf2020transformersExamples of constructing biased knowledge pair from different datasets.0.9 \small

constructing biased knowledge pair from different datasets.Knowledge Locating Resultsmore Knowledge Locating ResultsKnowledge Locating results of GPT2 (left) and GPT2-XL (right).Knowledge Locating results of GPT-Neo (left) and Llama (right).Average indirect effect of every token in the prompts in BERT.Qualitative Studyqualitative examples of our FAST.Debiasing Results on BERT and GPT2more Debiasing ResultsDebiasing Results on BERTDebiasing Results on BERT in terms of religion.Debiasing Results on GPT2Debiasing Results on GPT2 in term of race and religion.Debiasing Results on BEC-Pro and WinogenderDebiasing Results on BEC-Pro and Winogender.Analysismore AnalysisRobustness Analysis of Knowledge LocatingRobustness of Knowledge LocatingKnowledge Locating results of BERT.Location and Ratio of the Bias Layers Across Different Datasets.Location and Ratio of the Bias Layers on StereoSet.Fine-Tuning vs. Our FASTsecFine-Tuning vs. Our FASTStep2FASTFine-tuning (FT)_{}_{}Fine-Tuning vs. Our FASTDebiasing Results on BERT in terms of gender and race. The best result is indicated in \textbf. : the closer to 50, the better.0.7 \renewcommand

Fine-Tuning vs. Our FASTRobustness to the Number of Social BiasesEffectiveness of Continual DebiasingEffectiveness of Multi-layer DebiasingMulti-layer debiasing results and utility analysis on BERT.Multi-layer debiasing results and utility analysis on Llama-2-7b.Unility AnalysisMulti-layer debiasing results and utility analysis on Llama-2-7b.Ablation Study on Batch SizeAblation Study on Batch SizeAblation Study on Knowledge Batch SizeLimitation and Future Workssahoo2022detecting, dev2023buildingyu2022hate, ovalle2023m, fan2024biasalertScalibility to Larger ModelsBroader ImpactBroader ImpactAblation on the Number of External Parameters.table: Ablation on the Number of External Parameters.