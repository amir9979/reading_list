In order to reduce the search length, learning for planning problems can be drawn back to , which focused on learning domain-specific representations for planners through classical machine learning methods. Posing the task as a regression problem, learnt with neural networks was studied in . Post their success, more recent works explored various neural architectures and objective functions for this problem . These works serve to prove the effectiveness of neural networks for learning strong heuristics in A* search, however, their methods are not designed by studying the problem from the planner's perspective, i.e., by analysing the requirements of the planner to reduce the search length.

 Some works do look at the problem from this angle;  reformulate each step of the planner as a differentiable function, and sequentially execute them, with the loss taken after the final step. Backpropagating through time enables them to create a search-dynamics aware heuristic. Similarly,  learn heuristics by performing reinforcement learning on the actions taken by the planner. Such methods, while showing strong performance, incur a high training cost. In this work, we pose the problem as a regression task and take an alternate data-centric approach, where we analyse the planning algorithm, specifically A*, and find an optimal subset of nodes to train the neural network (in our case, language models). With this, we can constrain the training cost, while still learning a heuristic suitable for A*.    While in this work, we only study LMs being used as a heuristic in environments where possible actions are generated by rule-based methods, previous works have also explored using them as a world model, where the LLM generates an action given the state of the environment.  uses such a framework to build a tree and parse it with DFS/BFS, while  extends it to MCTS. The node to be expanded is selected with a heuristic computed by the LLM self-evaluating its generated actions. Such strategies are heavily dependent on the performance of this heuristic , which have been shown to be quite weak. In this work, we aim to improve such a heuristic by training it. Constraining our environments allows us to isolate the heuristic from the world model.  Besides a heuristic, LMs have been combined with external planners in various capacities. For instance,  uses an LLM with the LPG planner , which iteratively corrects errors in a plan. Seeding LPG with an LLM plan has been shown to work better than a random plan. LLMs have also been used to translate tasks to formal languages for symbolic solvers like PDDL and ASP. Combining such planners with LLMs has also been explored in dynamic settings to incorporate environment feedback . While these works primarily use off-the-shelf LLMs to improve symbolic planners, our work aims to train an LM.

 proposed to improve LLM heuristics by incorporating failure states into the input prompt, thereby providing a semantic gradient signal away from such states. This has further been incorporated into tree-based frameworks . Such failure states are discovered during the course of solving a problem, and thus the improvements brought about by them are restricted to that particular instance. On the other hand, we consider the setting where the weights are updated, thus bringing improving performance across problems. An alternate line of work addresses the weak performance of LLMs for planning with CoT prompting. These works  propose to train on the traces of tree-search algorithms, and with bootstrapping methods, aim to implicitly learn an improved heuristic. In contrast, we explicitly learn the heuristic by supervision.

 involves pruning the training dataset to only contain important datapoints, without a significant drop in performance. While various works exist for LM pre-training , to the best of our knowledge, we are the first work to study this in the context of heuristic learning. Our findings correlate with those of ; easier data is required for learning in the low-data regime.

 In identifying nodes required by the LM for generalisation, we categorise them as "easy", "medium" and "hard". Thus, our work also bears some similarities with curriculum learning methods, which propose a temporal ordering of data points of varying difficulty . Some works demonstrate that learning from easier points first, helps learn complex concepts later , others find the reverse to be more beneficial . The aim of curriculum learning is to improve the performance on hard data points, whereas our aim is to reduce the search length of A*, regardless of the difficulty of the training data.

A* Search is a tree-based search algorithm that aims to find a path in an environment, given a start node and a goal node by building a tree . The set of all nodes in the tree is defined as . For each node , A* search keeps track of two values, (i) historical cost , which is the distance between the start node and  and (ii) heuristic  which is an estimate of the true distance  between  and the goal node. Each node also stores the domain-dependent environment state , as a result of an action. For the search, A* maintains two lists, the frontier list  and the closed list , which are used to iteratively build the search tree. At the beginning, the closed list list is empty and the frontier list is initialised with the start node(alias root node). The search is terminated when either the goal state is encountered, or  is empty. Each iteration performs two steps, described below.

 This step picks the "most-promising" leaf node in the search tree. All leaf nodes are stored in the frontier list, implemented as a minimum priority queue on the node's fitness-value . If the state of the selected node is equal to the goal state, the search is terminated. Else, the next step is performed.   This step adds new children nodes to the selected node, thereby expanding the search tree. A child node is only added to the search tree if it is more promising than a future node(in the frontier list) or a previously expanded node(in the closed list). More formally, it is added to the search tree  there does not exist a node with the same state in either the frontier, or the closed list, with a lower  value. Finally, the selected node is moved from the frontier to the closed list.

We define the search length  of A* as the length of the closed list after termination of the search. The use of  makes A* an  search algorithm, significantly reducing the size of the search tree, when compared with uninformed search. The path from start to goal, defined as , is the sequence of  nodes from the root to the goal node. The start-to-goal path with minimum length is called the , denoted by . A* guarantees that the resulting path will be optimal if the heuristic is , i.e., . It can be shown that with  and non-trivial tie-breaking, A* will act as an optimal policy with . An inadmissible heuristic, however, does not guarantee sub-optimal solutions.

A* search is summarised in .

Our goal is train a language model , that, given a node , can predict the residual between the perfect heuristic and its estimated value, . From a puzzle instance , training instances can be derived for this task by considering each node in the search tree. For supervision, the perfect heuristic needs to be calculated, however, doing that for each node will require running A* again for all of them. This becomes very expensive, particularly since the  for a single puzzle can be > 10k. Therefore, we only consider nodes on the optimal path, since calculating their  is trivial : for any node ,. This is also the conventional setting used in previous works . Therefore, the training sequences  are given by .

We consider two losses to train , the L2 loss,

where  represents a forward pass of the LM; and the language modelling objective, given by,

We use encoder-decoder transformers for our experiments, and add a regression head  on the decoder that predicts  given the  token while training with . With , the (pre-trained) language model head  is used. The encoder input consists of a prompt, along with the details of a node.

Inference involves leveraging the trained LM in A* search. This is done by adding an LM inference with the previous heuristic calculation. During the expansion step, the child nodes are converted into a prompt and collated in a batch, which is used to predict  . This is added to , and search continues. Notably, only a single forward pass is performed per expansion. Additionally, we cache these prompts, such that if a state is revisited in a another node ,  can simply be retrieved, rather than predicted. For  trained with , we perform top-k decoding, with , along with self-consistency, predicting 3 sequences, as this works slightly better in practice.

We conduct our experiments on two domains, denoted in  and . All experiments are conducted on two test splits, the IID and OOD. is a standard maze puzzle that involves finding an unobstructed path from the start to the goal state. The state of a node  is characterised by the position of the player on the board. The admissible heuristic function used in the training data (and reference solutions) is the manhattan distance between the player and the goal positions. Training and validation is performed on sequences derived from mazes of size . The IID test split thus consists of mazes of the same size, while the OOD split consists of mazes of size . 

is a japanese puzzle game, that involves a player pushing one or more boxes to fixed docks. This puzzle is considerably harder than maze, since a few wrong moves can lead to deadlocked situations, rendering the puzzle unsolvable. The state of a node  is characterised by the position of the player on the board, and the position of the boxes. Note that all boxes and docks are identical. The admissible heuristic function used is the sum of the minimum manhattan distance between the player position and a box, and the sum of manhattan distances between the boxes and their assigned docks. Boxes are assigned to docks by solving the minimum cost assignment problem with the Hungarian algorithm. Training and validation is performed on 2 box problems, while testing is done on a mixture of harder 2, 3 and 4 box problems.

 The exact data generation process is described in .

We modify the metrics defined in , (i) inverse-length-ratio(ILR) to measure the differences in the search length, (ii) success weighted by cost(SWC) to measure the differences in solution length and (iii) Optimal \%, to measure the percentage of puzzles solved optimally. ILR measures the average inverse ratio between the search length  of an A* solution, to that of a reference . It is formulated as,

ILR can be averaged over various sets, (i) ILR-on-solved is averaged over all puzzles in the test set and (ii) ILR-on-optimal is averaged over all puzzles whose solutions are optimal. Suboptimal solutions, found with inadmissible heuristics, are often discovered with lesser , and  larger ILR.

SWC measures the average inverse ratio between the start-to-goal path length  of an A* solution, to that of an optimal reference, denoted by . It is formulated as, Additionally, to measure computational cost, we also propose a new metric, Inverse-Time-Ratio, which is defined as the average inverse ratio between the wall-clock time of an A* solution  and a reference solution , formulated as, 

ITR can be averaged over the entire test set (ITR-on-solved), or on the problems solved optimally (ITR-on-optimal).

When learning a heuristic with a language model, given a search node , we predict the difference  , where the final heuristic used during the search is . However, as with any neural network, there is some error in the prediction, resulting in a value close to , and in turn close to . We aim to study two aspects, (i) how this error affects , and (ii) how it affects optimality of the solution. To achieve this, we perform experiments with an oracle heuristic, that always uses  during the search. We artificially introduce error into this heuristic to study these effects. The error is introduced in different sections of the tree to identify correlations between the sets of nodes with low error,  and optimality.

 Here, we use to use the perfect heuristic  in certain sections of the tree, and  in other sections.  is calculated offline, by running Dijkstra's algorithm on the maze, starting from the goal. The tree is divided into three sets - ,  and . A node  is placed in the  set if , in the  set if , and in the  set if .  refers to all the nodes in the tree. We perform an ablation, where we use the perfect heuristic in each of these sections, and introduce noise in the others. The upper limit of each metric is given by using  in  sections, albeit with trivial tie-breaking. Results are shown in .

 As evidenced by the scores of  , low error on all the sets leads to the best performance. However, achieving that with a learned heuristic is difficult (discussed later). Hence, we try to identify smaller sets with the best scores. Amongst these, with the same , using  on nodes in the  set performs the best on both, ILR-on-solved and ILR-on-optimal, than using it on any other set. Moreover, using  in the  and  sets has diminishing returns, particularly with higher . 

While there does not seem to be a trend between SWC and Optimal \% amongst the three sets, both these metrics go down with increasing . This is not surprising, since the heuristic will be less admissible, thereby increasing the possibility of finding suboptimal solutions.

 With these experiments, we make the following conclusions, (i) ILR can be improved by having low error in the heuristic for nodes near the goal, (ii) there are diminishing returns to ILR when the heuristic has low error for nodes further away from the goal, and (iii) optimality is directly related to the error in the heuristics.

We have shown that in order to reduce the search length of A*, we need to predict the heuristic more accurately for nodes closer to the goal. Here, we explore how training on these nodes affects generalisation to other nodes, and in turn model performance on ILR. 

 We create four training splits from the puzzles by uniformly sampling nodes (on the optimal path) from the , ,  and  sets. Additionally, we also train on , ,  and , which excludes the corresponding set from the nodes to be sampled. For instance,  is trained on data sampled from the  and  sets. All training splits have the same size. We train the model on both domains with the following evaluations, (i) mean average error on validation splits containing nodes from each of the aforementioned splits and (ii) and ILR performance achieved by models trained on them. While (i) demonstrates the generalisation capability of the model, (ii) serves to extend the observations of the oracle heuristic to the LM.

 shows that each split generalises the best to itself, but shows poor generalisation to the others. The  set achieves the best generalisation to each split. Comparing ILR performance, the LM heuristic performs the best when trained on nodes from the  set, when compared to the performance of  and . However, this is still lesser than the performance of . This confirms that the trends observed with the oracle heuristic corroborate with those seen with the LM.

Amongst the exclusion sets, we observe that  achieves the worst generalisation and ILR performance on both domains and test splits. The generalisation between the other two sets is mixed, with  on maze, and the reverse on sokoban. However, both their validation metrics are below that of . 

Nodes in the  set can be considered "easier" than nodes in the other sets, since the LM requires fewer decision steps to predict their heuristic. We can thus also conclude that in our low-data regime, easier points are necessary for generalisation.

With these observations, we can conclude that the  set has the most contribution to reducing the search length in A*, and is required for enhanced generalisation with LM-based heuristics. Moreover, augmenting the training set with points from the  and  set also leads to better generalisation, as indicated by the reduced validation scores.  We generate mazes with a modified Prim's algorithm. The start and goal states are randomly chosen until the following criteria are met, (i) length of the optimal plan > , (ii) ratio between length of closed set after search and length of optimal plan is > . If either of these are not met within 10 tries, a new maze is generated. Criterion (i) ensures that the start and goal positions are not too close and (ii) ensures that there are sufficient number of additional expanded nodes. It serves as a surrogate for the measure of hardness  where  is the start node, proposed in . The surrogate is used since it is more aligned with the chosen metrics (ILR) in this work. However, this method only creates a maze with a single path to the goal. To get multiple paths, each node is designated to either be closer to the start, or to the goal, and walls are randomly broken at the boundary of these groups.  This dataset is adapted from the open-source boxoban dataset proposed in . For the training puzzles, we randomly shuffle the provided training set from the "unfiltered" split, followed by subsampling  boxes per puzzle. We use the same filters as maze, but with different hyperparameters. The IID test split uses the same criteria, but samples puzzles from the testing set of boxoban. To reduce the data creation time, we constrain the number of iterations required by A* to solve a puzzle between  and . The OOD split is curated to contain a mix of harder puzzles with varying number of boxes, length of optimal plans and higher number of iterations. All puzzles have size ,  and .  The exact details are in  and .

A* search is summarized in .

The language models have been trained on a regression task with context prompts, which are provided below. Since the experiments are performed with code models, we tailor the prompt accordingly. The same prompt is used for both domains, shown in , with the puzzle representations and legend in  for Sokoban and .

All models are trained for 40 epochs, with a learning rate of , batch size of 64 and optimized with Adafactor. We implement early stopping, with the model chosen by best performance on validation MAE, computed every epoch. Training is performed on 1 A6000 Ada GPU.

We show additional results demonstrating the effect of sampling from  with increasing model-scale, in  Recent works in AI planning have proposed to combine LLMs with iterative tree-search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these techniques is not trivial : LM-based heuristics are quite weak, incurring a high computational cost without a significant performance improvement. Existing methods to learn these heuristics do not consider the requirements of the planner, and typically need a lot of compute. Thus, in this work, we propose a distribution to downsample training data by identifying relevant data points to learn a performant heuristic, while constraining computational costs. To arrive at this model, we disentangle the requirements of the planner, in our case A* search, from that of the language model to generalise on this task. Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on nodes near the goal, and LMs need the same set of nodes for effective generalisation. With these insights, we can quantify the contribution of each node towards accelerating A* search, and subsequently derive a training distribution for learning LM-based heuristics. Following a recent work, we conduct our experiments on two classical planning domains, maze navigation and sokoban, with two test splits per domain, and two conventional loss functions. We reduce the number of iterations required to find the solutions by upto , with a wall-clock speed-up of upto . Introductionthinking-fast-and-slow:2011wu2024reasoningvalmeekam2023planninga-star-1968IDA-1985,MCTS-2006paul2021deepTo the best of our knowledge, we are the first work to establish correlations between the nodes of the search tree, and their contribution to reducing the search length of A*. Further, we propose a mathematical model to quantify the contribution of each node.     We study the training data requirements for training LM-based heuristics to generalise and compare them to those of A*, and somewhat surprisingly, find an overlap between them.     Subsequently, we propose a distribution to subsample nodes from the search tree, that outperforms uniform subsampling on 4 test sets and 2 standard loss functions. Moreover, we demonstrate the efficacy of our method by scaling the LM upto 720M parameters. Related WorksLearning Heuristics for PlanningMachine Learning Perspectiveyoon2006learning, fern2011firstarfaee2011learning, us2013learningchrestien2021heuristic, groshev2018learning, kirilenko2023transpathPlanner Perspectiveyonetani2021path, vlastelica2019differentiationspeck2021learning, orseau2023levin, orseau2021policyHeuristics with Language ModelsTree-search in LLMsyao2024treehao2023reasoningchen2024tree-0.3cmLLMs with external plannersvalmeekam2023planninggerevini2002lpgliu2023llm+yang2023couplingguan2023leveraging, dagan2023dynamicImproving LM-based Heuristicsshinn2024reflexionzhou2023languagelehnert2024beyond,gandhi2024streamOptimising Training DataCoreset Selectionpaul2021deep, marion2023less, abbas2023semdedupzhou2023algorithms,sorscher2022beyondCurriculum Learningbengio2009curriculumxu2020curriculummaharana2022curriculumPreliminariesA* SearchSelectionExpansioniffwhich in turn is equal to the number of iterationsinformedoptimal pathadmissibleThis can be easily proved by contradictionalg:astarProblem DefinitionRunning Dijkstra's algorithm is intractable for complex domains, like sokobanchrestien2021heuristic,us2013learning0.5emLoss functions _{L2} = (f(\theta, n) - d^*(n))^2

_{LM} = -\log p(d^*(n)|\theta) Inferencewang2022selfDomainslst:sokobanlst:mazeMaze NavigationSokobansec:data_generationMetricslehnert2024beyond     ILR = {N}\sum_{i=0}^N ^*_i}{}_i}

    SWC = {N}\sum_{i=0}^N {||}

    ITR = {N}\sum_{i=0}^N {_i} Disentangling A* and Heuristic Learningwidth=\textwidthimage-1.pngwidth=\textwidthimage-2.pngValidation MAE of a model trained on splits containing nodes from the \textit, \textit, \textit, \textit sets, and their corresponding exclusion sets. A lower value shows better generalisation.tbl:generalisationExperimental results by training on different splits of data, demonstrating the importance of the \textit split for generalisation to A* search on both, maze and sokoban.tbl:trained_heuristicsec:disentanglingRequirements of A*kirilenko2023transpathExperimentinitialmiddleendinitialmiddleendallalltbl:oracle_heuristicResultsallendmiddleinitialInference 1.Requirements for Heuristic LearningExperimental results by sampling from the proposed  distribution. Best scores are in \textbf. For , the value of  differs on each domain, with  for maze and  for sokoban.tbl:importance_samplingExperimentinitialmiddleendallexclusion setsinitialmiddleendinitialmiddleendResultstbl:generalisationallendmiddleinitialallendmiddleinitialallThe high ILR of \textit is therefore not representativeendInference 2endinitialmiddleProposed SolutionQuantifying a Node's Contribution to Inference 1          (n) = \log({|\pi^*| - g(n)}) Sampling Distributioneqn: contrInference 2 (n, \tau) = SoftMax({\tau}(n)), \forall n \in \pi^* initialmiddleExperimentsExperimental Settingsfull-datatbl:importance_samplingsec:hyperparametersResultsTraining TargetinitialModel Scaletbl:model_scaleComputational Costbylander1994computationaltbl:computational_costConclusionLimitationscustomAppendixA* Searchalg:astar return      SelectionExpansionData Generationsec:data_generationMaze\urltakahashi2019learning\urlSokobanguez2019investigationtbl:datasets_mazetbl:datasets_sokobancodegreenrgb0,0.6,0codegrayrgb0.5,0.5,0.5codepurplergb0.58,0,0.82backcolourrgb0.95,0.95,0.92prompt     language=Python,     backgroundcolor=\color,     commentstyle=\color,     keywordstyle=\color,     numberstyle=\tiny\color,     stringstyle=\color,     basicstyle=\ttfamily,     breakatwhitespace=false,     breaklines=true,     captionpos=b,     keepspaces=true,     numbers=left,     numbersep=5pt,     showspaces=false,     showstringspaces=false,     showtabs=false,     tabsize=2 Python import torch def get_improved_heuristic(heuristic: int, difference: int):     '''         A function that takes in the admissible A* heuristic and adds to it the difference, to return a heuristic closer to the optimal cost to the goal. The difference should be calculated keeping in mind the optimal cost of the puzzle.     '''     return heuristic + difference

# The difference is calculated by observing the {domain} puzzle and deducing the optimal cost to goal. The heuristic is subtracted from this optimal cost # {puzzle_legend} puzzle_str = "{puzzle_str}" improved_heuristic = get_improved_heuristic({heuristic}, -1.7emPrompt used while training the language model. \{curly braces\} denote a placeholder.lst:promptA* Search Algorithmalg:astarPromptssec:promptslst:promptlst:sokobanlst:mazeHyperparameterssec:hyperparametersAdditional Resultstbl:model_scaleExperimental results showing the effects of the proposed sampling strategy on larger models, upto 720M parameters. Base Model refers to the variant of code-t5(base/large). Code-t5-small results have been provided in the main paper.tbl:model_scale