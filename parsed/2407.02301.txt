The motivation of CFinBench is to evaluate the financial knowledge of large language models in the context of Chinese. Inspired by predecessors~, we also focus on the advanced knowledge and complex reasoning

abilities, which, compared to traditional NLP capabilities, pose a greater challenge to the increasingly advanced LLMs of today. In practice, we build CFinBench based on the real-world examination questions used in China for assessing financial professionals across multiple dimensions. We include 3 question types: single-choice, multiple-choice and judgment, as exemplified in Figure~. Compared with the single-choice questions alone in most existing works~, a broader range of question types can more comprehensively assess the capabilities of LLMs. Specifically, for single-choice questions, each question has four options, with only one correct answer. For multiple-choice questions, each question has four or five options, with at least two correct answers. For judgment questions, each question requires a direct judgment of whether the statement is correct or wrong. With prompts, LLMs are expected to answer these questions correctly.

In the evaluation of large language models, a diverse array of tasks is often preferred to comprehensively assess their capabilities. A hierarchical evaluation framework enables a more nuanced understanding of the abilities of LLMs. Instead of categorizing the financial tasks solely based on their subjects~, we thoroughly explore the characteristics of Chinese financial system, and reorganize the financial tasks into more reasonable categories. Specifically, the process starts with acquiring fundamental knowledge in financial subjects, followed by passing essential professional qualifications, and subsequently refining skills through practical experience in industry applications. Additionally, adherence to laws and regulations is a critical aspect that demands careful consideration. In practice, we include 4 first-level categories and 43 second-level categories, which are summarized in Figure~.

Our dataset is primarily composed of mock exams sourced from publicly accessible channels. Notably, some questions of financial qualification and financial practice originate from internal examinations of the financial departments of Chinese companies and are non-publicly available, which are challenging to acquire through web crawling.

The collected data come in various formats, including PDF, EPUB, Microsoft Word documents and web pages. Documents in PDF format and EPUB format are parsed into text using PyMuPDF and EbookLib respectively. We standardized all single-choice questions to have exactly four options, excluding those with fewer options and randomly removing excess wrong options from those with more than four. Similarly, for multiple-choice questions, to maintain uniformity, we only retain questions with four or five options.

Following predecessors~, all the collected questions go through a standard data preprocessing pipeline including cleaning and de-duplication. For data cleaning, we first remove non-Chinese paragraphs with the inexpensive n-gram models like fastText~. Then a series of filtering rules and heuristics are performed, such as only keeping lines with valid punctuation, discarding consecutive newlines and whitespace characters, or removing unsemantic and garbled lines. For data de-duplication, we adopt MinHash algorithm~ for internal de-duplication and de-duplication with external public data~.

To enhance data diversity and mitigate data contamination problem, we also adopt the strategy of question rephrasing based on GPT4~. We observe that the collected raw data exhibits a significant class imbalance, with a notable scarcity of judgment questions and a substantial surplus of single-choice questions. To address this issue, we prompt GPT4 to rephrase a portion of the single-choice questions into judgment questions, while maintaining semantic consistency, as exemplified in Figure~ (a). Furthermore, to mitigate the problem of data contamination, we first randomize the option order~. In practice, this includes both random shuffle and 'farthest option swapping', where the correct option is exchanged with the incorrect option that is farthest away. Subsequently, we prompt GPT4 to rephrase the questions based on the shuffled options, similarly preserving semantic consistency, as exemplified in Figure~ (b). Following this, all the questions undergo several rounds of rigorous human cross-validation. The statistics of the final dataset are summarized in Appendix.

We randomly split the questions into a development set, a validation set, and a test set within each second-level category. The development split per category consists of three examples to facilitate few-shot evaluation. A portion of the development examples are also annotated with detailed explanations to enable few-shot chain-of-thought settings~, which will be discussed in Appendix. The validation set and test set are divided in a ratio of 2:8, where the validation set is for hyperparameter tuning and the test set is for full evaluation.

We employ the OpenCompass~ framework to perform model inference. Specifically, during the generation process, we set both the temperature and the top  to 1.0, and employ greedy decoding. The input token length is limited to 2048, and the output token length is limited to 64, which is sufficient for the questions of choice and judgment. Right truncation is performed for input prompts exceeding the length limitation. All models are inferred in both zero-shot and three-shot settings, which are exemplified in Appendix.

We adopt accuracy to measure the match between model prediction and gold answer. Specifically, for single-choice questions, if multiple valid options are predicted by the model, we only select the first option as the final answer predicted by the model. For multiple-choice questions, if any of the options predicted by the model are not among the gold answer, we directly classify it as wrong. Otherwise, we score it based on the number of predicted answers (out of a full score of 1). At last, we calculate the final score for each category based on: .

To give a comprehensive view of the status of LLMs in a Chinese financial context, we evaluate a wide spectrum of large language models, as depicted in Table~. Specifically, our experiments cover 46 open-source LLMs from various families, including Llama~, Qwen~, ChatGLM~, Baichuan~, InternLM~, Phi~, DeepSeek~, XuanYuan~, FinMA~, Gemma~, TigerBot~, Skywork~, Yi~ and Mistral~. We classify models into different categories according to their size, including greater than 65B, approximately equal to 30B, 10B-20B, 5B-10B, and less than 5B. Considering the legal issues, we only report the results of two publicly recognized API-based LLMs,  ChatGPT~ and GPT4~. In addition, the proprietary finance-specific model YunShan~ is included.

In Table~, we report the 0-shot and 3-shot accuracy of each first-level category on the test split under the answer-only setting. As can be seen, the Chinese-oriented Yi1.5-34B~ lead the benchmark, with a mean accuracy just reaching 60.16\%, highlighting the challenge presented by CFinBench.  In addition, the accuracy of some other Chinese-oriented models such as Qwen-72B~, Qwen1.5-72B~, XuanYuan2-70B-Base~ and XuanYuan-70B-Base~ also exceed 56\%. The accuracy of GPT4~ is around 55\%, which also shows obvious advantage compared to other models. In the size range of 10B-20B, Qwen-14B~, InternLM2-20B~ and XuanYuan-13B-Base~ are in the lead, with accuracy exceeding 46\%. Notably, in the size range of 5B-10B, the proprietary Chinese finance-specific model YunShan-7B~ is in the absolute leading position, with an accuracy of over 52\%, which is even higher than the accuracy of some 70B models. Also, Yi1.5-9B~, Qwen1.5-7B~ and ChatGLM3-6B-Base~ also achieve the accuracy over 45\%. During the size range of less than 5B, Qwen1.5-4B~ is the only model that achieves an accuracy of more than 40\%. The other models like YunShan-1.5B~, Phi3-3.8B-Instruct~ and Qwen1.5-1.8B~ also achieve good performance with an accuracy of more than 35\%. In conclusion, there is still significant room for improvement for current LLMs in the Chinese financial domain.

As we can see from Table~, the performance of most models demonstrates improvement when some examples are provided. However, in the case of InternLM2-7B and ChatGLM-6B,  the zero-shot setting outperforms the few-shot setting. We guess that this is because these models have acquired the ability to fully understand the questions without the need for examples during the pre-training or fine-tuning. The introduced examples may mismatch with their training methodology, which leads to the decrease in accuracy~.

In Table~, as the model size increases, the accuracy of Qwen1.5-0.5B, Qwen1.5-1.8B, Qwen1.5-4B and Qwen1.5-7B increases accordingly,  33.37\%, 37.69\%, 43.34\% and 48.36\% respectively at the 3-shot setting. Similarly, the accuracy of Yi1.5-34B is increased by 10.72\% over Yi1.5-9B at the 3-shot setting. However, this does not mean that increasing the model size will definitely improve the performance. 

Impressively, two finance-specific models XuanYuan~ and YunShan~ achieve very competitive accuracy. This can be attributed to the fact that both models are mixed with high-quality financial corpus during pre-training and fine-tuning, resulting in better grasp of financial knowledge. We think the same is true for the general base models such as Yi~ and Qwen~ that perform well in Table~.

Since we do not publicly release the labels for the test split, we provide the average accuracy on the validation split as a reference for developers. The results of 0-shot and 3-shot on the validation split and test split under answer-only setting are reported in Table~. We can observe that the difference between the results of the same model on validation split and test split is very small. For example, the 0-shot accuracy of GPT4 on validation split and test split differ only by 0.14\%, suggesting that developers can use the accuracy on validation split as a good indicator to iterate their models faster.

To better engage in natural conversation, the chat version are often derived from base model by alignment techniques~, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). As observed in Table~, the accuracy of some models' chat version is improved when compared to the base version, such as Qwen1.5-32B, InternLM2-20B, Baichuan2-13B,  At the same time, the accuracy of some models' chat version have declined, such as Yi1.5-34B, ChatGLM3-6B, and Qwen1.5-1.8B. The varying alignment strategies of the models lead to different results.

To further explore the models' reasoning capabilities, in addition to the answer-only (AO) setting, we also perform some experiments on the chain-of-thought (COT) setting~. Evaluation on COT setting requires the model to generate explanations for a given question and then give the final answer based on the generated explanations. Specifically, we obtain the explanations examples only for multiple-choice questions manually by professional financial practitioners. The experimental results are reported in Table~.

As observed in Table~, the models achieve comparable or lower average accuracy than in the answer-only setting. This suggests that COT prompting does not necessarily improve results, which is also evidenced in other benchmarks like FinEval~ and C-Eval~,  FinEval~ is another representative benchmark for evaluating the Chinese financial advanced knowledge of LLMs. In Table~, we report the few-shot accuracy on CFinBench and FinEval with various LLMs. It can be seen that the accuracy of the highest Yi1.5-34B reaches nearly 87\%, while ours is around 60\%. Likewise, the lowest InternLM2-7B accuracy is still over 62\%, while ours is only around 43\%. This all suggests that CFinBench is more challenging and better able to distinguish the performance of different models.

To better engage in natural conversation, the chat version are often derived from the base model by alignment techniques~, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In Table~, we report more results of the representative chat models.

We list the prompt examples utilized in the evaluation process, including zero-shot and few-shot in answer-only scenarios (~Figure and Figure), zero-shot and few-shot in chain-of-thought scenarios (~Figure).

In Table~, we enumerate the comprehensive statistical information of the dataset.

For limitations, ours focuses on Chinese financial system and is therefore not suitable for assessing financial knowledge in other countries. The proposed CFinBench provides an important basis for evaluating the LLMs' mastery of Chinese financial knowledge. However, since we have open sourced the questions and answers of the validation split, if they are improperly used to train the large language model, the accuracy of the model may be falsely high. Therefore, the results on CFinBench are only a reference. The true quality of the model depends on the performance of the user in the practical scenario.

The proposed benchmark is under the terms of the Apache-2.0 license. We encourage code sharing and respect the copyright of the original author, and also allow code modification and redistribution (as open source or commercial software).

Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) : whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) : whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) : whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) : whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments of 50 representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 60.16\%, highlighting the challenge presented by CFinBench. The dataset and evaluation code are available at .

Financial SubjectFinancial QualificationFinancial PracticeFinancial Lawhttps://cfinbench.github.io/Introductionchatgptopenai2023gpt4touvron2023llama,touvron2023llama2,llama3yang2023baichuanteam2023internlmzeng2022glmhendrycks2020measuringhuang2024cgu2024xiezhizhong2023agievalshiller2013financewu2023bloomberggptxie2023pixiutouvron2023llamashah2022fluexie2023pixiulu2023bbtzhang2023finevalFinancial SubjectFinancial QualificationFinancial PracticeFinancial Lawyoung2024yibai2023qwenzhang2023xuanyuanRelated WorkLarge Language Modelschatgptopenai2023gpt4touvron2023llama,touvron2023llama2,llama3llama3yang2023baichuanbai2023qwenteam2023internlm, cai2024internlm2zeng2022glmwu2023bloomberggptyang2023fingptxie2023pixiuchen2023discli2023cfgptzhang2023xuanyuanwang2023panguEvaluation Benchmarkswang2018glue, xu2020clue, borkan2019nuanced, rajpurkar2018know, rajpurkar2018know, narayan2018donhendrycks2020measuringsrivastava2022beyondliang2022holistichuang2024cli2023cmmluzhang2023evaluatingzhong2023agievalgu2024xiezhishah2022fluexie2023pixiuxie2024finbenlu2023bbtxie2023pixiulu2023bbthu2024nolei2023cfbenchmarkzhang2023cgcezhang2023xuanyuanzhang2023finevalzhang2023xuanyuanzhang2023finevaltab0shah2022flue,lu2023bbt,zhang2023fineval,lei2023cfbenchmarkCFinBenchOverviewhuang2024c,fei2023lawbench,zhang2023finevalfig1zhang2023fineval, zhang2023xuanyuan, clark2018think, antfineva, huang2024cTaxonomyzhang2023fineval, zhang2023xuanyuanfig0: The purpose of the financial subject is to test whether LLMs can memorize the essential foundational knowledge in financial subjects. Specifically, 11 subjects are included: Political Economy, Western Economics, Microeconomics, Macroeconomics, Industrial Economics, Public Finance, International Trade, Statistics, Auditing, Economic History, and Finance. These subjects collectively provide a comprehensive framework for understanding the intricacies of economic systems, market structures, and financial institutions. By delving into these areas, individuals can develop a profound understanding of economic theories, policy analyses, market dynamics, and financial instruments, thereby enabling them to make informed decisions, analyze economic trends, and navigate the complexities of global financial markets. In summary, these subjects are crucial in equipping professionals with the foundational financial knowledge.

	Financial Subject: The objective of the financial qualification is to examine whether LLMs can obtain necessary qualified certifications for finance professionals. We include 8 qualifications: Tax Practitioner Qualification, Futures Practitioner Qualification, Fund Practitioner Qualification, Real Estate Practitioner Qualification, Insurance Practitioner Qualification, Securities Practitioner Qualification, Banking Practitioner Qualification, and Certified Public Accountant (CPA). These qualifications demonstrate an individual's expertise and proficiency in specific areas of finance, such as taxation, financial markets, investment management, and accounting. By obtaining these qualifications, professionals can enhance their knowledge and skills in areas such as financial analysis, risk management, and financial planning. These qualifications indicate the valuable entry tickets for financial practitioners.

	Financial Qualification: The category of financial practice is to examine whether LLMs can fufill the specific tasks in financial practical jobs. We include 13 titles in practical financial jobs: Junior/Intermediate Auditor, Junior/Intermediate Statistician, Junior/Intermediate Economist, Junior/Intermediate Banking Professional, Junior/Intermediate Accountant, Tax Consultant, Asset Appraiser, and Securities Analyst. These practices involve the application of financial concepts and techniques to real-world problems, requiring professionals to possess a deep understanding of financial markets, instruments, and regulations. By engaging in these practices, professionals can develop expertise in areas like financial reporting, data analysis, economic modeling, risk assessment, and investment analysis. In a nutshell, the ability to accomplish practical financial tasks is one of the key indicators of the competence of financial professionals.

	Financial Practice: The purpose of the financial law is to test whether LLMs can comply with financial laws and regulations. Specifically, it includes 11 exams of laws and regulations: Tax Law I/II, Tax Inspection, Commercial Law, Securities Law, Insurance Law, Economic Law, Banking Law, Futures Law, Financial Law and Civil Law. These laws provide the legal foundation for financial transactions, investments, and operations, and are essential for professionals to understand in order to navigate the financial landscape effectively. Familiarity with these laws enables individuals to appreciate the legal implications of financial decisions, identify potential risks and liabilities, and ensure compliance with regulatory requirements. In summary, proficiency in financial laws can reduce the occurrence of illegal activities and minimize the risk of a 'one-vote veto' in the careers of financial practitioners. Financial LawData ConstructionData SourcesData Processingyuan2021wudaocorpora, penedo2023refinedweb, wei2023skywork, huang2024cjoulin2016fasttextbroder1997resemblancezhang2023fineval, zhang2023xuanyuan, antfineva, openfindata, lu2023bbtwang2022self, xu2023wizardlmfig2berglund2023reversalfig2ExperimentsSetupData Splitwei2022chainInference Details2023opencompassEvaluation MetricsModelstab1touvron2023llama2,llama3bai2023qwenzeng2022glm,du2022glmyang2023baichuanteam2023internlm, cai2024internlm2gunasekar2023textbooks,textbooks2,abdin2024phideepseekv2zhang2023xuanyuanxie2023pixiuteam2024gemmachen2023tigerbotwei2023skyworkyoung2024yijiang2023mistralchatgptopenai2023gpt4wang2023panguResultstab1young2024yibai2023qwenbai2023qwenzhang2023xuanyuanzhang2023xuanyuanopenai2023gpt4bai2023qwencai2024internlm2zhang2023xuanyuanwang2023panguyoung2024yibai2023qwenzeng2022glm,du2022glmbai2023qwenwang2023panguabdin2024phibai2023qwenAnalysisFew-shot examples are helpful in most cases.tab1gu2024xiezhi,li2023cmmluScaling up the model size usually results in better performance.tab1Domain specific pre-training and fine-tuning are helpful.zhang2023xuanyuanwang2023panguyoung2024yibai2023qwentab1Results on the validation split.tab2The performance of chat models.ouyang2022trainingtab3ConclusionChecklistFor all authors...

Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?

  Did you describe the limitations of your work?

  We describe our limitations in Appendix.Did you discuss any potential negative societal impacts of your work?

  We discuss the potential negative societal impacts in Appendix.Have you read the ethics review guidelines and ensured that your paper conforms to them?

If you are including theoretical results...

Did you state the full set of assumptions of all theoretical results?

	Did you include complete proofs of all theoretical results?

If you ran experiments (e.g. for benchmarks)...

Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?

  The dataset and code are available at \url Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?

	Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?

	Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?

If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

If your work uses existing assets, did you cite the creators?

  Did you mention the license of the assets?

  Our evaluation code is developed based on OpenCompass~\cite, which uses the license of Apache-2.0.Did you include any new assets either in the supplemental material or as a URL?

  Did you discuss whether and how consent was obtained from people whose data you're using/curating?

  Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?

If you used crowdsourcing or conducted research with human subjects...

Did you include the full text of instructions given to participants and screenshots, if applicable?

  Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?

  Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?

AppendixMore experimentsResults on chain-of-thought setting.kojima2022large, wei2022chaintab1appendix_tab1zhang2023finevalhuang2024cComparison with other similar benchmark.zhang2023finevalappendix_tab2More results on chat models.ouyang2022trainingappendix_tab3Prompt examplesappendix_fig1appendix_fig2appendix_fig3Data statisticappendix_tab4Limitations and potential negative societal impactsLicensehttps://www.apache.org/licenses/LICENSE-2.0