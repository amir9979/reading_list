As our starting point, basic soft context compression approaches aim to compress an arbitrary token sequence to a fixed length sequence of soft tokens~, which we call overall compression.

To achieve this goal, a two-pass pipeline is performed to compress and then leverage the soft token sequence.

The first pass compresses the token sequence into a soft token sequence. Assuming we have a token sequence  to compress to  soft tokens, we append a special summary token sequence  to  and input them to the compressor, obtaining the output hidden state

Then the hidden states  correspond to  are used as the compressed soft token sequence, thus we have

The second pass leverages the soft token sequence as a soft prompt. When generating the output for a context  conditioned on the compressed sequence ,  is used as the alternate of , thus we have the decoder output

To train the compressor and decoder models, the language modeling training objective is applied to  as an indirect supervision since we cannot obtain the gold answer of soft tokens.

The basic compression approaches have to face the challenge of compression loss. However, the compression loss is difficult to control. The higher the compression ratio, the greater the compression loss. Even if trained with reconstruction loss, the decoder can make mistakes when recovering words from compressed tokens, especially on rare words, just like human~.

This feature is harmful to tool-using language models, since key information loss such as misspelled names of tools or parameters will directly lead to failure. Therefore, we propose the selective compression strategy as a more controllable approach to keep the key information despite the compression ratio, where the key information retains raw text tokens.

Given a tool documentation as the context  to be compressed, we split it into disjoint sub-sequences  whose union is . Each  is rather a key information sub-sequence (e.g., name of a parameter) or a sub-sequence that can be compressed (e.g., functionality description of the tool). Following the notations in basic soft context compression, we have the compressed token sequence A notable challenge in context compression for tool-using language models is the variability in documentation length. The basic compression methods typically compress the documentation into a uniform sequence of fixed length, denoted as . This approach has limitations: for lengthy documentation, setting a small  leads to a significant loss of information, adversely affecting performance. Conversely, a large  hinders the effective compression of shorter documents, thereby reducing the compression ratio. Additionally, applying selective compression strategies, which divide the documentation into sub-sequences of varying lengths, intensifies these issues.

We believe it is a better approach to compress tool documentation according to a preset compression ratio . To realize this approach, we propose the block compression strategy to support the variable  with fixed . The core idea is to chunk the sequence to be compressed into a variable number of blocks, each compressed to  soft tokens.

We chunk the sequence to compress  into  chunks . Concatenating the compressed version of these chunks, we obtain the final compressed sequence

Note that the last chunk is not always full, which will make the compressed sequence at most  soft tokens longer than expected. Therefore,  should be a small number.

The aforementioned strategies offer a concise and precise approach to compress tool documentation for tool-using language models. As illustrated in Figure~, our final method integrates both of them based on the basic soft context compression.

From the perspective of block compression, we can unify the two strategies in practice by regard key information sub-sequences as special blocks which we do not compress. Specifically, we first split the key information sub-sequences (i.e., the key blocks), then chunk the other sub-sequences into blocks (i.e., the plain blocks).

Next, we input the blocks into the compressor. To reach high efficiency and keep more context information, we compress all the blocks in one documentation in parallel. As shown in the compression part of Figure~, we append one block of summary tokens to the input sequence for each plain block, and obtain all the compressed blocks at once.

Finally, as described in the selective compression strategy, we concatenate all the blocks to form the compressed documentation, which is then used by the decoder.

Following the basic soft context compression approaches~, we initialize the compressor and the decoder with a pre-trained language model, then jointly train them. Actually, we use the same model as the compressor and the decoder, when we input summary tokens, it outputs compressed blocks, otherwise, it works as an ordinary language model. The compressor and decoder need pre-training to acquire the capability of using soft tokens. Different from existing approaches, our pre-training format changes due to the integration of our two strategies. Specifically, we randomly chunk pre-training data as key blocks and plain blocks, and perform the same parallel compression manner as shown in Figure~.

The training objective is language modeling, thus we apply the per-token cross-entropy loss on the decoder output. With the gradient propagated by the compressed documentation, the loss can supervise the decoder as well as the compressor in an end-to-end manner.

In addition,~ propose to add an auxiliary loss of reconstructing the raw text  from the compressed soft token sequence , which agrees to our goal of keeping key information, so we take this idea into account in our method, implementing a variant of our approach with reconstruction loss. In practice, we follow~ to use the trainable soft prompt to switch the decoder between ordinary mode and reconstruction mode.

To evaluate our approach, we conduct experiments to train models with context compression and test the performance on tool-using benchmarks. Our goal is to investigate the variations in model performance under different compression ratios and compression strategies.

In all of our experiments, we use the same base model as both the compressor and the decoder, and use LLaMA-7b~ to initialize all the base models. According to the manner of compression, we categorize base models into three cases:

In these cases, block compression is always enabled so that we can track the variation of model performance with a controllable compression ratio.

Note that we refer to existing compression approaches such as RMT~ and AutoCompressor~ as basic compression, and the overall context compression case is equivalent to basic compression plus block compression. Therefore, we study the influence of block compression strategy through extra analysis experiments in Section~ which provide fair comparison between our approach and these existing soft compression approaches.

For general context, it is difficult to key information for selective compression. However, when we focus on the documentation of tools, it becomes evident that the information that models must precisely retrieve is the name. Therefore, we follow a simple but sound definition of key information as the names of tools and parameters.

We consider two training objectives in our experiments. The first is language modeling in the supervision of the decoder output. It is the basic objective for all the base models including the no-compression case. The second is the reconstruction objective, requiring the decoder to recover the raw text from compressed soft token sequences. Reconstruction is only for models with compression, acting as an auxiliary objective. When the reconstruction objective is on, we use the sum of reconstruction and the language modeling loss as the final loss.

The reconstruction objective is from ICAE~ which shares the same motivation of keeping the raw information and compatible with our approach. From this perspective, ICAE is parallel to our approach. Therefore, we study the effects of the reconstruction loss as long as it is possible. Specifically, switching the reconstruction objective on and off, each base model in the compression cases has two variants. We report and analysis results of both variants.

Following existing soft compression approaches~, we pre-train base models on general corpus at first instead of directly fine-tuning them on downstream tasks. The only exception is the upper-bound baseline, i.e., a fine-tuned LLaMA without context compression, for which pre-training is omitted because it does not need to acquire how to use soft tokens.

We use SlimPajama~ as the pre-training dataset. SlimPajama is a deduplicated version of RedPajama~, which is a community reproduction of the LLaMA~ pre-training dataset. To pre-train LLaMA with compression, we randomly sample data within 2k context length to construct a subset of 1B tokens. Then, we train all the models with compression on the same subset for one epoch.

We maintain the compression manner during pre-training consistent with the manner in practice. For overall compression models, we randomly select a prefix for each sample, with the length ranging from 0.5k to 1.5k, and then use the compressed prefix as input, with the remaining portions serving as output. For selective compression models, we randomly mark sub-sequences of the prefix as key information according to a random proportion ranging from 0 to 1 for each sample.

Given the heavy computation cost of pre-training, to ensemble support for variable compression ratio in one model, we assign a random compression ratio ranging from 1 to 16 to each sample, and always set the length of the summary sequence as 2.

Some tool-using language models including the official baseline of API-Bank are fine-tuned on instruction-tuned models~. To unify the settings and make our base models prepared to efficiently adapt to downstream tasks, we follow them to instruction-tune the pre-trained compression models as the final base models. Specifically, we use the ShareGPT dataset released by OpenChat~ as the instruction-tuning dataset. We trunk ShareGPT into a sequence length of 2k and train one epoch for all the models, with the other settings and training procedure the same as pre-training.

We evaluate our approach on tool-using benchmarks API-Bank~ and APIBench~. Both of them can be modeled as a standard case in the decoder part of Figure~, in which tool-using language models accept one or more tool documentation as well as the user query as input and then output the tool call.

The dataset consists of multi-turn dialogues, where the user can ask the model to call external APIs. Each tool documentation is a JSON dictionary as exemplified in Figure~. We use the  subset of API-Bank in our experiments, where each of the user queries is a dialog history ending with an instruction to use a tool.

The dataset simulates a scene where an automated agent finds a suitable model on a platform (e.g., Hugging Face Hub) to fulfill the user's query. Therefore, the input contains a query and the model card of candidate models as the tool documentation, and the output is an API call to drive the model. However, the dataset is originally for testing retrieval-augmented tool-using language models. Specifically, only the top-ranked candidate model retrieved is provided to the decoder, and it cannot be guaranteed that the retrieval is correct, thus introducing the possibility of cascading errors. To weaken the impact of the retrieval module, we use the BM25 retrieval module in the official codebase to extend the number of candidates to up to 5, and make sure the correct answer is within. We shuffle the order of documentation to avoid potential position bias. The original dataset contains three subsets (i.e., , , and ), we use their union as a whole dataset.

We use the official training and test set for both of the datasets, whose statistics are shown in Table~. As shown in Figure~, we always compress different tool documentation separately, then concatenate all the compressed documentation before inputting them to the decoder. To ensure the consistency between training and testing, we train a separate model for each combination of the compression ratio, the base model, and the dataset. We train all the models on the corresponding dataset for two epochs at once.

Both of the benchmarks use the accuracy of API calls as the metric. API-Bank provides a local sandbox to run the APIs, and check the running result to judge whether the API call is correct or not. APIBench checks the answer through AST matching, without running the API. We follow the official test approaches for both of the benchmarks.

Table~ demonstrates the performance of three cases of base models. In general, selective context compression outperforms overall context compression, reaching similar or even higher performance compared to the no-compression case. Also, we find that adding reconstruction loss can be harmful to performance, and selective context compression can close the performance gap caused by reconstruction loss.

In these two benchmarks, trends of performance according to the compression ratio differ. Table~ shows that documentation in API-Bank is more concise than those in APIBench, thus intuitively harder to compress. Results show that the performance of overall context compression noticeably decays when the compression ratio becomes higher, which supports this intuition. On the other hand, only the weakest base model, which is overall context compression with reconstruction loss, shows obvious performance degradation on APIBench. This phenomenon suggests that APIBench is much easier than API-Bank, having the potential to keep satisfactory performance under a higher compression ratio.

Furthermore, we dive into the error cases to explore whether selective context compression keeps key information or not. Under our intuitive definition of key information as the tool names and parameter names, we count the number of error cases caused by name error, i.e., the model predicts a wrong name of APIs or parameters.

From results aggregated in Table~, we find selective compression models make fewer mistakes in name errors. On APIBench, this phenomenon is more obvious. Although overall compression can reach comparable performance with the no compression baseline, the number of name errors grows with the compression ratio. In contrast, selective compression keeps the number of name errors even lower than baseline, less affected by the compression ratio.

Another interesting finding is that the overall compression base model with reconstruction loss produces even more name errors, suggesting that without priors it is hard for the model to realize the importance of names.

Since supporting of controllable compression ratio relies on block compression, the methodology of observing the performance in different compression ratios does not work when studying block compression itself. To make a fair comparison between cases with and without block compression, we step back to the basic soft context compression. This section also plays the role of providing fair comparison between existing soft compression approaches, namely RMT~ and AutoCompressor~, and the proposed block comporession strategy.

Actually, the overall context compression base model is a basic context compression model with the integration of block compression. Therefore, we train a new basic context compression base model without block compression with the same data and pre-training-fine-tuning pipeline with the overall context compression base model. We thereby can analyze the influence of block compression through comparing the performance of these two base models under the same length of the compressed summary token sequence. We follow AutoCompressor~ to set the length of summary sequences to 50.

Table~ demonstrates experiment results. Apart from the plain setting of separately compressing each documentation, we add another setting to observe the performance under a higher compression ratio, where we regard the concatenation of multiple documentation as a whole. With either setting, block compression significantly improves basic context compression.

To study the effects of selective compression strategy and the necessity of selective strategy in pre-training, we evaluate all four cases of strategy combination during pre-training and fine-tuning. Results are shown in Table~, which can be seen as an extended version of Table~.

We find that introducing selective compression benefits the performance even if the final compression manner is overall compression. Moreover, the base model can adapt to selective compression through fine-tuning, though having a performance gap to the best combination. To reach the best performance, the model should use selective compression from the beginning to the end. The conclusion is supported by Table~ both with and without the reconstruction loss.

Although we use open datasets from existing work, there not exists evidence supporting the necessity of non-key information such as the descriptions. Theoretically, the model may guess the functionality of a tool only using the names of the APIs and parameters.

Therefore, we conduct experiments on the case that non-key information is deleted. In other words, the tool documentation consist of almost only the names.

The results are shown in Table~. To understand the results, please note that tools in the API-Bank test set are unseen during training. In contrast, APIBench shares the same tool library during training and testing. Thus, when fine-tuned and tested with the same type of input, the model has the chance to memorize the tools in APIBench. However, a performance gap also exists in this case. To summarize, we can conclude that the non-key information is necessary.

Apart from soft compression which we use, another way to achieve tool documentation compression is hard compression, such as prompting ChatGPT to summarize the documentation. However, hard tokens are able to carry less information in the same context length, which is the reason we consider soft compression at first.

To illustrate the manner of hard compression, we use GPT-4 Turbo to compress the non-key part of the tool documentation, which is a very close setting to our main experiment despite we use ChatGPT as the compressor.

We explain the experiment details on API-Bank, which are highly similar to the case on APIBench. We compress all  fields since other non-key fields like datatype consist of very short text. Specifically, we use the following prompt and switch on the JSON-only output mode:

Also, we carefully check the output and leverage regenerating to ensure any other field is kept as is.

To provide better intuition, we give an example as follows (formatted for easy reading).

We can see from the example that plain text cannot compress concise input with a high compression ratio, while soft compression has the chance to compress a short description into a single soft token at an over 10 compression ratio.

The only difference on APIBench is that we compress the  as well as the  field. The other fields are always very short.

Next, we fine-tune using the baseline setting in the paper, namely fine-tuning a LLaMA-7b model fine-tuned on ShareGPT. To avoid inconsistency between training and testing, we also evaluate the case where the training data are also compressed.

Table~ lists the accuracy and Table~ lists the averaged compression ratio over the datasets, where we find GPT-4 summarization is less efficient. Tested with GPT-4 summarized input, the baseline model achieves lower performance. When trained with GPT-4 summarized data, the performance goes even lower. Note that the official training set of API-Bank uses a dedicated set of tool documentation which are model generated thus far more messy and harder to summarize than the test data. In contrast, APIBench uses the same set of tool documentation in the training set and the test set. These results imply a possible flaw that untrained hard summarization may introduce information loss or error on tool-using tasks.

Please note that our work mainly focuses on developing an efficient approach to compressing tool documentation based on the soft context compression framework, instead of a comparison of soft/hard compression approaches on specific tasks. Therefore, the results of hard summary baselines act as additional data to help readers better understand our motivation and better illustrate the differences between soft/hard compression approaches. Anyway, the performance of hard summarization/compression has a limited relation with the integrity of our work.

Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying the input window as well as slowing down the decoding process. Given the progress in general-purpose compression, soft context compression is a suitable approach to alleviate the problem. However, when compressing tool documentation, existing methods suffer from the weaknesses of key information loss (specifically, tool/parameter name errors) and difficulty in adjusting the length of compressed sequences based on documentation lengths. To address these problems, we propose two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models. 1) Selective compression strategy mitigates key information loss by deliberately retaining key information as raw text tokens. 2) Block compression strategy involves dividing tool documentation into short chunks and then employing a fixed-length compression model to achieve variable-length compression. This strategy facilitates the flexible adjustment of the compression ratio. Results on API-Bank and APIBench show that our approach reaches a performance comparable to the upper-bound baseline under up to 16x compression ratio.

Introductionnakano2021webgpt, OpenAI_2023, patil2023gorilla, cheng2023bindingpatil2023gorilla, li2023api, qin2023toolllm, tang2023toolalpacafig:taskfig:taskge2023context, chevalier2023adapting, bulatov2022recurrent, wang2024lomali2023apipatil2023gorillaWe introduce concise and precise context compression for tool-using language models, with strategies for minimizing key information loss under variable compression ratio.     Our approach on two tool-using benchmarks demonstrates negligible performance loss under up to 16x compression ratio.     We explore different combinations of training objectives and compression strategies, and provide a recipe for context compression training for tool-using language models. fig_method.pdf 		Overview of our method for tool documentation compression. 		When compressing a tool's documentation, we cut out the key information as key blocks (red) and chunk the rest into plain blocks (green). 		We use the concatenation of key blocks and compressed plain blocks (yellow) as the compressed documentation. 		We supervise the decoder output conditioned on compressed documentation to train the compressor and the decoder end-to-end. 	fig:methodRelated WorkTool-Using Language Modelscobbe2021training, nakano2021webgpt, komeili-etal-2022-internet, thoppilan2022lamda, gao2022pal, huang2022inner, yao2022react, cheng2023binding, schick2023toolformerOpenAI_2023yang2023gpt4tools, xu2023tool, patil2023gorilla, li2023api, qin2023toolllm, tang2023toolalpacaContext Compressionbulatov2022recurrent, ge2023context, wang2024lomali2023compressing,jiang2023llmlingua, jiang2023longllmlinguachevalier2023adapting,ge2023contextMethodBasic Soft Context Compressionbulatov2022recurrent, chevalier2023adapting H = ((T) \Vert (S))   (T) = H[L_T, L_T + L_S)   H' = (C \Vert (T'))  Selective Compression Strategyge2023context 	C &= \Vert_i C_i  \\ 	C_i &= \left\{ 	 		& (T_i) & T_i  \\ 		& (T_i) &  \\ 	- & aligned.-1Block Compression Strategy C = \Vert_i (T_i)  Concise and Precise Context Compressionfig:methodCombining Selective and Block Compression Strategiesfig:methodTraining Compressor and Decoderchevalier2023adapting, ge2023contextfig:methodge2023contextge2023contextExperimentsBasic SettingsBase Modelstouvron2023llamaNo compression: this case corresponds to the fine-tuned LLaMA without context compression, which we regard as the upper-bound approach because it is not affected by compression loss. 	Overall context compression: these models act as our baselines, performing overall compression in the manner of basic soft context compression approaches. 	Selective context compression: these models benefit from our proposed selective compression strategy and demonstrate the performance of our approach. bulatov2022recurrentchevalier2023adaptingsec:Effects of Block Compression StrategyKey Information for Selective CompressionTraining Objectivesge2023contextPre-training Compression Modelschevalier2023adapting, ge2023contextDatasetcerebras2023slimpajamatogether2023redpajamatouvron2023llamaCompression MannerInstruction-Tuningli2023api, tang2023toolalpaca, alpaca, vicuna2023wang2023openchatTool-Using BenchmarksAccuracy on the test set of two tool-using benchmarks. The performance of selective compression is seldom affected by the compression ratio, however, the performance of overall compression noticeably decays as the compression ratio increases.tab:exp_sel_vs_ovali2023apipatil2023gorillafig:methodAPI-Bankfig:tasklevel1-apiAPIBenchhuggingfacetensorflowhubtorchhubFine-tuning on Downstream Taskstab:data_statfig:methodMetricTool-Using Evaluationtab:exp_sel_vs_ovatab:data_stat5ptExploration on the effects of selective compression in different training stages. Compression models can adapt to selective compression with fine-tuning. The best combination is to consistently train with selective compression. 4 to 16 represents the compression ratios.tab:exp_seltab:exp_sel_vs_ova_name_errorEffects of Block Compression Strategysec:Effects of Block Compression Strategybulatov2022recurrentchevalier2023adaptingchevalier2023adaptingtab:exp_blkEffects of Selective Compression Strategytab:exp_seltab:exp_sel_vs_ovatab:exp_selConclusionLimitationsAcknowledgementscustomAdditional ResultsThe Necessity of Non-key Informationtab:exp_del_nonkeyComparison with Hard Summarizationdescription Here is an API document.

Replace each "description" field with a brief summary and keep the other parts as is. The summary should remove redundancy and express the text as concisely as possible, ensuring that allkey information are preserved. Only output a single json without the quote block.

# the raw doc

# doc after GPT-4 summarization example\_codedescriptiontab:exp_gpt4_acctab:exp_gpt4_comp_ratio