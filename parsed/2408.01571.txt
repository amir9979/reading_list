Counterfactual explanations (CEs), introduced by , offer actionable insights by suggesting minimal changes to an input that would alter a model's classification. Unlike gradient-based explainability methods such as saliency maps , which highlight influential features, CEs aim for changes that maintain realism and adherence to the data distribution, distinguishing them from adversarial attacks .

In medical imaging, CEs are invaluable for providing clinicians with insights into alternative diagnoses through plausible image modifications. This capability is critical for enhancing trust in diagnostic models by ensuring CEs remain within the medical data distribution. Generation of medical imaging CEs has evolved with the application of generative models like Variational Autoencoders (VAEs)  and Generative Adversarial Networks (GANs) . Recent advances in controlled image generation with Diffusion Models  have introduced sophisticated methods for creating condition-specific CEs. Techniques including classifier-guided  leverage gradients from pretrained classifier and ones using classifier-free guidance  condition the generation process on labels or classifier saliency maps to produce CEs.

Our method stands out by utilizing a Diffusion Autoencoder (DAE)  for CE generation within the diffusion model's latent space. This approach enables direct edits to latents, streamlining CE without needing external classifiers, and simplifying the process.

Supervised Learning approaches have successfully solved many problems in image processing. Nevertheless, their dependency on the size of the annotated dataset is a significant obstacle in domains in which data annotation is time-consuming and relies on expert annotators. In Self-Supervised Learning (SSL), networks use unlabeled training data to learn meaningful feature representations through an auxiliary task, which are then used for downstream tasks. 

Specifically in the generative SSL setting, it is common to synthesize novel examples with a generative model and use them as a dataset for training an external model for a downstream task, such as classification . Another line of work uses feature representations learned by the generative network itself for a discriminative task . 

 demonstrated that latents learned by StyleGAN  can be used for downstream tasks such as regression and classification in a fully supervised manner. As a direct inspiration to our work,  use StyleGAN latent codes to predict the magnitude of a visual attribute by measuring its distance from a hyperplane matching a linear direction with a semantic meaning.  migrated this approach to Diffusion Models, thus circumventing GAN Inversion .

Medical image analysis requires distinguishing between binary classification, multiclass classification, ordinal regression, and regression. Binary classification differentiates between two distinct classes, such as the presence or absence of a pathology. Multiclass classification assigns cases to one of several categories without considering any order among them. In contrast, ordinal regression predicts a rank-ordered score that reflects the severity of a condition. Regression, on the other hand, predicts a continuous numerical value rather than discrete, ordered categories. While regression is used for predicting exact measurements, ordinal regression is used when the outcome is categorical but ordered, such as grading the severity of a disease. This ordered grading is crucial since it enables clinicians to monitor disease progression, adjust treatment plans, and prioritize cases based on severity (e.g., for DR ). Although this study focuses on ordinal regression, our method could also be applied to regression tasks where predicting a continuous outcome is required.

We specifically study VCFs, the most prevalent osteoporotic fractures in those over 50, causing significant pain and disability . Radiologists use the Genant scale  to measure fracture severity from CT images. Deep Learning can automate VCF detection  , however, only a few works have considered VCF grading, all fully-supervised  . Compared to fracture detection, grading is an even more imbalanced task since medium to severely fractured vertebrae account for only a small portion of overall data.  Closest to our approach,  and  train auto-encoders for vertebra shape reconstruction and then use the learned latent codes for downstream fracture detection. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We use DAE  as a generative unsupervised feature extractor. DAE leverages the capabilities of Denoising Diffusion Probabilistic Models (DDPMs)  and Denoising Diffusion Implicit Models (DDIMs)  for unsupervised feature extraction. DAE distinguishes itself by incorporating a semantic encoder, designed to transform input images into a semantic latent space , that captures semantically meaningful information. This space is different from the DDPM and DDIM noise latent space , which was shown to lack high-level semantics . Using  enables the model to generate high-fidelity reconstructions and facilitate downstream tasks with a higher degree of semantic awareness. Moreover, it was shown that the semantic latent space is characterized by a linear data manifold, similar to StyleGAN's StyleSpace , facilitating meaningful interpolation between latent representations.

The architecture of DAE is characterized by its use of conditional DDIM, which performs dual functions: it acts as a stochastic encoder to encode the input image into a noise representation  and as a decoder to reconstruct the image from the combined semantic and noise latents. This method allows the model to effectively retain and utilize semantic details throughout image reconstruction. Training the DAE model involves minimizing the loss function:

where  is the input image,  is the timestep,  is the noise component and  is a neural network.  Training the DDIM backbone and the semantic encoder simultaneously ensures that the model produces semantically meaningful and visually coherent outputs.

We obtain the semantic latent representation  for a subset of training samples for which labels are available and train linear classifiers (linear regression and SVM) to predict the existence of the target pathology. The decision boundary of a binary linear classifier is represented by the hyperplane given by its normal equation:

where  is a semantic direction corresponding to the pathology existence,  is an input image latent, and  is a bias term. The magnitude of pathology in grading tasks is estimated using the distance of the latent  to the hyperplane : 

A simple linear regression is fitted to calibrate this distance to the respective pathology scale. Finally, the continuous values are rounded to the nearest grade to obtain the clinical grading in ordinal categories, matching the ground truths.

To generate CE images, the semantic latent code can be changed in the direction  and together with the original stochastic latent decoded by the conditional DDIM to a new image. For binary CEs, we reflect a given latent sample to an equivalent position on the opposite side of the decision boundary, maintaining the original distance from the boundary within the latent space. For a latent sample , the counterfactual is:

To generate a counterfactual of a particular pathology grade, the calibration utilized in the regression process is inverted to determine the magnitude of the required change along the semantic direction. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We experimented with several ordinal regression and classification tasks, some suffering from high class-imbalance (Table~).

 We used the public VerSe dataset  and an in-house dataset acquired at Klinikum Rechts der Isar and Klinikum der Universität München , containing a total of 12019 sagittal 2D CT slices of vertebrae. Each slice has a size of  pixels centered around a single vertebra, though multiple surrounding vertebrae are also visible. For each slice, the existence of a VCF in the center vertebra is indicated, and a small subset also includes a Genant grading, ranging from G0 (Normal/No fracture) to G3 (Severe/Over 40\% reduction). Of the 1248 fractured samples, 220 have a grading (74 G1, 102 G2, 44 G3). We applied no image augmentations and kept other training parameters with their defaults. We only use samples from the VerSe dataset to train the fracture classifiers and evaluate their grading.

 The SPIDER dataset  contains a total of 1446 3D T2-weighted MRI volumes. We preprocess these volumes to extract a random 2D  sagittal slice, each centered around a single IVD. Each IVD's matching Pfirrmann grade  is indicated. The Pfirrmann grading system is a widely recognized scale for assessing the degree of IVD degeneration in MRI. It ranges from Grade I, indicating no degeneration with a homogeneous structure and normal disc height, to Grade V, which signifies severe degeneration with a collapsed disc space and inhomogeneous structure with areas of hypointensity. 

 The RetinaMNIST dataset  comprises 1200  pixel Fundus camera images, each labeled with its corresponding DR severity grade. DR is a diabetes complication that affects the eyes and is characterized by damage to the blood vessels of the light-sensitive tissue at the back of the eye (retina). The severity of DR is categorized into five stages: from Grade 0 (no DR), representing no apparent retinal damage, to Grade 4 (proliferative DR), indicating advanced disease with a high risk of vision loss. Due to the small size of the dataset, we further applied image augmentation while training the DAE: image rotation (), image flip, grid distortion, and zoom (), each with a probability of .

 The BraTS dataset  consists of 484 3D FLAIR T2-weighted MRI volumes. In the preprocessing step, we extract a random 2D  slice from each volume and a binary label indicating the presence of peritumoral edema in each slice. This condition represents fluid accumulation around the tumor site, often indicative of the tumor's aggressiveness and the body's response to its presence.

 The MIMIC-CXR dataset  is an extensive collection of chest radiographs labeled to reflect common conditions that can be identified through radiographic imaging. In the preprocessing step, each chest X-ray image is resized to a  resolution to standardize input size across the dataset. The DAE was trained on 166512 unlabeled images. For training the classifiers, we chose a subset of labeled anterior-posterior (AP) images for lung edema, opacity, pneumonia, and cardiomegaly and used labels extracted from reports by the CheXbert~ labeler. The severity grades for the ordinal regression of edema are taken from  following their dataset splits.

We trained the DAE for  steps with a batch size of  on a single Nvidia A40 GPU. We used the official DAE implementation by . The semantic encoder used codes of . To encode images and create CEs,  reverse diffusion steps were used to encode the images back to the stochastic latent . The stochastic latent was then concatenated to the semantic latent in  forward diffusion steps to retrieve the original image . More training hyper-parameters are given in Sec.~. All grades apart from the minimal are considered positive for training the SVM and linear regression classifiers. For VCFs, since the labels were noisy, only G2 and G3 were considered as the positive class. For the image reconstruction baseline, we used StyleGAN2  with Encoder4Editing (E4E) . For the classification and regression baseline, we used DenseNet121 and trained it from scratch with the same data pipeline.

To evaluate image reconstruction, we measure the perceptual similarity between the original and encoded image using Learned Perceptual Image Patch Similarity (LPIPS) and the general image generation quality using the Fréchet Inception Distance (FID). For classification performance we use ROC-AUC and  score. We calculated the macro  and Mean Average Error (MAE) for grading performance.

We begin by evaluating the DAE's capability to accurately reconstruct images from latent codes using the VCF task. As shown in Table~, the DAE semantic encoder outperforms StyleGAN2 E4E in reconstructing vertebrae images, as evidenced by LPIPS scores. Fig.~ supports this finding, illustrating E4E's limited ability to capture fracture-relevant features. Despite the overall similarity in appearance between the original and E4E-reconstructed images, the fracture cavity (highlighted in red) is mostly lost. Additionally, the FID measurements in Table~ demonstrate that DAE generates higher-quality image distributions.

We conduct extensive experiments to measure the discriminative capability of the DAE latents in detection and regression in multiple tasks. For VCFs, Table~ shows that both linear layers and SVM can effectively separate fractured from healthy vertebrae by constructing a separating hyperplane with a respective AUC of 0.96 and 0.93. Results reported by  further highlight the advantages of DAE as a feature extractor in comparison to Autoencoder (AE) and Variational Autoencoder (VAE). Although the linear layer achieves better detection results, the SVM's hyperplane shows better performance in the linear regression of Genant grades. In Table~, our method achieved comparable results to a fully supervised DenseNet121 baseline on other detection tasks. 

Besides feature extraction, our method provides inherent CE generation. In this setting, the generative capability of the model can aid the interpretability of the decision support system. In Fig.~, the input image (left), after being encoded into the latent space (center), can be manipulated towards both classes, e.g., from healthy vertebra to severe fracture and vice versa. Similarly, in Fig.~, the input image (in the blue box) is manipulated towards the opposite class relative to the binary decision boundary, i.e., from healthy to pathology and vice versa.

Fig.~ demonstrates that our CEs can be extended to visualize whether the model is well calibrated to the ordinal regression of Genant grades by showing the model's perception of each grade for a given vertebra. While the last row in Fig.~ is not well calibrated, the rightmost image augments a barely visible feature. This exaggeration of anatomical changes could guide the radiologist's attention and help estimate the potential progression of pathological changes barely visible in the original image.

Additional examples in Fig.~ highlight the limitations of our method. A qualitative evaluation by a clinician revealed several issues: The Pfirrmann grade V CE shows only a slight signal loss with minimal height loss, suggesting it should be graded as grade II. In the DR examples, the model introduces additional blurring when transitioning from diseased to healthy states. The removal of peritumoral edema is only partial in the second case. Additionally, in the same task, clinicians noted size changes between the CEs and the original images, indicating a potential model bias related to the MRI slice position along the axial plane.

While the well-calibrated samples in Fig.~ show impressive visual results, the VCF grading metrics in Table~ reveal the limitations of our method. The linear separability of classes in DAE's semantic latent space using 2D slices cannot compete with the fully supervised, end-to-end baseline and 3D methods, motivating an extension to three dimensions. 

One reason for this might be a failure to disentangle the fracture of adjacent vertebrae from the central vertebral body that is classified. This can be observed in the second to last example in Fig.~, where the center vertebra remains unchanged while the top one changes with the severity of the fracture. At the same time, this showcases the interpretability of our approach by visualizing the model's understanding of different grades, highlighting its misguided feature attribution. In future work, the network could be explicitly guided to attend to the central vertebrae, or the generation process could be conditioned on anatomical features, thus disentangling these from disease progression.

The t-SNE projection of latents in Fig.~ clearly shows that the unsupervised DAE clustered the vertebral levels ranging from T1 to L5. Since fractures occur more frequently in the lumbar spine, a cluster of fractures can be observed there with many outliers.  We hypothesized that training with all levels could aid the data imbalance since only very few fractures are present in the data per vertebral level. However, the better separatability of Genant grading in the visually similar subset of vertebra L1-L4 (Fig.~, right) suggests investigating independent models for the different spine segments. 

To challenge the assumption of a linear relationship between the distance to the hyperplane and the severity of the fracture, we fitted a polynomial regression to grades G0, G2, and G3. The results in Table  show an improvement over our simple regression model, indicating the presence of a non-linear relationship that warrants further investigation in future research.

This work has demonstrated how CEs can reveal model biases by uncovering the inner representations of different classes and regression scores. For instance, manipulating vertebral images towards the healthy direction introduced a lung artifact, effectively translocating the vertebrae upwards within the spine where fractures are uncommon (Fig.~). Another example is the bias related to brain slice position along the axial plane and the presence of edema (Fig.~). 

Beyond revealing such model biases, CEs offer crucial insights into AI models, lifting the "black box" and empowering clinicians (and researchers) to understand the models' inner decision-making. Further, CEs might potentially drive the discovery of novel imaging biomarkers . 

Future studies should validate the clinical applications of CEs based on DAE. These studies should focus on several key areas. First, they should explore the potential of CEs for imaging biomarker discovery by unveiling which features drive classification. Second, researchers should assess whether generated CEs can effectively demonstrate potential disease progression, helping clinicians anticipate future developments. Third, studies could investigate using CEs to visualize reverse disease progression, e.g. aiding in planning reconstructive surgeries such as bone cement filling in vertebral bodies. Lastly, these investigations should aim to identify model biases, ultimately improving the overall reliability of the algorithm in real-world clinical settings. Through these diverse applications, CEs could provide valuable insights into disease mechanisms and treatment planning while enhancing the interpretability and trustworthiness of AI models in clinical practice.