Manipulation Classification~ is a task meticulously designed to ascertain whether multimodal data encompasses fabricated elements. To investigate LVLMs' proficiency in identifying multimodal content altered through various manipulative techniques, we designed six types of manipulation. The first four methods focus on visual alterations, while the last two target textual modifications. % relationship between classification and label Out-of-Context (OOC) Classification~ aims to evaluate the coherence and correspondence of context across various modalities. Unlike the aforementioned manipulation techniques that require modifying images and texts, OOC Classification combines real but misused images and texts. If the image and claim are contextually aligned, we define the relationship as true. Conversely, if the image and claim are not contextually aligned, we define the relationship as false. % To increase the difficulty, the NewsCLIPpings dataset uses methods such as CLIP and SBERT to find the most similar misused images.  We collected multimodal samples from the NewsCLIPpings dataset~, using embedding methods such as CLIP and SBERT-WK~ to extract the most similar misused images, for  in OOC Classification.

Veracity Classification~ involves assessing whether the context from one modality aligns with or accurately reflects the context from another modality.  Based on the image evidence, the LVLMs need to predict the truthfulness (Supported, Refuted) of the claim. We curated a subset of the Mocheg dataset~ for this task. If the image supports the truthfulness of the claim, we label the relationship between the image and the claim as ``Supported'' indicating a true label. Otherwise, it is labeled as ``Refuted'' indicating a false label. % {What is the relationship between `Supported' with `True' mentioned in Sec2.2.} This is a cross-modal semantic transformation task designed to . 

% 介绍任务种类%  The Manipulation Classification involves determining whether multimodal news is fabricated. The OOC  Classification assesses whether the image and claim are inconsistent. Lastly, the Veracity Classification evaluates whether the claim is true based on the image. Although these tasks are distinct, LVLMs only need to respond with ``yes'' or ``no'', facilitating a clearer understanding of their fact-checking capability to identify potentially abusive usage of multimodal disinformation.% define labels To unify the three tasks and facilitate a more effective analysis of benchmark results, we simplify the tasks into binary classification, we define the label . The Manipulation Classification task involves determining whether multimodal news is fabricated, with labels indicating ``'' (Non-Fact.) or ``'' (Fact.) The OOC Classification task assesses whether the image and claim are inconsistent, with labels indicating ``'' (Fact.) or ``'' (Non-Fact.). Lastly, the Veracity Classification task evaluates whether the claim is true based on the image, with labels indicating ``'' (Fact.) or ``'' (Non-Fact.).

% With the rapid advancement of LVLMs, an increasing number of models are being developed. To provide a comprehensive overview of the current status of LVLMs within the given time constraints, we have selected 12 representative models for this benchmark. These models feature diverse architectures, various training methodologies, originate from different companies, and vary in size. To provide an exhaustive perspective on the current state of LVLMs within the context of multimodal fact-checking, we conducted evaluations on 12 representative accessible LVLMs. Our selection encompasses a range of models from diverse organizations, differing in size, which allows for a thorough understanding of the capabilities and limitations of LVLMs in handling multimodal content concerned with facts.

For the open-source and accessible LVLMs, we adopt the representative models like LLaVA-NeXT~, InstructBLIP~, Qwen-VL~, Yi-VL~, InternVL~, CogVLM~, MiniCPM-V-2~, mPLUG-Owl~, Emu2~ and MiniGPT-v2~. As two of the most powerful closed-source LVLMs, both GPT-4V and Claude3-Haiku are also included in our testing scope.

We define a multimodal content  as a tuple consisting of an image  and an accompanying claim  to be fact-checked. 

Given that our benchmark comprises three distinct tasks, we have developed three task instructions  specifically designed to enhance the multimodal fact-checking capabilities of the LVLMs. 

To enable the model to answer binary classification questions, we carefully design three questions for three tasks and incorporate the image  and claim  into them.

At the end of each prompt template, we specify the required output format : ``Answer yes or no.''. % cot TODO As demonstrated in Figure~, to explore the effect of different prompt strategies like Chain-of-Thought (CoT)~ or In-Context Learning (ICL) prompting, we utilized the four following prompt methods for the : , ~, , and ~.

% For Manipulation Classification, the task \(T_1\) is defined as: % For Out-of-Context Classification, the task \(T_2\) is defined as: % For Veracity Classification, the task \(T_3\) is defined as:  

We conduct extensive experiments on the  to evaluate a total of 12 representative LVLMs: 1) ~; 2) ; 3) ~; 4) ~; 5) ~; 6) ~; 7) ~; 8) ~; 9) ~; 10) ~; 11) ~; 12) ~. % TODO% %     \item ~, developed by OpenAI, is a version of the GPT-4 architecture that includes capabilities for processing and generating images in addition to text. %     \item ~, developed by Anthropic, possesses sophisticated vision capabilities comparable to other leading models. It can process a wide range of visual formats, including photos, charts, graphs, and technical diagrams.%     \item ~ is a generative multimodal model with 37 billion parameters, designed to enhance task-agnostic in-context learning capabilities through effective scaling. %     \item ~ is a large-scale vision-language foundation model, scaling up the vision foundation model to 6 billion parameters and progressively aligning it with the LLM, using web-scale image-text data from various sources. %     \item ~ is a powerful open-source visual language foundation model that achieves state-of-the-art performance on multiple cross-modal benchmarks by using a trainable visual expert module for deep fusion of vision and language features.%     \item ~ is the new version of LLaVA~, with improved reasoning, OCR, and world knowledge capabilities.%     \item ~ introduces a novel vision-language instruction-tuning framework utilizing BLIP-2 models to enhance zero-shot generalization performance across diverse vision-language tasks.%     \item ~ is Alibaba Cloud's multimodal large vision-language model that excels in multilingual text recognition, fine-grained understanding, and multi-image interleaved conversations, significantly outperforming other large vision-language models in various benchmarks.%     \item ~, developed by DAMO Academy, is a training approach that enhances LLMs with multimodal capabilities by integrating a foundational LLM with a visual knowledge module and a visual abstractor module, using a two-stage method to align image and text.%     \item ~ is a unified vision-language model designed for diverse tasks such as image description and visual question answering, utilizing unique task identifiers for improved performance and efficiency.%     \item ~ is an open-source multimodal vision-language model from the Yi LLM series, excelling in content comprehension and multi-round image conversations, and leading in recent English and Chinese benchmarks.%     \item ~ is a robust multimodal large language model designed for efficient end-side deployment. It is built on the foundation of SigLip-400M and MiniCPM-2.4B, connected by a perceiver resampler.%     % \item VILA~ VILA is a visual language model that excels in video and multi-image understanding, enhanced by interleaved image-text pretraining, deployable on the edge with AWQ 4bit quantization and TinyChat framework, offering advanced capabilities like video reasoning and in-context learning.%     % \item phi3 vision%  To ensure our results are reproducible, we set the temperature as 0 without any sampling mechanism. We use the accuracy and macro-averaged F1 score as the evaluation metrics. More implementation details and baseline descriptions are provided in Appendix .

% % few-shot set up% %     \item GPT-4V  1%     \item LLaVA-NeXT~ 1%     \item InstructBLIP~%     \item Qwen-VL~ 1%     \item Yi-VL~%     \item mPLUG-0wl~%     \item MiniGPT-v2~%     \item CogVLM~%     \item VILA~ %     \item InternVL~%     \item MiniCPM-V-2%     \item Emu2~ 1%  In Table~, we present the average outcomes of 12 accessible and representative LVLMs in a zero-shot setting on the . From the results, we derive the following observations: 

1) For the overall performance of the LVLMs on the Manipulation Classification, the open-sourced model LLaVA-NeXT achieves the best performance with the 56.9\% macro-averaged F1 score. It is worth noting that the most lightweight model, MiniCPM-V-2, also performs well across a spectrum of manipulation techniques, with the highest 63.2\% accuracy and 56.2\% macro-averaged F1 score. Counterintuitively, the more powerful closed-source models, namely GPT-4V and Claude3-Haiku, fail to produce promising results in this task. 2) This discrepancy underscores the complexity of model architecture and the necessity of task-specific tuning, revealing that sheer computational power does not guarantee superior performance in such a specialized task for manipulation classification. None of the tested models exceed a performance threshold of 57\%, revealing a vulnerability of vision-language foundation models in this multimodal fact-checking subtask. 3) In OOC Classification, GPT-4V stands out as the preeminent model with the highest 75.8\% accuracy and 75.2\% macro-averaged F1 score. In terms of Veracity Classification, Qwen-VL is distinguished by its considerable macro-averaged F1 score of 69.3\%. 4) Overall, we can find almost all the LVLMs could achieve better performance on OOC Classification but worse on Manipulation Classification, and performance on Veracity Classification lies in the intermediate range. This pattern underscores the rational distribution of task difficulty within our proposed benchmark, , which comprehensively spans a spectrum from challenging to straightforward multimodal fact-checking tasks.

During benchmarking, we identified a Yes/No Bias issue with the tested LVLMs, where it tends to consistently respond with either ``yes'' or ``no''. We have chosen two key metrics to evaluate the Yes/No bias of the model for the Manipulation Classification task: 1) False Positive Rate (FPR)~ and 2) False Negative Rate (FNR)~. % Definitions:% %   \item True Positive (TP): The number of correctly predicted positive cases (``yes'').%   \item False Positive (FP): The number of incorrectly predicted positive cases (``yes'').%   \item True Negative (TN): The number of correctly predicted negative cases (``no'').%   \item False Negative (FN): The number of incorrectly predicted negative cases (``no'').% % %     \item False Positive Rate, FPR%         \[%         FPR = {FP + TN}%         \]%     \item False Negative Rate, FNR%          \[%         FNR = {FN + TP}%         \]%  In Figure~, models such as GPT-4V, Claude3-Haiku, Yi-VL, and InternVL tend to answer ``no'' more frequently. Conversely, models like Emu2, MiniGPT-v2, and InstructBLIP are more inclined to answer ``yes''. Meanwhile, LLaVA-NeXT, CogVLM, Qwen-VL, and mPLUG-Owl exhibit a balanced performance without a strong bias towards either affirmative or negative classifications. Given that these models were not specifically trained for this task, the presence of such biases is not unexpected. This underscores the necessity of , aiming to guide the enhancement of fact-checking capabilities in LVLMs for future developments.

To explore the impact of model size on factual capabilities, we analyzed two families of LVLMs: InstructBLIP and LLaVA-NeXT, which both utilize the same language backbone, i.e., Vicuna~, and employ similar CLIP models, with InstructBLIP using EVA CLIP-g and LLava-NeXT using CLIP ViT-L/14. Specifically, we examined InstructBLIP (7B), InstructBLIP (13B), LLava-NeXT (7B), LLava-NeXT (13B), and LLava-NeXT (34B). As shown in Figure~, the following observations were made: 1) In Manipulation Classification, there is a minimal correlation between the model size of the specific LVLMs family and the performance. % aligning with our main findings in Table~.  2) Regarding OOC Classification and Veracity Classification, the model performance generally improves with the increased model size.

% The image size in InstructBLIP and LLaVA-NeXT are same 224*224, InstructBLIP uses  EVA CLIP-g,LLaVA-NeXT  CLIP ViT-L/14;% CLIP ViT-L/14 llava% EVA CLIP-g instructblip% https://laion.ai/blog/large-openclip/% % zero-shot cot 不会影响性能% few-shot cot 没啥大用 As Table~ and Table~ show, the impact of CoT in the zero-shot setting varies across different models and tasks on . For Manipulation Classification, most LVLMs exhibit improved performance with CoT, particularly in terms of the F1 score, as seen in GPT-4V, where the F1 score increased from 45.6\% to 50.6\%, and in Claude3-Haiku, where it rose from 36.8\% to 42.7\%. In the case of OOC Classification, CoT proves beneficial for some LVLMs, such as Qwen-VL, while it negatively affects others, like Claude3-Haiku. For Veracity Classification, CoT generally does not significantly impact performance and may even reduce it for certain models.

In few-shot settings, as shown in Figure~, CoT does not enhance the performance of Qwen-VL (7B) and LLaVA-NeXT (13B). For Qwen-VL(7B), CoT has a minimal to slightly positive impact on performance across all shot scenarios, without notably influencing the rate of valid responses. Conversely, CoT impacts LLaVA-NeXT (13B) inconsistently. The possible reasons for these observations include the underdeveloped ability of the LVLM to handle multiple image inputs and the excessive length of the rationale, which diminishes the model's ability to understand the task effectively.

% 引出 yes/no bias qwen 全是 no% llava 没有接受专门的 few-shot训练,出现不可控现象 To thoroughly investigate the impact of In-Context Learning (ICL) on model performance, we selected Qwen-VL (7B) and LLaVA-NeXT (13B) that support multiple image inputs to conduct few-shot experiments on Manipulation Classification.  We calculated the macro-averaged F1 scores and the rates of valid responses for both LVLMs, where the valid response means the response without ambiguous outputs. 1) The results, as illustrated in Figure~, indicate that the implementation of few-shot learning does not enhance the fact-checking capabilities of these models. 2) For the performance of Qwen-VL in Figure~, the few-shot prompt (i.e., ICL) did not result in a performance improvement. Instead, we found that it induced model inertia, leading it to predominantly respond with ``no'' in most instances. 3) For LLaVA-NeXT, as the number of shots increases, the average number of valid responses from the model progressively decreases, as shown in Figure~. We speculate that this number decline of valid responses to our prompt is likely due to the model's lack of sufficient training on multi-image scenarios.  We provide more qualitative analysis in Appendix .

For Manipulation Classification, we designed six types of manipulation, selecting data from the DGM4 dataset~ and constructing additional datasets ourselves. The initial data was sourced from the VisualNews~ datasets. The DGM4 dataset complies with the Apache-2.0 license. The VisualNews dataset is available upon request. 

The Out-of-Context Classification data is sourced from the NewsCLIPpings dataset , while the Veracity Classification data is obtained and sampled randomly from the Mocheg dataset . The NewsCLIPpings dataset is available upon request. Mocheg dataset complies with the Apache-2.0 license.

We utilized the high-performance computing platform and employed Slurm to request 2-4 A800 GPUs for benchmarking.

Recent advancements have seen LLMs excel across various domains, with major tech companies developing high-performing proprietary models such as OpenAI's GPT-3~  and GPT-4~, Google's PaLM~ and Gemini~, and Anthropic's Claude. These models, however, are often only accessible via specific APIs or not at all. In contrast, the AI community has embraced the emergence of open-source LLMs, making significant contributions like MistralAI's Mistral-series~, Google's UL2-20B~ and Gemma~, Tsinghua University's GLM-130B~, and Meta's OPT~ and the LLaMA series~, enhanced by extensive alignment efforts~.

LVLMs have significantly advanced the understanding of both textual and visual data within a unified framework~. Innovative models such as Flamingo~ and PaLM-E~ have demonstrated the ability to integrate visual and textual information effectively, without the need for task-specific training. Concurrently, the development of diverse multimodal datasets~ stemming from GPT-4 and GPT-4V~ has spurred the fine-tuning of models like LLaVA~, MiniGPT-4~, mPLUG-Owl~, InstructBLIP~, and others~, highlighting a trend towards more versatile and real-world applicable multimodal systems.

Previous studies have established that language models (LMs) can function as repositories of factual knowledge, serving effectively as knowledge bases~. This reservoir of factual information acquired during pretraining proves beneficial for knowledge-intensive tasks, such as question-answering and fact-checking~.  used cloze tests involving triples and tailored prompts to evaluate the factual knowledge embedded in language models, while  focused on optimizing prompt design to enhance factual retrieval from these models.

Despite these advancements, the reliability of these methods has been questioned.  highlighted the inconsistency in rank-based probing methods when using paraphrased contexts. Similarly,  argued that biased prompting and the leakage of correct answers can often lead to an overestimation of an LM's knowledge retention. On the other hand,  employed question-answering formats to gauge models' uncertainty about specific facts, suggesting a different approach to measure factual accuracy. Our methodology aligns more closely with the approaches of , which involve querying models directly to self-evaluate their accuracy in delivering factual responses, offering a more direct assessment of their knowledge capabilities.  But differently, this work focuses on the multimodal nature of fact checking to explore the complex reasoning capability of LVLMs.

%  Multimodal Fact-Checking refers to the systematic process of identifying counterfactuals or inconsistencies between facts across different modalities within multimodal data . Common manifestations of multimodal misinformation include claims about digitally manipulated context  and the amalgamation of context from disparate modalities and contexts . The former is predominantly associated with deepfake technologies , while the latter is linked with cheapfake methodologies . An essential Multimodal Fact-Checking pipeline consists of evidence retrieval and the adjudication process. Evidence retrieval furnishes the foundational basis for subsequent multimodal judgments. Within the adjudication phase, tasks are delineated into distinct categories, such as Manipulation Classification, Out-of-Context Classification, and Veracity Classification.

Manipulation Classification  is a task meticulously designed to ascertain whether multimodal data encompasses fabricated elements. Out-of-context Classification  aims to evaluate the coherence and correspondence of context across various modalities. Veracity Classification  involves assessing whether the context from one modality aligns with or accurately reflects the context from another modality. Collectively, these tasks constitute the comprehensive process of multimodal fact-checking. In this work, we employed six different manipulation techniques to assess whether LVLMs can detect manipulations in multimodal news. Data from the NewsCLIPpings dataset is used to challenge LVLMs' ability to discern semantic differences between real images and real text, specifically for OOC classification. Similar to text, the cross-modal Veracity task is used to evaluate LVLMs' ability to perform factual inference across different modalities.

Traditional multimodal benchmarks have been centered around specific skills such as visual recognition~, image description~, and visual commonsense reasoning~. However, the advent of advanced LVLMs has necessitated the development of new benchmarks to keep pace with their robust zero-shot capabilities, which often exceed those measured by conventional metrics. This has exposed shortcomings in their ability to match answers accurately, highlighting issues with robustness. To address these limitations, the research community has introduced several innovative benchmarks, such as MME~, MMBench~, MM-Vet~, SEED-Bench~, GOAT-Bench~, LAMM~ and MMCode~. These benchmarks are designed to facilitate structured evaluations of complex multimodal tasks and reveal the flaws of traditional methods. Distinct from these, our proposed benchmark is tailored to systematically assess multimodal factual knowledge, especially concerning disinformation detection in the realm of deepfakes and cheapfakes. This testbed would allow for a more thorough exploration of LVLMs' trustworthy awareness concerning a wider range of task types associated with multimodal factuality.

For the open-source LVLMs, test set leakage is not a concern, as the literature explicitly delineates the datasets and instruction-tuning procedures employed in their training, none of which encompass the multimodal data utilized in our . However, we cannot fully guarantee the exclusion of potential data leakage with GPT-4V and Claude3-Haiku, as its internal workings remain opaque. Nevertheless, as evidenced by the results in the experiments, where all LVLMs were evaluated directly on the , the absence of significant test set leakage is implied. This is inferred from the fact that direct application of the LVLMs did not yield disproportionately high performance, which would be expected if the models were benefiting from test set leakage.

We further provide the detailed results of the 12 representative LVLMs on the Manipulation Classification with respect to the six manipulation methods, as depicted in Table~-.

To verify the model's understanding of manipulation tasks, we designed prompts for six different manipulation tasks and tested them on twelve models (see  ). As shown in Figure , the model's performance on each sub-task was consistent with that of a single prompt. This suggests that the model struggles with manipulation fact-checking. For the Background Change task, the scenarios we set might have been too simple, making it easy for the model to detect the manipulations.

To better understand the reasoning process of the model in fact-checking, we are conducting a study on the correct and incorrect reasoning processes of the GPT-4V model.  Figure~ illustrates an instance where GPT-4V fails to identify manipulated content, specifically a face swap involving Joe Biden and another individual. This oversight underscores a significant limitation of GPT-4V in accurately recognizing individuals within images. The model's rationale primarily emphasizes overall scene consistency and plausible historical context, but it fails to detect the specific manipulation of Joe Biden's identity. In contrast, Figure~ showcases GPT-4V's successful identification of manipulated content by accurately discerning the discrepancy between the emotional state depicted in the image and the corresponding caption. Todd Stern's smiling expression contrasts with the caption's description of him angrily rebuffing a suggestion. GPT-4V effectively recognizes this emotional mismatch, demonstrating its capability to evaluate the coherence between visual and textual elements.

In zero-shot settings, the model's performance relies solely on its understanding of the instructions, its comprehension of the images and claims, and ultimately making a judgment based on this understanding(see also~). The main results indicate that the model's fact-checking ability is weak. As discussed in , the Yes/No Bias also highlights this issue.

In few-shot settings, the model does not gain insights from the examples. As Figure~ shows,  LLaVA-NeXT's usable response rate decreases, and it starts outputting gibberish instead of ``yes'' or ``no''. Specifically, in few-shot with CoT conditions, LLaVA-NeXT does not generate its own reasoning process but merely copies the rationale from previous examples. For example, one output from LLaVA-NeXT is, ``Answer yes or no. Rationale: The image shows what seems to be an unnatural or edited blend of faces, particularly noticeable in the features of the man and the child. This indicates that the image may have been digitally altered.'', which is already included in the demonstrations of the prompt.