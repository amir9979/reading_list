Instruction tuning is a widely-employed method for enhancing the instruction-following capability of LLMs~. Data quality significantly outweighs quantity when it comes to instructional tuning. Several studies~ demonstrate that fine-tuning models with only a small subset of data from the original dataset, i.e., the Alpaca dataset~, can yield results that greatly surpass those obtained from fine-tuning models using the entire dataset. Other researchers  have explored the evolution of training data towards increased complexity and diversity when preparing datasets for instruction tuning. Instead of perceiving instruction tuning merely as a process of distilling the entire dataset at once from a teacher model, ~refine instruction with each iteration through a teacher model.  

%或者叫LLM 自动标注 Knowledge distillation from an advanced, proprietary LLM into a weaker, accessible open-source LLM has gathered notable attention~. As a way of distilling from stronger LLMs, some researchers utillize a teacher LLM for data augmentation and annotation to fine-tune student LLMs~. Researchers propose different techniques to synthesize data from LLMs across various tasks and domains.  introduce a self-reflective critic-and-revise framework to generate scientific questions-answer pairs using an LLM to address the data scarcity challenge in the science domain.  synthesize a mathematical dataset from LLMs by bootstrapping questions from existing datasets and then rewriting the questions from multiple perspectives.  and  employ LLMs to generate and annotate datasets for training a sentence encoder and an LLM judge.

Despite  point out that there may exist a systematic bias in the automatic evaluation using an LLM, e.g., GPT4~, the LLM-as-a-judge paradigm has become widely adopted. Techniques such as pairwise comparison and reference-guided grading are employed to reduce assessment bias. The LLM-as-a-judge paradigm, known for being cost-effective and exhibiting high correlation with human annotators, has been utilized across multiple benchmarks~. Several studies~ also prompt an LLM to score the responses generated by models, with the aim of improving the process of instruction tuning.

The overview of our framework is demonstrated in Figure~.  Overall, the~ (TAPIR) framework is designed to mitigate the issues stemming from unbalanced task distributions and the heterogeneous difficulty of instructions. 

We first view TAPIR from a single-round perspective. The~ module is designed to select challenging instructions for a student LLM to learn, which enhances the model's task-solving capabilities. Next, based on the seed dataset, we propose~ that ensures a balanced representation of tasks, thereby preventing the skew in model performance.

To enhance the effectiveness of instruction tuning, we extend TAPIR to the multi-round scenario. Here, our framework incorporates the principles of curriculum planning. We systematically increase the complexity and difficulty of tasks, thereby enabling the student LLM to progressively develop and refine its capabilities.

%我们选择一个初始的数据集,从中筛选种子数据

Our methodology begins with the establishment of the student model . This model is initialized using a foundational pre-trained LLM, such as LLaMA2~ or any alternative decoder-only LLM when required. Concurrently, we set up the teacher LLM  and the LLM judge  from more powerful, and often proprietary, LLMs (such as ChatGPT or GPT-4). In our implementation,  and  are instantiated by the same LLM, which we access via API calls.

To capture the diverse NLP task proficiencies of the teacher LLM, our approach incorporates a variety of prompt templates. We employ a publicly available instruction tuning dataset, namely the Alpaca dataset~, as our foundational training corpus. This dataset comprises a collection of instruction-response pairs, , where each  represents the -th instruction. The corresponding response  used in our work is generated by the teacher LLM .

To curate a high-quality seed dataset, we propose the~ (MFD) metric, which allows us to selectively filter initial seed data from existing instruction tuning datasets.  Our process begins by fine-tuning the student LLM  on the dataset , resulting in an initial model . Next, we employ  to generate the response for each  in , i.e., . This exercise assesses the student LLM's internal ability to adequately learn from  through simple straightforward training (which does not requires us to obtain a very strong student model here). Consequently, the MFD score for each instruction  is determined as follows:

The judge LLM  assesses the divergence between the teacher-generated response  and the student-generated response  for a given instruction . The prompt template to facilitate this assessment is shown in the appendix . Here, the judge LLM  is tasked with evaluating the ``goodness'' of the student model's response  (i.e., ) and the teacher's response  (i.e., ) with scores as output, in the range from 1 to 10. To compile our seed dataset, we establish a threshold ; only those instruction-response pairs where the judge's rating meets or exceeds  are included:

Employing the MFD metric strategically compels the student LLM to engage with more challenging instructions, averting the model's potential bias towards mastering less complex ``shortcuts''  (i.e., easy tasks). This practice, therefore, elevates the model's task-solving capabilities to a higher upper bound.

Upon acquiring the seed dataset , we initiate the distillation process from the teacher LLM. It is important to note that imbalanced task distributions in the training data significantly influence the performance of SFT more than the sheer volume of data. To counteract this, our methodology enforces a balanced task representation within . We introduce the~ (DTR) technique, which is integrated into the SFT process.

Let  represent the set of all task types. Empirical evidence suggests that certain tasks (specifically mathematical problem solving, logical reasoning, coding) play a pivotal role in enhancing the intrinsic abilities of student LLMs , despite their potential under-representation in public datasets. Consequently, we elevate the sampling probability for these critical tasks. We define  as the probability distribution over the task types in , and we denote the task type of a given pair  as . During the training phase, each pair  is sampled from the seed dataset  with replacement strategy, applying the task probability  as the re-sampling weight. A comprehensive enumeration of tasks, along with their probabilities, together with the prompt templates for task classification are shown in the appendix.

As far as the task types are considered, we further observe that, learning from direct responses from the teacher LLM only is not enough. For instance, a straightforward solution or a simple explanation to a mathematical problem, as provided by the teacher LLM, may not offer adequate instructive cues for the student to internalize the underlying reasoning process. Drawing inspiration from prior research~, we have refined our distillation approach to incorporate task-specific prompts for re-writing better responses. Particularly, the Chain-of-Thought (CoT) paradigm~ is applied to the more challenging reasoning tasks to generate detailed responses for better guidance. Refer to Table~ for an example of CoT.

To summary, our instruction distillation process is both~ and~. Let  be the response re-writing version of  . The token sequences are further denoted as  and  with  being the sequence length of . Therefore, the re-sampled auto-regressive causal language modeling loss  for LLM distillation is defined as:

%%%(\Phi)= & -\sum_{(x_i,y_i)\in D_S}\sum_{l=1}^{L_i}[\log\Pr(_{i,l+1}\vert x_i,\\%& _{i,1\cdots l},\Phi)\cdot \Pr((x_i,y_i))]%% where the student LLM  is parameterized by , and .

The aforementioned techniques are designed to cultivate a proficient student LLM  within a single training cycle. However, the sole reliance on a single round may not ensure 's optimal performance. Moreover, it is essential for student LLMs to engage with simpler instructions to avert the catastrophic forgetting of basic tasks. Curriculum learning strategies~ typically start with simpler task aspects or tasks and incrementally progress to more complex challenges. To this end, we augment our approach with the~ (MCP) technique, which aims to enhance the student 's capabilities across successive rounds.

In each training round , the proportion of challenging instructions within our seed dataset is incrementally augmented by a factor of . It is important to note that the initial seed dataset  comprises a curated set of tasks characterized by their higher difficulty. When  is set to 1, the entire training corpus consists exclusively of these ``hard'' samples from . By progressively increasing  through subsequent rounds, we systematically raise the complexity of the learning tasks.  %Furthermore, this progression is seamlessly integrated into the MCP framework, enabling us to derive t The loss function for the -th round is defined as follows: %%%& (\Phi,r) = \alpha(\Phi) -(1-\alpha)\sum_{(x_i,y_i)\in D\setminus %D_S}\sum_{l=1}^{L_i}\\%& [\log\Pr(_{i,l+1}\vert x_i,_{i,1\cdots l},\Phi)\cdot \Pr((x_i,y_i))]%% Finally, we present our MCP training algorithm in Algorithm~.

In this study, the student LLM is initialized using LLaMA2 .  Therefore, we benchmark our model against the following state-of-the-art LLMs that are similarly fine-tuned on the same base model : Alpaca , LLaMA2-Chat , Vicuna , and Recycled WizardLM .  Notably, both LLaMA2-Chat and Vicuna have undergone training on datasets that are substantially larger than the one used for our student LLM. Recycled WizardLM has shown outstanding results on benchmarks such as AlpacaEval. To the best of our knowledge, Lion~ is the most recent work for distilling large proprietary LLMs, based on adversarial learning. We also take this work as our baseline.

%  We filter our seed dataset from the Alpaca dataset~, which consists of 52K instruction-following samples. This dataset was developed using the self-instruct approach and generated by the~ model.

 In our work, we utilize the pre-trained LLaMA2 7B model as the student LLM and employ ChatGPT~ as the teacher and judge model.  %We use various prompt templates to have ChatGPT play roles as a judge and a task classifier. For optimization, we utilize the Adam optimizer~, setting the learning rate at , the warm up rate at 0.03 and a batch size of 32. The training process spans three epochs with a maximum sequence length of 2048 with the bfloat16 precision. We implemented two models, namely  and .  TAPIR-7B-S is trained in single round   without the incorporation of curriculum learning.  In default, we set the threshold  for seed dataset creation. TAPIR-7B-M, on the other hand, represents the fully-realized, multi-round version of our approach, where all the proposed methods have been applied.  is set to 0.3 in default. In each round, the sampling weight for challenging instructions is increased by 0.2 in the three rounds. All the experiments are run on a server with NVIDIA A100 (80GB) GPUs. The 3-round iterations may require a total of 190 GPU hours to complete.

 In our work, the inference of TAPIR models is configured to favor creativity while maintaining the coherence of generated contents. Specifically, the temperature was set to 0.5. We set the maximum generation length at 2048. All other settings were left at their default values, based on the default settings of LLaMA2 .

For automatic evaluation, we utilize AlpacaEval 2.0  and MT-Bench  as benchmarks. AlpacaEval 2.0's leaderboard presents an automated assessment framework specifically designed for LLMs. This approach effectively evaluates LLM performance by comparing the model's outputs against reference responses from GPT4 Turbo , which are then automatically annotated by GPT4 Turbo. The evaluation culminates in the calculation of win rates. Studies indicate that the results from AlpacaEval correlate closely with those of human expert annotations. MT-Bench is another comprehensive and widely-used benchmark designed to test the proficiency of LLMs in following instructions. It encompasses a broad array of single and multi-turn dialogue scenarios covering diverse domains, including writing, reasoning, mathematics, and coding tasks. Within MT-Bench, the evaluation mechanism also relies on GPT4 Turbo to serve as an internal judge that rates model responses.

In Table , we report the ablation results of our method. In the table, ``Single Round'' refers to our trained model without MCP, which slightly under-performs our full implemented model (i.e., ``Full Implement.''). It shows that the MCP technique can boost the performance of the student LLM by curriculum planning through multiple rounds. ``Full Alpaca'' is the model fine-tuned on the original Alpaca dataset, and ``Seed Alpaca'' is the setting where our model is trained on the selected Alpaca dataset, which is filtered by the MFD metric. The results show that models trained on a subset of the Alpaca dataset, refined using our method, outperform those trained on the complete dataset. Additionally, we have compared the efficacy of our re-writing technique before and after the improvement (denoted as ``Seed Alpaca (R)''), demonstrating that our approach enhances the answer qualities.

Figure  provides an in-depth examination of TAPIR's training progression by charting its performance on AlpacaEval 2.0 and MT-Bench across successive training iterations. The scores reveal that our novel task-aware curriculum distillation framework steadily boosts the student model's capabilities with each round. 

To better visualize the performance across various tasks, we compare the response quality scores of TAPIR, LLaMA2-Chat, and Lion against those of ChatGPT based on Vicuna-Instructions~. We employ the prompt from Table  and conduct a pairwise comparison using GPT-4 to evaluate the relative quality of the generated responses. We present the relative response quality scores from the three models across various sub-tasks compared to ChatGPT in Figure~. The results show that our trained model consistently outperforms baselines across most tasks.

 The Alpaca dataset does not have task type labels. We utilized ChatGPT to categorize the tasks in the seeds and assign task labels. 

In Figure , we present the visualization of the task distribution of the Alpaca dataset alongside the task distribution re-sampled by our method. Our categorization of task types is derived from the evaluation tasks of WizardLM. Our dataset features a more uniform distribution of tasks, which over-samples  tasks of only a small percentage, such as code debugging and law. Among all the tasks, logical reasoning and mathematical problem solving have the largest proportions, which follows the practice~ to improve task solving abilities of student LLMs.

% 附录详细表格,写几句分析生成文本的区别 To clearly compare the quality of responses generated by our model with those from other baseline models, we present several case studies drawn from the Vicuna-instruction dataset~ in the Appendix . We utilize the scoring methodology depicted in Figure , employing ChatGPT's responses as references to enable GPT-4 to evaluate these model response cases. This judgment is carried out through a pairwise comparison and the final score is the average score of two rounds.

Table  shows that when the model is asked to play as a sports commentator, TAPIR vividly describes the final winning play of a championship game, capturing the excitement with dynamic language. Lion provides an analysis on how to commentate such moments, not fully complying with the task. LLaMA2-Chat misinterprets the instruction. It does not deliver a description of the play and therefore does not fulfill the request. Table  demonstrates an instruction to estimate a huge number using common sense. Although TAPIR erroneously assumes a constant blink rate without taking into account periods when people are asleep, TAPIR's calculation appears to be more precise. Lion, on the other hand, makes an error by stating the number of blinks per hour as the number of blinks per day. LLaMA2-Chat provides no actual calculation for the number of blinks in a lifetime and instead focuses on factors that could affect blinking. In Table , TAPIR writes a Python program that correctly implements the dynamic programming approach to calculate the -th Fibonacci number. Lion, on the other hand, provides an incorrect and irrelevant explanation and code. LLaMA2-Chat also presents an incorrect response by suggesting that it is not possible to find the -th Fibonacci number using dynamic programming.