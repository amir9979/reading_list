To answer the questions raised above, we need to learn effective representations from paired image-text samples. Prior work has shown that contrastive learning is a suitable approach for learning representations from paired data . Let  denote an image encoder and  denote a text encoder that maps images and text to a common representation space, respectively. Given a batch of training samples , where  and  denote the  image and text instances respectively, the InfoNCE loss  is optimized by minimizing the distance between the representations of an image and its corresponding text, , while maximizing the distance between unrelated image-text representation pairs, : _(x_i,t_i;B) = -  +  ,

where  denotes similarity between two vectors (e.g. cosine similarity), and  is a temperature parameter. For simplicity of notation, we drop  and denote the loss for  by . Multimodal contrastive learning trains encoders  and  by minimizing Eq.~ over the pairs in : _ = __B _^N  _(x_i,t_i).

To study the transferability and effectiveness of general-domain representations (RQ1), we consider encoders trained on a large-scale dataset of general image-text pairs. By a full or partial freeze of each of these encoders as shown in Figure~ (a and b), we will study the effectiveness of representations of the corresponding modality for medical tasks. In particular, we consider the following cases:

Partial fine-tuning while freezing early layers on a network allows for building high-level features on top of the early layers' features. In addition to leveraging general-domain features, this approach offers two additional benefits: (i) it reduces the number of learnable parameters, which improves robustness when training on a small amount of data, and (ii) it reduces the computational cost.

The extreme cases of a full encoder freeze (cases 2 and 3) imply a constant representation space for the corresponding modality. However, adaptation of the other modality's encoder allows for aligning its representations to that of the frozen encoder. A similar approach was taken in ImageBind , where the text and image encoders were frozen, but encoders of four other modalities were tuned to align with the frozen encoders.

Vision-language contrastive learning maximizes alignment between image-text pairs. While this focuses on inter-modal alignment, a vast body of work in self-supervised learning has been devoted to unimodal representation learning. Leveraging unimodal representation learning in a multimodal framework has the potential to enhance representations by considering the relationship between examples within each modality . To explore this, we consider two unimodal contrastive learning approaches and combine each of them with multimodal contrastive loss during training.

For this purpose, we employ SimCLR  which is one of the pioneering techniques for visual self-supervised learning. In this technique, two views of an image are created by applying a series of random augmentations (e.g. crop-and-resize, color jitter, Gaussian blur, etc.) to the source image. Then, an encoder is learned by minimizing the contrastive loss (i.e., minimizing the distance between two views of an image in the representation space, and maximizing the distance between two views of different images) as illustrated in Figure~ (c). Given a batch of data, , and random augmentations,  and , SimCLR learns encoder  and projector  (usually a multi-layer perceptron) by minimizing:  _ = ___((_1(B))), ((_2(B))) .

Inspired by , we also perform contrastive learning between representations of an original image and its masked version, Figure~ (d). Following , we divide the image into non-overlapping patches and then use a masking operator, , to randomly mask a certain fraction of patches. The unimodal contrastive loss in this case is defined as, _ = ___((B)), (((B))) .

Finally, in each case, the text and image encoders are trained by optimizing, _ + (1-) _,

where,  is the image-text contrastive loss (Eq.~), and  is a tradeoff parameter.

We study how changing the granularity of features affects multimodal representation learning. To learn fine-grained features, we investigate CoCa , which performs simultaneous image-text contrastive learning and image captioning as shown in Figure~ (e). The addition of autoregressive caption generation via a text decoder conditioned on the image representation aims for detailed granularity . The loss is defined as , where for an image-text pair , the captioning loss, , is defined as, _(x,t) = -_(t_|t_, (x)).

Here,  is a trade-off parameter,  is a text decoder,  denotes the -th token of ,   denotes all tokens up to the -th token in , and  denote the representation vector of .

Going in the other direction, to study the effect of learning high-level (coarse-grained) features in multimodal contrastive learning, we follow FLIP . This method masks a large portion of image patches and applies contrastive loss between the visible patches and text, Figure~ (f). In addition to enforcing the vision encoder to learn higher-level features, masking 50-75\% of patches also reduces the computation cost by 2-4. 

 We train  contrastive vision-language models, categorized along three key dimensions: transferability (RQ1), unimodal learning (RQ2), and fine-grained representation learning (RQ3). Table~ presents the benchmarked methods and their descriptions. Here,  follows the prescription of contrastive learning between a masked image and the corresponding text, proposed by FLIP . The training data consists of 2.8 million image-text pairs encompassing radiology (1.4M), histopathology (1.2M), and general medical images (e.g., endoscopy). Please see Figure ~ for a few samples. These pairs are sourced from four medical datasets including PMC-OA , Quilt-1M , MIMIC-CXR , and ROCO . These datasets contain a broad spectrum of textual content such as scholarly articles, clinical reports, and social media posts. We use the training splits of Quilt-1M, MIMIC-CXR, and ROCO for pretraining, while the test splits are later used for retrieval in downstream evaluation. The PMC-OA dataset is entirely used for pretraining. Please refer to Section  for more details on the pretraining data. 

 We perform an extensive evaluation of all models on a total of  retrieval, classification, and visual question-answering (VQA) tasks. Within our evaluation, we also explore models' generalization performances on two modalities including dermatology and ophthalmology that are not  present in pretraining data. Table  presents the datasets used for all evaluations. The image and text retrieval tasks encompass both image-to-text () and text-to-image () retrieval. To evaluate the () performance, we utilize test sets from the ROCO, Quilt, and MIMIC-CXR datasets. For each case, we report Recall@200 results. The image classification tasks include both zero-shot classification and linear probing across 22 tasks, including histopathology (9 tasks), radiology (8 tasks), dermatology (3 tasks), and ophthalmology (2 tasks). Visual question-answering is performed using the Mixture of Enhanced Visual Features (MEVF) method  without using a decoder. We use our trained vision and text encoders to encode the image and question, respectively. This task contain open- and close-ended questions and is mapped as a classification problem and performed on PathVQA , VQARAD  datasets.

Due to the extensive nature of this study, it is challenging to perform model selection for each contrastive learning variant. Therefore, we first perform hyperparameter tuning and model selection with vanilla contrastive learning. Hyperparameter tuning was mainly performed as a grid search on batch size and learning rate. More details are available in Section . We evaluate three image encoders, ResNet-50 , ViT-B/16, and ViT-B/32 transformers . For the text encoder we used the default GPT/77 encoder used by CLIP .  We pretrain each candidate on our pretraining data (2.8M pairs) and then evaluate its performance by measuring validation loss, retrieval Recall@200, and VQA accuracy. The validation loss is measured on the validation set of the PMC-OA dataset. The retrieval performance is measured for both image-to-text and text-to-image cases on ROCO, Quilt, MIMIC-CXR, and DeepEyeNet datasets. VQA accuracy is measured on PathVQA and VQARAD datasets. For VQA tasks we use a context length of 12 instead of 77 for the text encoder, similar to . Table~ presents the results for these encoders. Findings from this experiment suggest that ViT-B/16 is the best-performing encoder compared to other encoders in our study. Specifically, it achieves the best validation loss and outperforms the other encoders on 6 out of 8 retrieval and 1 out of 2 VQA tasks. Therefore, we consider ViT-B/16 as our default vision encoder and consider the performance of this model as the baseline for other experiments throughout the paper.

In general, we find a positive response to our first research question, and a negative response to the second research question. Finally, findings on the third question suggest that learning fine-grained representations can be beneficial over course-grained representation for medical domain.  Further details are provided below.

As summarized in Table , we investigate four contrastive learning approaches to examine the transferability of representations from the general domain to the medical domain. The retrieval and VQA results are presented in Table , while the classification results are shown in Figure .  We observe that in 12 tasks (6 in retrieval and 6 in classification), partially freezing the image encoder outperforms the baseline.  These findings suggest that transferability from the general domain is a viable route for representation learning in the medical domain. In particular, primitive features learned in the early layers of a vision transformer on a large, general domain dataset do not require much adaptation by further pretraining on medical data. Accordingly, a partial freeze of the image encoder trained on general domain data could lead to performance improvement in the medical domain.  The same phenomenon, however, is not observed when partially freezing the text encoder.  Only 6 fine-tuning tasks (5 in classification and 1 in VQA) benefit from partial freezing of the text encoders.  Finally, fully freezing either of the encoders hurts performance. This is not unexpected, as representations learned for general-domain data may not be fully transferrable to the medical domain. Yet, for full encoder freezing, 5 tasks (3 in classification and 2 in retrieval) benefit from this approach for text encoders, while 1 classification task showed improvement with full freezing of image encoders. 

The results of our experimentation on two contrastive learning approaches enabling joint unimodal and multimodal representation learning are presented in Table  (retrieval and VQA) and Figure  (classification). While neither of the variants in this study perform well across all tasks, masked CL does improve performance in the 2 VQA tasks, 2 classification tasks, and one retrieval task (). A few explanations are possible. For instance, a different trade-off in the loss combination could result in a better performance. It is also possible that optimizing the unimodal and multimodal loss functions requires very different feature sets, making the resulting representation space ineffective for our downstream tasks. Further investigation is required to better understand these results.

In this study, Fast CL and Image Captioning are the two contrastive learning methods aimed at encouraging learning of the coarse-grained and fine-grained representations, respectively. As shown in Table , fine-grained learning improved performance in 4 retrieval tasks and 2 VQA tasks. Additionally, Figure  demonstrates that learning fine-grained features enhanced performance in 4 classification tasks. On the other hand, Fast CL outperforms the baseline in 2 retrieval tasks and 4 classification tasks. These results suggest that learning fine-grained features could be useful in the medical domain. This observation is supported by the fact that local details are often important in medical diagnosis tasks and aligned with the findings of previous works showing the importance of learning fine-grained features .

Based on a holistic view of the results obtained from the experiments, we observe that in the medical domain, for the task of retrieval, Image Partial Freezing yields the overall highest results based on Tables ,  and . For both zero-shot learning and linear probing, the standard CLIP baseline shows the overall best results according to Figures  through . Finally, for VQA, according to Tables  and , Masked CL and Image Captioning can yield the best performances.