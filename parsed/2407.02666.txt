articlepreprintcorl_2024amsmathamssymbmathtoolsamsthmcapitalize,noabbrevcleverefplaintheoremTheoremsectionpropositiontheoremPropositionlemmatheoremLemmacorollarytheoremCorollarydefinitiondefinitiontheoremDefinitionassumptiontheoremAssumptionremarkremarktheoremRemarkhyperrefurlutf8inputencT1fontencscalefnt,letltxmacro\oldtextsc\textsc\textsc1\oldtextsccaptionsubcaptionmicrotypetextgreekamsfonts,amsmath,amssymbgraphicxbooktabs,tabularxnicefracgraphicsmultirowlipsumfont=footnotesizecaptionalgorithmalgorithmicadjustboxsoulxcolorpdfpageslistings   basicstyle=\ttfamily\small,  %   breaklines=true,             %   frame=single,                %   backgroundcolor=\color,  %   columns=fullflexible,        %   showspaces=false,            %   showstringspaces=false,      %   breakindent=1em,             % xspaceenumitemwrapfig\T\mathcal\D\mathcal\L\mathcal\mc\mathcalmygreenrgb0.1, 0.6, 0.1\new\textcolor\M\mathcal\S\mathcal\A\mathcal\did_\Dprior\mathcal_\todo1\ac1\lms1\edit1titlesec0pt4pt plus 2pt minus 2pt2pt plus 2pt minus 2pt0pt4pt plus 4pt minus 2pt2pt plus 2pt minus 2pt0pt3pt plus 4pt minus 2pt0pt plus 2pt minus 2pt\oursVLM-PC\xspaceBlue9rgb0.098,0.3,0.9DarkBluergb0,0.08,0.45hyperref   colorlinks,   urlcolor  = Blue9,   linkcolor = Blue9,   citecolor = Blue9, Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models%   Annie S. Chen \\   Stanford University \\   \And   Alec Lessing \\   Stanford University \\   \And   Andy Tang \\   Stanford University \\   \And   Govind Chada \\   Stanford University \\   \And   Laura Smith \\   UC Berkeley \\   \And   Sergey Levine \\   UC Berkeley \\   \And   Chelsea Finn \\   Stanford University

We consider generative VLMs, also known as multimodal language models, which take as input  including images  and prompt text  and outputs text  from a distribution over textual completions . We label each of the robot's prior behaviors  with a command , a textual description of the corresponding behavior. We also define levels of magnitude  for each that define the duration  that  should be executed. While there are many ways to acquire locomotion policies, e.g., via traditional model-based techniques or learning-based approaches, we use the built-in controller provided by the Go1 robot. To make these policies amenable to being used by a VLM, we choose to represent the policies as skills (as opposed to less interpretable, low-level actions such as joint angles or foot contact patterns) that we acquire by varying several parameters (x- and y-velocity in the robot frame, gait type, body height, yaw speed, and duration), the details of which can be found in Appendix~. For example,  could be ``Climb" or ``Crawl", and  is ``Small'', ``Medium'', or ``Large''. At timestep , the VLM is prompted to output high-level action , which leads to the robot executing low-level actions  for the number of seconds dictated by . Through prompt engineering, we ensure that the VLM outputs the skill and magnitude in a specified format that allows us to extract the high-level skill command for the robot to execute.

We propose a system, Vision-Language Model Predictive Control (), that uses a VLM to account for these errors and successively refine strategies, so that the robot can autonomously adjust from strategies that fail and try others. Summarized in Figure~,  combines two key insights to effectively enable VLMs to serve as an effective high-level policy:  (1) reasoning about information gathered by the robot in its environment and (2) selecting actions by planning ahead and iteratively replanning during execution. The VLM we use in all of our experiments is GPT-4o. We tuned the prompts to take into account the setting of legged locomotion and the limited view from the robot's camera. Full prompts and an example log of the VLM's chats are shown in Appendix~.

In large foundation models, techniques like chain-of-thought~, where the model is prompted to output intermediate reasoning steps, have been shown to significantly improve the model's ability to perform complex reasoning.  We aim to leverage such techniques to equip the VLM to better understand and reason through the environment and provide more effective high-level commands to the robot. In particular, we want the VLM to reason through the history in the environment and the progress made with the commanded skills before deciding on the next skill, in order to determine if the robot should try a new strategy. As such, we include as input to the VLM an image representing the robot's current view along with the full history of interactions (including the robot's previous images and the previous outputs of the VLM) and a prompt, i.e. the input at timestep  is ,  which contains for each previous query step , each previous image  and prompt  along with VLM output .  We then prompt the VLM to first reason through what progress the robot has made using the history of commands selected and the current position and orientation of the robot. 

Due to partial observability, there is often no clear answer as to which skill is most appropriate for a given situation and without physical experience, the VLM is not grounded in the robot's low-level capabilities (i.e. the VLM understands that the robot can crawl, but it does not know exactly how it crawls and whether this crawling skill will actually be successful in the current situation). So, we use an approach akin to model predictive control~, wherein we prompt the VLM to produce the immediate skill to execute by planning multiple steps , ..., , into the future and reasoning about the consequences of the actions.  This allows the VLM to foresee different possible strategies that might be applicable to the current situation, so it may better adjust in the future if the next chosen skill does not make progress. Then after the robot executes the skill corresponding to , the VLM repeats this planning for each step during deployment. To implement this, we specifically prompt the VLM to make a multi-step plan taking into account the latest visual observation , compare the new plan to the prior existing plan, and use the one that seems more applicable. 

We use a Go1 quadruped robot from Unitree. The robot is equipped with an Intel Realsense D435 camera mounted on its head, which provides an egocentric view of the environment, which is the only source of information the robot has about its surroundings. We configure the default controller to correspond to a set of prior behaviors: walking forward, crawling (at a low height), climbing (which can overcome stair-height obstacles), walking backward, turning left, and turning right. This same set of behaviors is used for all experiments, and details of the skills are in Appendix~.  In each setting, we report the average and median wall clock time in seconds (where lower is better) needed to complete the task along with the success rate across five trials for each method. If the robot does not complete the task within 100 seconds of executing actions, we consider it a failure. For each method in each setting, we report these metrics across five trials.

To evaluate each method, we conduct trials in five real-world indoor and outdoor settings.  The settings test the robot's ability to adapt to varying terrain conditions, requiring agile skills and dynamic strategy adjustments based on new information. The goal in each setting is to reach the ``red chew toy''. The robot only receives information from its camera and does not have access to a map of the environment. The tasks are shown in Figure~, annotated with the goal and an example path through the course, and described as follows:\\  The robot first must crawl under a couch, determine that it is a dead end, back up and turn to walk around the couch, climb a cushion it cannot pass without climbing, and finally locate the toy. \\  The robot first faces a couch that it must crawl under to the opposite side, then faces several stools blocking its path with a narrow gap between them, and must determine that it cannot fit through the gap and must turn and go around to locate the red chew toy. \\  The robot first faces bushes that it must turn from and go around, then faces a series of small logs that it must climb over, and finally locates the red chew toy. \\  The robot first faces a series of bamboo plants that it must turn from and go around, then a bench that it must crawl under, and then find the red chew toy.\\  The robot first faces a curb that it must climb over, a dirt hill that it must walk up, a wooden plank that it must climb over, and finally locate the red chew toy between the bushes.

We compare  to several variants that differ in the amount of context provided to the VLM and the amount the VLM is prompted to plan: (1) : The VLM is prompted with only the current image and the prompt, and is not provided with any history of interactions but is still prompted to output a multi-step plan of skills at each timestep. (2) : The VLM is prompted with the full history of interactions, including the robot's previous images and the previous outputs of the VLM, but is only prompted to plan a single skill at each timestep. (3) : The VLM is prompted with the full history of interactions, including the robot's previous images and the previous outputs of the VLM, and is prompted to make a multi-step plan of skills at each timestep. As a baseline, we additionally compare to (4) , which randomly selects a skill and magnitude to execute at each timestep.

As shown in Figure~, on average across all five settings,  successfully completes the task 64\% of the time, almost 30\% more than the second best method (No Multi-Step), which succeeds on average 36\% of the time.  is also on average over 20\% faster at completing the target task as the next best method, showing that including both history and multi-step planning are important for improving the use of these VLMs in providing high-level commands in a variety of settings. In Figure~, we find that particularly on Indoor 2, Outdoor 1, and Outdoor 2,  is more than twice as successful as the next best method. No Multi-Step is the second best method, and does comparably to  (which does multi-step planning) on Indoor 1 and Outdoor 3, indicating that in some situations, multi-step planning does not significantly help, although it does not hurt performance. No History fails in almost every setting except Outdoor 3, as it often gets stuck behind obstacles that require trying multiple different strategies. Random fails in every setting, showing that each setting requires nontrivial reasoning for the robot to succeed.  We provide examples of typical interactions with the VLM in Figure~ without history, without multi-step planning, compared to , and we find that with ours, the VLM is able to both reason through effectiveness of prior strategies and plan ahead to try coherent new strategies to overcome the current obstacle.

As large foundation models trained on Internet-scale data are used as these high level planners, they can leverage in-context learning, where examples or instructions are included as context in the input to the model~. We provide an extension of our method including in-context examples, called VLM-PC+IC, where we include in the first prompt several additional images, taken from the egocentric view at different points in the environment, as well as a label for each of them with the best command to take. This provides the VLM with more context about the environment and the best strategies to take at key points. As shown in Table~, we find that in two of the obstacle courses, this can significantly improve performance.  While inexpensive to obtain, this does require human labeling of several images from the deployment environment with the best command to take, which may not be feasible in all deployment settings. Nonetheless, this extension further reinforces the importance of providing useful context to the VLM and having it use this context to make informed decisions, and shows that these labeled examples can be useful context on top of the history of experiences in the environment.

We obtain different behaviors from the default controller by modulating the parameters passed in. Specifically, we control x- and y-velocity in the robot frame, gait type, body height, yaw speed, and duration to achieve different skills. The parameters used for each skill are described in the tables below, along with the duration corresponding to each magnitude. After the action is done executing, the robot will stay frozen in the position it was left in at the end of the last action, e.g. if the last action was to crawl, the robot will stay low to the ground. We additionally provide our GPT-4o query hyperparameters. 

In the following, we include the prompts used for , where the text highlighted in grey indicates text that is used for all comparison methods (including No History and No Multi-Step Plan). The text in green corresponds to the prompting for reasoning over history and is included in  and the No Multi-Step Plan prompts. The text highlighted in blue corresponds to prompting for multi-step planning, which is included in  and No History. The text in yellow corresponds to reasoning over the historical multi-step plan and is included only in the full  prompt. When included, the ICL prompt went immediately after the first paragraph of the initial prompt and consisted of a short explanation followed by example egocentric views and one or two actions the robot might take when it each view. For  and No Multi-Step Plan methods, the ``Initial Prompt'' below was given at the start and repeated after every six responses. Otherwise the ``Successive Prompt'' was given in all queries after the first. Note that we used GPT-4o as the VLM for all of our experiments, and additional prompt tuning may be necessary for other VLMs. Anecdotally, we tried using the Gemini Flash model and found that it did not reason as effectively with these prompts.

In the pages afterward, we display a full example log with  on the Outdoor 2 obstacle course, where the prompts and input images are included in blue and the output of the VLM at each step is included in green. See our anonymous website for videos of our results.

Equal Contribution. Correspondence to Annie Chen (\href). Videos of our results can be found on our website: \url.     Legged robots are physically capable of navigating a diverse variety of environments and overcoming a wide range of obstructions. For example, in a search and rescue mission, a legged robot could climb over debris, crawl through gaps, and navigate out of dead ends. However, the robot's controller needs to respond intelligently to such varied obstacles, and this requires handling unexpected and unusual scenarios successfully. This presents an open challenge to current learning methods, which often struggle with generalization to the long tail of unexpected situations without heavy human supervision. To address this issue, we      investigate how to leverage the broad knowledge about the structure of the world and commonsense reasoning capabilities of vision-language models (VLMs) to aid legged robots in handling difficult, ambiguous     situations. We propose a system, VLM-Predictive Control (VLM-PC),      combining two key components that we find to be crucial     for eliciting on-the-fly, adaptive behavior selection with VLMs:      (1) in-context adaptation over previous robot interactions and (2) planning multiple skills into the future and replanning.     We evaluate VLM-PC on several challenging real-world obstacle courses, involving dead ends and climbing and crawling, on a Go1 quadruped robot. Our experiments show that by reasoning over the history of interactions and future plans,     VLMs enable the robot to autonomously perceive, navigate, and act in a wide range of complex     scenarios that would otherwise require environment-specific engineering or human guidance.  vision-language models, on-the-fly adaptation, legged locomotionIntroductionsec:introkuindersma2016optimization,hwangbo2019learningwidth=1.0\textwidthfigs/teaser6.png-2mm         \small          \textbf           By leveraging the commonsense reasoning abilities of pre-trained VLMs to adaptively select behaviors, \ours allows legged robots to quickly adjust strategies when encountering a wide range of situations, even backtracking when appropriate. \textbf An example trajectory of the robot tasked with finding the red chew toy amid obstacles using \ours--it first crawls under a couch, then backs out of it when it finds it is a dead end, turns to walk around the couch, climbs over a sizeable cushion, and finally locates the toy. \textbf An overhead view of the trajectory with \ours. \textbf An example trajectory of the robot's behavior using a VLM naively, where the robot gets stuck and cannot adapt. \textbf Visualization of the robot's egocentric POV that is provided to the VLM at different points along the trial along with excerpts of reasoning with \ours at those points.     -7mmfig:teaserahn2022can,huang2022language,driess2023palmwei2022chain,kojima2022largegarcia1989model,morari1999model,finn2017deep,ebert2018visualfig:teaserRelated Worksec:relateddai2014whole,kuindersma2016optimization,Hutter2016ANYmalA,Park2017BoundingCheetah,Bellicoso2018DynamicLT,Bledt2018MITC3,Katz2019MiniCAhaarnoja2018learning,tan2018sim,Yang2019DataER,yu2019sim,lee2020learning,yang2021learning,agarwal2023legged,peng2020learning,rudin2022advanced,smith2022legged,caluwaerts2023barkour,he2024agile,zhuang2023robot,cheng2023extremecutler2014reinforcement,rajeswaran2016epopt,sadeghi2016cad2rl,tobin2017domain,peng2018sim,tan2018sim,yu2019sim,akkaya2019solving,xie2021dynamics,margolis2022rapid,haarnoja2023learningmargolis2023walkyu2019sim,peng2020learning,yu2020learninglee2020learning,kumar2021rma,fu2023deepmiki2022learning,agarwal2023legged,zhuang2023robot,yang2023neuralchen2023adaptbacon2017option,peng2019mcp,lee2019learning,sharma2020learning,strudel2020learning,nachum2018data,chitnis2020efficient,pertsch2021guided,dalal2021accelerating,nasiriany2022augmentingahn2022can,huang2022language,huang2022inner, macmahon2006walk,driess2023palm,misra2016tell,stepputtis2020language,kollar2010towardliang2023code,singh2023progpromptyu2023language,sha2023languagempc,mirchandani2023large,arenas2023prompthuang2022innerzha2023distilling, liang2024learninghuang2023voxposer, nasiriany2024pivot, belkhale2024rtshah2023navigation,shah2023lmtang2023saytapouyang2024longProblem Statementsec:settinglee2020learning, margolis2023walk, cheng2023extremechen2022youVision-Language Model Predictive Control (\ours)sec:methodwidth=0.99\textwidthfigs/method5.png         \small          \textbf          Our method uses a pre-trained VLM to provide high-level skill commands for a legged robot to execute.          Given the robot's current view and history of interactions, the VLM is first prompted to reason through the robot's current state and progress with the history of commanded skills, and is then prompted to make a new multi-step plan, compare it to the prior plan, and adjust if needed. The robot executes the first skill in the plan, and the VLM is queried again.      fig:methodInterfacing Robotic Locomotion Skills with VLMssec:app-skillsUsing VLMs for Adaptive Behavior Selectionfig:methodsec:app-logsUsing in-context reasoning to adapt on-the-fly.wei2022chain,kojima2022large,zeng2022socraticMulti-step planning and execution.garcia1989model,morari1999model,finn2017deep,ebert2018visualExperimental Resultssec:expsExperimental Setupsec:app-skillswidth=1.04\textwidthfigs/envs_final.png         \small          \textbf We evaluate \ours on five challenging real-world settings, each of which presents unseen obstacles designed for the robot to get stuck, and requires commonsense reasoning to solve. For each setting, we give a third-person view of the obstacle course as well as an example path through the course, with three different egocentric views (labeled 1, 2, 3) at different points to show the diversity of scenes the robot encounters from its viewpoint.     -4mmfig:settingsEvaluation Settings.fig:settings\textbf\textbf\textbf\textbf\textbfComparisons.No HistoryNo Multi-Step\oursRandomMain Resultswidth=1.02\textwidthfigs/avg_results2.png         \small          \textbf           Across all five settings on average, \ours significantly outperforms Random, No History, and No Multi-Step on average and median time to complete the task and success rate, performing roughly 30\% more successfully than the next best method.      -4mmfig:avg-resultsfig:avg-resultsfig:settingsfig:qual-analysiswidth=1.01\textwidthfigs/qual_analysis3.png         \small          \textbf With \ours (Right), the VLM can both analyze the efficacy of previous commands and prepare new, coherent plans to tackle the current obstacle, by combining benefits from multi-step planning from No History (Left) and reasoning over history from No Multi-Step (Center).     fig:qual-analysisAdding Labeled In-Context Examples Can Improve Performance\small \textbf We find that in two of the obstacle courses, leveraging the VLM's in-context learning capabilities by providing additional images labeled with the best command can significantly improve performance.tab:iclmin2022rethinking,dong2022surveytab:iclDiscussion and Limitationssec:conclusionWe thank Zipeng Fu and Yunhao Cao for help with hardware, and Jonathan Yang, Anikait Singh, and other members of the IRIS lab for helpful discussions on this project. This work was supported by an NSF CAREER award, ONR grant N00014-21-1-2685 and N00014-22-1-2621, Volkswagen, and the AI Institute, and an NSF graduate research fellowship.exampleAppendixsec:appSkill Details and Hyperparameterssec:app-skillsPrompts and Logssec:app-logspages=-, width=1.3\textwidthprompts.pdfpages=-bamboo_chat.pdf