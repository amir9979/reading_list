This step aims to aggregate the information of each time step in a time series.  is the input multivariant time series, and  is the injection matrix.  is the -th row of  and  is the -th column of .

We learn  and  in a similar way as attention. After this operation, we send  and  to the next nonlinear element-wise mixing step for information exchange.

 In this step, the product of element pairs allows for a thorough computation on the features of . To consider the global correlation between element pairs, the output of information exchange undergoes a Softmax layer to weight the importance of all . However, traditional dot product operation considers all time series of , and the combination with Softmax leads to redundant computation on element pairs. To address this issue, we generate a series of convolutional kernels whose size is equal to  as follows

based on which we can further derive  % \( \sum W_jX_i \times \sum W_jX_i \rightarrow \sum_k \sum_m W_{j,k} W_{j,m} X_{i,k}X_{i,m}\). For the attention mechanism, its computation of time series is equal to our model but greater on channel aspect (different channel of time series) in element view (more details will presented in Appendix ):

Then, through the Softmax function, we gain the weighted kernel (score map) as follows,

Finally, we add an embedding to the kernel as a random shift operation similar to  Attention-Free Transformer ,

The embedding is initialized in random and it is similar to an attention weights shift as in . %  In this step, the convolutional kernels operate on the corresponding elements in . To alter the receptive field on , we introduce a structured transposition matrix. This matrix aggregates the elements within a special range, which are then convolved by the kernels as follows.% % % %     & W^M = \sum_i W^{M_{i}},\\%     & s.t. W^{M_{i}} = \alpha_i Mask_i(W)\\%     & M_{i,j} = X_iW^M_j = X_i[t_\tau]W^M_j[t_\tau]\\ % % % where  is a series of time indexes.  is a learnable weight to focus on important factors.

This module uses a self-attention kernel named pectralime indow lock (S2TWB) to convolve the time series based on the convolution theorem in Eq.  as follows

where  is a convolution operation and  is a multiply operation.  is the Fast Fourier Transform. We aim to generate a series of kernels as  and aggregate them together to generate the kernel   as follows

where  is the time series length and  ( to ) are basis operators with learnable parameters . Here we use  function to present the basic operator, and we have

Then the kernel  of S2TWB can be reformulated as follows

where . Finally, we apply FFT on kernel  and  as follows, Next, we will demonstrate how to use FFT and kernel  to adaptively change the receptive field. FFT is a convolutional method for time series as mentioned in section . As shown in Figure , S2TWB uses FFT to project kernel and time series  in the time domain to the spectral domain and multiply them, after which the kernel generated in the time domain convolves time series like the CNN kernel. Specifically, the kernel  is combined linearly with several waves as shown in the left side of Figure . Then the kernel allocates different attention weights to different positions of a time series based on the distance between itself and the target position in the time domain as shown in the left part of Figure . Convolution via FFT only relies on the relative position but not on the absolute position, which is more flexible.

The overall mathematical presentation of the denoising function can be written as follows

where  is the output of the representation,  is the input and  is the structured receptive field. Then we learn a convolution operation as follows

which presents the  convolves the .

% % % % [1]% \REQUIRE Incomplete observe data , % \ENSURE  noise prediction model --GCS% %     \STATE obtain Q, K through~~ equation ;%     \STATE obtain spectral window through equation ~~ ;%     \STATE obtain global kernel by ~ equation ; %     \STATE obtain M by ~ equation , equation ; %     \STATE obtain output by ~ equation ;% \ENDFOR% %  The convolutional kernel  projects onto the time series data , essentially performing a dot product between two vectors. In Score-CDM, the transformation matrix  projects a portion of the vector  onto unit elements, and the result is multiplied by a convolutional kernel with a size of . This is equivalent to convolving  with a kernel of size .

For the attention mechanism, after time-mixing and element-mixing, the temporal values on the nodes are thoroughly blended, resulting in . At this point, applying Softmax yields attention weights that span the entire time series.

We compare Score-CDM with four classic models including Attention Free Transformer, Transformer, TCN, and Dlinear (MLP-based Model) from four aspects, whether can be coded as attention or convolution, the computational metrics (element-wise or dot-product), and through full or structured transformation. Through the comparison, one can see that Score-CDM can be considered as a convolution model with a global attention score map, and its receptive field is flexible. For a clearer comparison among these methods, one can refer to the Appendix.

  As shown in Appendix Table 1, we compare our method with five transformer models including Attention Free Transformer, Transformer, Performer , Linear Transformer , and Reformer  in terms of time complexity and space complexity. The comparison shows the efficiency of Score-CDM whose time complexity is the same as Linear Transformer and space complexity is the same  AFT. %  %{}% In total, we describe the training process of DiffGCS in Algorithm~. We design a similar training process and backward process as PriSTI .  In the training process, we train a map from diffusion step  to noise . In other words, our model learns to predict noise intensity in each diffusion step. We finally get a noise estimation function  to denoise data step by step. To capture intervariate correlation, we additionally add an attention mechanism into  to directly compute the data in variate dimensions. Its input is  and its output is the predicted noise.

% % 	% 	% 	 {\bf Input:} A sample of incomplete observed data , the adjacency matrix , the number of diffusion steps , the optimized noise prediction model .\\% 	 {\bf Output:}{ Missing values of the imputation target .}  % 	% \BlankLine%     [1]%     % \State Utilize linear interpolation on the sample of observed data  to obtain the interpolated conditional information ;%     \STATE ;%     \STATE Set ;%  where the shape of  is same to the missing part in ;%      }%     \STATE %     \STATE %     \EndFor%     % 

When using the trained noise prediction model  for imputation, we aim to impute the incomplete MTS data , and the interpolated conditional information  is constructed based on all observed values.   The model receives  and  as inputs and generates samples of the imputation results through the reverse process in Eq. ().  %More details on the imputation process of our proposed framework are shown in the appendix. We evaluate the performance of our model on three real-world datasets METR-LA, AQI-36, and PEMS-BAY that are widely adopted for MTS imputation in previous works. METR-LA and PEMS-BAY are the traffic flow datasets collected from traffic sensors in Los Angeles County Highway and Bay Areas in California. AQI-36 is collected from 36 AQI sensors distributed across the city of Beijing. The detailed dataset statistics are given in the appendix. %We use the following three datasets for evaluation: METR-LA, AQI-36, and PEMS-BAY. We will introduce them in the appendix.%  is a dataset used in traffic flow prediction and imputation. It contains 207 traffic sensor nodes in Los Angeles County Highway with minute-level sampling rate.%  is collected from 36 AQI sensors distributed across the city of Beijing. This dataset serves as a widely recognized benchmark for imputation techniques and includes a mask used for evaluation that simulates the distribution of actual missing data. % % For a specific month, such as January, this mask replicates the patterns of missing values from the preceding month. Across all scenarios, the valid observations that have been masked out are employed as targets for evaluation. We derive a spatial matrix from the spatial distribution for further analysis.%  is an open dataset used for traffic flow prediction and analysis, primarily covering the transportation network of the Bay Area in California, USA. The dataset comprises 325 sensor nodes with a sampling interval of 5 minutes, and it contains a total of 16,937,700 data points.% For meaningful experiment comparison, we randomly mask each dataset with the ratio of  and .%In total, each dataset will be artificially masked with 25\% or 50\% values in random. % {Revise: For the two datasets METR-LA and PEMS-BAY, we partition the entire data into training, validation, and testing sets by a ratio of . The preprocessing method for the dataset is non-repetitive sampling.} For AQI-36, we select Mar., Jun., Sep., and Dec. as the test set, the last 10\% of the data in Feb., May, Aug., and Nov. as the validation set, and the remaining data as the training set. % It is worth noting that since TRMF cannot impute on samples other than the training set, its training set is all the observations. % The time window length  for AQI-36 and  for MELR-LA and PEMS-BAY.% The software and hardware environments of our experiments is as follows: %AMD EPYC 7371 CPU, NVIDIA RTX A5000, Python 3.6.13, Pytorch 1.7.0.% The linear interpolation is implemented by torchcde, and spatial attention with virtual node downsampling is implement based on Linformer. % The code to reproduce the experiments on AQI-36 is provided in the supplementary meterial.% For the dataset AQI-36, we adopt the same evaluation strategy as the previous work . For the traffic datasets METR-LA and PEMS-BAY, more details will show in Appendix. 

% we artificially inject some missing values by following  to construct the imcomplete data. We evaluate the model on two data missing scenarios, block missing and point missing. In the block missing scenario, we first randomly mask 5\% of the time series data, and then for each sensor we mask its data ranging from 1 to 4 hours with a probability  as in  to mimic sensor failure. For the point missing case, we randomly mask  of all the time series observations. %In sensor failure secenary , which is denoted as S-Failure, is setted followup the previous work--PriSTI .% The missing rate of each dataset under different missing patterns has been marked in Table . All evaluations are performed only on the manually masked parts of the test set.%  We compare Score-CDM with the following baselines. % ,  ,  ,  ,  ,  ,  , and  .

To study whether each module of Score-CDM is useful, we also compare Score-CDM with the following two variants.  In the experiment, the baselines Transformer, SAITS, BRITS, GRIN, and SPIN are implemented by the code provided by the work . 

 We apply Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) defined as follows to evaluate the model performance.

where  is the imputed time series at time  and  is the corresponding ground truth.

% % % 

We first compare the performance of different methods in the point missing scenario with the data missing rate  setting to  and , respectively. The experiment result is shown in Table . The best result is highlighted in bold font, and the second-best result is underlined. From Table , one can observe that Score-CDM consistently outperforms all the baseline methods in both cases and over all the datasets. Specifically, Score-CDM improves the performance of the best baseline PriSTI by - in terms of MAE on AQI-36, by - on PEMS-BAY dataset, and by - on METR-LA dataset. This demonstrates that Score-CDM can effectively balance the local and the global temporal information of time series. Compared with attention-based diffusion models CSDI and PriSTI, Score-CDM performs better in all the datasets, verifying the effectiveness of the extracted score map by SCM and the self-attention kernel of S2TWB to construct the flexible receptive field. Compared with RNN-based methods BRITS, the performance improvement of Score-CDM is much more significant. For example, the RMSE of BRITS on AQI-36 is 28.76 when , while the RMSE of Score-CDM is dropped to only 12.14. GRIN is also an RNN model, but its performance is much better than BRITS by incorporating GNN models. One can see that SPIN performs best among all the attention-based methods, indicating that integration of both temporal and spatial information can significantly enhance model performance for MTS imputation. However, the performance of SPIN is still inferior to PriSTI, which suggests that diffusion models is truly powerful in MTS imputation due to their strong generative capability. TimesNet's performance is moderate among all the methods. This is because its receptive field is smaller than attention methods, and thus is less effective to capture long-term temporal features in time series data.

For the block missing scenario, we set the sensor missing probability  to mimic that  sensors fail in 1 to 4 hours without any time series data observations. We compare Score-CDM with six strong baselines. The result is shown in Table . It shows that Score-CDM still outperforms the baseline methods on the three datasets, demonstrating its superior performance in the block data missing scenario. PriSTI achieves the best performance among all the baselines, but it is still inferior to Score-CDM. For example, Score-CDM outperforms PriSTI by more than 3\% in terms of MAE on METR-LA  AQI-36, and by more than 12\% on PEMS-BAY. To further evaluate the performance of different methods under very high point data missing percentages, we compare Score-CDM against the baselines when  and . The result is shown in Table . It demonstrates again that Score-CDM outperforms all the baselines when the available time series observations are very sparse. 

% BRITS perform not very well among all baseline models due to the limitations of the MLP model in effectively capturing spatial correlations and higher-order relationships between nodes, resulting in inadequate spatial encoding. SAITS exhibits slightly better performance, indicating that Diagonally-Masked Self-Attention (DMSA) can alleviate the impact of self-redundant information and enhance temporal prediction performance.% GRIN, as an advanced autoregressive multivariate time series imputation method, outperforms other RNN-based models like BRITS by extracting spatial correlations. However, compared to diffusion model-based methods (PriSTI, CSDI), GRIN still lags behind, possibly due to the inherent issue of error accumulation in autoregressive models.% SPIN outperforms other models (except diffusion models), highlighting the effectiveness of the improved Hybrid Spatiotemporal Attention mechanism in enhancing the performance of the original Transformer for spatiotemporal sequences. TimesNet shows slightly lower results than SPIN, indicating that although it can adaptively organize data for temporal convolution, this convolution approach remains local, limiting the ability to learn global spatiotemporal dependencies.% The performance improvement of CSDI over SPIN on AQI-36 suggests that using the Transformer as a denoising neural network in the diffusion model can effectively enhance its performance, especially with a specific design. However, CSDI performs worse than SPIN but PriSTI performs better showing that the traditional diffusion model for imputation has not used spatial conditions adequately. Furthermore, PriSTI exhibits additional improvement over CSDI but worse than CDM-f, indicating that incorporating spatial information promotes better representation learning in the model but it is still limited to the properties of its denoising function.%, which is widely used for measuring the performance of the probabilistic imputation methods.% The evaluation metrics for deterministic imputation are Mean Absolute Error (MAE) and Mean Squared Error (MSE), both of which reflect the absolute error between the imputation value and the ground truth.% The evaluation metric of probability imputation is CRPS , which evaluates the compatibility of the estimated probability distribution with the observed value. % We introduce the calculation details of CRPS as follows.% For a missing value  whose estimated probability distribution is , CRPS measures the compatibility of  and , which can be defined as the integral of the quantile loss :% % %     (D^{-1},x) & =\int^1_0 2\Lambda_{\alpha}(D^{-1}(\alpha), x)d\alpha,\\%     \Lambda_{\alpha}(D^{-1}(\alpha), x) & =(\alpha-_{x<D^{-1}(\alpha)})(x-D^{-1}(\alpha)),% % % where  is the quantile levels,  is the -quantile of distribution ,  is the indicator function.% Since our distribution of missing values is approximated by generating 100 samples, we compute quantile losses for discretized quantile levels with 0.05 ticks following  as:% %     (D^{-1},x) \simeq \sum_{i=1}^{19}2\Lambda_{i\times 0.05}(D^{-1}(i\times 0.05), x)/19.% % We compute CRPS for each estimated missing value and use the average as the evaluation metric, which is formalized as:% %     (D, )=\in}(D^{-1},)}{||}.% % [h]% {1.3}% % 行间距放大为原来的1.5倍% \small% % Please add the following required packages to your document preamble:% % % % \usepackage[normalem]{ulem}% % {\ul}{}% % Please add the following required packages to your document preamble:% % % % \usepackage[normalem]{ulem}% % {\ul}{}% % Please add the following required packages to your document preamble:% % % % \usepackage[normalem]{ulem}% % {\ul}{}% % Please add the following required packages to your document preamble:% % % % \usepackage[normalem]{ulem}% % {\ul}{}% {cll|cccc|cccc|cccc}% \hline% Figure 3 shows the distribution of RMSE and MAE bars for the three variants compared to SaSDim. It is evident that SaSDim-GConv performs significantly worse than the other variants by 25\% on 25\% data missing scenario, because it does not consider the geographical correlations between locations. The Constraint module is used to modulate the adjacency relationships between nodes. SaSDim-noConst(noC), which does not use spatial constraint conditions, achieves 12\% higher RMSE error than SaSDim on 25\% missing scenario and 90\% higher RMSE error on 50\% missing scenario. It confirms that spatial constraint conditions are helpful in aggregating the neighboring information. SaSDim-lowO directly applies first-order derivatives in the denoising process. Although SaSDim-lowO is superior to other variants, it still performs worse than SaSDim, significantly lagging behind SaSDim with a 15\% higher RMSE and 20\% higher MAE in the 25\% data missing scenario. This indicates that SaSDim can learn better representations through scaling down noise for denoising. This result verifies that the Across Spatial-temporal Global Convolution module and the Probabilistic High-Order SDE Solver module are both indispensable for improving the model performance.To examine whether the designed two modules SCM and S2TWB work, we conduct the ablation study to compare Score-CDM with its two variant models w/o and w/o. Figure  shows the result. One can see both modules are useful to the model, as removing each one of them will lead to remarkably performance drop in all four cases. One can also see that SCM has a larger impact on the model performance compared with S2TWB, because removing it leads to a more significant performance decline. This implies that the global temporal features are critical to MTS imputation and the proposed SCM can effectively capture the global features. When 25\% point data are missing on the PEMS-BAY dataset, the performance of w/o drops by over 5\% compared with Score-CDM in terms of MAE, and the performance drop is up to 8.7\% when the 50\% data are missing, which verifies S2TWB is also important to the performance improvement. This indicates a pronounced periodicity of the traffic flow time series in the PEMS-BAY dataset, characterized by a prevalence of local temporal features. The proposed S2TWB in Score-CDM can effectively capture this periodicity by extracting the corresponding frequencies in the spectral domain. %A similar improvement of around 5\% is observed in METR-LA, validating the similarity in spatiotemporal cycles and trend patterns between PEMS-BAY and METR-LA. Lastly, the minimal enhancement observed in the AQI-36 dataset suggests a more gradual variation in air quality indicators over the short term, indicating a certain stability within seasons. In practice, our processing of AQI-36 data differs from the aforementioned traffic datasets, avoiding biases introduced by climatic variations between seasons. This ensures that the model learns stable representations. To further show the effectiveness of Score-CDM, we give a case study to visualize the learned score map in Figure . To make a comparison, we also present the score map learned by Diff-former. Diff-former applies the attention mechanism as the denoising function of the diffusion model to learn the score map and extract time features. We select a traffic flow time series whose length is 24 and with two channels from METR-LA. The darker color in the figure represents a higher score, while the light color represents a smaller score. It shows that Score-CDM can better capture both local and global temporal features compared with Diff-former as the high attention scores are distributed over different locations on the score map, while the high score of Diff-former only located at one or two areas in its score map.  %When color is deeper and each line is closer, frequency is more major which represents the importance of it and the width of its receptive field. The left side in Figure  shows that the raw data has a few global frequencies and the most of frequencies are local and non-major, which demonstrates the time representation ability of Score-CDM. The right side of this figure shows that attention-based models are intended to over-focus on a few positions that merge all ranges of time. Overall, Score-CDM not only achieves good performances but imputes well intuitively. 

In Figure , we give a case study to show the traffic flow time series imputation results of 5 road sensors by Score-CDM in METR-LA dataset. Each subfigure represents the imputation result of a sensor, and the time windows of all sensors are in one day (24 hours). The red crosses represent observations, and dots of various colors represent the ground truth of the missing values. The solid green line is the imputation result by Score-CDM, and the green shadow represents the quantile between 0.05 to 0.95. One can see that the imputed values denoted by the green curves are very close to the ground truth missing time series points and the observations, which demonstrates the desirable time series data imputation performance of Score-CDM.