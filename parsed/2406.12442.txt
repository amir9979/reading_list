The enlargement of the language model scale brings about emergent abilities including in-context learning~. To utilize LLMs for reasoning tasks,  propose CoT prompting, which extends in-context learning with step-by-step reasoning processes to elicit reasoning in LLMs.  Subsequently, a substantial number of works based on CoT are proposed to further enhance LLMs' reasoning performance~.  For instance,  and  explore using codes to express the reasoning processes. Most of these existing approaches focus on the content of the prompt (e.g., question decomposition~) and the external usage of CoT prompt (e.g., tree searching~). We focus on the format and internal structure of CoT prompts and propose the Abstraction-of-Thought format, which is complementary to existing approaches. Our approach can potentially be combined with existing methods for further improvement.

Although LLMs equipped with CoT prompts can achieve advanced reasoning capabilities, there is still a significant gap between open-source smaller-scale models and large models.  To bridge this gap, a promising and popular approach is finetuning language models to learn reasoning~.  A series of studies have found that finetuning models on data containing CoT reasoning processes could enhance the reasoning ability~.  The predominant way is to train models on instruction tuning datasets and their enhanced versions.  For instance, the instruction tuning dataset FLAN collection~ includes CoT data on a small subset of tasks to improve the model's performance under CoT prompts.  CoT Collection~ further supplements the remaining tasks in FLAN with CoT reasoning processes, covering 1.84 million instances.  Orca~ enhances FLAN by prompting LLM with task-specific prompts, thereby training smaller models for cautious reasoning. In this paper, we follow previous work to collect data based on FLAN for fair comparison. We design the methodology for gathering AoT reasoning process and collect  to facilitate better training of models for reasoning.

Previous works study different aspects of reasoning with abstraction, including entity abstraction~, event abstraction~, spatial-temporal abstraction~, and conceptualization abstraction~. Recently, ~ proposes to use abstract placeholders in the reasoning chain and call domain tools to supplement specific knowledge, thus allowing the model to use the tools effectively. In this paper, we explore how to improve the basic abstract reasoning of language models from the perspective of reasoning formats and training data.

To align LMs to the AoT reasoning format, we collect the  for finetuning LMs.  comprises reasoning processes in AoT format, characterized by the following features: (1)  incorporates reasoning processes expressed in both natural language and programming language.  Inspired by previous work~, solutions to some complex problems are more appropriately to be expressed with programming language. This approach not only facilitates the use of external modules as tools to improve reasoning accuracy~, but also enhances the faithfulness of the reasoning process~.  Models trained on such hybrid reasoning strategies can flexibly choose the suitable reasoning approach based on the type of test question, thereby handling more complex reasoning problems. (2)  is built upon an instruction-tuning dataset that covers a wide range of tasks.  This allows the  to encompass questions from various tasks, rather than being confined to a specific domain or task. (3)  is collected through an automated pipeline, which allows it to scale up without relying on the efforts of human annotation.

We follow previous work~ to use FLAN Collection~ as our source dataset. FLAN Collection includes diverse NLP datasets sourced from P3~, Super-NaturalInstructions~, Flan~, and additional dialogue and code datasets. We follow~ to exclude datasets whose data are not publicly accessible and datasets with an excessive number of tokens. Ultimately, we focus on 216 datasets that are consistent with the CoT Collection~. We manually divide the 216 datasets into two parts,  (203 datasets that are more suitable to be solved in natural language) and  (13 datasets that are more suitable to be solved in programming language). We utilize a proportional stratified sampling method to sample 400k instances from the original data for the subsequent AoT response generation. Details of the dataset division and sampling can be found in Appendix~.

Manually annotating the AoT reasoning process for all instances is labor-intensive and time-consuming.  We adopt a method of synthesizing data with LLMs~ to automate the process of collecting AoT responses.  We first manually create instructions and 3 demonstrations, to exploit the instruction-following and in-context learning capabilities of LLM for generating AoT responses (details can be found in Appendix~).  We designed two types of prompts for the datasets in  and , respectively. To minimize the difficulty of generation, we only consider 2 levels of abstraction in this paper. The correct answer to the question is included in the prompt to help the model focus on the generation of the reasoning process. We use GPT-3.5-Turbo as our back-end LLM and generate with greedy decoding. Since we are more concerned about the impact of the reasoning format on the model's reasoning ability, we do not meticulously design different demonstrations for each dataset like previous work~.  We use Python as the programming language.

After generating the AoT response, we perform validation and filtering to ensure high quality. For AoT in natural language, we examine whether the answers predicted in the response are consistent with gold answers. To prevent degeneration where different inference steps describe the same content, we stipulate that there should not be excessive similarity between different steps. Specifically, we calculate the Jaccard similarity of words between different steps and require it to be below a threshold of 0.5. For AoT in the programming language format, we execute the code provided in the response and check whether it correctly prints or returns the gold answer. For instances that fail to meet the requirements, we ask the LLM to regenerate 10 times (with a temperature parameter of 0.7). We retain the first response that meets the requirement. If none of the 10 responses meet the requirement, we discard the instance. After the filtering process, we retain 348k instances. We randomly sampled 100 examples (50 in natural language and 50 in programming languages) and manually checked the quality of the AoT responses. We find that 96\% of AoT responses are valid. Table~ reports the statistics and Appendix~ lists some samples of .

Table~ presents the zero-shot BBH performance of LMs finetuned in different ways. The AoT-finetuned models demonstrate a remarkable performance enhancement compared to the baselines. For example, in comparison to Llama-3-8B-CoT, Llama-3-8B-AoT achieves an absolute improvement of +9.7\%. This trend is consistent across all models studied, which suggests that aligning models with the AoT reasoning format could enhance their reasoning ability more effectively than aligning them to the CoT. AoT guides models to think from a more abstract perspective first, addressing the essence of the problem at a higher level, which potentially enhances their reasoning ability.

It is worth noting that the AoT-finetuned models displayed more substantial improvements in algorithmic tasks than in NLP tasks. The algorithmic tasks, which require capturing the internal reasoning rules of the questions without relying on external knowledge, pose a greater challenge to the model's reasoning ability~. As a result, the performance on algorithmic tasks is usually lower. In contrast, NLP tasks depend not only on reasoning ability but also on external knowledge, where LMs could face the bottleneck in external knowledge. AoT-finetuning guides the model to carry out reasoning with abstraction, with a focus on strengthening the reasoning ability, thus yielding a more noticeable improvement in algorithmic tasks. We further discuss the performance of subtasks (), case study (), and computational cost comparison () in the Appendix.

We also evaluate the effect of AoT under the few-shot setting, the standard setting proposed by~. For few-shot CoT demonstrations, we use three questions and their CoT rationales which are provided by the official repository. For the AoT prompt, we employ the same questions and manually create the AoT rationales.  Consistent with our , we use the Python program to express the reasoning process for some tasks. Prompts can be found in Appendix~.

Table~ shows the results.  For models that have not been finetuned, using prompts in AoT format achieves remarkable performance improvement compared to those in the CoT format. For example, the pre-trained Llama-3-8B achieves an absolute improvement of 15.1\% with the AoT prompting. This suggests that AoT could be more effective in stimulating the reasoning ability of pre-trained LMs. Furthermore, by aligning the pre-trained models to AoT with the , the models demonstrate improved performance under AoT prompts. For instance, Mistral-7B-AoT achieves an absolute improvement of 6.7\% over the vanilla Mistral-7B, validating the effectiveness of our . Further discussions including the few-shot performance of instruction-finetuned models are in Appendix~.

In preceding experiments, we utilize the CoT Collection~ for the CoT-finetuning.  CoT Collection differs from our  in two confounding factors, besides the reasoning format:  (1) A different LLM is used during data collection;  (2)  additionally employs the hybrid reasoning strategy, representing reasoning both in text and code.  To verify the role of the AoT format, we conduct an ablation study to attempt to eliminate the influence of these confounding factors.  We construct a new training dataset, .  The  uses the same LLM (i.e., GPT-3.5-Turbo) as the  to collect data, ensuring the reasoning capability of the back-end LLM is consistent.  Moreover,  also adopts the hybrid reasoning strategy.  Specifically, we prompt the LLM to convert the reasoning processes of  from AoT into CoT while keeping the main reasoning content the same. Considering the computational cost of the LLM, we sample 10k data from , and carry out the ablation experiment on these data.  We finetune the models on these same 10k questions, but with reasoning processes in different formats: (1)  from CoT Collection, (2)  as described above, and (3)  from . More implementation details can be found in the Appendix~.

As shown in Table~, among all formats of training data, AoT achieves the best results over the CoT format (both CoT and the AoT2CoT). With the same back-end LLM and hybrid reasoning strategy, AoT still outperforms AoT2CoT. This demonstrates that the main factor contributing to our method's improvement is the reasoning format.  For the CoT format, AoT2CoT outperforms CoT, especially in algorithmic questions. This suggests that integrating both text and code for LLM training and reasoning could be an effective way.

To investigate the impact of the training data scale, we train models with different numbers of training samples. As shown in Figure~, the AoT-finetuned models achieve a steady improvement as the scale of training data increases. Across all scales, AoT-finetuned models outperform CoT-finetuned models. Moreover, finetuning using 10k AoT data can yield desirable performance, even better than the case of finetuning using 348k CoT data. This demonstrates both the effectiveness and efficiency of our .

We analyze the responses of AoT-finetuned models from the following perspectives.

: The proportion of the text/code being utilized among all responses.

: The proportion of responses that contain/output predicted answers and follow the AoT format among text/code responses.

: The proportion of responses whose predicted answers are correct among text/code responses.

Table~ presents the results of the responses on BBH in the zero-shot setting. First, models prefer to reason with natural language for most problems, rather than programming language.  For example, only 26.6\% of responses from Llama-3-8B-AoT use code. Second, some models struggle to output code in a valid format.  Of the code responses generated by Llama-2-7B-AoT, only 56.9\% can be executed and output the answer, possibly due to the model's insufficient coding capacity acquired from pre-training.  Third, reasoning with code often achieves higher accuracy than reasoning with text.  The accuracy of code responses from CodeLlama-7B-AoT reaches 61.1\%, whereas text responses only have an accuracy of 43.6\%.  This suggests that reasoning with programming language could be a promising direction~.

To better understand the failure modes and future challenges, we randomly sample 100 problems on which Llama-3-8B-AoT fails and manually annotate their error types. Mostly, incorrect reasoning steps constitute the main cause of errors (38\%).  The model also suffers from lack of necessary knowledge (16\%), misunderstanding of tasks (15\%), and hallucination (12\%). The reasoning process sometimes trivially repeats the conditions (12\%) or fails to be executed (5\%). Finally, a smaller percentage of correctly predicted answers are misjudged by automated indicators (5\%). We present detailed error definitions, examples, and proportions in Appendix~ Table~.

We follow CoT Collection~ to preprocess the FLAN Collection~ data as our source data. The CoT Collection contains 1.84M samples from 216 tasks. Each sample consists of a question, an answer, and a CoT rationale. We discard the CoT rationale and use the GPT-3.5-Turbo to generate the AoT reasoning process based on the question and the correct answer. We manually divided the 216 datasets into two parts,  (203 datasets that are more suitable to be solved in natural language) and  (13 datasets that are more suitable to be solved in programming language). The AoT-Text consists of the following tasks:

The AoT-Code consists of the following tasks:

We utilize a proportional stratified sampling method to sample 200k instances from the AoT-Code datasets and 200k instances from the AoT-Text datasets. The proportional stratified sampling method ensures that the proportion of each dataset is the same before and after sampling, thus maintaining the overall data distribution.

Listing~ shows the system prompts we use for collecting AoT responses. We use different prompts for AoT-Text and AoT-Code and clearly state the requirements of AoT format. We also include 3 demonstrations in the prompt.

 retains 348k high quality data after filtering. The average number of steps for responses in AoT-Text and AoT-Code is 2.8 and 2.2, respectively. We randomly sample 3 samples from each of AoT-Text and AoT-Code and present them in Listing~.  is in English.

Table~ lists the models involved in our experiments, including their names, versions, and corresponding URL links.  We follow the licences (which can be found in the URL links) of these models to use them. For the open-source models, we use the model weights provided by Huggingface.

In the zero-shot setting, we directly use the test question as the input to the models.  For our models with CoT/AoT-finetuning, we are able to extract the predicted answers from the responses in a fixed format.  For open-source instruction finetuned models (e.g., Llama-3-8B-Instruct), we utilize a simple yet effective instruction to guide the model to output the answer in a fixed format.  The instruction we use is: ``''

In the few-shot setting, we conduct experiments with two type of prompts: CoT prompt and AoT prompt. The CoT prompt is collected from the official GitHub repository of BBH. For each test task, the CoT prompt contains three demonstration questions along with their CoT rationales. For the AoT prompt, we employ the same questions and manually create the reasoning processes in AoT format.  In consistency with the , we use the Python programming language to express the reasoning process for some tasks.

For the following tasks we use a programming language reasoning process: 

For the following tasks we use a natural language reasoning process: 

Listing~ presents examples of the Few-shot AoT prompts for BBH.  Considering that the full prompts for all tasks in BBH is long, we show two demonstrations in the prompts here for brevity (one in natural language and one in programming language). We will release the prompt we used after publication.

To construct AoT2CoT for ablation study, we use LLM to transform the reasoning process from AoT format to CoT format. We sample 10k of data from the  (5k from AoT-Text and 5k from AoT-Code). Our aim is to transform the reasoning format while keeping the reasoning content is roughly the same. We prompt GPT-3.5-Turbo (the same back-end LLM for collecting ) to transform the input reasoning process into CoT format. In order to avoid the model generating new reasoning content in response to the question, we do not input the question and answer, but only the reasoning process. Listing~ gives the specific prompt we use.

Table~ presents the tasks and number of questions within the BBH benchmark. Following~, tasks are divided into two categories: NLP tasks and algorithm tasks. Tasks ``Logical deduction'' and ``Tracking shuffled objects'' consist of 3 sub-tasks. The questions and answers in BBH are in English.

Table~ shows the performance on subtasks of BBH. We also introduce human performance as a reference~. We can observe that AoT-finetuning compared to cot-finetuning is able to achieve improvements on multiple reasoning tasks on multiple models.

Table~ presents the 3-shot performance on BBH, including those that are instruction-finetuned (such as Llama-3-8B-Instruct).  A consistent trend can be observed where instruction-finetuned models achieve lower few-shot performance compared to models that are merely pre-trained. For example, Llama-3-8B-Instruct achieves an overall accuracy of 56.7\% with the official few-shot CoT prompt, while Llama-3-8B can achieve 60.0\% with the same prompt. For the CodeLlama-7B-Instruct and Llama-2-7B-Chat, a significant performance decrease is noted in the instruction-finetuned versions.  Upon examining their responses, we find that in most cases, the models did not follow the demonstration format for answering.  For the Llama-3-8B-Instruct and Mistral-7B-Instruct, the responses adhered to the demonstration format.  In this scenario, using the AoT prompt yields better results than the CoT prompt.

Listing~ shows success cases for the AoT-finetuned Llama-3-8B. The AoT-finetuned model demonstrates a certain degree of ability to reason with abstraction. For example, for the Case 3, the model first defines the tools needed to solve the problem at an abstract level, i.e., the  class and the  function. Subsequently, the model then utilizes these tools to solve the problem based on the concrete conditions of the question. Listing~ presents the error cases.

Considering that our AoT format encompasses multi-level abstractions, the average length of AoT responses tends to be longer than that of CoT responses.  Consequently, the AoT-finetuned model may takes a higher computational cost during inference to generate more tokens.  To clarify the impact of this in practical applications, we conduct a brief comparison of computational costs.  We calculate the average length of responses from Llama-3-8B-CoT and Llama-3-8B-AoT on the BBH questions.  On average, the responses of Llama-3-8B-CoT consist of 107.1 tokens, while the responses of Llama-3-8B-AoT consist of 188.9 tokens. Thus, AoT responses takes approximately 80\% more tokens than CoT responses. 

To estimate the generation time during inference, we use the vllm library on a server with 8 A100-SXM4-40GB GPUs, setting the batch size to 1.  The average time for Llama-3-8B-AoT to generate one AoT response is 1.96 seconds.  However, this estimation should only be taken as a rough reference, as it may be influenced by various factors such as hardware conditions.

Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning.  However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks to bridge this gap by introducing a novel structured reasoning format called Abstraction-of-Thought (AoT). The uniqueness of AoT lies in its explicit requirement for varying levels of abstraction within the reasoning process.  This approach could elicit language models to first contemplate on the abstract level before incorporating concrete details,  which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT) method. To align models with the AoT format, we present , a generic finetuning dataset consisting of 348k high-quality samples with AoT reasoning processes, collected via an automated and scalable pipeline. We finetune a wide range of language models with  and conduct extensive evaluations on 23 unseen tasks from the challenging benchmark Big-Bench Hard. Experimental results indicate that models aligned to AoT reasoning format substantially outperform those aligned to CoT in many reasoning tasks. IntroductionDBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:journals/corr/abs-2303-08774DBLP:conf/nips/Wei0SBIXCLZ22DBLP:journals/corr/abs-2309-15402,DBLP:journals/corr/abs-2401-14295,DBLP:conf/nips/KojimaGRMI22,DBLP:conf/iclr/ZhouSHWS0SCBLC23,DBLP:conf/nips/YaoYZS00N23DBLP:journals/corr/abs-2402-13116,DBLP:conf/nips/ZelikmanWMG22,DBLP:conf/acl/ShridharSS23,DBLP:conf/acl/HoSY23,DBLP:conf/icml/FuPOSK23,DBLP:journals/corr/abs-2307-02053saitta2013abstractionyang2012intelligentfig:introfig:introDBLP:journals/coling/Cohen87saitta2013abstractionDBLP:conf/nips/Wei0SBIXCLZ22DBLP:journals/corr/abs-2211-12588,DBLP:conf/icml/GaoMZ00YCN23DBLP:conf/icml/LongpreHVWCTZLZ23DBLP:journals/corr/abs-2309-05653DBLP:conf/acl/SuzgunSSGTCCLCZ23DBLP:journals/corr/abs-2206-04615Related WorkChain-of-Thought PromptingDBLP:journals/tmlr/WeiTBRZBYBZMCHVLDF22DBLP:conf/nips/Wei0SBIXCLZ22DBLP:journals/corr/abs-2309-15402,DBLP:journals/corr/abs-2401-14295, DBLP:conf/nips/KojimaGRMI22,DBLP:conf/nips/YaoYZS00N23DBLP:journals/corr/abs-2211-12588DBLP:conf/icml/GaoMZ00YCN23DBLP:conf/iclr/ZhouSHWS0SCBLC23DBLP:conf/nips/YaoYZS00N23Training Language Models for ReasoningDBLP:journals/corr/abs-2402-13116DBLP:journals/corr/abs-2402-13116,DBLP:conf/nips/ZelikmanWMG22,DBLP:conf/acl/ShridharSS23,DBLP:conf/acl/HoSY23,DBLP:conf/icml/FuPOSK23,DBLP:journals/corr/abs-2307-02053,DBLP:conf/acl/HsiehLYNFRKLP23,DBLP:conf/acl/MagisterMAMS23,DBLP:journals/corr/abs-2210-06726,DBLP:journals/corr/abs-2305-13888,DBLP:journals/corr/abs-2309-05653DBLP:conf/icml/LongpreHVWCTZLZ23DBLP:conf/emnlp/KimJKJYSS23DBLP:journals/corr/abs-2311-11045,DBLP:journals/corr/abs-2306-02707Reasoning with AbstractionDBLP:conf/eacl/DurmeMS09,DBLP:conf/ijcai/SongWWLC11,DBLP:conf/aaai/GongZZ16DBLP:journals/corr/abs-2206-01532,DBLP:journals/corr/abs-2311-09174DBLP:conf/cvpr/0017JZZ21DBLP:journals/corr/abs-2404-00205DBLP:journals/corr/abs-2401-17464width=.98\textwidthfigures/aot.pdfIllustration of Abstraction-of-Thought (AoT) format with natural language (upper half) and programming language (lower half).     Unlike the unconstrained CoT, AoT explicitly requires that different levels of abstraction be presented in the reasoning process.     Here are examples of 2-level abstraction AoT.     In AoT, the high-level parts (represented in , i.e., \DarkRed) plan and organize the entire reasoning process from an abstract perspective, while low-level parts (i.e., ) carry out concrete and detailed reasoning steps.      The high-level parts are abstractions of the low-level parts, clarifying their functionality and objectives.     For clarity, we omit some code snippets in AoT with programming language.fig:aot-6mmAbstraction-of-Thought (AoT)saitta2013abstraction,yang2012intelligentDBLP:journals/coling/Cohen87fig:aot \setlength \setlength

\nonumber     s_i & = a_i^{1} \circ a_i^{2} \circ \dots \circ a_i^{k} , \\     \tau_{AoT} & = s_1 \circ s_2 \circ \dots \circ s_n , alignedfig:introfig:aot\textbackslash boxed\{\}The \ourdatasetOverviewHybrid reasoning strategy.DBLP:conf/icml/GaoMZ00YCN23,DBLP:journals/corr/abs-2211-12588DBLP:journals/corr/abs-2401-00812DBLP:conf/ijcnlp/LyuHSZRWAC23Broad task coverage.Scalability.Source Dataset SelectionDBLP:conf/emnlp/KimJKJYSS23,DBLP:journals/corr/abs-2311-11045DBLP:conf/icml/LongpreHVWCTZLZ23DBLP:conf/iclr/SanhWRBSACSRDBX22DBLP:conf/emnlp/WangMAKMNADASPK22DBLP:conf/iclr/WeiBZGYLDDL22DBLP:conf/emnlp/KimJKJYSS23DBLP:conf/emnlp/KimJKJYSS23AoT-TextAoT-Codesec:appendix_dataAoT Response GenerationDBLP:conf/emnlp/KimJKJYSS23,liu2024bestsec:appendix_dataDBLP:conf/emnlp/KimJKJYSS23, DBLP:journals/corr/abs-2311-11045Data Validation and Filteringtab:statisticsec:appendix_dataExperimentsEvaluation Dataset.DBLP:conf/acl/SuzgunSSGTCCLCZ23DBLP:journals/corr/abs-2206-04615Held-OutNLPAlgAllsec:appendix_bbhSetting and Baselines.DBLP:conf/emnlp/KimJKJYSS23,DBLP:journals/corr/abs-2311-11045CoTAoTDBLP:conf/emnlp/KimJKJYSS23Models.DBLP:journals/corr/abs-2307-09288DBLP:journals/corr/abs-2308-12950llama3modelcardDBLP:journals/corr/abs-2310-06825GPT-3.5DBLP:journals/corr/abs-2303-08774sec:appendix_modelImplementation Details.DBLP:journals/corr/abs-1909-08053\textbackslash boxed\{\}Zero-Shot Performancetables/table_zero_shottab:Zero-shotDBLP:conf/acl/SuzgunSSGTCCLCZ23sec:appendix_by_taskssec:appendix_case_studysec:appendix_computational_costtables/table_few_shotFew-Shot PerformanceDBLP:conf/acl/SuzgunSSGTCCLCZ23sec:appendix_few_shottab:Few-shotsec:appendix_few_detailstables/table_aot2cotAblation Study on Reasoning FormatDBLP:conf/emnlp/KimJKJYSS23CoTAoT2CoTAoTsec:appendix_aot2cottab:aot2cotAblation Study on Training Data Scalefig:ablation_scaleResponse AnalysisUsage Rate (UR)Format Correctness (FC)Answer Correctness (AC)tab:ablation_model_analysisDBLP:journals/corr/abs-2211-12588tables/table_model_analysisError Analysissec:appendix_case_studytab:table_errorConclusionLimitationsDBLP:journals/corr/abs-2204-05862,DBLP:conf/nips/Ouyang0JAWMZASR22anthology,customImplementation Detailssec:appendix_details\ourdatasetsec:appendix_dataDBLP:conf/emnlp/KimJKJYSS23\hrefDBLP:conf/icml/LongpreHVWCTZLZ23AoT-TextAoT-Code+2mm adversarial@droberta, olid, ai2, natural, question, qanta, health, quail, jeopardy, jigsaw, detoxifying, poki, qnli, cb, google, subjqa, multi, casino, task, babi, wiki, ddo, anli, craigslist, ohsumed, strategyqa, adversarial@dbidaf, cad, semeval, emo, overruling, hippocorpus, qed, diplomacy, piqa, smcalflow, super@record, schema, ai2@ARC-Easy, nlu, ruletaker, news, com2sense, civil, circa, quartz, diqa, semeval, hybridqa, evaluation, ecqa, storycommonsense, miscellaneous, snli, clariq, blimp, financial, hatexplain, hope, numersense, x, xcsr, qa, mcscript, mwsc, persent, trivia, hate, coached, scitail, drop, rte, anli, qrecc, ms, quac, wikitext, nlg, mutual, gwsd, yahoo, essential, swag, torque, wiki, cola, winowhy, disfl, roc, semeval, codah, mocah, atomic, crows, mnli, tweetqa, scruples, conv, stereoset, break, duorc@SelfRC, dialogre, ambigqa, iirc, miam, pubmed, deal, ai2@ARC-Challenge, coda, spolin, wiki, hateeval, timetravel, duorc@ParaphraseRC, recepie, kilt@hotpotqa, curated, sciq, freebase, squad, help!, bless, squad, task, sbic, quoref, com, wnli, haspart, personachat, argkp, ethos, open, race@high, proto, sarcasm, web, abductive, curiosity, imppres, race@middle, adversarial@dbert, eurlex, head, defeasible, equity, qrecc, wiki, eqasc, bard, wiqa, dream, liar, anli, scitailv1.1, tellmewhy, cod3s, dstc, indian, aquamuse, glucose, social, air, missing, narrativeqa, scitldr, mrqa, meta, go, casehold, scifact, super@boolq, ade, dailydialog, starcon, commonsenseqa, openbookqa, quarel, propara, event2mind, inquistive, tom, wiki, cosmos, afs, medical, creak, yoruba, semeval, xl, super@multirc, opp, esnli, grailqa, root09, qasper, ropes, gooaq, cos, perspectrum, xquad, trianglecopa, mctaco +2mm+2mm aqua, big, cfq, conala, gsm8k, leetcode, math, math, mathmatics, mathqa, prost, svamp, synthetic +2mmlst:prompt_aot_collectlst:Aot_Collection_Samplescaption=, label=listings/prompt_aot_collect.mdcaption=, label=listings/Aot_Collection_Samples.mdModelssec:appendix_modeltab:exp_models\hreftables/table_modelsZero-Shot Promptssec:appendix_zero_shotAnswer the question and put the final answer in \textbackslash boxed\{\}.Few-Shot Promptssec:appendix_few_shot\hrefBoolean expressions, Dyck languages, Geometric shapes, Logical deduction five objects, Logical deduction seven objects, Logical deduction three objects, Multistep arithmetic two, Navigate, Object counting, Penguins in a table, Temporal sequences, Tracking shuffled objects five objects, Tracking shuffled objects seven objects, Tracking shuffled objects three objects, Web of lies, Word sorting.Causal judgement, Date understanding, Disambiguation qa, Formal fallacies, Hyperbaton, Movie recommendation, Reasoning about colored objects, Ruin names, Salient translation error detection, Snarks, Sports understanding.lst:prompt_bbh_aotcaption=, label=listings/prompt_bbh_aot.mdAoT2CoTsec:appendix_aot2cotlst:prompt_aot2cotcaption=, label=listings/prompt_aot2cot.mdBBH Benchmarksec:appendix_bbhtables/table_bbhtab:bbhDBLP:conf/acl/SuzgunSSGTCCLCZ23Additional Experiment Resultssec:appendix_add_resultPerformances on Subtaskssec:appendix_by_taskstab:bbh_by_tasksDBLP:conf/acl/SuzgunSSGTCCLCZ23Few-Shot Performancesec:appendix_few_detailstab:few-shot-detailtables/table_few_detailsCase Studysec:appendix_case_studylst:success_casesPersonswap\_giftslst:fail_casestables/table_errorComputational Cost Comparisonsec:appendix_computational_cost\hreftables/table_by_taskscaption=,backgroundcolor=\color, label=listings/success_cases.mdcaption=,backgroundcolor=\color,label=listings/fail_cases.md