Evaluating stories is a difficult task . In the social sciences literature, multiple criteria have been suggested, often divided into cognitive and emotional factors . However, the consensus around the criteria to be used in the NLP literature is still weak .  distill the indicators used in the social sciences literature into 6 criteria (Relevance, Coherence, Empathy, Surprise, Engagement, Complexity), which we will use in our paper as well.

While human evaluation remains the gold standard of evaluation, it is costly and time-consuming. We therefore need to develop automatic measures that can act as substitutes for human judgment, ideally for each of the criteria. Such automatic measures could be used to improve language models,  a loss function or for chain-of-thought prompting .

Automatic measures (, ROUGE , BERTScore , BARTScore ) have been repeatedly shown to correlate moderately to poorly with human judgment, especially when applied to tasks other than the one they were designed for .  put forth the particular limitations of reference-free measures. For ASE,  and  also observe weak correlations between automatic and human ratings, whether they be reference-based or reference-free. This highlights the need for better automatic evaluation methods. To tackle this issue, this paper investigates the use of LLMs to annotate stories with ratings w.r.t. given criterion.

LLMs are increasingly being tested for automatic text annotation,  sentiment analysis  , named entity recognition  or event structure modeling .  demonstrate that labeling performed by GPT-3 can achieve the same performance as human labeling and be up to 96\% more cost-efficient.  show that GPT-3 performs well for text classification tasks, but struggles with more complex tasks such as named entity recognition.  design a test for creativity and show that LLM-generated stories pass fewer tests than human stories, and that using LLMs for ASE yields no positive correlations.

We seek to generalize their findings through the use of source-available models and a finer analysis and discussion of LLM performance.

The importance of designing efficient prompts for large language models such as GPT-3 has been extensively investigated in recent years.  notably find that zero-shot prompting can perform similarly to few-shot prompting, and even exceed it. They explore the design of metaprompts that prime the language model to better solve a given problem.  treat the prompt engineering process as an optimization problem, use search algorithms guided by LLMs to solve it and attain human-level performance.  and  review different strategies that have been applied to augment large language model abilities, -to-most prompting , ask-me-anything prompting , and zero-shot chain-of-thought reasoning .

We choose to investigate whether LLMs perform better with simple or detailed guidelines, and with zero- or one-shot Eval-Prompts.

The ASG task commonly involves the generation of a story from a short sentence called a , which we will henceforth call .

Given an evaluation measure  ( scoring algorithm, an LLM), a story-prompt , and a story , we define the ASE task as the production of an evaluation score .

In this paper, we choose to use LLMs as ASE measures. We will refer to the prompt that is fed to the LLM as the , to distinguish it from the story-prompt. See  for an example of the use of an LLM for story evaluation.

 We use the criteria introduced by , who designed , a benchmark for story evaluation. They compiled a set of six orthogonal criteria from the social sciences literature:

Given the importance of good prompt engineering , we design four different Eval-Prompts for the generation of ratings. For each of our Eval-Prompts, we provide the model with a story-prompt and a corresponding story. Then:

 (simple rating): we ask the model to rate the story on a scale from 1 to 5 on one of the six criteria;

 (rating with explanation): same as Eval-Prompt 1, and we ask the model to explain its answer;

 (rating with explanation and guidelines): same as Eval-Prompt 2, and we provide the model with the detailed guidelines from the original annotation protocol by ;

 (rating with explanation and human story): same as Eval-Prompt 2, and we provide the model with the human story associated with the same story-prompt. We explicitly tell the model that the human story is given only for reference purposes.

Different Eval-Prompt examples are shown in .

 For  systems and  story-prompts, let  be the story generated by system  for story-prompt . For a (human or automatic) measure , we denote by  the score associated to . Let  be a correlation coefficient, 's  , Spearman's   or Kendall's  . We note  the measure provided by the -th human annotator.

A naive method to compare ratings from two measures would be to compute how much they differ from each other for each story,  calculating the average L1 distance between a given evaluation method  and the human ratings, , . However, this method suffers from the central tendency bias---the tendency of an individual to rate most items on a survey in the middle of a rating scale---which is often observed in Likert scales  and could be explained by the participants' tendency to base their judgment on a least mean squares estimator rather than a maximum a posteriori estimator . We therefore choose more robust measures of meta-evaluation: system-level and overall correlations.

 We take the correlation of the vectors containing the mean score of all stories for each system, for  and . This strategy measures how much  and  agree when comparing different systems. Formally:

The segment-level correlation, often used in conjunction with the system-level one in the meta-evaluation literature , is not adapted to ASE since stories generated from the same story-prompt are not required to be similar, while  of a sentence should look alike. We therefore use the overall correlation, which we define below.

 We take the correlation between the full vectors containing the scores of  or  for a given story for every system. Formally:  Correlations between two automatic measures on the same annotated dataset are not independent. As advised by , we use the Williams test  to evaluate the strength of an increase in dependent correlations .

Given three features ,  and  of a population of size , Williams's  test for whether the correlation between  and  equals the correlation between  and  is formulated as follows:

where  is the correlation between  and  and

Williams's  statistic follows a Student's -distribution with  degrees of freedom. In particular, the Williams test %is more powerful than the equivalent for independent samples (the Fisher  to ) as it takes the correlations between  and  into account.

Furthermore, since we perform a large quantity of tests, we choose to correct -values for multiplicity. As advised by , we control the false discovery rate using the Benjamini-Hochberg (BH) method % at level  . Given  -values  sorted in increasing order, the BH method consists in %finding the largest  such that . The null hypothesis would then be rejected for the first  tests. This is equivalent to computing adjusted -values  and replacing the -values from largest to smallest.

Following recent recommendations to move beyond simplistic ``statistical significance'' tests , we report all -values for transparency. %and avoid using an arbitrary threshold (\ 0.05). We choose to use a gradual notion of evidence for our statistical analysis, as suggested by .

We conduct a user study in which we ask human raters to identify potential issues in LLM explanations.  introduced an error annotation schema called  that we adapted for ASE. We manually reviewed a random sample of 20 explanations from Beluga-13B on Eval-Prompt~3 and selected the most relevant error types. Then, we randomly sampled another 100 explanations and, for each explanation, we asked 3 human workers to annotate it w.r.t. following five error categories:

We recruited workers on Amazon Mechanical Turk. We estimated that a HIT would take around one minute, so we set the reward at  per hour. To ensure that annotators spoke fluent English, we restricted access to the experiment to the UK, the US, Canada, Australia and New Zealand. 

We use the  dataset  which contains 1,056 stories generated from story-prompts from the  dataset , with both pretrained language models: , , , ,  and ; and ASG-specific models: ,  and . These stories were annotated with scores from human raters on the six criteria introduced in  and 72 automatic measures. We reproduce the original procedure from : for reference-based evaluation measures (), we use the human story from  as the reference for the generated story. Because of space constraints, we display only the evaluation measures that are the most used in the literature: , , , , , , , . The results are similar for the other automatic measures.

Since the release of the  dataset, language models have made significant advancements. We therefore felt the need to expand  with more recent models. We selected  (Llama-7B) as a new baseline and 4 high-performing models (at the time of selection) of different sizes on the HuggingFace Open LLM Leaderboard:  (Platypus2),  (Llama-30B),  (Beluga-13B),  (Mistral).

We submit each of the four Eval-Prompts 3 times on all 1,056 stories on each of the 6 criteria, and we then extract the ratings automatically from the generated answer via a regular expression. Since story evaluation on multiple prompts and multiple criteria was more computationally demanding, we limited our experiments to the smaller 13B and 7B models. We used the 4 following models: Beluga-13B, Mistral,  (Llama-13B), and  (ChatGPT). We also ran the ASE experiments with Llama-7B, which failed at the task too often for the results to be exploitable,  generating nonsensical conversations between itself and the user. We use  for Llama models and  for ChatGPT (default suggested values).

Llama2  models were trained on a closed ``new mix of data from publicly available sources''. Beluga-13B and Mistral-7B are Llama2 models fine-tuned on Orca-style datasets which contain triplets of ``System message--User query--LLM response'' for a large collection of tasks . Beluga-13B is fine-tuned on StabilityAI's closed internal dataset, while Mistral-7B is fine-tuned on the open OpenOrca dataset . ChatGPT  is a closed-source model trained on a closed internal dataset that includes the CommonCrawl, Books1 and Books2 datasets.

% % Fabian: avoid single line on next page We used the  library  and the OpenAI API for our experiments.

First, we want to verify if LLMs provide stable answers. The default decoding strategy for LLMs (both Llama models and ChatGPT) is top- sampling, which involves random variability in the generation process. We evaluate how consistent LLMs are with themselves through an inter-rater reliability (IRR) estimation. For each task, we interpret the three different LLM ratings as coming from three different annotators and we use the intra-class correlation coefficient (ICC), which is the most relevant one for our case study: unlike Cohen's and Fleiss's kappas  or Krippendorff's alpha , which quantify IRR based on all-or-nothing agreement, the ICC incorporates the magnitude of the disagreement to compute its IRR estimate, with larger-magnitude disagreements resulting in lower ICC than smaller-magnitude disagreements . We specifically use the ICC for  (ICC2k) ; with the assumption that the  aspect can approximate the random aspect of the generation.

ICC2k values for Eval-Prompt 1 for Beluga-13B, Mistral-7B and human ratings are displayed on . Comparing LLM consistency and human inter-rater agreement values should be done with caution: human raters may have subjective appreciations of the Likert scale despite guidelines, while LLM consistency depends mostly on parameters that dictate output variability,  or top-. %, since the factors of answer variability differ between each setting, but That said, we reckon that it is still useful to display human IRR values as a baseline. We observe that LLMs have very high consistency overall for all criteria; the lowest value is Mistral-7B's ICC for Surprise (0.66), which is still fairly high. Confidence intervals are also smaller than for human ratings.

Here, we study the Kendall correlations between LLM and human ratings on corresponding criteria. For the ``Beluga-13B 1'' column in , the first value is the correlation between Beluga-13B Relevance ratings and averaged human Relevance ratings for Eval-Prompt 1, then Coherence ratings, etc. % In addition, as a way of integrating all the information GPT-3 could possibly garner from a given story, we compute the average of GPT-3 Task 1 ratings over the six human criteria (the ``DaVinci/ChatGPT AVG 1'' columns) and display the corresponding correlations.

Assuming we want an automatic measure to perform as well as an individual human rater would, we need a baseline for comparison. Therefore, we also compute the average correlations between individual human ratings and % average human ratings, which we compiled into the same figures for the sake of readability (the ``Human'' column). Since the individual human rating is included in the average human rating, both measures are not independent, so the column acts as an upper-bound. % similarly to the  lower-bound,  LLM ratings generally correlate with human ratings similarly to automatic measures, if not better. % Simpler Eval-Prompts (1 and 2) yield the best results overall. Making the Eval-Prompt more complex by providing guidelines (Eval-Prompt~3) or a human story (Eval-Prompt~4) may confuse LLMs since the story is generally rather long already. Overall, Beluga-13B is the best performer, achieving higher correlations (0.25 on average) than both other LLMs and automatic measures (0.18). The better results (as compared to Llama-13B (0.16) and Mistral-7B (0.20)) suggest a positive influence of fine-tuning and model size respectively. The inferior performance of ChatGPT (0.18) is difficult to explain since OpenAI does not disclose the details of its architecture, its training process and, most importantly, its training data. Nonetheless, an important takeaway is that current source-available models can effectively compete with closed-source models: this is good news for NLP research, since observations made on closed-source models cannot easily be generalized.

First, we observe that human baseline correlations are noticeably higher than non-LLM automatic measures: while human annotators tend to reach a consensus when ranking systems (averaging correlations of 0.73), non-LLM automatic measures are moderately to poorly correlated from human judgment (with values ranging from 0.13 to 0.57).

Meanwhile, Llama models display very high correlations, with Beluga-13B performing almost as well as human raters (0.70 vs 0.73). ChatGPT shows a somewhat erratic performance (correlations range from 0.07 to 0.73), which is overall comparable or inferior to Llama models. Also, LLMs generally outperform other automatic measures (0.70 for Beluga-13B compared to 0.57 for ).

The fact that correlations are sometimes higher than the baseline can be explained by the subjective nature of the task: human annotators may exhibit higher variability in their ratings than the stable LLMs.

 shows the BH-adjusted -values of the Williams tests for the increase in correlations with a given criterion between Beluga-13B average Eval-Prompt~1 ratings (row) and other measures (column).

For overall correlations, there is strong statistical evidence that Beluga-13B correlates better with human judgment than many non-LLM automatic measures (~<~0.01 for many tests). Evidence is more moderate to weak when comparing Beluga-13B and other LLMs. For instance, between Beluga-13B and ChatGPT, -values lie between 0.01 and 0.14. While the performance of Beluga-13B still leaves a lot of room for improvement, it performs better than non-LLM automatic measures.

For system-level correlations, statistical evidence for better performance appears weaker: ~>~0.11 for all tests. However, one should keep in mind that the ratings (averaged over more than 1,000 stories) used to compute system-level correlations hold more information than the individual ratings of the overall correlations. Therefore, while statistical evidence is weaker, the averaged nature of the correlations and the significant numeric increases in correlations (0.70 for Beluga-13B vs 0.57 for /) suggest that Beluga-13B is more reliable at ordering systems compared to non-LLM measures.

First, LLMs show very high self-consistency. Overall correlations remain weak, although LLMs display marginal improvements over non-LLM automatic measures, backed with strong statistical evidence. At the system-level, LLM correlations with human judgment are high, but statistical evidence is weaker. In conclusion, while LLMs still cannot be relied upon to evaluate a single story, they appear more reliable than non-LLM automatic measures for comparing different models and selecting the best one. % Fabian: removed for spacing In this section, we discuss the influence of the Eval-Prompt on the consistency and distribution of the generated LLM ratings.

Here, we analyse the influence of the Eval-Prompt on LLM consistency. ICC2k values for Beluga-13B ratings w.r.t. different Eval-Prompts are shown on  (other LLMs display similar behavior). The influence of Eval-Prompts appears limited: providing guidelines (Eval-Prompt 3) tends to decrease self-consistency for all criteria except Complexity with a discernible effect (as shown by the confidence intervals), but ICC values remain very high. LLMs are therefore remarkably consistent in their grading, no matter the Eval-Prompt.

We show the average Likert ratings per LLM per Eval-Prompt on . Compared to Eval-Prompt~1, Eval-Prompt 2 seems to have limited influence on the ratings for all models, often leading to overlapping confidence intervals. Eval-Prompt 3 causes a statistically discernible decrease in ratings for Beluga-13B and Llama-13B, and a discernible increase for ChatGPT. Eval-Prompt 4 has a similar effect, with the decrease also observable with Mistral-7B. The significantly lower ratings of ChatGPT partly stem from the fact that it was not asked to rate the new Llama-generated stories, which were generally highly-rated.

Overall, it seems that more detailed Eval-Prompts (3 and 4) tend to decrease the ratings for Llama-models while having an opposite effect for ChatGPT. We tried to separate ratings per generative model or per criterion but were unable to identify a more specific pattern: we therefore chose to show only the aggregated results for the sake of clarity.

Here we analyze the influence of Eval-Prompts on correlations between LLM ratings and human ratings.

Eval-Prompt 2 overall correlations are very close to Eval-Prompt~1 correlations for all models: simply asking for an explanation has limited influence on correlations. Eval-Prompt 3 tends to decrease correlations for all models: providing guidelines makes the model less accurate, counter-intuitively. Eval-Prompt 4 (providing a human story for reference) has a similar effect.

Eval-Prompt~2 has limited effect on correlations again, except for Beluga-13B for whom it seems to increase correlations. Eval-Prompt 3 decreases correlations, with a marked effect in Llama-13B. Finally, Eval-Prompt~4 seems to cause a small increase in correlations, contrary to its decreasing effect on overall correlations.

First, regardless of Eval-Prompt complexity, LLMs behave consistently when prompted multiple times. Asking for an explanation (Eval-Prompt 2) has negligible effect on ratings, while more complex Eval-Prompts (3 - providing guidelines and 4 - providing a reference human story) have a more discernible influence (positive or negative). As for correlations with human ratings, providing guidelines (Eval-Prompt 3) consistently seems to lower correlations, whereas providing a human story for reference (Eval-Prompt 4) has opposite effects for overall or system-level correlations. % For the ASE task, asking LLMs to explain their answer (Eval-Prompt 2) and giving them a reference (Eval-Prompt 4) seems to have limited influence on ratings and may increase explainability, but giving prompts that are too detailed or complex (Eval-Prompt 3) may impair LLM performance.

In this section, we analyze to what extent the explanations provided by LLMs are consistent w.r.t. ratings, , they differ from criterion to criterion, whether they are semantically relevant and, for Eval-Prompt 3, whether they are compliant with the provided guidelines. We will focus on Beluga-13B since it had the best correlations with human judgment, as shown in .

First, we want to ascertain whether Beluga-13B provides different explanations for each of the human criteria. We gather the explanations provided by Beluga-13B on human stories for each criterion and use the  library  to compute their corresponding embeddings. % feed them to a LLM to obtain their embeddings. We use the  API} since they trained and fine-tuned a specific Masked Language Model for that task. We then use a 2D UMAP projection  (with parameters  and ) to visualize how the embeddings are distributed.  shows the visualization of the UMAP projection: Beluga's explanations are overall well-separated w.r.t. corresponding criteria. 

Since Beluga's explanations seem to vary from one criterion to another, we evaluate whether they make sense from a semantic point of view%: extracting keywords is an efficient method to gain insight into this question . We use the YAKE! keyword extractor, which significantly outperforms other state-of-the-art methods  : we show selected 3-gram keywords from the top-30 per criterion on . The results are consistent with : keywords are overall different for each criterion. We can also see here that they are semantically relevant.

We display the results of our user study (designed in ) in . We also display the IRR, which we computed using Gwet's agreement coefficient 1 (AC1) . Gwet's AC1 is known to perform well for IRR estimation on binary classification tasks such as our user study: it was designed to be more stable and less affected by prevalence and marginal probability than Cohen's kappa, and this was confirmed by practical experiments .

We can see that Beluga-13B produces near-impeccable syntax, at least according to annotators (2\% of ``Poor Syntax''). It also does a good job at producing coherent text (11\% of ``Incoherence''), and mostly understands the guidelines (13\% of ``Wrong Guideline''). However, it tends to repeat itself somewhat (20\% of ``Superfluous Text'') and, most notably, tends not to substantiate its claims with direct references to the story (31\% of ``Unsubstantiated Claims''). Overall, annotators tend to agree with one another, as showed by the high values of Gwet's AC1.

The substantial rate of ``Unsubstantiated Claims'' and the fact that ---despite the Eval-Prompt explicitly asking for it---beg the question of whether Beluga-13B truly understands the given task. We discuss this question further in .

LLM explanations seem to be specific to each considered human evaluation criterion; however, a finer analysis with a user study reveals that LLMs often struggle with following guidelines and substantiating their explanations.

In this section, we discuss the performance of LLMs at the ASG task compared to human and previous models' performance, as we expanded the  dataset with stories generated from more recent models. Since Beluga-13B and Mistral-7B display very high system-level correlations with human ratings (see ), we use their ratings as proxy for human ratings.  and  show the average Beluga-13B and Mistral-7B ratings for Eval-Prompt 1 per model per criterion for a few  models (GPT-2, HINT) and the Llama models.

We observe that LLMs perform remarkably well, getting higher ratings than older models (GPT-2) and even human stories. Beluga-13B and Mistral-7B both seem to prefer the outputs from larger LLMs (Platypus2-70B, Llama-30B) to their own outputs, suggesting that the LLM grading process cannot be explained simply by a proxy for perplexity. Interestingly, in both tables, Mistral-7B gets slightly higher ratings than Beluga, with some differences being statistically discernible, which could be explained by differences in fine-tuning data.

 Larger models (Platypus2-70B, Llama-30B) exhibit the best ASG performance, with LLM ratings at least equal to those of human stories. However, our setting involves short stories of between 500 and 1,000 words; generating longer stories may prove more difficult since maintaining large-scale coherence may become an issue.

In this section, we verify whether the LLM pretraining data contains the WritingPrompts dataset to check for model contamination, as advised by , and to what extent ASG performance is related with data exploitation,  reproduction of training examples.

We use the  method  which is based on the hypothesis that unseen data will contain more outlier words with low probability than seen data. Furthermore, it does not require additional training. Given a sentence and an LLM's probability distribution of the next token,  the top-\% of tokens with the highest negative log-likelihood and computes their average log-likelihood. We can then detect if the sentence was included in pretraining data by thresholding this average. We follow  and use  for our two experiments.

We sample 1,000 stories from the WritingPrompts dataset , from which the  human stories come.  shows the predicted contamination rates of the WritingPrompts sample. Since they are very low, this strongly suggests that the WritingPrompts sample was not included in the pretraining data of the evaluated models. We can reasonably surmise that the same applies to the whole WritingPrompts dataset.

We use the BooksMIA dataset , which contains 9,870 samples of books labeled 0 if included in the Books3 dataset (commonly used for pretraining LLMs) or 1 if released in or after January 2023. Since the BooksMIA data is labeled, we compute the area under the ROC curve (AUC) obtained with . Results are shown on .

We observe that the AUC detection score is higher for larger models, , it is easier to detect if a book was in the pretraining data of a larger LLM. The definition of the  also means that larger LLMs tend to produce text that is more similar to their pretraining data, such as fiction books, which could help explain their better ASE ratings.

 The better performance of larger LLMs for ASG may be partially explained by their tendency to generate text that is more similar to their pretraining data,  novels.

The ASE task is a very subjective one: LLM performance at ASE and ASG must be seen as a reflection of  preferences and may therefore include biases,  their pretraining data.

Furthermore, we performed most of our experiments in a zero-shot setting without further training; it would be interesting to compare our results with future work involving fine-tuning or reinforcement learning with human feedback on data specific to ASE.

Also, we did not conduct our experiments with LLMs that were optimized for long inputs and outputs, such as GPT-4.

Finally, we mainly used source-available LLama models and found that they performed at least as well as ChatGPT, a proprietary model. We encourage the NLP community to favor the use of such models, as the growing presence of closed models hinders research transparency and reproductibility.

% The most pressing issue for ASG would be the design of specific measures for each human criterion, as all automatic measures (including GPT-3) have L1 distance values to human ratings that are similar to random normally-distributed ratings, and overall correlations with human judgment still remain moderate to weak. Similarly to how ChatGPT's dialogue-oriented fine-tuning seemed to skew it towards Coherence, RLHF may be a promising solution for the fine-tuning of LLMs for a specific criterion.%Another research direction would be to study  how  LLMs can be fine-tuned to better generate stories, as ASG systems have yet to reach human-like performance. Similarly to how ChatGPT's dialogue-oriented fine-tuning seemed to skew it towards better Coherence, RLHF may be a promising solution.%how well LLMs can generate stories that would have high evaluations in a specific criterion. This work was performed using HPC resources from GENCI-IDRIS (Grant 2022-AD011013105R1) and was partially funded by the grants ANR-20-CHIA-0012-01 (``NoRDF'') and ANR-23-CE23-0033-01 (``SINNet'').

We would also like to convey our appreciation to TACL Action Editor Ehud Reiter, as well as to our anonymous reviewers, for their valuable feedback.

  Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning and deep understanding. Meanwhile, Large Language Models (LLM) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers. Introductionli2013storyrowcliffe2004storytellingmiller2008powerturner2014creativelombardo2012storytellinggeorge2014creativejunior2023storybrown2020languagethoppilan2022lamdachowdhery2022palmtouvron2023llamae.g.clark2021alledilivre2023concourscelikyilmaz2020evaluationchhun-etal-2022-humanfig:demo_examplefig:llm_schema We compute and analyze the correlations between LLM ratings with human annotations on criteria specific to story evaluation; we find that, while overall correlations are moderate to weak, system-level correlations are very high, suggesting that LLMs can produce reliable model rankings for ASE;     A comparison between LLMs and current ASE measures. We examine the effects of using different Eval-Prompts on the consistency and distribution of LLM ratings. We find that adding detailed guidelines does not necessarily improve performance and that LLMs are remarkably self-consistent;     An analysis of the influence of prompt engineering on LLM performance. We analyze the explanations provided by LLMs through different methods, including a user study, and find that LLMs' understanding of the ASE task is perfectible. Most notably, they struggle at explaining their ratings with substantiated claims;     Insights on LLM explainability for ASE. The high system-level correlations of LLMs with human ratings enable us to use them to rate other LLMs for ASG. We find that LLMs perform at least as well as humans for the generation of short stories, and that their performance may be explained by their tendency to produce output that is similar to their pretraining data. An analysis of LLM performance in ASG.sub:methodology\url: 150k rating and explanation annotations (1,056 stories, 6 criteria, 4 Eval-Prompts, 3 tries, 2 models);     ASE experiments: 1,500 human annotations of LLM explanations;     User study: 384 stories generated by Llama models with corresponding LLM annotations to expand the  dataset of . ASG experimentHANNAchhun-etal-2022-humansec:related_worksec:llms_for_asgsec:analysissec:discussionsec:conclusionswidth=\textwidthpictures/llm_schema.pdfSchema of the performed ASE experiments. RE, CH, etc. are the considered human criteria (\autoref). ``EP'' means ``Eval-Prompt'', defined in \autoref. For the user study (\autoref), we randomly sampled 100 explanations from our experiments.fig:llm_schemaRelated worksec:related_workHuman Evaluationmccabe1984makes, dickman2003fourbae2021preliminaryfan2018hierarchical, guan-etal-2020-knowledge, rashkin-etal-2020-plotmachines, goldfarb-tarrant-etal-2020-contentchhun-etal-2022-humane.g.wei2022chainAutomatic Evaluatione.g.papineni2002bleulin2004rougezhang2019bertscoreyuan2021bartscorezhang2004interpreting, novikova2017we, colombo2022glassdeutsch2022limitationsguan-etal-2020-knowledgechhun-etal-2022-humanAutomatic Annotatione.g.qureshi2022novelenkhsaikhan2021autovauth2021automatedwang2021wantding2022gptchakrabarty2023artPrompt Engineeringreynolds2021promptzhou2022largewei2022emergentwhite2023prompte.g.zhou2022leastarora2022askmakojima2022large         \\         : You have become death, destroyer of worlds.\\         \\

        \\         Rate the story on a scale from 1 to 5 on Surprise (how surprising the end of the story was). Rating:         Eval-Prompt 1Promptblack\textbf: You look up to see all of them in fear. You just must fix this soon. Slowly, just like your Father always had instructed him, you look down and see all your foes dead and beaten down. You can't resist the urge to touch the wounds. For there is nothing you can do about it. 0.1cm         \\         : You have become death, destroyer of worlds.\\         \\         : You look up to see all          \\         :\\         1 — The ending seemed completely obvious from the start, or doesn't make any sense at all.\\         2 — The ending was easily predictable after a few sentences.\\         3 — The ending was predictable after half of the story.\\         4 — The ending surprised you, but would have been difficult to predict.\\         5 — The ending surprised you, and still seemed as if it could very reasonably have been predicted, ie, there were enough clues in the story.\\         \\         Rate the story on a scale from 1 to 5 on Surprise (how surprising the end of the story was) and explain your answer. Use the provided guidelines. Rating:         Eval-Prompt 3PromptTarget StoryGuidelines0.1cm         \\         : You have become death, destroyer of worlds.\\         \\         : You look up to see all          \\         : I saw the button. It was simple, red, no words on it as I already knew what it did. I mean I built the button, I built what happens          \\         Rate the target story on a scale from 1 to 5 on Surprise (how surprising the end of the story was) and explain your answer. Do not rate the human story; it is here only for reference. Rating:         Eval-Prompt 4PromptTarget StoryHuman StoryExample Eval-Prompts for the Surprise criterion. Eval-Prompt 2 is the same as Eval-Prompt 1 with ``explain your answer'' added at the end. ``Prompt'' (bold) refers to the story-prompt.fig:gpt3_promptsMeta-Evaluation of LLMs for ASEsec:llms_for_asgMethodology for ASEsub:methodologypromptalabdulkarim2021automaticstory-promptASE Definition.e.g.Eval-Promptfig:demo_exampleASE Criteria.chhun-etal-2022-humanHANNA (, how well the story matches its prompt),      RelevanceRE (, how much the story makes sense),      CoherenceCH (, how well the reader understood the character's emotions),     EmpathyEM (, how surprising the end of the story was),     SurpriseSU (, how much the reader engaged with the story),     EngagementEG (, how elaborate the story is). ComplexityCXMethodology.zhao2021calibrateEval-Prompt 1Eval-Prompt 2Eval-Prompt 3chhun-etal-2022-humanEval-Prompt 4fig:gpt3_promptsMeta-Evaluation Measuresssec:math_backgroundNotations.e.g.pearson1895viispearman1961proofkendall1938newe.g.i.e.stevens1971issuesdouven2018bayesianSystem-level correlation (). K^_{m_1,m_2} &\triangleq K \left({N} ^_{m_1},{N} ^_{m_2} \right),\\  \quad ^_{m} & \triangleq \left[ \sum\limits_{i=1}^N m(y_i^1),\dots, \sum\limits_{i=1}^N m(y_i^S)\right].\nonumber -0.5\baselineskipma-etal-2019-results,bhandari2020ree.g.Overall Correlation (). K_{m_1,m_2} & \triangleq K (_{m_1},_{m_2}),\\  \quad _{m} & \triangleq \left[\left(m(y_i^j)\right)_{(i,j) \in \{1, \dots, N\} \times \{1, \dots, S\}}\right].\nonumber -0.5\baselineskipStatistical Testing (\autoref).graham-baldwin-2014-testingwilliams1959regression,moon2019williamssteiger1980tests t =  - r_{13}) )}}{{(n-3)} +  + r_{13})^2}{4} (1 - r_{23})^3}},  K = 1 - {r_{12}}^2 - {r_{13}}^2 - {r_{23}}^2 + 2 \, r_{12} \, r_{13} \, r_{23} . jafari2019andbenjamini1995controllingamrhein2019scientists, wasserstein2019moving, mcshane2019abandonmuff2022rewritingHuman Evaluation of ASE Explanationsssub:user_studydou-etal-2022-gptScarecrow% \item : there is no explanation provided by the LLM despite asking for it;: parts of the explanation are grammatically incorrect or wrongly-worded;     % \item : the explanation makes factually incorrect assertions;     Poor Syntax: parts of the explanation are self-contradictory, logically wrong, or simply do not make sense and do not fit the other categories;     Incoherence: the explanation does not respect the provided guidelines;     Wrong Guideline: parts of the explanation contain text that repeats itself or generation artefacts;     Superfluous Text: the explanation fails to make explicit references to the story to substantiate its reasoning. Unsubstantiated ClaimsExperimental DetailsDataset.HANNAchhun-etal-2022-humanWritingPromptsfan2018hierarchicalBERTGenerationrothe2020leveragingCTRLkeskar2019ctrlGPTradford2019languageGPT-2radford2019languageRoBERTaliu2019robertaXLNetyang2019xlnetFusionfan2018hierarchicalHINTguan2021longTD-VAEwilmot2021temporalsub:methodologychhun-etal-2022-humane.g.BLEUHANNABLEUpapineni2002bleuROUGElin2004rougechrFpopovic2015chrfBERTScorezhang2019bertscoreSUPERTgao2020supertBLANCvasilyev2020fillBARTScoreyuan2021bartscoreBaryScorecolombo2021automaticASG Models.HANNAHANNALlama-2-7b-chat-hf\urlPlatypus2-70B-instructLlama-30b-instruct-2048StableBeluga-13BMistral-7B-OpenOrcaASE Models.Llama-2-13b-chat-hfGpt-3.5-turboe.g.touvron2023llama2mukherjee2023orcaOpenOrcabrown2020language, ouyang2022trainingtransformerswolf-etal-2020-transformersAnalysis of the resultssec:analysis: How do LLMs compare w.r.t. evaluation methods, both human and automatic?     ASE1: How does the Eval-Prompt influence the consistency and distribution of LLM ratings?     ASE2: How explainable is the evaluation performed by LLMs?     ASE3: Relying on ASE results, how do LLMs perform at ASG?     ASG1: How does pretraining data help predict ASG performance? ASG2ASE1: Comparison with Current Evaluation Measuressub:ase1_analysisAutomatic Annotation Consistencyssub:icc1cohen1960coefficient, fleiss1971measuringhayes2007answeringhallgren2012computingaverage random ratersvallat2018pingouinrandomtab:icc1e.g.Correlations with Human Annotationsssub:correlationsfig:story_level_kendall_mixed1_correlationsOverall Correlations (\autoref).System-level Correlations (\autoref).BARTScoreIntra-class coefficients type 2k for Beluga-13B ratings with 95\% confidence interval. Higher is better.tab:icc2Average Likert ratings per LLM per Eval-Prompt. The asterisk signals the fact that ChatGPT was only asked to rate the original \texttt dataset without Llama-generated stories. Higher is better.tab:average_ratings_per_llmStatistical Testing.fig:williams_belugaBARTScoreBERTScoreTakeawaysASE2: Influence of the Eval-Promptsub:ase2_analysisInfluence on Consistencysub:influence_icctab:icc2Influence on Ratingsssub:influence_ratingstab:average_ratings_per_llmInfluence on Correlationsssub:influence_correlationsOverall Correlations (\autoref).System-level Correlations (\autoref).TakeawaysASE3: Explainability of Ratingssub:ase3_analysisi.e.sub:ase1_analysisVisualization of Explanation EmbeddingsSentenceTransformersreimers-2019-sentence-bertmcinnes2018umapfig:umap_explanationKeyword Analysiscampos2020yaketab:explanation_keywordsfig:umap_explanationUser Study on LLM Explanationsssub:user_study_resultsssub:user_studytab:error_ratesgwet2008computing, fergadis2022irrcacwongpakaran2013comparison40\% of all Eval-Prompt 3 ratings are not supported by an explanationsec:discussionTakeaways.Average Beluga-13B ratings for Eval-Prompt 1 with 95\% confidence interval. Higher is better.tab:average_beluga_ratingsAverage Mistral-7B ratings for Eval-Prompt 1 with 95\% confidence interval. Higher is better.tab:average_mistral_ratingsASG1: LLM Performance in ASGsub:ASG1_analysisHANNAfig:system_level_kendall_mixed1_correlationstab:average_beluga_ratingstab:average_mistral_ratingsHANNATakeaways.ASG2: Influence of Pretraining Data on ASG Performancesub:ASG2_analysismagar-schwartz-2022-datae.g.shi2023detectingshi2023detectingModel Contamination.fan2018hierarchicalHANNAtab:wp_contaminationData Reproduction.shi2023detectingtab:book_auci.e.Takeaways.e.g.Discussion on LLM performancesec:discussionmahowald2023dissociatingbubeck2023sparkskahneman2011thinkingchhun-etal-2022-humanmahowald2023dissociatingbubeck2023sparksConclusionssec:conclusionsPractical Takeawayssub:practical_takeaways In particular, LLMs display very high system-level correlations with human judgment;     Used with prompts based on specific criteria, LLMs are currently the best proxy for human evaluation of story generation (\autoref). exhibiting very high intra-class coefficient values;     LLMs are remarkably self-consistent (\autoref), they struggle to explain their answers with substantiated claims;     LLMs understand the ASE task only partially (\autoref): Providing a human story for reference (Eval-Prompt 4) yields mixed results;     For ASE, providing detailed guidelines (Eval-Prompt 3) did not lead to improved correlations with human ratings (\autoref). with larger LLMs exhibiting the best performance;     LLM stories have at least equal ASE ratings to human stories (\autoref), the higher ratings of larger LLMs may be due to their ability to produce output similar to existing books. Pretraining data helps explain LLM performance at ASG (\autoref):Limitations and Future Directionsaveragee.g.Acknowledgmentstaclacl_natbib