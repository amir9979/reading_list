We utilize CoT Prompting  to extract CoTs for a raw dataset  from LLMs, where  is the question and  is the golden answer. Specifically, we first create a CoTs Extraction Prompt  that contains several human-curated question-CoTs pair examples and the task description, which can be found in Appendix . For each , we extract CoTs as:

where  means concatenation. Then, we classify the CoTs annotated dataset into two datasets according to the final answer's correctness in the annotated CoTs, same as Zelikman et al.. One is the CoTs-original correct dataset   and the other is CoTs-original wrong dataset .

We define dual CoTs data as contrasting CoTs that follow similar reasoning steps but reach divergent conclusions compared to the original. To provide a deeper understanding, we also present several examples of dual CoTs in Appendix . In the following, we will introduce how to generate dual CoTs datasets including  contrasting to , and  contrasting to .

 To generate correct CoTs contrasting with the originally wrong CoTs, inspired by Rationalization , we design an Answer Hint Prompt  that shares the same examples with  but with different organizational structures. The template of  can be found in Appendix . Each example in the context and the final provided question will be inserted with a hint that tells LLMs the answer first before CoTs. Thus, due to the same in-context examples and hint answers, teacher LLM can rectify its original wrong CoTs data with similar reasoning steps but correct answers. For each , we rectify CoTs as follows and then have the Rectified CoTs dataset :  To generate incorrect CoTs contrasting with the originally correct CoTs, a straightforward approach is to use AHP with incorrect hint answers to prompt LLMs to produce wrong CoTs. However, in practice, we find that LLMs rarely follow the incorrect hints and still generate correct CoTs. This may be due to the simplicity of the questions, which fall within the LLMs' knowledge range. Additionally, LLMs, having undergone Reinforcement Learning from Human Feedback (RLHF) , may resist providing unhelpful answers. Therefore, we design a Contrastive CoTs Prompt () to entice LLMs to generate incorrect CoTs, leveraging their strong in-context learning capabilities. The prompt template can be found in Appendix . Specifically, to ensure high-quality incorrect CoTs, we randomly sample negative examples from  and positive examples from , pair them, and place them into the CCP as curated joint in-context examples. For each , we corrupt CoTs as follows and then have the corrupted CoTs dataset :  After preparing the dual CoTs, we first fine-tune student models on the teachers' original correct CoTs dataset  and rectified CoTs dataset . The training objective is as follows:

where the merged correct CoTs dataset , and  denotes the student with the base inference ability after the initial fine-tuning.

 Inspired by  who leverage fine-grained quality signals to align human preference, we propose a key reasoning steps learning (KRSL) method to further encourage students to comprehend the reasons behind both correct and wrong CoTs from the teacher.

 We pair the teacher's original correct CoTs dataset  with its corrupted CoTs dataset , creating an originally correct dual CoTs dataset , where  and  are dual to each other; similarly, the teacher's inherently wrong dual CoTs dataset . By merging them, we obtain the ultimate dual CoTs datasets , which is prepared for the subsequent learning of key reasoning steps.

 Then we employ the minimum edit distance to identify the key steps in both correct reasoning and wrong reasoning, as shown in Figure .  In this way, students can identify less frequent text segments that are inserted or replaced in wrong CoTs compared to correct CoTs, and vice versa. These text segments are considered key reasoning steps. After that, we assign token-level weights to facilitate fine-grained learning for correct CoTs and wrong CoTs in  respectively:

where ,  and  represents the weight of -th token in the correct CoTs (semantically same with ). We set the weights to zero to ignore the impact of identical tokens in the dual CoTs.

 Finally, to ensure that the student makes correct decisions on key steps in correct reasoning, we optimize the student model on these tokens with weighted negative log-likelihood. Conversely, to prevent the student from making key steps present in wrong reasoning, we optimize the student model on these steps with weighted positive log-likelihood. The sum of both is taken as the final loss. The optimization objective is as follows:

where  consists of 27 challenging tasks that span arithmetic, symbolic reasoning, etc. This collection is mainly composed of multiple-choice questions, along with a minority of open-ended questions. To underscore the superiority of our method, we divide the BBH dataset for each subtask into a training set (BBH-train) for distillation and a test set (BBH-test) for in-domain evaluation, following a 4:1 ratio.

 is derived from the BIG-Bench (BB) , which includes 203 tasks covering linguistics, mathematics, common-sense reasoning, etc. To simplify our evaluation, we refine the selection of tasks from BB by identifying those associated with keywords such as "multiple-choice" and "reasoning." Additionally, we exclude any tasks that are part of the BBH dataset, narrowing our pool to 61 distinct subtasks. For each of these subtasks, we randomly sample up to 100 instances, culminating the BB-sub dataset.  is a benchmark that assesses LMs on reasoning capabilities using human exams across various fields, including English, Math, Law, and Logic. We focused on the English multiple-choice questions within this benchmark to evaluate our method's effectiveness.  comprises asy and hallenge from middle and high school science exams. ARC-E features simpler questions, while ARC-C includes more challenging ones. We use their test sets for evaluation. Detailed statistics for all mentioned benchmarks are provided in Appendix .

 We employ the modern and widely-used open-source language model, LLaMA2-7B , as our student SLM. For the teacher model, given its performance and cost-effectiveness, we employ OpenAI's advanced black-box LLM, ChatGPT, specifically using the "" variant for extracting CoTs with the same manual prompt that is used in . We employ LoRA  for parameter-efficient fine-tuning of the student SLMs. We empirically set  in KRSL as 1.0 and  as 0.025. Our experiments leverage a mixed-precision training strategy, carried out on 4  A100 GPUs. We employ vLLM to enhance inference speed, using a greedy decoding method for text generation on a single A100 GPU. More training details and hyperparameter settings can be found in Appendix .

 We compare EDIT with the following baselines: (1)  under various settings, e.g., Zero-shot (+ CoT) or Few-shot (+ CoT). (2) , which is a standard CoTs distillation method that directly fine-tunes student SLMs on CoTs data. (3)  is a multi-task CoTs distillation strategy that aims to optimize both the prediction of answers and the learning of CoTs concurrently. (4)  aims to bolster the reasoning consistency in the student SLMs by integrating counterfactual data into its training regimen. To ensure a fair comparison and ablate the impact of increased data sizes due to dual CoTs, we implement two additional settings: (5) . We perform random repeat sampling on the baseline's original training data until the volume matches that of EDIT; (6) . We train the Std-CoT using all data included in EDIT, adding the marker "" before the negative sample's question to differentiate it from positive reasoning.

We compare EDIT with the baselines across both IND and OOD datasets in Table  and illustrate the results by answering the following research questions.

 From the table, it is evident that the student SLMs with distillation outperform those that were not distilled. This demonstrates that the reasoning ability of LLMs can be effectively transferred to SLMs by distilling CoTs.

 It can be observed that our proposed method EDIT outperforms the distillation baselines on both IND and OOD datasets, achieving an average improvement of 4.7 \% compared to the standard CoT distillation (Std-CoT), which demonstrates the effectiveness and generalizability of EDIT.

 Ablation results in the table show that removing the rectified wrong CoTs (w/o RWC) and removing key reasoning steps learning (w/o KRSL) result in performance degradation on almost all IND and OOD, emphasizing the importance of both components. On the one hand, the rectified teachers' mistakes aid the students in learning diverse ways of thinking. On the other hand, KRSL directs the student's attention to crucial steps in the dual CoTs, thereby improving the reasoning ability of the students. Additionally, we note that although KRSL and DPO   share very similar learning principles, DPO performed unexpectedly poorly in this scenario. Detailed experiments and analyses are provided in Appendix .

 We compare EDIT with baselines (6) and (7) to ensure a fair comparison due to the twice data amount of dual CoTs. Results in Table  show that while Std-CoT benefits from additional data, it underperforms compared to EDIT across most tasks. EDIT's superiority stems from its method of learning key reasoning steps beyond mere imitation, allowing students to learn from mistakes. Additionally, Std-CoT with Dual CoTs outperforms that with Repeat Sampling in OOD tasks by incorporating counterfactual reasoning, reducing overfitting and better generalizing the reasoning. This supports our view that simple fine-tuning with correct teacher data is insufficient for true reasoning learning.

 To better adapt to the community's varying computational resource requirements, we conduct experiments on models of different sizes, including TinyLLaMA-1.1B, LLaMA2-7B and 13B. The results in Figure  show that EDIT outperforms the baselines across different model sizes. Particularly on benchmarks with broader evaluation dimensions such as BB-sub and AGIEval, significant improvements are observed regardless of the model size. This suggests that the more challenging a task is, the more it requires genuine reasoning rather than mere imitation, highlighting the benefits that EDIT brings to student SLMs.

 To cater to the community's diverse model preferences, we conduct experiments on models of different architectures, including CodeLLaMA-7B  , LLaMA3-8B  , and Mistral-7B-v0.2  . As shown in Figure  (middle), EDIT consistently outperforms its variant w/o KRSL and the baseline Std-CoT across all model architectures. Notably, the performance gap is significantly larger for the stronger model, Mistral, indicating that our method provides greater benefits with more powerful base models.  We conduct an ablation study on the key reasoning steps in KRSL where students learn exclusively from either the correct or wrong reasoning steps (referred to , we set  or , respectively). The results shown in Figure  (left) indicate that learning key reasoning steps solely from either correct or wrong CoTs leads to a decline in performance. This demonstrates that joint learning from both correct and wrong key reasoning steps is more beneficial for enhancing students' reasoning capabilities. Furthermore, we observe a greater performance drop in the absence of key steps in correct CoTs (w/o Correct) compared to the absence of key steps in wrong CoTs (w/o Wrong), suggesting that key steps from correct CoTs have a more significant impact on students' learning.

 We also explore which component of the dual CoTs dataset in KRSL plays a more significant role: the originally correct dual CoTs  or the inherently wrong dual CoTs . From the Table , compared to using , employing  resulted in superior performance, even with less data, which demonstrates that  has higher data quality compared to . The dual CoTs constructed from the inherent wrong CoTs of teachers more effectively highlight the key steps in reasoning.

Beyond accuracy in reasoning, the quality of CoTs is crucial for interpretable AI. Therefore, we leveraged the sota LLM, GPT-4, to score the quality of CoTs generated by Std-CoT, EDIT, and teacher LLMs. The evaluation focused on which CoT best reflects the key reasoning steps in the problem-solving process, with the prompt template detailed in Appendix . The distribution of the evaluation scores is shown in Figure  (right), where we observe that the score distribution for CoTs generated by EDIT is closer to that of the teacher compared to Std-CoT. This illustrates that EDIT is more effective in learning the key reasoning steps, resulting in the production of high-quality CoTs.

To more clearly show the quality of key reasoning steps in generated CoTs, we present 5 cases sampled from BBH, AGIEval, and ARC, compared with Std-CoT and teachers, as detailed in Appendix . Tables  and  show that the reasoning form of the student SLMs distilled by Std-CoT is very similar to that of the teacher. However, the student SLMs distilled by EDIT exhibit a changed way of thinking, leading to the correct answers. Table  reveals nearly identical reasoning among the three, yet in the critical reasoning steps 7 and 8, Std-CoT fails to make the correct decisions, whereas EDIT correctly executes stack operations. Cases from OOD datasets, shown in Tables  and , indicate that EDIT can accurately analyze problems and provide more logical reasoning.

In this subsection, we delve into the influence of various mistake patterns on the EDIT. Based on the observation of mistake data, we utilize GPT-3.5 to categorize them into four types, including , ,  and . The results of EDIT trained on these mistake patterns are shown in Table . We can see that KRSL on  consistently outperforms other mistake patterns, with KEs and MCEs having a relatively smaller impact. This suggests that LEs provide a broader range of reasoning patterns that are relevant for mathematical, commonsense, and symbolic reasoning. As for KEs and MCEs, since these types of mistakes are more specific compared to LEs, it is not easy for the model to learn a general reasoning solution from these mistakes. Therefore, learning the key reasoning steps from logical reasoning errors is the most effective way among them.

The results of the model size ablation study on IND datasets are presented in Figure . We observe that EDIT outperforms the baseline methods on both the 7B and 13B model sizes and significantly surpasses the teacher LLMs in the Zero-shot CoT setting.

Here we show 5 cases in Table , , ,  and  to clearly compare the CoT generated by EDIT with the teacher LLM and the standard CoTs distillation (Std-CoT). We utilize  and  to denote whether the CoT is correct or incorrect, respectively.

We ask  to classify all the teacher's wrong CoTs and list the statistic result for mistake pattern data in Table .   To fairly assess the influence of different single mistake patterns (LEs, KEs and MCEs), we ensure consistency in data size and the proportion of challenging problem data () for each pattern. Since the available data for MCEs is the smallest, we randomly select 356 instances from  and 56 instances from , creating three dual CoT datasets—, , and —each with 412 samples. Then we conduct experiments using these datasets in KRSL and the results are shown in Table .

We note that the learning objectives of KRSL, utilizing both positive and negative examples, closely resemble preference alignment algorithms like RLHF and DPO . Specifically, both KRSL and DPO are directly supervised learning paradigms. However, there are key differences:

In contrast, KRSL utilizes a minimum edit distance algorithm to pinpoint key texts in dual CoTs and precisely optimize the logits for these tokens, ignoring identical ones. This makes KRSL more suitable for learning from dual CoTs compared to DPO. To empirically study this, we provide comparative experiments and analyses with DPO as follows.

We compare KRSL with DPO by implementing DPO in the EDIT and training LLaMA2-7B on complete dual CoTs data using the  implemented in the TRL , with the following settings:  of 1e-5, a , a  ratio of 0.3,  of 0.1, a  of 512,  of 1024, 10 training , and a  of 16. The results (Table ) show significant performance degradation with DPO. Thus, we check the model's generation results in Table  and find that the output pattern almost completely collapses, outputting only the answer without the intermediate reasoning process. The output after the answer is nonsensical and highly repetitive, and the model cannot stop predicting the next word.

Table , ,  and  show the data statistics of AGIEval, ARC, BIG-Bench Hard (BBH) and BIG-Bench Sub (BB-sub), respectively.

In our study, we ensure consistency in the hyperparameter settings across all baselines, including our proposed EDIT approach, to maintain the fairness of our comparative analysis. Here, we detail the hyperparameter configurations employed in our experiments.

 The number of training steps is determined based on the size of the training dataset, the batch size, and the number of gradient accumulation steps required. We maintain a consistent batch size across all baselines to eliminate any performance discrepancies that could arise from varying batch sizes.

 Our initial exploratory experiments focused on the standard CoTs distillation method using the LLaMA-2 model. We found that while the batch size had minimal impact on performance, the learning rate was a critical factor. We tested learning rates of 1e-4, 2e-4, and 3e-4, observing optimal performance at 2e-4 across the standard CoT and other distillation baselines, as well as our EDIT approach. Consequently, we set the learning rate to 2e-4 for all methods involved in our study.

 Throughout our training process, we monitored the training loss curve and noted that it generally plateaued by the 15th epoch, indicating that the models had achieved convergence. Therefore, we set the number of epochs to 15 for 7B models. The process of determining the number of epochs for other model sizes followed a similar pattern. To mitigate the potential risk of overfitting and to ensure our evaluation reflects the most effective model configuration, we systematically selected checkpoints from the epoch that demonstrated the best performance on the IND task. These checkpoints were then used to evaluate performance on OOD tasks.

The hyperparameters in training and inference can be found in Table  and Table  respectively. In the KRSL, the second phase training in EDIT, the learning rate is empirically set as 5e-6.

Our experimental code is based on modifications of Meta's open-source , utilizing the FSDP framework and training the model in parallel on four 80GB A100 GPUs. In our experimental setup, training a 7B model during the SFT stage takes approximately 40 minutes per epoch. For KRSL, each epoch takes around 90 minutes. With the same settings, training the Mistral model will see about a 10\% increase in training time. We will release our code in the future.

We use the prompt template shown in Table  to call the ChatGPT API to generate the CoTs for the BBH-train datasets.

We list the Answer Hint Prompt templates in Table , which imply the teacher LLMs to generate the CoTs based on the given answers following the in-context examples.

We list the Contrastive CoTs Prompt templates in Table , which query the teacher LLMs to generate the CoTs with similar rationales to the original ones but divergent answers by following the few examples provided with contrastive CoT pairs.

We list the evaluation prompt templates of CoTs quality in Table .

For mistake pattern mining, we employ the prompt template delineated in Table , which includes the definitions of the four distinct mistake patterns. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \clearpage% \newpage% % %%% BEGIN INSTRUCTIONS %%%% The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page% limit. % Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:% %     \item You should answer , , or .%     \item  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.%     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). %    % \item {\bf The papers not including the checklist will be desk rejected.}% % {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.% The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "" is generally preferable to "", it is perfectly acceptable to answer "" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "" or "" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer  to a question, in the justification please point to the section(s) where related material for the question can be found.% IMPORTANT, please:% %     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},%     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}%     \item {\bf Do not modify the questions and only use the provided macros for your answers}.%  % %%% END INSTRUCTIONS %%%% % \item {\bf Claims}%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: Our claim have been made in the abstract and reflect our paper's contribution and scope.%     \item[] Guidelines:%     %         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. %         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. %         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. %     % \item {\bf Limitations}%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: We have discussed the limitations of our work in appendix.%     \item[] Guidelines:%     %         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. %         \item The authors are encouraged to create a separate "Limitations" section in their paper.%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.%     % \item {\bf Theory Assumptions and Proofs}%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This article does not involve any theory or proof.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include theoretical results. %         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. %         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. %     %     \item {\bf Experimental Result Reproducibility}%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: We provide detailed experimental settings, including hyperparameter settings and model checkpoint selection, etc., in the appendix.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. %         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example%         %             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.%         %     % \item {\bf Open access to data and code}%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification:  The code is not included at this time but will be released after the review process to ensure reproducibility of our results.%     \item[] Guidelines:%     %         \item The answer NA means that paper does not include experiments requiring code.%         \item Please see the NeurIPS code and data submission guidelines () for more details.%         \item While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines () for more details.%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.%     % \item {\bf Experimental Setting/Details}%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification:  We have provided these details in the experiment section and appendix.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.%         \item The full details can be provided either with the code, in appendix, or as supplemental material.%     % \item {\bf Experiment Statistical Significance}%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: In preliminary experiments, due to the cost of large models, we tested our method with three different random seeds to assess the impact on experimental results. We found that the variations were minimal () and did not affect the overall conclusion, namely that our proposed method significantly outperforms the baselines. Therefore, the error bars are likely to be minor.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)%         \item The assumptions made should be given (e.g., Normally distributed errors).%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.%     % \item {\bf Experiments Compute Resources}%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: We provide the computational resource consumption in the appendix.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. %         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). %     % \item {\bf Code Of Ethics}%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: Our work is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.%     \item[] Guidelines:%     %         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).%     % \item {\bf Broader Impacts}%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This work does not have any potential societal impact.%     \item[] Guidelines:%     %         \item The answer NA means that there is no societal impact of the work performed.%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).%     % \item {\bf Safeguards}%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This paper does not release any dataset or models.%     \item[] Guidelines:%     %         \item The answer NA means that the paper poses no such risks.%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. %         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.%     % \item {\bf Licenses for existing assets}%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: We strictly obey the license of used assets.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not use existing assets.%         \item The authors should cite the original paper that produced the code package or dataset.%         \item The authors should state which version of the asset is used and, if possible, include a URL.%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,  has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.%     % \item {\bf New Assets}%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This paper does not release any new assets.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not release new assets.%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. %         \item The paper should discuss whether and how consent was obtained from people whose asset is used.%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.%     % \item {\bf Crowdsourcing and Research with Human Subjects}%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? %     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This paper does not involve crowdsourcing nor research with human subjects.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. %         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. %     % \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This paper does not involve crowdsourcing nor research with human subjects.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. %         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. %         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.%     % footnote0 As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion () of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak-riven key reasonng step distillaion (), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps. Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets. Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps. Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs. EDITEDITCode can be found at \urlIntroductionsec_introfew-shot-learners, hoffman-scaling-law, palm, openai2023gpt4few-shot-learnerswizardllm, lionalpacavicuna2023acl-teach-slm-reason, acl-lm-are-reasoning-teachers, specializing-slmemergent-ability, chain-of-thought-promptingacl-findings-distill-step-by-step, explanations-make-better, mind-s-mirrorWe calculated the edit distance and its average proportion in the overall sequence on the dual CoT dataset mentioned in our subsequent methods \S\ref.struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these stepsbad-case-exEDITEDITkey-step-exWe reveal a shortfall in the previous distillation methods, where the simple SFT paradigm may result in students mimicking the teacher's reasoning forms but making errors or omissions in key reasoning steps, thus diminishing the versatility of CoTs.     We propose mistake-driven key reasoning step distillation, which allows students to learn key reasoning steps from our specifically designed dual CoTs data, further improving reasoning.     Extensive experiments validate the effectiveness of our method across both IND and OOD datasets, showing that EDIT can reduce errors in key reasoning steps for students.     We investigate how different mistake patterns impact EDIT and find that logical errors provide the more significant benefits than knowledge or mathematical calculation errors. Related Workssec_relatedCoT Reasoning.palm, emergent-abilitychain-of-thought-promptinglet-s-think-step-by-step, self-consistency, language-models-can-self-improveemergent-abilityflan-t5Knowledge Distillation from LLMs.hinton-kdopenai2023chatgptalpaca, vicuna2023, intruct-tune-with-gpt4, lionacl-teach-slm-reason, acl-lm-are-reasoning-teachersexplanations-make-better, acl-findings-distill-step-by-step, mind-s-mirrorspecializing-slmtailored-learning-slmLearning from Mistakes.reflexionstudy-asssistantICLR24COKscottmistake-gpt4-correctllama2wizardllmopenai2023gpt4width=\linewidthimg/EDIT-overview.pdf\textbf (1) We first retain all CoTs data annotated by teacher LLMs (2) and ask teacher LLMs to generate dual CoTs data using our designed two comprehensive prompts. (3) Then we fine-tune student SLMs on both original correct and rectified-after CoTs data. Finally, we apply key reasoning step learning on the pre-tuned student SLMs by identifying the minor difference between the dual CoTs.EDIT-overviewMistake-driven Key Reasoning Step DistillationmethodEDIT-overviewacl-teach-slm-reason, acl-findings-distill-step-by-stepCoTs Annotated by LLMscot-annotatechain-of-thought-promptingappendix:gen CoTs     CoT \sim LLM\left(\oplus q \right) To support our assumption of CoT correctness, We randomly sample 100 examples to manually check the logical consistency between the CoT and the final answer and find that the CoTs generated by ChatGPT generally support the final answer.starDual CoTs Generationappendix: ex of dual cotsRectify Wrong CoTs.starappendix:icl-answer-hint-prompt     CoT^{-+}_{} \sim LLM\left(\oplus q \oplus a \right) Corrupt Correct CoTs.rlhfCEPappendix:icl-contrasting-cots-prompt     CoT^{+-}_{} \sim LLM\left(\oplus q \oplus CoT^{+}_{} \right) Training Student with CoTsEDITSurpervised Fine-tuning on Correct CoTs.     \pi_{sft} = \arg\max_{\pi} _{q, CoT \sim ^{+}_{merge}}\left[\log \pi(CoT \mid q)\right] Key Reasoning Steps Learningfiga\textit\textitkey-step-ex

    \omega^{+}_{t} &=              \alpha, & \\         0, &  \\     ,      ^_ &=     . cases, &          0, &  \\     if  is deleted or replacedotherwisealignedeq_weights\textit     \max_{\pi_{sft}} _{q, CoT^{+}_{}, CoT^{-}_{} \sim ^{}_{dual}}     \quad\left[(\pi_{sft}, q, CoT^{+}_{}, \omega^{+}_{}) - (\pi_{sft}, q, CoT^{-}_{}, \omega^{-}_{})\right]

    \left(\pi, q,CoT,\omega\right)=-\sum_{CoT_t\in CoT}\omega_t\log \pi(CoT_t\mid q,CoT_{<t})

ExperimentsExperimental SetupIn-domain (IND) Dataset: BIG-Bench Hard (BBH)bbhOut-of-domain (OOD) Dataset: (1) BIG-Bench Sub (BB-sub)bb \url.(2) AGIEvalagieval(3) AI2 Reasoning Challenge (ARC)arcARC-EARC-Cappendix:data-statModels \& Implementation Details.llama2gpt-3.5-turbo-0613bbhlora\urlvllmappendix:hyperparameterBaselines.Teacher \& Vanilla StudentStd-CoTacl-teach-slm-reasonMT-CoTexplanations-make-betterSCOTTscottStd-CoT w/ Repeat SamplingStd-CoT w/ Dual CoTs\linewidth!

Results (Accuracy, \%) of the main experiment. w/o RWC represents that student models are distilled without using the rectified teacher's wrong CoTs in the first step of EDIT and w/o KRSL denotes that the second step KRSL in EDIT is removed. The improvements of EDIT and its variants, w/o RWC and w/o KRSL, over the average best baseline are indicated by subscripts.tab:main-resultMain Resultstab:main-resultCan CoT distillation improve the performance of students?Can EDIT further enhance the performance of students compared to other distillation methods?How significant are the improvements in EDIT attributed to the rectified wrong CoTs and the key steps learning, respectively?DPOappendix:comparision with dpoIs the increased training data / compute cost a key factor in EDIT's effectiveness?tab:main-resultAblation StudyEDIT is universally applicable to SLMs with various sizes.\urltinyllamafig:ablation-on-model-size-oodEDIT is universally applicable to SLMs with various architectures.llama2llama3mistralfig:3width=\linewidthimg/ablation-on-four-model-size.pdfAblation results on model size for four OOD datasets. The dotted line indicates the performance of the teacher LLM under the Zero-shot-CoT setting. Due to the space limitation, we present the results on the IND dataset in Appendix \ref.fig:ablation-on-model-size-oodCorrect key reasoning steps have a greater impact than incorrect ones.EDITfig:3The quality of 4- dual CoTs data is more important than quantity.tab:krsl data sourceAnalysisQuality of Generated CoTs: From the Perspective of Key Reasoning Stepsappendix:eval promptfig:3Case Studyappendix:case studytab:bbh-reasoning-colored-obj-casetab:bbh-movie-recommend-casetab:bbh-dyck-lang-casetab:agieval-aqua-rat-casetab:arc-caseMistake Pattern MiningLogical Errors (LEs)Knowledge Errors (KEs)Mathematical Calculation Errors (MCEs)Other Errors (OEs)tab:result of mistake patternConclusionplainmainLimitationsDue to the considerations of costs, such as API calls and GPU training expenses, we only use ChatGPT as the teacher LLM and the widely available open-source model LLaMA2 as the student. Employing GPT-4 as the teacher to provide high-quality examples of annotated CoTs and dual CoTs could better demonstrate the effectiveness of our proposed method.     Currently, most assessments of CoT distillation focus primarily on accuracy , which is insufficient because safe LLMs rely heavily on trustworthy CoTs. We hope the community to develop standards for evaluating the quality of CoTs, rather than relying solely on automatic assessments by GPT-4. acl-teach-slm-reason, acl-lm-are-reasoning-teachers, decompose-question-aclfindings, tailored-learning-slmExample of Dual CoTsappendix: ex of dual cotstab:example of cot on lestab:example of cot on kestab:example of cot on mcestab:example of cot on lesA casual judgment dual CoTs example from BIG-Bench Hard where the wrong CoT shows a logical error.tab:example of cot on les\arraystretch1\arraystretch1A movie recommendation example from BIG-Bench Hard where the wrong CoT shows a knowledge-based error.tab:example of cot on kes\arraystretch1\arraystretch1A multistep arithmetic dual CoTs example from BIG-Bench Hard where the wrong CoT shows a mathematical calculation error.tab:example of cot on mces\arraystretch1Additional ExperimentAblation Study on Model Size for In-domain Datasetappendix:ablation on model size for INDfig:ablation-on-model-size-ind\arraystretch1A failure case in EDIT w/ DPO from BIG-Bench Hard.tab:dpo failure case\arraystretch1Case Studytab:bbh-reasoning-colored-obj-casetab:bbh-movie-recommend-casetab:bbh-dyck-lang-casetab:agieval-aqua-rat-casetab:arc-case\Large\textbf\Large\textbfappendix:case studyMistake Pattern Miningappendix:ex mistake pattern mininggpt-3.5-turbo-0613tab:stat-mistake-patterntab:result of mistake patternKRSL v.s. DPOappendix:comparision with dpoDPOKRSL requires the model to learn from highly similar positive and negative samples (dual CoTs) for identifying key reasoning steps while DPO usually uses completely different positive and negative samples from human preference data.     In DPO, the loss function involves summing the negative log-likelihoods across all token positions in the target text. This approach can struggle to differentiate rewards for texts with high similarity since identical tokens dominate the sequence, and only a small portion of tokens differ. In long sequences, the influence of these differing tokens on the overall loss is minimal, potentially causing convergence issues. dpo\_trainerhttps://github.com/huggingface/trllearning ratecosine learning rate schedulerwarmupDPO betamaximum prompt lengthmaximum lengthepochsbatch sizetab:dpo v.s. krsltab:dpo failure caseExperimental DetailsDataset Statisticsappendix:data-stattab:agieval_sattab:arc_sattab:bbheval_sattab:bbsubeval_satClassification statistics of mistake data patterns.tab:stat-mistake-patternHyperparameters Settingsappendix:hyperparameterTraining Steps and Batch Size.Learning Rate.Epochs and Evaluation Strategy.tab:train-hyperparameterstab:infer-hyperparametersComputation Budgetllama-recipes\urlPrompt TemplatesCoTs Extraction Promptappendix:gen CoTstab:prompt-gen-CoTsAnswer Hint Promptappendix:icl-answer-hint-prompttab:Answer Hint PromptContrastive CoTs Promptappendix:icl-contrasting-cots-prompttab:Contrastive CoTs PromptEvaluation Prompt of CoTs Qualityappendix:eval promptgpt4-eval-promptMistake Pattern Mining Promptappendix:mistake-pattern-miningtab:prompt-mistake-pattern\arraystretch1A reasoning about colored objects case from BIG-Bench Hard.tab:bbh-reasoning-colored-obj-case\arraystretch1% {!}{\arraystretch1A movie recommendation case from BIG-Bench Hard.tab:bbh-movie-recommend-case\arraystretch1\arraystretch1A dyck languages case from BIG-Bench Hard.tab:bbh-dyck-lang-case\arraystretch1\arraystretch1A AQuA-RAT case from AGIEval.tab:agieval-aqua-rat-case\arraystretch1\arraystretch1A case from AI2 Reasoning Challenge.tab:arc-case\arraystretch1\{Task Description\}. Your response should conclude with the format "Therefore, the answer is".Q: \{Task Example Question No.1\}A: Let's think step by step. \{Human-Curated-CoTs No.1\}.Q: \{Task Example Question No.2\}A: Let's think step by step. \{Human-Curated-CoTs No.2\}.Q: \{Task Example Question No.2\}A: Let's think step by step. \{Human-Curated-CoTs No.3\}.Q: \{QUESTION\}A: Let's think step by step.CoTs extraction prompt template of gpt-3.5-turbo for generating the CoTs data.tab:prompt-gen-CoTs\{Task Description\}. Your response should conclude with the format "Therefore, the answer is".Q: \{Task Example Question No.1\}H: \{The correct answer is \}A: Let's think step by step. \{Human-Curated-CoTs No.1\}.Q: \{Task Example Question No.2\}H: \{The correct answer is \}A: Let's think step by step. \{Human-Curated-CoTs No.2\}.Q: \{Task Example Question No.3\}H: \{The correct answer is \}A: Let's think step by step. \{Human-Curated-CoTs No.3\}.Q: \{QUESTION\}H: \{The correct answer is \}A: Let's think step by step.Answer Hint Prompt templates for rectifying the wrong CoTs data based on the hint answers.tab:Answer Hint Prompt \\  \\  \\  \\ \{Task Description\}. You need to complete the  which requires you to give themost likely incorrect answer to the  and the rationale for the incorrect answer.The incorrect answer and rationale in the  must be different from the correctanswer and rationale in the .: \{Task Example Question No.1\}: \{Corrected CoT No.1\}: \{Wrong CoT No.1\}: \{Task Example Question No.2\}: \{Corrected CoT No.2\}: \{Wrong CoT No.2\}: \{Task Example Question No.3\}: \{Corrected CoT No.3\}: \{Wrong CoT No.3\}: \{USER\_QUESTION\}: \{Corrected CoT\}: Contrastive CoTs Prompt templates for mistaken the correct CoTs data. The examples are sampled from the teachers' original wrong CoTs data and its corrected CoTs. In this way, teacher LLMs can expose the reasoning flaws in problems that were originally solved correctly.tab:Contrastive CoTs Prompt \\  You are a helpful and precise assistant for assessing the quality of the response.: \{QUESTION\}: \{ANSWER\}\{ASSISTANT1\}\{ASSISTANT2\}\{ASSISTANT3\} We would like to request your feedback, in the form of scoring, on which of theresponses from AI Assistant 1, 2 and 3 effectively demonstrates the key reasoning steps insolving this question. Key Reasoning Steps refer to certain crucial steps in the process oflogical reasoning or problem-solving. These steps play a significant role in the thinkingprocess and have a notable impact on subsequent reasoning. Each student will receive anoverall score on a scale of 1 to 10, where a higher score signifies that the assistant'sresponse is more effectively demonstrates the key reasoning steps for the question.Please provide a comprehensive explanation, avoiding any potential bias and ensuring thatthe order in which the responses were presented does not affect your judgment. And thenoutput three lines indicating the scores for AI Assistant 1, 2 and 3, respectively.Output with the following format:Evaluation evidence: <your evaluation explanation here>Score of AI Assistant 1: <score>Score of AI Assistant 2: <score>Score of AI Assistant 3: <score>Prompt template of GPT-4 for assessing CoTs quality. In the analysis, we use this template to eval the quality of CoTs generated by Std-CoT, EDIT and the teacher LLM respectively.gpt4-eval-prompt You are a helpful assistant who is good at identifying types of reasoning mistakes.There are now three types of inference errors, as follows:(a). Logical reasoning errors. This type of error involves the logical structure of reasoning,including assumptions, reasoning rules, argument chains, etc. Among logical errors, studentsmay make errors such as invalid reasoning, insufficient or incorrect assumptions, and jumps inreasoning. Students may make errors in selecting reasoning strategies or methods. The chosenmethod may not be suitable for a specific problem, or may lead to misleading reasoning.(b). Knowledge errors in reasoning. This type of error involves misunderstanding or incompleteunderstanding of facts, concepts or knowledge, conceptual confusion, and cognitive biases.(c). Numerical calculation errors. This type of error involves mathematical calculation errors,which may include incorrect calculations, conversions or errors in the processing of numericalvalues.(d). Other errors. All other errors that do not belong to the above three categories.I will give you a dictionary with the following fields and meanings:\{\quad "input": reasoning question.\quad "right\_output": the correct answer.\quad "wrong\_output": the wrong answer.\}You need to first form your own opinion about the problem based on the reasoning questions and thecorrect answers, and then analyze the reasons for the mistakes in the wrong answers in "Rationale:".Then give your classification results in "Category:", e.g., (a), (b) or (c), etc. If an answerinvolves errors in multiple categories, you should point them out and connect them with '+' signin the category. For example, if an answer involves logical errors and mathematical calculationerrors, then the category should be a+c.You must output with the following format:Rationale: <your analysis process and explanation of the final classification results>Category: <only fill in with a or b or c or a+b or a+c or b+c or a+b+c or d.>Prompt templates of GPT-3.5 for classifying the mistakes. In the analysis, we use this template to classify the mistake data used in EDIT.tab:prompt-mistake-pattern