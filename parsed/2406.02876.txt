In bilingual neural machine translation (NMT), given a source sentence with  tokens  and its target sentence with  tokens , %  and NMT models are generally optimized by the cross-entropy loss: % %  where  is the index of each decoding step,  is the target-side previous context for , and  represents the model parameter.

While in MNMT, for the language pair ,  where  and  represent the source language  and the target language , and  denotes the target language tag, the MNMT models are generally optimized by the cross-entropy loss: % 

In this paper, we focus on the analysis of the placement of  and explore the better way to indicate the target language for zero-shot translation.

Pioneered by , the LT strategy has become the  strategy to build the unified MNMT model. %  Recently, many varieties are proposed to adjust the placement of the LT and we list several popular samples in Tab.. %  Among these strategies, the T-Enc and S-Enc-T-Dec strategies are the most widely used ones to build MNMT models. %  The first proposed T-Enc strategy performs the best on zero-shot translation . %  And S-Enc-T-Dec is also widely used in establishing MNMT models, , M2M100 , mBART , and so on. %  Although some studies  propose to place double tags on the encoder side, the capability of indicating the target language still remains insufficient . %  Therefore, this paper mainly focuses on the two most widely used strategies,  T-Enc and S-Enc-T-Dec, to investigate the influence of the placement of LT.

The  issue describes the wrong target languages translated by the MNMT models on zero-shot translation . %  Prior studies  reveal that the spurious correlations between language pairs within supervised data aggravate this issue and make efforts to overcome it in terms of adjusting training strategy, modifying the residual connection and generating auxiliary data. %  Besides,  points out that data noises also make the  issue, and  enhance the model's awareness to the vocabulary of target language during generation. %  However, the cause of fundamental language tag strategies, which perform various on this issue, remains unclear.

To quantify this issue, we adopt the language rate as the metric to observe the error language distribution of zero-shot translation. %  We adopt the langdetect toolkit to identify the language of generated sentences, following prior studies . %  And we calculate the language rate of language  as follows:

where  denotes the -th generated sentence,  denotes the scale of the test set, and  denotes the language detect function. %  The  denotes the language rate of the desired target language, where the higher language accuracy denotes the slighter  issue.  %  Generally, language accuracy is regarded as an indicator of the performance of zero-shot translation, where the higher accuracy usually guides to the better performance.

To further observe the variation of language rate throughout decoding steps, we calculate the rate of continuous intervals with 5 words in it along generated sentences. %  To accurately detect the language of short text, we choose the  toolkit, which has higher accuracy in detecting the short texts. % We calculate it as follows:% % %     (^{(i)}) = \lceil \vert ^{(i)} \vert / len \rceil,\\% %  % % %     (\ell) = ^{N}\sum_{j=1}^{num(^{(i)})}_{lang(_{j}^{(i)}) = \ell}}{\sum_{i=1}^{N} (^{(i)})} \\ \times 100\%,% % % where  denotes the -th generated sentence with  words, and  denotes the -th segment in  with  words,  denotes the scale of the test set, and  denotes the language detect function.% We use the off-the-shelf  toolkit to identify the language of generated sentences, where  is set to 1,  following existing studies. % We define the language rate of the desired target language as , where the higher language accuracy denotes the slighter  issue. %   In this section, we first conduct experiments on the OPUS-100 dataset, which contains 100 languages, to count the error distribution of the  issue. %  Prior study  points out that the noise in data is an important factor of the  issue. %  Thus, we also count the error distribution with the denoised data, which filters the noise language pairs with target sentences in wrong languages, following .

As shown in Tab., the mainstream LT strategies suffer from the serious  issue. %  Most mainstream LT strategies endure the grave  issue, which ranges from 39.40\% to 57.55\% even after data denoise, signifying that the language generation is disturbed by English. %  Besides, the T-Enc and ST-Enc strategies tolerate the severer  issue than other strategies, indicating that the target tag on the encoder side may be mixed with the indication to the source language. %  Error distributions on both data settings suggest that the placement of LT has a non-ignorable impact on the  issue. %  To understand the impact of LT better, we explore the language variation of language generation and the encoder's modeling tendency within different LT strategies. %  Specifically, we employ the widely-used T-Enc and S-Enc-T-Dec strategies on the denoised OPUS-100 dataset to conduct analysis experiments.

In this section, we explore the variation of language indication in generation, by calculating the fine-grained language rate throughout decoding steps. 

As shown in Fig., in terms of language accuracy, the S-Enc-T-Dec strategy exhibits a considerable decrease of approximately 20\%. %  Besides, as shown in Fig., the rate of mistranslating into English of S-Enc-T-Dec increases by around 20\%. %  These variations signify that the indication from the S-Enc-T-Dec strategy is rapidly degraded and switched to English after generating a few tokens. %  Hence, we conclude that the language indication of the S-Enc-T-Dec is decreasing after a few tokens, resulting in the bias to the most common language in the training set, , English. %  Conversely, the T-Enc strategy exhibits a more sufficient and stable language indication throughout the decoding steps. %  Comparing both the above strategies, we conclude that the indication from the encoder could be more sufficient and stable and avoid being diminished during generation, while this setting suffers from the  issue.

To invest the target language indication in the encoder and the  issue, we focus on two questions:  %  1) %  2) To answer the first question, we visualize the variation of language representation along the encoder layers, by calculating the similarity between the source and target languages in the zero-shot sentence pairs on the OPUS-100 dataset. %  Following , we adopt the average-pooled encoder layer output as the sentence representation, and then calculate their cosine similarity. % 

As shown in Fig., for the 6-layer encoder, the similarity scores of both LT strategies maintain the upward trend from the 1st layer to the 5th layer, and drop in the 6th layer, which is consistent with . %  The variation suggests that the encoder tends to generate language-agnostic representation from different languages first and then reduce the similarity across languages. %  In fact, the representation of the top layer in the T-Enc strategy tends to be target-language-specific, whereas the one of the S-Enc-T-Dec strategy tends to be source-language-specific, which is verified in Appendix . %  Hence, we could answer the first question that the encoder has a first-agnositc-second-specific tendency on language representation, the target LT mainly indicates the desired target language on the top encoder layer.  %  Further, we conjecture that the target LT is mixed with the source language features in the first stage of the tendency, resulting in the  issue.

%  To answer the second question and verify our conjecture, we apply a simple operation to the T-Enc strategy. %  Specifically, we mask the target language tag in the 4 shallow encoder layers and restore it in the 5th layer. %  As shown in Tab., this operation could significantly mitigate the  issue in the T-Enc strategy, reducing to 13.01\% () and 0.39\% () on this issue. %  The remission on  issue verifies our conjecture and responds to the second question that the first-agnostic-second-specific tendency introduces bias into the target LT in the first stage.

In conclusion, we summarize our findings on the impact of the placement of LT on the  issue as follows: %  1) The language indication from the encoder side is more sufficient and stable, without being diminished during decoding;  %  2) The target language is mainly indicated on the top encoder layers, and placing the target LT at the bottom layer of the encoder introduces the  issue.

% 3) Data noise is not the core factor of  issue. Based on the conclusions in  and , we propose anguage onverter trategy () to enhance the language indication to mitigate the  issue, which further improves the quality of zero-shot translation.

According to our conclusions, as shown in Fig~, we divide the encoder layers into the shallow layers and the deep language converter layers by the -th deepest layer. %  In the shallow layers, we place the source language tag in front of the sentences to avoid the  issue. %  In the deep language converter layers, we introduce the target language embedding as auxiliary signals to prompt the desired target language. % by converting each input state of these layers to be target-language-specific.% % Since the target language embeddings contain target language-specific features , the indication of the target language could be sufficient, while introducing target information into the top encoder layers could avoid the language confusion as stated in \S. As pointed out by prior studies~, the target language embeddings contain target language-specific features, which could provide sufficient indication of the target language into the top encoder layers.

Then the final calculation of the self-attention block  in language converter layers is as follows: % %     a_{ij} = )W^Q)((h_j + e^{t})W^K)^T}{},% % %     z_i = \sum_{j=1}^{n} )}{\sum_{k=1}^{n}exp(a_{ik})}(h_j + e^{t})W^V,%  where  denotes the -th state of input tokens to each converter layer,  denotes the output states of self-attention block in each converter layer,  denotes the calculation of self-attention and  represents the LayerNorm function.  %  Since the language tags have already been included in the vocabulary, LCS introduces no extra parameters. %  Besides, we maintain the cross-entropy loss to optimize the MNMT model.

Moreover, we place the target language tag in front of the decoder input to indicate the target language better. %  We verify the effectiveness of this placement in Appendix .

We conduct experiments on three popular datasets, MultiUN, TED, and OPUS-100 . % which can be classified as Small, Medium, and Large according to the language scale.%  The statistics of the datasets of each translation direction are presented in Tab..  %  (Please refer to Appendix~ for more details.) %  For direct comparison, we report scores of noise and denoised data versions of OPUS-100. %  For all these datasets, English is the central language in the training sets, serving as either the source or the target language in each sentence pair.  %  Following , we consider translation directions involving English as supervised translation and directions between non-English languages as zero-shot translation.

% We implement the Transformer models based on the open-source toolkit fairseq  with mixed precision  and set beam size to 5 and length penalty to 1.0 during inference, following existing studies . We employ open-source toolkit fairseq to implement the Transformer models, with mixed precision . %  During inference, we set beam size to 5 and length penalty to 1.0, following existing studies . %  We apply SacreBLEU to calculate BLEU and report the averaged scores for all models. %  More details about the dataset, training, and evaluation can be found in Appendix .

We respectively establish MNMT models in the several LT strategies in  and our LCS on the three datasets and list the results in Tab..  %  Besides, we conduct experiments in the T-Enc-T-Dec, which seems to combine the indication from both encoder and decoder sides. %  Moreover, we also report the results of the denoised OPUS-100 dataset for a comprehensive comparison, where MultiUN and TED are low-noise datasets.

As shown in Tab., our proposed LCS yields the highest language accuracy and best performance of zero-shot translation on all datasets among all LT strategies. %  Specifically, compared to the widely-used T-Enc strategy, LCS effectively mitigates the  issue by improving language accuracy up to 95.28\% (), 96.21\% (), 85.35\% (), and 86.67\% () on zero-shot translation of MultiUN, TED, OPUS-100 (noise and denoised) datasets, respectively. %  Furthermore, LCS outperforms the T-Enc strategy by 3.07, 3.30, 7.93, and 2.93 BLEU scores improvements on zero-shot translation of these datasets, respectively.  %  While bringing conspicuous improvement to zero-shot translation, LCS maintains the performance of supervised translation. % % Unfortunately, limited to the ability of the language detect toolkit, the distinction on medium- and low-resource languages is low accuracy, which reduces the performance on these related translation pairs. Unfortunately, since the language detection toolkit is lowly accurate for medium- and low-resource languages, the performance on related translation pairs is reduced in the denoised OPUS-100. %  In this case, LCS performs well and robust on zero-shot translation on the noise version of the OPUS-100 dataset, compared to the denoise version.

We also probe the performance of our method in prior aspects, , the distribution of the  issue, and the language variation in the encoder. %  As shown in Tab., LCS could yield the highest language accuracy, and the lowest rates on the  and  issue. %  These scores suggest that LCS could provide the most accurate target language indication for zero-shot translation, and avoid being mixed with the indication of source language. %  Since LCS performs better than T-Enc on the  issue, we consider that LCS also provides sufficient and stable indication during generation. %  Besides, in terms of language representation variation, our proposed LCS yields the lowest score in the 6-th layer than the T-Enc strategy, meaning that LCS could generate more target-language-specific representation to indicate the target language better.

We evaluate the generalizability of LCS on OPUS-100 by applying it to the following stronger approaches:

% \noindent \textbullet \ .% We first train the model in the S-Enc-T-Dec strategy with 100K steps, and fine-tune the model in our strategy, with the same total training steps as other models.% \noindent \textbullet \  .% DN, based on the T-Enc strategy, introduces the denoising auto-encoder training objective. % We based on the S-Enc-T-Dec strategy to train the model with the more important training objective.% \noindent \textbullet \  .% DisPI, based on the S-Enc-T-Dec strategy, removes the residual connection of the encoder middle layer to yield the language-agnostic representation.% \noindent \textbullet \  .% CTS, based on the S-Enc-T-Dec strategy, introduces the contrastive learning training objective to close the representation gap of similar sentences.% We apply this objective to our shallow encoder layers since the function is similar.% % The function is similar to our vanilla encoder, and % \noindent \textbullet \  .% LEE adopts the target language embedding added to each state at the decoder side to indicate the desired target language without any LT strategies.% We base on the S-Enc-T-Dec strategy to combine the LEE and our strategy.% \\ .% EAG conducts large-scale pseudo-parallel language pairs for zero-shot translation directions.% We list this approach for the purpose of comparison..  As shown in Tab., LCS is effective in enhancing the performance of existing approaches. %  Specifically, with regard to zero-shot translation and language accuracy, most approaches achieve significant improvements with the application of LCS.  % % Although a few approaches achieve relatively small improvements with the enhancement of LCS on the small-scale dataset, we conjecture it may be attributed to the fact that prior approaches are already quite effective on this dataset and indicate the target language is easier when trained with a small-scale language set.% % Subsequently,  On the OPUS-100 dataset, all approaches yield significant improvements with the application of LCS, with 11.65, 10.91, 10.98, 3.32, and 2.36 average BLEU scores improvements on the noise data version, respectively. %  Compared to the noise version of the OPUS-100 dataset, LCS also could yield similar improvements on the denoise version, even achieving the 15.90 BLEU score by the FT method and the 87.86\% accuracy by the CTS method. %  Further, on the pretrained multilingual model, LCS exhibits advanced performance to enhance the performance of mBART on both supervised and zero-shot translation. %  While in terms of supervised translation, the application of LCS generally maintains or slightly improves the performance of most approaches. %  In conclusion, these results demonstrate that LCS is well-compatible with other approaches and can achieve further improvement when combined with them, without introducing extra parameters.

% 2 &  &  & \;\;5.26 & \;\;4.42 \\% 3 & 15.51 & 16.86 & \;\;5.03 & \;\;9.48 \\% 4 & 15.06 & \;\;4.29 & 17.25 & \;\;6.19 \\% 5 & 14.75 & 14.62 &  & 18.26 \\% 6 & 14.22 & 14.66 & 18.11 &  \\ The selection of hyperparameter  is an important factor of LCS. %  Indeed, the selection of  mainly relies on the variation of language similarity among encoder layers, which is described in Section~. %  We expand the variation into the deeper encoder with 12 layers, and display the variation in Fig.. %  We list the performance of different values of  in the 6-layer and 12-layer encoder in Tab..

As the results show, the optimal selection of  is around the inflection point of the variation of language similarity. %  We could observe from Fig. and Tab., when the selected value  is around the inflection point, LCS could yield better translation quality. %  For most settings of , the performance of LCS on zero-shot translation is much better than other methods (as shown in Tab. and Tab.). %  We further probe the selection of  in other deeper encoders (24-layer and 48-layer), and conclude that the better range selection of  is the nearby integers of 15\% of the encoder depth.

% Besides,  We conduct experiments to compare the effectiveness of LCS with the T-Enc and S-Enc-T-Dec, where we set  to 2 for 6- and 12-layer encoders, 5 for the 24-layer encoder, and 6 for the 48-layer encoder. %  As shown in Fig., although T-Enc and S-Enc-T-Dec could yield better translation on supervised translation with deeper encoders, both cannot perform much better than the 6-layer encoder on zero-shot translation. %  Compared to them, with deeper encoders, LCS exhibits an upward trend on zero-shot translation, where the 48-layer encoder improves around 2.50 BLEU score compared to the 6-layer encoder. %  Besides, in deep encoders, LCS maintains a similar supervised performance with the S-Enc-T-Dec strategy.

% % %  % In this section, we examine the effect of , which represents the number of language converter layers, on the noise OPUS-100 dataset. % % % We report the results of the denoise version in Appendix .% To evaluate the translation quality of the supervised and zero-shot translation, we conduct experiments on the encoder on 4 different depths, , 6, 12, 24, 48, we train 24- and 48-layer encoder with the assistance of DeepNorm.}, and keep the decoder with 6 layers.% % % We list the results of the supervised and zero-shot translation in Tab. and Tab., respectively.% % % In fact, the practical setting of hyperparameter  gradually increases along with the depth of the encoder.% % % However, the increasing trend is slow and a little search is needed for the optimal , , only setting  to 5 or 6 for the 24- and 48-layer encoder could yield great performance on both translation scenarios.% We also conduct experiments to compare the effectiveness of LCS with the T-Enc and S-Enc-T-Dec, where we set  to 2 for 6- and 12-layer encoders, 5 for the 24-layer encoder, and 6 for the 48-layer encoder.% % % As shown in Fig., although T-Enc and S-Enc-T-Dec could yield better translation on supervised translation with deeper encoders, both cannot perform much better than the 6-layer encoder on zero-shot translation.% % % Compared to them, with deeper encoders, LCS exhibits an upward trend on zero-shot translation, where the 48-layer encoder improves around 2.50 BLEU score compared to the 6-layer encoder.% % % Besides, LCS maintains the similar supervised performance with the S-Enc-T-Dec strategy in deep encoders.% In conclusion, we could apply LCS to deeper encoders, with a little search for hyperparameter , to boost the performance of zero-shot translation. We select four languages distributed in various language families, Arabic (Ar), English (En), Russian (Ru), and Chinese(Zh), following . %  In final, the training set consists of 6M sentence pairs. %  We select Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, the dropout rate of 0.1, the maximum learning rate of 0.0007 and label smoothing rate of 0.1. %  We share the vocabulary for all languages and segment words into subwords using byte pair encoding (BPE)  with 40k merge operations, following .  %  In training, we set the maximum batch size per GPU to 4096 tokens and trained on 8 GPUs with 300K steps.

It includes 60 languages in total  and we choose the top 20 languages following . %  In final, the training set consists of 3.5M sentence pairs. %  We choose Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, dropout rate of 0.1, maximum learning rate of 0.0005 and label smoothing rate of 0.1. %  In this dataset, we use SentencePiece to segment words into subwords with 64k merge operations, following . %  In training, we set the maximum batch size per GPU to 6400 tokens and trained on 8 GPUs with 100K steps.

It includes 100 languages in total and consists of 55M training sentence pairs with up to 1M samples per language pair. %  We choose Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, dropout rate of 0.1, maximum learning rate of 0.0005 and label smoothing rate of 0.1. %  In this dataset, we use SentencePiece  to segment words into subwords with 64k merge operations, following . %  In training, we set the maximum batch size per GPU to 6400 and trained on 8 GPUs with 400K steps. %  We train models in both noise and denoise data versions with the same parameter settings and similar epochs.

Since the number of languages in mBART is less than OPUS-100, we mainly select the six languages in the zero-shot test set of OPUS-100 (, Arabic, German, French, Russian, and Chinese), and add English to conduct experiments. %  The training set is still English-centric. %  We select the mBART-Large with 25 languages. %  During training, we set dropout to 0.3, and learning rate to 0.00003 with 2500 steps to warmup. %  We set the maximum batch size per GPU to 1024 tokens and trained on 8 GPUs with 100K steps. %  For fine-tuning mBART in LCS strategies, we set  to 2.

Inspired by the successful application of DeepNorm  and BranchNorm , we utilize DeepNorm to stabilize the training of deep models (24-layer and 48-layer), which applies such a constraint to the early stage of model training. %  And the  and  are following DeepNorm, as the following formulas:

where  and  denote the depth of the encoder and decoder for a standard Transformer. % Besides, we adapt the warmup steps from 4000 to 8000 to train models stably.%  In MultiUN, we calculate the case-insensitive sacreBLEU scores, following , and calculate case-sensitive sacreBLEU scores for TED and OPUS-100 datasets, following . %  We respectively average the scores of overall supervised or zero-shot translation directions to report in tables. %  Specifically, we apply different tokenizer for all Chinese testset, compared to the rest languages.