We adopt the same judge prompt as in .

We adopt this prompt from AlpacaEval, which is proved to have high correlation with human judges.

% % % % % The Open Assistant data also provides multiple ranked human responses per prompt from which we can construct evaluation fine-tuning data. We prompt the  using the Pointwise Judge Prompt . Then after getting all the CoT thoughts and scores, we accept them into the training set if the ranking of their scores agrees with the human rankings in the dataset. We resample the training set by discarding some of the data that receives the most common score so that the scores are not too skewed, as we observe many samples receive a score of 4. This results in% 1,630 train examples which do not overlap with any prompts we used for further tuning.% % % % %  For the SFT model, we train for a total of 10 epochs using a learning rate  and global batch size of 32. We employed cosine learning rate scheduling and saved a checkpoint after every epoch. We selected checkpoint from epoch 5 as the final model.

For all DPO training, we also trained for 10 epochs, with a learning rate of ,  and global batch size of 32. We adopted cosine learning rate scheduling.

For Self-Rewarding training, during Iteration 1 we set  for actor data creation and applied a filter to exclude pairs where the chosen response length exceeded  characters. We selected the checkpoint from epoch 5 for this iteration. In both Iteration 2  3 we continue with  and chose checkpoints from epoch 1 and epoch 2 respectively. For Iteration 4, we adjust  to  and selected the checkpoint from epoch 2.

For  training in Iteration 1 we set  for actor data  creation, and we filtered out pairs with chosen response length exceeding  characters. Additionally, for the judge data creation, we filtered out pairs if the chosen judgment length exceeded . We selected checkpoint from epoch 6 for this iteration. In Iteration 2, we increased  to  and set the threshold to  for judge data filtering, we selected the checkpoint from epoch 4. In Iteration 3 we maintain  at  and chose the checkpoint from epoch 2. Finally, in Iteration 4, we further increased  to  and again selected the checkpoint from epoch 2.

Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms  have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. %the self-improving process. To address this issue, we introduce a novel  step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills.  Surprisingly, this unsupervised approach improves the model's ability to judge  follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9\% to  on AlpacaEval 2, and 20.6\% to  on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. yuan2024selfrewarding39.4\%29.1\%intromethodexperimentrelatedLimitationssec:limitationsyuan2024followingwang2024helpsteer2meta-judgeIteration 3Conclusionsec:conclustionyuan2024selfrewardingwu2024selfyuan2024selfrewardingwu2024selficlr2024_conferenceiclr2024_conferenceAppendixJudge Promptprompt:judge_prompt Review the user's question and the corresponding response using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: \\ \\ - Add 1 point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content. \\ - Add another point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer. \\ - Award a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results. \\ - Grant a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus. \\ - Bestow a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer. \\ \\ \\ User:  \\ \\  response/response \\ \\ After examining the user's instruction and the response: \\ \\ - Briefly justify your total score, up to 100 words.\\ - Conclude with the score using the format: ``Score:  total points'' \\ \\ Remember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. \color\{response\}yuan2024selfrewardingGPT4 Judge Promptprompt:gpt4_pairwise im system\\ You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.\\  im im user\\ I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.\\ \\  Instruction\\ \\ \{\\     |    |``instruction'': ````'''',\\ \}\\ \\  Model Outputs\\ \\ Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.\\ \\ \{\\     |    |\{\\     |    ||    |    ``model'': ``m'',\\     |    ||    |    ``output'': ````''''\\ |    |\},\\ |    |\{\\     |    ||    |    ``model'': ``M'',\\     |    ||    |    ``output'': ````''''\\ |    |\}\\ \}\\ \\  Task\\ \\ Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M.\\ \\  Best Model Identifier\\  imTraining Detailstraining_detail