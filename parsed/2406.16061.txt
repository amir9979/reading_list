Starting from a finite set of tokens , called hereafter the vocabulary, an autoregressive language model can be seen as a collection of probability distributions  over  conditioned on elements of , i.e. sequences of up to  tokens. We assume the existence of an end-of-sentence (EOS) token in , denoted , that can represent a full stop or a line-break for example.  \\ To generate text, a pre-trained language model  with an input  is queried autoregressively and samples tokens , where . % %     s_1 \sim \plm(.\mid q), \ s_2 \sim \plm(.\mid qs_1), \ s_3 \sim \plm(.\mid qs_1s_2) \dots,%  The generation process stops at the first index  for which . Here,  refers to the concatenation of the tokens . \\ When interacting with language models, and more specifically in CoT reasoning, we are interested in generating sentences , i.e. sequences from  that end with the  token, rather than an arbitrary amount of tokens. When prompted with a sentence  or a sequence of sentences , the language model can therefore autoregressively generate a new sentence . \\ Given a question  (e.g., a math problem), we define a chain-of-thought as a sequence of  sentences , where  is the final answer. Assuming the existence of a binary function  that assesses the correctness of the sentence  to the question , our goal is to tune a pre-trained model  to generate a chain  from a question  such that .

Our approach first requires access to a dataset of reasoning traces, called a CoT dataset, , where each training example includes a  , and a  (or ) comprised of  sentences, , of which the last element,  is a valid  to the question , i.e., . Naturally, the number of steps  needed to reach the answer to  depends on the question itself. \\ Such datasets  are generally human-made. Examples of publicly available reasoning datasets include the arithmetic datasets GSM8K , AQuA-RAT , MAWPS  as well as the commonsense reasoning datasets StrategyQA , Creak , e-SNLI , ECQA , QASC , QED , Sen-Making . 

 From such a dataset, we can construct a dataset  of prompt-response pairs, where each example  contributes  pairs: . % :% %     \gD_{}^{} = \bigcup_{i=1}^N \bigcup_{k=1}^{n_i} (x_iz_i^{1:k-1}, z_i^k).%  Such a dataset can be used for supervised fine-tuning, during which the parameters  of the base language model  are updated to minimize the SFT loss:

where  = ||. In principle, if the data is representative of the target task and if the model generalizes well, the SFT phase should increase the likelihood of . Put differently, because each  in the training dataset is a step towards a valid answer , then after supervised-fine-tuning, on similar examples, the model should encourage the sentences that unroll the reasoning and help discover a valid answer to the initial question.

 From , we also construct a preference dataset , comprised of triplets of the form . The  and the  answers are obtained directly from , but for each  (which is actually either a question or a concatenation of a question and a certain number of initial reasoning steps), we need an invalid reasoning step. Naturally, an arbitrary sequence of tokens would be invalid, but it will provide no useful signal to the model if it is fine-tuned with RLHF or with preference optimization methods such as DPO using such a preference a dataset. Ideally, the  answers should be almost correct reasoning steps, or contain errors that either a language model or a human are expected to make. Naturally, the  answers can be obtained using human annotators explicitly asked to generate wrong but close-enough answers. In this work however, we investigate two simple and complementary ways of defining such a dataset:

An illustration of this dataset creation process is provided in  of . \\ After an SFT phase where  is fine-tuned into  using  by minimizing the loss in , any preference optimization method can be used on  . For instance, DPO  fine-tunes  into a model  that minimizes the following loss:

where

Here  is the sigmoid function and  is a scaling hyperparameter. As an alternative to using two separate SFT and preference optimization phases, ORPO , a more recent approach, combines both steps and replaces the loss in  with:

where  is a weighing hyperparameter. %To illustrate the process of building preference triplets, we provide in TODO a full example obtained form a specific instance of the GSM8K CoT dataset  in Appendix TODO. From the GSM8K training dataset, we construct the SFT dataset  as described in . The train set of GSM8K consists of  examples, with an average of  reasoning step per example, leading to to an SFT dataset of  examples. We then fine-tune the based model on this dataset using low-rank adaptation  for efficient parameter updates, processing each example 3 times. The learning rate used in , and the batch size is 16. For LoRA, we use rank 64 matrices and a scaling parameter . It is noteworthy that GSM8K examples contain calculation annotations (between <<>>, as shown in the examples provided in ). These annotations can be used to call external tools (e.g., python scripts or calculators) to perform calculations, rather than asking the LLM to perform the calculation. While we made no such usage of external tools, we tried both keeping and removing the annotations from the text before SFT, and found no significant difference in terms of performance. We thus decided to process the dataset without annotations. \\ Details about the choice of the hyperparameters are provided in .

 In addition to testing the fine-tuned model on the GSM8K's test set, we assess SFT's out-of-distribution generalization, both on the harder math word problem AQuA, and on the non-mathematical task ARC-Challenge. We report the accuracies in . As expected, and as confirmed by other studies , fine-tuning the model on the reasoning steps helps improve the performances on questions requiring reasoning that come from the same distribution. Unsurprisingly however, the performances on AQuA and ARC-Challenge drop after the SFT stage, confirming the overfitting issues of SFT, and their limited generalization to unseen examples . This is confirmed by  in , where we reduce the number of training epochs (on GSM8K) and observe better performances on AQuA. In the next subsections, we investigate whether preference optimization algorithms can lead to even further performance boosts on the three evaluation tasks.

From , we construct a preference dataset  using digit corruption as explained in . Given the stochasticity of the digit corruption approach, we ensure that the rejected answers are indeed invalid, by repeatedly generating reasoning steps until they differ from the ground truth reasoning steps. For reasoning steps that do not include digits, we simply do not include them in the preference dataset.

We fine-tune the SFT model on the obtained preference dataset using DPO with a scale factor , with the same LoRA configuration as SFT. We use the AdamW optimizer  with a learning rate of  along with a linear schedule for the learning rate, as explained in .

 We report the accuracies post DPO tuning in . The significant performance increase in GSM8K (a relative ) shows how merely corrupting digits to create  reasoning steps improves the mathematical reasoning abilities of Falcon2-11B. Our approach helps boost performances on the AQuA task, with a relative increase of , even without using any example from the AQuA train set during training. We note however, that there is no benefit on the ARC-Challenge task. We suspect that it is because it does not require the same type of skills as GSM8K and AQuA. In , we investigate whether other schemes could boost ARC performances.

While DPO has emerged as the go-to method for preference optimization, several variants  claim to address some of its shortcomings: overfitting, inefficient learning, memory utilization. A recent attempt at benchmarking these approaches  has shown that DPO still outperforms its variants on a variety of tasks. In this section, we make use of our constructed preference dataset to further compare DPO to its variants. Unlike , we also consider  ORPO  which combines both SFT and preference optimization. 

 We report the accuracies on the GSM8K test dataset in . We find that the variants of DPO do not lead to improved performances, even with extensive hyperparameter tuning for each method separately (). This confirms the recent observations from  Unlike , when construct  using weak LLM generation, as described in , there are a few parameters to take into account: which weak LLM to use? how to prompt said LLM? how to post-process the resulting sequences?  \\ We first consider instruct version of the Gemma model , Gemma-2B-it, to generate answers. We use the prompt provided in . We then filter out the responses that do not start with ``Next step: '', and simply do not create the corresponding triplet in the preference dataset. The generation stops at the first line-break or full stop. We also consider the larger Llama-7B  and its chat version, to assess the effect of the weak LLM size.  \\ When using a weak LLM to generate rejected answers, it is not unlikely that the LLM outputs valid reasoning steps, in which case, including the resulting triplet in the preference dataset might hurt generalization of the resulting model. We experimented with the robust version of DPO , which accounts for the ambiguity in the preferences, but that did not result in improved performances. We therefore consider to corrupt the digits of the generated sequences similar to the digit corruption scheme alone. In  of , we study the effect of post-generation digit corruption, and find that digit corruption is essential for downstream tasks. We also compare using the chat version of Llama-7B with the prompt template of  to using its base version with few-shot examples only, and find that using the base version yields to better performances.  \\ Lastly, we consider an , where we Falcon-11B fine-tuned with DPO as described in  as a weak LLM. We report in  the accuracies on the three tasks, using the three weak LLMs with post-generation digit corruption. While the weak LLM scheme does not perform as well as the digit corruption scheme alone on the math world problems, it is noteworthy that with Llama-7B and the iterative approach, the performance on ARC-Challenge improves over the base model.

A natural question at this point is to consider the effect of the size of the preference dataset on the resulting model fine-tuned with DPO. Given that our proposed approach allows us to generate arbitrarily many wrong reasoning steps per valid reasoning step (e.g., we can corrupt the digits in many ways, and prompt weak LLMs multiple tims), we can construct preference datasets with triplets  that contain redundant  pairs, with different  answers. We thus consider using three rejected answers for the digit corruption, Gemma-2B-it, and Llama-7B experiments. We also consider fine-tuning on a dataset consisting of both digit corrupted answers and Llama-7B-generated answers (themselves digit corrupted), and a dataset containing three times as many rejected answers. \\ We report in  the results of the different schemes on the three tasks at hand. Strikingly, simply tripling the number of rejected answers for the digit corruption scheme leads to a an accuracy of  on the GSM8K task, which represents a relative increase of  over the base performances. Additionally, mixing different sources of rejected answers (Llama-7B and digit corruption), can lead to increased performances on ARC, indicating that diversifying the sources might help with generalization to other tasks.

To further test our approach, we perform some of the experiments above using Mistral-7B  as a base model. We report the results in , and find that all approaches lead to better performances on the GSM8K benchmark than the base model, which scores . With this experiment, we confirm that digit corruption is the strongest of the two schemes we proposed.

%  So far, we have used the GSM8K training data to construct the SFT and preference datasets. Naturally, given the proximity of the corresponding questions in the test set, fine-tuning on such datasets would mostly help boost the performances on the GSM8K benchmark. In this section, we consider using the AQuA training set to create  and . For SFT and DPO, we fine-tune with the same hyper-pararameters as the ones we found best for the experiments on GSM8K. We report the results in , using Falcon2-11B as a base model, and confirm that using the AQuA training set is more helpful for the AQuA benchmark ( relative increase) than using the GSM8K training set.

We hypothesized that fine-tuning a language model to predict the next reasoning step only should help improve performances on reasoning benchmarks. Our results in the main paper confirm this hypothesis. However, it is natural to wonder whether using multiple reasoning steps to predict could be beneficial. More specifically, for SFT, we compare our approach (which requires fine-tuning on  pairs) to fine-tuning on  pairs. Similarly, for DPO, we compare our approach (that requires fine-tuning on  triplets) to fine-tuning on  triplets. Using Falcon2-11B as a base model, and GSM8K as a data source and for evaluation, we found that with this change, the performance drops from  to  for SFT, and from  to  for DPO with digit corruption. \\ We also tested replacing the inputs  with , where  is a sequence corresponding to 3-shot examples, and found that while the SFT performance slightly increases to , the DPO performance significantly drops to .

% *denotes equal contribution\arabic   Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the reasoning performances of language models. While the  answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating  answers: digit corruption, and weak LLM prompting. Our approach leads to increased accuracy on the GSM8K, AQuA-RAT, and ARC benchmarks for Falcon2-11B and Mistral-7B. For example, the approach can lead to up to a relative  increase in accuracy on the GSM8K benchmark without any extra annotations. This work suggests that spending resources on creating more datasets of reasoning traces would further boost LLM performances on informal reasoning tasks.chosenrejectedOur code is publicly available at \url.Introductionreasoningintelligenceinformal reasoninghuang2022towardsrae2021scaling,bommasani2021opportunities,cobbe2021trainingCoT;wei2022chainwei2022chainkojima2022largestanovich2000individual,kahneman2003mapsreasoningkojima2022largeemergentwei2022emergent%\includegraphics[width=\textwidth]{main_figure.png}width=1.35\textwidth, centerimg/dpo_reasoning-enhance_new.pdfIllustration of the creation process of a preference dataset with two complementary approaches to generate rejected answers. The preference dataset is used to fine-tune a reference model using a Direct Preference Optimization (DPO) or one of its variants, after a supervised fine-tuning (SFT) step. %For a clearer view of the details, please zoom in on the image. %This procedure aims at increasing the likelihood of generating valid reasoning steps and at the same time reducing the likelihood of generating invalid ones.     fig:main_figurewei2022chain,kojima2022largeho2022largeuesato2022solvingni2023learninghong2024orpogabriel2020artificial,ji2023ai,klingefjord2024humanRLHF;christiano2017deep,ouyang2022training,bai2022trainingPPO;schulman2017proximalDPO;rafailov2024directIPO;azar2023ipoSLIC;zhao2023slicKTO;ethayarajh2024ktoziegler2019finetuning,stiennon2020learningcobbe2021trainingling-etal-2017-programalmazrouei2023falconjiang2023mistralclark2018thinkfig:main_figurePORT: Preference optimization on reasoning tracesProblem setuppromptedor until the prompt  exceeds  tokens, but we disregard this case by assuming a very large context window.Proposed approachsec:approachquestionreasoning tracerationaleanswercobbe2021trainingling-etal-2017-programkoncel-kedziorski-etal-2016-mawpsgeva2021aristotleonoe2021creakcamburu2018esnliAggarwal2021ExplanationsFCkhot2019qasclamm2021qedwang-etal-2019-makeSFT data:     {!}{},      valid reasoning stepsPreference data:(prompt, chosen, rejected)promptchosenpromptrejectedrejected For each pair  from , we prompt a smaller language model (hereafter also referred to as  LLM) with  and use the response to define the corresponding  answer . By incorporating the resulting triplet in the preference dataset, we naturally incentivize the base model to avoid errors of the type made by the weak LLM. % To avoid cases where the weak LLM produces actual valid step-wise responses,     %we propose to either filter out the generated responses  that are judged similar to  by a classifier obtained with zero-shot in-context learning from another LLM, or      %we propose to further corrupt  by changing the digits randomly, as explained above.  This process can be used to generate multiple rejected answers  per prompt . LLM generation:weakrejected In datasets that involve mathematical reasoning, most reasoning steps  include digits. Without modifying any non-digit character of , we replace each digit with one from  to  with equal probability. Similarly, this approach can be used to generate multiple rejected answers  per prompt . Digit corruption:tab:dpo-examplessec:dataset_exampleseq:sft_lossrafailov2024direct          \gL_{}(\vtheta) = - \E_{(x, y_w, y_l)\sim \gD_{}^{}} \left[l(x,y_w,y_l;\vtheta)\right],

     l(x,y_w,y_l;\vtheta) = \log &\sigma \left( \beta \log {p_{}(y_w \mid x)} - \right.\nonumber\\  &\left.\beta \log {p_{}(y_l \mid x)} \right). hong2024orpoeq:indiv_dpo_loss

     l(x,&y_w,y_l;\vtheta) = - \lambda \log \sigma \left( \log {1 - p_\vtheta(y_w \mid x)} - \right. \nonumber \\ &\left. \log {1 - p_\vtheta(y_l \mid x)} \right) - \log p(y_w \mid x),    Experimentssec:experimentsempirically investigate the proposed approachcompare the two schemesinvestigate the effect of the preference optimization methodEvaluation:cobbe2021trainingA harder math word problem: the Algebra Question Answering with Rationales dataset , which includes approximately  algebraic word problems, each presented with a rationale leading to one of five multiple-choice options (A to E). We use the accompanying test set of  examples for evaluation.     https://github.com/google-deepmind/AQuAAQuA;ling-etal-2017-programA non-mathematical dataset: The AI2's Reasoning Challenge , that covers multiple science subjects. The questions are split into  and  sets. Questions in the Challenge set cannot be solved with retrieval or co-occurence methods. Each question admits one valid answer amongst a set of typically four options. There is no ground-truth chain-of-thought reasoning provided for this dataset. We use the test set of the ARC-Challenge set, that consists of  examples, for evaluation. https://allenai.org/data/arcARC;clark2018thinkEasyChallengeeval-harnessopenai2023gpt4sec:used_promptsBase model:\urlalmazrouei2023falconsec:mistraljiang2023mistralTraining data:sec:using_aquatraincobbe2021trainingdatasetssec:approachsec:dataset_examplesSupervised fine-tuningsec:exp_sftsec:approachLoRA;hu2021lorasec:dataset_examplessec:training_detailsResults:tab:base_vs_sft_vs_dpouesato2022solvingni2023learningtab:aqua_n_epochssec:additional_resultsPreference optimization with digit corruptionsec:dpo_digitcorrsec:approachlosh2019decoupledsec:training_detailsResults:tab:base_vs_sft_vs_dpobadsec:dpo_weakllmVariants of DPOazar2023ipo,ethayarajh2024ktosaeidi2024insightssaeidi2024insightshong2024orpoResults:fig:dpo-variantssec:training_detailssaeidi2024insightsPreference optimization using weak LLMssec:dpo_weakllmsec:dpo_digitcorrsec:approachteam2024gemmasec:used_promptstouvron2023llamachowdhury2024provablyfig:llama_variantssec:additional_resultssec:used_promptsiterative approachsec:dpo_digitcorrtab:comparison_of_weak_llmsIncreasing the size of the preference dataset(prompt, chosen, rejected)(prompt, chosen)rejectedfig:larger-datasetUsing Mistral as a base modelsec:mistraljiang2023mistralfig:mistral-resultsUsing AQuA as source datasetsec:using_aquatab:aqua-resultsRelated workWhat is reasoning?mchugh2018reasoninghuang2022towardsdeductiveinductiveabductivebronkhorst2020logicalformalinformalReasoning in LLMs:austin2021program,hendrycks2021measuring,liang2023holistic,clark2018thinkwei2022chainyao2024treebesta2024graphpfau2024letslewkowycz2022solving,taylor2022galactica,chen2021evaluatingzelikman2022star,huang2022large,gulcehre2023reinforced,yuan2023scaling,singh2023beyond,hosseini2024vni2023learningcobbe2021training,uesato2022solvingkhalifa2023gracePreference optimization:bradley1952rankschulman2017proximalrafailov2024directazar2023ipo,zhao2023slic,cai2023ulma,ethayarajh2024ktorafailov2024direct,tunstall2023zephyrhong2024orpoConclusionLimitationschung2024scalingsampleDataset examplessec:dataset_examplestab:dpo-examplesone John puts  from his piggy bank savings last week to repair his car. How many dollars are left in his piggy bank?\\    He saved money for 2 years, which is equal to 12 x 2 = <<12*2=24>>24 months. The amount of money he saved is <25*24=600>>600. But he spent some money so there is  = <<600-400=200>>200 left.  200.    Five coaster vans are used to transport students for their field trip. Each van carries 28 students, 60 of which are boys. How many are girls?\\    There are a total of 5 vans x 28 students = <<5*28=140>>140 students. If 60 are boys, then 140 - 60 = <<140-60=80>>80 of these students are girls.  80 Question:Rationale:Question:Rationale:Used promptssec:used_prompts   You are expert grade-school science teacher. Given the following question, provide justification for the answer.\\   Question: . Answer Choices: . Answer: ... The Answer is .\\   You need to add a two to three sentences rationale before ``The answer is '', justifying the correct answer. questionoptionsanswer letteranswer lettersec:dpo_weakllm   You are an obedient assistant. Your task is to reason about the following question. Write only the next step of the reasoning chain. Your answer should include exactly one following reasoning step and has to be exactly one sentence long! The answer should start with "Next step: ". Here are two examples:\\

Question: {prompt_example_1} Next step: {first_step_example_1} Next step: {second_step_example_1} Next step: {third_step_example_1} Next step: {fourth_step_example_1} Next step: {final_answer_example_1}

Question: {prompt_example_2} Next step: {first_step_example_2} Next step: {second_step_example_2} Next step: {third_step_example_2} Next step: {fourth_step_example_2} Next step: {fifth_step_example_2} Next step: {sixth_step_example_2} Next step: {final_answer_example_2}

Question: {prompt} Next step: {first_step_ground_truth} ... Example of preference dataset obtained from the \textbf ``Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'', and the corresponding \textbf ``Natalia sold  clips in May. Natalia sold  clips altogether in April and May. The solution to the problem is .'' This example is obtained from the GSM8K dataset \citep. The \textbf column represents steps from ground-truth rationale, \textbf are examples obtained by digit corruption, and \textbf are examples obtained by prompting the Llama-2-7B-chat model \citep  tab:dpo-examplesTraining detailssec:training_detailsNumber of epochs used for SFT:almazrouei2023falconSFT learning rate:SFT batch size:LoRA parameters:hu2021loraOptimizer:tieleman2012lecturelosh2019decoupledFurther DPO hyperparameters:touvron2023llamasec:dpo_weakllmsec:experimentsfig:dpo_hp_search%\includegraphics[width=\textwidth]{main_figure.png}width=1.\textwidth, centerimg/hp_search.pdfDPO hyperparameter search. The y axis corresponds to the accuracy on the test set of GSM8K.fig:dpo_hp_searchHyperparameters for DPO variants:ethayarajh2024ktoazar2023ipofig:dpo_variants_hp_searchsec:dpo_digitcorrhong2024orpoeq:indiv_orpo_lossfig:dpo-variantsAdditional Resultssec:additional_resultstab:aqua_n_epochsfig:llama_variantssec:used_promptsAblations