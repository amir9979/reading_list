Both FLORES+ and Belebele dataset are licensed under CC-BY-SA-4.0.  The pretrained models , , , and  are under Apache-2.0 license.  is under Apache-2.0 and Community License for Baichuan2 Model.  is under MIT license and Deepseek License Agreement.  are licensed ``AS IS''.  is released under the Meta Llama 3 Community License Agreement. 

All experiments were run on an NVIDIA RTX A6000 GPU. On average, it took around 50 hours to evaluate one model on all languages in both experiments. In total, approximately 400 GPU hours were used. 

Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public discourse and governing a multi-ethnic society, and has gradually transitioned from a pluralist to a more assimilationist approach since 1949. We explore the impact of these influences on current language technology. We evaluate six open-source multilingual LLMs pre-trained by Chinese companies on 18 languages, spanning a wide range of Chinese, Asian, and Anglo-European languages.  Our experiments show Chinese LLMs performance on diverse languages is indistinguishable from international LLMs.  Similarly, the models' technical reports also show lack of consideration for pretraining data language coverage except for English and Mandarin Chinese. Examining Chinese AI policy, model experiments, and technical reports, we find no sign of any consistent policy, either for or against, language diversity in China's LLM development. This leaves a puzzling fact that while China regulates both the languages people use daily as well as language model development, they do not seem to have any policy on the languages in language models.

Introductiontab:companiesRelated Workconneau-etal-2020-unsupervised,brown2020language,workshop2022bloomabadji-etal-2022-towards,imanigooghari-etal-2023-glot500lee2020EFLbird-chiang-2012-machineHistorical background on China's language and AI policyLanguage Policymullaney2011comingAI PolicyChinese national Cybersecurity Law, Data Security Law, and Personal Information ProtectionModel ExperimentsModelstransformerswolf-etal-2020-transformersWe will release code.Qwen1.5-7BqwenYi-6Bai2024yiDeepSeek-LLM-7Bdeepseek-llmInternLM2-7Bcai2024internlm2XVERSE-7Bxverse-model-cardBaichuan2-7B-Baseyang2023baichuanLlama3-8Bllama3modelcardMistral-7B-v0.3jiang2023mistralTechnical ReportsQwenXVERSEInternLM2tab:modelsLanguages and Tasksgoyal-etal-2022-florestab:lang_codesExperiment 1: Evaluating language model perplexity with the FLORES+ datasetFLORES+https://github.com/openlanguagedata/floreswe use the \texttt split of FLORES+.apd:exp1-samplerust-etal-2021-good,ali2023tokenizerfig:tok-efficiencygonen-etal-2023-demystifying,xiao2023streamingllmMie2016Canfig:nll-sumNLLExperiment 2: Evaluating Zero-shot Reading Comprehension using Belebele datasetholtzman-etal-2021-surface, wiegreffe-etal-2023-increasingbandarkar2023belebelenllb-22zero-shot MRC accuracyfig:bele_accuracyzero-shot MRC accuracyInternLM2XVERSEPopulation and Economyethnologuefig:corr-speakersNLLzero-shot MRC accuracyThe population for Standard Malay (zsm) is not provided in \citet, so we use the population for Malay (msa) instead.NLLzero-shot MRC accuracytab:corrfig:corr-gdpDiscussion and ConclusionLimitationshttps://github.com/modelscope/modelscopeAcknowledgementsLeft blank for anonymous submissionemnlp2023acl_natbibOverview of selected languages and their respective categories. Languages categorized as ``Chinese ethnic minorities'' indicate languages that are also spoken by some populations in China that the PRC determine as ``minority ethnic group''.tab:lang_codesInformation about the developers of Chinese open-source multilingual language models.tab:companiesTechnical details according to technical reports and model cards of the models. Information not reported is shown as `-'.tab:modelsExperiment 1: Sample sentences from FLORES+ English datasetapd:exp1-sampleOn Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.``Panama Papers'' is an umbrella term for roughly ten million documents from Panamanian law firm Mossack Fonseca, leaked to the press in spring 2016.Experiment 2 promptbandarkar2023belebele Given the following passage, query, and answer choices, output the letter corresponding to the correct answer. ### Passage: With the change from the quarter to the half mile run, speed becomes of much less importance and endurance becomes an absolute necessity. Of course a first-class half-miler, a man who can beat two minutes, must be possessed of a fair amount of speed, but endurance must be cultivated at all hazards. Some cross country running during the winter, combined with gymnasium work for the upper part of the body, is the best preparation for the running season. ### Query: According to the passage, which of the following would be the most beneficial for a runner preparing for the upcoming season? ### Choices: (A) Practicing cross country running in the summer (B) Focusing on cultivating speed while training (C) Beating a three minute time (D) Utilizing the gym to work out the upper body ### Answer: Model and Dataset InformationLicensinglicenseYi-6BXVERSE-7BInternLM2-7BMistral-7B-v0.3Baichuan2-7B-BaseDeepSeek-LLM-7BQwen1.5-7BLlama3-8BRuntimeCorrelations between performance metrics of experiment 1 and 2 with log(language speakers) for all 18 languages and log(national GDP) for Lao, Burmese, Thai, Malay, Vietnamese, Indonesian, Korean, and Japanese. Language performance as measured by both experiments metrics are highly correlate with data resources, as measured by number of speakers and GDP, where increased resources correlates with better performance.tab:corrscale=0.5figures/tok_bar.pdfTokenization by different models and languages. Fertility on the y-axis is defined as number of tokens divided by number of characters in a sentence. Therefore, the higher the number means a sentence is more fragmented by tokenizers. Overall, Lao (lao\_Laoo), Burmese (mya\_Mymr) and Tibetan (bod\_Tibt) are the most fragmented languages. Baichuan2 and XVERSE produce more balanced tokenization result across languages. fig:tok-efficiencyscale=0.5./figures/scatter_gdp.pdfLooking at the languages spoken by China's neighboring countries, performance metrics correlates with national GDP, and the trend of Chinese models and international models are highly similar. fig:corr-gdp