Our objective can be formulated as a Bayesian optimization problem, where the goal is to select a set  so as to maximize the posterior probability of , i.e.

where  is the posterior probability. Applying Bayes rule we get:

Note that the last term does not depend on , so it can be ignored when optimizing over . Introducing a prior over model parameters , we get:

We will refer to the two terms as the conditional and marginal terms, respectively. Note that the conditional and marginal terms together make up the negative pointwise mutual information between the selected and downstream data, which has deep connections to prior work on active learning and active sampling .

Given that we have access to prior knowledge from the dataset , we can replace the uninformed prior over  with an empirical Bayes prior that conditions on  to obtain:

As this integration is still intractable, we now make our main simplifying assumption which is to replace this integration over parameters by a point estimate: 

where  is a model trained on  and  is a model trained on both  and   (in practice, we use a model that is pre-trained on  fine-tuned on ). 

Moreover, this approximation leads to computational benefits by avoiding the full combinatorial optimization of subset selection. In particular, once we condition on a single model , and assuming the distribution over points  is independent, i.e. , we have:

which simplifies to:

This gives our CoLoR-Filter criteria that we use to select data. This optimization selects the points with the largest conditional loss reduction (CoLoR), i.e. the points where the negative log-likelihood loss of the conditional model  is lower than the marginal model . Intuitively, this selects data points that are more likely under the conditional model than the marginal model.

While the factorization that results from our point estimate of the parameters is computationally convenient, it makes an important simplifying assumption. In particular, the CoLoR-Filter objective no longer encourages the selection of a diverse dataset, as scores are applied independently to each point. In practice, this is remedied by a few considerations: (1) we can run CoLoR-Filter on a corpus that has already been deduplicated to prevent degenerate duplications, (2) for large , we must select many different data points, and (3) each datapoint is itself a sequence that may contain diverse signal across tokens. We should also note this is not a unique property of CoLoR-Filter and also happens in other methods that do offline scoring like DSDM and DSIR. We defer a detailed discussion of the nuances of this issue to .

Since the CoLoR-Filter objective is written as a difference of logs, it can also be written as a log of the ratio between probabilities under  and . If data were actually sampled from , then this ratio would be the importance weight needed to reweight samples so that they are from the model defined by . Note that DSIR  directly attempts to perform importance sampling from  to  instead of optimizing performance on the downstream data. Thus, DSIR ends up with a somewhat related algorithm except in DSIR: (1) there is no language model, just features of a full data point (hashed n-grams), and (2) the algorithm samples rather than optimizes.

Another closely related approach is DSDM  which uses a TRAK Datamodel estimator  to score datapoints and then selects the top- points. The motivation and setting of DSDM are similar to CoLoR-Filter, but DSDM relies on TRAK which constructs a linear approximation of the influence that data points have on each other. Instead, CoLoR-Filter operates directly in function space by comparing the loss between models directly rather than relying on linear approximations or Datamodels .

%  CoLoR-Filter is inspired by and builds on the RHOLoss approach introduced in prior work  with subtle but significant differences in the setting: the original RHO paper focuses on cases where the hold-out data is sampled from the same distribution as  over multiple epochs of training. In contrast, we focus on selecting data to target downstream distributions that are different from  and where we only take a single pass over the data. Here, we derive a straightforward adaptation of RHOLoss to our setting, which we call RHO-down.

We now derive RHO-down in our setting, aiming to illustrate the connections between RHO-down and CoLoR-Filter. First, RHO-down approximates the full subset selection problem from  by a greedy (sequential) approximation where samples are added to  one (batch) at a time. Using a batch size of , the th-sample would be ideally added according to the following criterion:

where  ranges from  to  sequentially. RHO-down then uses a point estimate of the parameters (as we do in CoLoR-Filter):

Finally, the RHO-down authors found that updating the conditional term to depend on  was unstable, so they instead approximate this by a fixed model :

First, RHO-down approximates the full subset selection problem from  by a greedy approximation where samples are added to  one (batch) at a time. 

From here, RHO-down uses a point estimate of the parameters (as we do in CoLoR-Filter):

Finally, the RHO authors found that updating the conditional term to depend on  was unstable, so they instead approximate this by a fixed model :

Note that while both CoLoR-Filter and RHO-down approximate the posterior over parameters with a point estimate, RHO-down makes a few additional approximations. This is largely a result of RHO-down attempting to increase data diversity by using a sequential approach to selection that conditions on the previously selected data . This is an understandable goal, but it introduces more approximations, can cause instability by creating a non-stationary data distribution, and is computationally expensive since the data selection is no longer parallelizable. A continued discussion of the pros and cons of online selection is in .

 We also consider a version of the algorithm that we call ``RHO-down + prior'' that replaces  in the RHO-down algorithm with  to incorporate the prior information. This corresponds to conditioning on both  and  instead of only . Intuitively, this method can better leverage stronger features learned on the larger  to integrate the information from the small .

In our experiments, we will consider four algorithms based on the above derivations. In this section we go through each of these in turn.

 Our proposed algorithm is presented formally in . Compared to the derivation, the main difference is the introduction of , a hyperparameter that acts as a compute-performance trade-off controlling how expensive and aggressive the data selection is. Rather than selecting data from all of , we take a random subset  of size . Thus, larger  subselect more aggressively, but at the cost of more computation. A full discussion of this cost is in .

 As an ablation of CoLoR-Filter, we follow prior work  and include a baseline that only uses the conditional model to select data. Essentially, this is CoLoR-Filter if we always assume that  in Line 4 of . 

 We present a practical variant of RHO-down in  based on the derivation presented in . The main changes to make a practical algorithm are (1) the introduction of  as in CoLoR-Filter, and (2) performing the algorithm batch-wise instead of using single data points.

 We can also incorporate the prior data  into  by simply replacing Line 1 where  is trained on  with a procedure where we first pre-train  on  and then fine-tune it on . 

% % % [1]% %   \REQUIRE Conditional dataset , marginal dataset , train dataset , \\ \qquad \quad training budget , selection multiplier , batch size %   \STATE Sample a subset  of size %   \STATE Pre-train  on , pre-train  on , and initialize a model %   %     \STATE Sample a batch  of size  from %     \STATE Select a subbatch  of size  from  by: %     %         \bar B_t = _{x \in B_t}  \log \Pr(x| \thetacond_t) - \log \Pr(x| \thetamarg_{t})%     %     %     \STATE Update  to  by training on %     \STATE Optionally: Update  to  by training on %     \STATE Optionally: Update  to  by training on %   \ENDFOR% % % 

To evaluate the computational cost of the various algorithms, we use units of ``model forwards'' per token where we assume that a backward pass is twice as expensive as a forward pass . Note that our 150m models take about 5e8 FLOPs per model forward of a single token . The cost of running the selection algorithms depends on  and  defined as follows:   is the size of the prior data ,  is the size of the selected dataset ,  is the hyperparameter controlling how aggressively we subselect data.  Note that we assume that  is so small that the cost of training a model on  is negligible towards the total cost (and all the methods we consider just fine-tune a model once on ). We will also be careful to note when computation can be done in parallel before training versus computation that must happen serially during a training run. Offline algorithms like CoLoR-Filter can take advantage of parallelism to improve efficiency. In this section, we go through each method in turn and aggregate the computational costs in .

We also include another parameter  to cover the case where we select data using small models and use it to train a larger model . Specifically,  is the ratio of cost of one model forward of the  target model compared to the small auxiliary models used for data selection. For example, in our experiments, when we use 150 million parameter models to select data and then train a 1.2 billion parameter model on the resulting data, then . Training thus costs  across all methods since we run a forward and backward for the large model on all  sequences.

 The cost of selection is  forward passes. But, this selection process is  parallelizable. Training the prior model costs  forwards since . And training a model on the selected data costs  forward passes. So the total cost is , but the  scoring computation can be done in parallel. 

 The conditional-only method is almost the same as CoLoR-Filter, except we only need  forward passes for selection since we only run one model over the data. The cost is thus , with  being parallelizable.

 The cost of selection is still  forward passes. Then we need an additional  to backward the output model (since the forward is already handled during scoring). Note that we need to evaluate the marginal model online, so it is not parallelizable, but the conditional model is fixed and can be computed offline. So, the cost is , and the  conditional model computation can be done in parallel. 

 For the version with an added prior, we just add  cost for training the prior. Thus, the cost is  with  parallelizable.

Overall, the methods all have comparable costs, with Conditional Only being the cheapest and RHO-down + Prior the most expensive. The main difference is that CoLoR-Filter and Conditional Only are easily parallelized while RHO-down and RHO-down + Prior are not. It should also be noted that when doing experimentation, offline methods like CoLoR-Filter also benefit from being able to re-use likelihoods multiple times, while RHO-based methods need to recompute the serial cost any time that some hyperparameter of the algorithm.

 We train language models with 150 million non-embedding parameters using the OLMo codebase  and following hyper-parameter choices from . Unless otherwise noted, we use 150m models as the auxiliary models () as well as the target model . Full hyperparameters are described in detail in . 

We take  to be a small dataset of 25 million tokens sampled from the Project Gutenberg Books data subset of Dolma ,  to be a dataset of 3.1 billion tokens from C4 , and  to be all of C4.  We select a dataset  of 3.1 billion tokens (which is approximately the ``chinchilla optimal'' amount for models of this size). To get  or , we fine-tune or train for one epoch on . 

 To evaluate the efficacy of our data selection, we report cross-entropy loss of next token prediction on a held-out dataset  from the same distribution as  (Books). 

 The simplest baseline we consider is  sampling, which has been shown to be a strong baseline for C4 pre-training . We consider all four algorithms described in : ,   , , and . And as one extra baseline, we also include  which estimates n-gram importance weights between  and , and similarly has a parameter like  that controls how aggressively to subselect.

Note that while it is in a similar setting to ours, we do not include DSDM  as a baseline since there is no publicly available code and based on the appendix of that paper, it it much more computationally expensive than the methods we consider.

We first run the domain transfer experiments on 150m models, sweeping across  that controls the selected subset size.  In  we plot how the final performance scales with  across methods. We see that CoLoR-Filter has the best scaling performance with increased , with no sign of saturation for . We hypothesize that by using strong models to select the data, CoLoR-Filter is able to more effectively scale to larger  than the other methods. In  in , we plot the learning curves (evaluated on the held-out validation set) for the four methods introduced in . There, we see especially clean scaling for CoLoR-Filter across the entire learning curve, substantially outperforming random selection with much less data, similar to .

 Finally, we also conduct an experiment in scale generalization (partially shown in ) using the data selected by our 150m auxiliary models to train a 1.2b target model.  In  we show learning curves for a sweep over . We still see consistent gains as we scale  for a fixed number of training tokens.  Interestingly, if we fix the total number of tokens we are  (i.e. where the lines end when we run out of C4), then the final performance with  is better than all other values of . This shows how a strict subset of tokens can outperform a superset (e.g. ). We should also point out here the computational savings when using CoLoR-Filter. As an example, consider  where we match the performance of 25 billion randomly selected tokens with about 1.5 billion filtered tokens. Considering the computational costs discussed above with  and measuring  in billions of tokens, the total cost for training the CoLoR-Filter model is  while the cost for training on 25 billion random tokens is , illustrating a more than 5x total compute savings to achieve the same performance on Books. A full plot visualizing the cost in FLOPs for all  is in . %This only tells us the performance on one task, so in the next section we consider targeting a variety of tasks at once that are more substantially different from language modeling. We target the 8 tasks from the OLMo paper : Hellaswag , PIQA , ARC-challenge and ARC-easy , Openbook QA , SciQ , BoolQ , and Winogrande . Each of these datasets has a separate train split. We use these train splits to construct  as follows: for each question we concatenate the question and the correct answer formatted as a grammatical continuation. Overall, this results in a small  dataset of 7.4 million tokens.  and  are the same as before. And we again get  by fine-tuning  for one epoch on .

 We evaluate on held-out data from each downstream task test or validation sets (using val if test is not publicly available). We use the evaluation procedure from OLMo  which follows  for evaluating these multiple-choice tasks using the rank classification approach of . We report aggregat perfromance across tasks as well as the task-specific performance.

 We use the same baselines as in .

While the curves themselves are noisier now due to the noisier nature of accuracy evaluation on small datasets compared to cross entropy on a large one, the same trends hold as we saw for domain transfer to Books. CoLoR-Filter in particular is scaling the best as we increase . Other methods do not illustrate the same clean scaling as we increase , which is nearly linear on a log scale for CoLoR-Filter, as seen in . Full learning curves are in .

We can also look at the performance broken down by task and illustrated relative to training on an equivalent amount (3.1 billion tokens) of randomly selected data for  illustrated in . We see especially large gains on Hellaswag, ARC easy, Openbook QA and SciQ and actually see performance decreases on BoolQ and Winogrande. However, we should note that at this scale and with all data selected from C4, we actually found BoolQ and Winogrande to be quite noisy and not even correlated with training on 8x as much random data, so it is not clear how much weight to place on those results. Across the other tasks, the gains of CoLoR-Filter over the baselines are clear. It is an interesting direction for future work to probe more deeply into how task-dependent the gains from targeted data selection can be.

 We also consider scale generalization to a 1.2b target model and illustrate the full results of a sweep over  in . Again we find significant benefits of CoLoR-Filter across scales. A full table of per-task results is in .  Again we notice that training on a strict subset of data can outperform a larger dataset.

We can again do out the calculation of computational savings for .  It now takes about 3 billion tokens for CoLoR-Filter to match the performance of training on 25 billion random tokens. This amounts to a total cost of , which is still an upwards of 2.5x reduction in compute to achieve the same average performance across the suite of tasks. A full plot visualizing the cost in FLOPs for all  is in .

Note, we also conduct a few more experiments and ablations in the appendix:  considers using CoLoR-Filter in-distribution to target C4 loss,  considers applying CoLoR-Filter batchwise rather than globally,  considers finetuning on  after targeted pre-training, and  inspects some of the selected and excluded examples.

First, we simply plot the CDFs of the conditional loss reduction (CoLoR) score function used to select the data. We find that there are relatively few outliers and the CoLoR scores are fairly concentrated and normally distributed. Moreover, we note that the mean CoLoR in both experiments is positive, meaning that the conditional model actually has higher losses on the datapoints in C4 than the marginal model. This makes sense because the conditional model has been finetuned on  which is out of distribution relative to C4.

Now we just list a few representative examples to give a flavor for the types of outliers that exist under our ranking of sequences and the sorts of typical sequences that are selected versus excluded. The sequences are sampled randomly from different quantiles of the distribution and we shorten all the sequences so that they fit more easily on the page. 

 shows outliers when targeting Books and  shows more typical examples when targeting Books. Generally, we found that the documents with very high scores contain things like old English, poetry, and tables of contents that are particularly unusual in books compared to the rest of the internet. Other things like fiction and dialogue are also highly scored. Negative outliers typically have things like poorly encoded text or advertisements.

 shows outliers when targeting downstream tasks and  shows more typical examples when targeting downstream tasks. Here the patterns are less clear since the target tasks are more diverse, but we did observe many scientific and wiki-style documents with high scores as well as some descriptions of physical interactions that may be useful for common sense tasks. Again, the negative outliers tend to have things like poorly encoded text or advertisements. 

% % The development of the CoLoR-Filter for data selection has notable broader impacts on both machine learning and society. It enhances efficiency in language model training, leading to reduced computational resources and environmental footprint, while its scalability democratizes access to high-performing models. The method's success in diverse downstream tasks promises advancements in fields like medical text processing and legal analysis. However, it also raises concerns about dataset bias, necessitating continuous evaluation and updates. Future research should focus on ensuring models do not inherit biases from the selected training data, extending applications, improving efficiency, and implementing safeguards to maximize societal benefits while minimizing risks.% % All training is conducted on an internal cluster using H100 GPUs. On one GPU, each 150m training run for 3.1b tokens takes about 4 hours, running the auxiliary models offline and in parallel can be faster. Training the 1.2b model to completion takes about 2 days on 4 GPUs. %  Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.

In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks. 

Code: 

Filtered data: Introductionrae2021scaling, penedo2023refinedweb, cerebras2023slimpajamaeval-harness, magnusson2023paloma, engstrom2024dsdm, chang2024surveysec:derivationsmindermann2022prioritizedsec:booksraffel2020exploringsec:downgroeneveld2024olmomindermann2022prioritizedfig:1bSetting and Derivationssec:derivationsmindermann2022prioritized,evans2023badxie2023dataengstrom2024dsdmsec:relatedBayesian Data Selection     \min_{S \subset \Dtrain, |S| = n} -\log \Pr (\Ddown | S),

    \min_{S \subset \Dtrain, |S| = n} -\log \Pr(S| \Ddown) + \log \Pr(S) - \log \Pr(\Ddown)

     \min_{S \subset \Dtrain, |S| = n} _{} + _{} Prior work \citep has referred to the models that estimate these two terms as the ``reference'' and ``learner'' or ``actor'', respectively. We opt for the names conditional and marginal for clarity in connections to the Bayesian viewpoint.lindley1956measure, moore2010intelligent, houlsby2011bayesian, bickfordsmith2023prediction, kirsch2023a, rainforth2024modernCoLoR-Filtersec:color-filter      \min_{S \subset \Dtrain, |S| = n} -\log \int_\theta\Pr(S| \theta)\Pr(\theta | \Ddown, \Dprior) + \log \int_\theta \Pr(S | \theta)\Pr(\theta| \Dprior)

     \approx \min_{S \subset \Dtrain, |S| = n}  -\log \Pr(S | \thetapriordown) + \log \Pr(S | \thetaprior),

    \min_{\{x_1,\dots, x_{n}\} \subset \Dtrain}   -\log \prod_{i=1}^n \Pr(x_i| \thetapriordown) + \log \prod_{i=1}^n \Pr(x_i | \thetaprior)

     \min_{\{x_1,\dots, x_{n}\} \subset \Dtrain}  \sum_{i=1}^n -\log \Pr(x_i| \thetapriordown) -  (-\log \Pr(x_i | \thetaprior)) A note on data diversity.app:onlineRelated Algorithmssec:related_algsConnection to importance sampling.xie2023dataConnections to DSDM.engstrom2024dsdmilyas2022datamodels, park2023trakilyas2022datamodelsConnections to RHO-down.mindermann2022prioritizedeq:bayes      \approx \min_{x_i \in \Dtrain}   -\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | \Ddown, x_{<i}) + \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| x_{<i}),

     \approx \min_{x_i \in \Dtrain}  -\log \Pr(x_i| \thetadownx) + \log  \Pr(x_i | \thetax)

     \approx \min_{x_i \in \Dtrain} -\log \Pr(x_i| \thetadown) + \log  \Pr(x_i | \thetax). eq:bayes      \approx \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | \Ddown, x_{<i}) + \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| x_{<i})

     \approx \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -\log \Pr(x_i| \thetadownx) + \log  \Pr(x_i | \thetax)

     \approx \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -\log \Pr(x_i| \thetadown) + \log  \Pr(x_i | \thetax) app:onlineRHO-down + prior.Further Related Worksec:relatedActive \& Curriculum learninghoulsby2011bayesian, bickfordsmith2023prediction, kirsch2023alindley1956measure, rainforth2024modernpukelsheim2006optimalsener2018active, ash2019deep, ash2021gonee.g.graves2017automatedmindermann2022prioritizedlin2024rho1evans2023bad, tack2024learningData curation practices in pre-trainingwenzek2020ccnet, elazar2023s, sorscher2022beyond, allenzhu2024physicsbrown2020languagelee2022deduplicatingtouvron2023llama, touvron2023llama2raffel2020exploring, rae2021scaling, together2023redpajamaschuhmann2022laion, abbas2023semdedup, fang2023dataxie2023datajoulin2017bag, brown2020languagetirumala2024d4hoffmann2022training, meta2023llama3gunasekar2023textbooksAlgorithmssec:algsCoLoR-Filter Prior data , downstream data , training data , budget ,  subset size multiplier    % \STATE Sample a subset  of size  Pre-train  on     fine-tune to get  on  initialized from     Select a random subset  of size  from     Select data:    Selected dataset  to train  on. alg:color-filter-0.4cm         S =  n_{x \in D_\tau}  -\log \Pr(x| \thetacond) + \log \Pr(x| \thetamarg)     -0.5cmFrom Derivations to Practical AlgorithmsCoLoR-Filter.alg:color-filtersec:computeConditional only.evans2023badalg:color-filterRHO-down Downstream data , train data , budget , subset size multiplier , batch size     Train  on     Initialize a random  and     Randomly select a batch  of size     Select data:      Update  to  by training on       Selected dataset  to train  on. alg:rho-0.4cm         \bar B_t = b_{x \in B_t} - \log \Pr(x| \thetacond) + \log \Pr(x| \thetamarg_t)     -0.5cmRHO-down.alg:rhosec:derivationsRHO-down + Prior.alg:rhoComputational Costsec:computefleuret2023littlehoffmann2022training, casson2023transformerflopstab:costScale transfer.evans2023badlargeEven though there are 8x as many parameters in the large model, the FLOP multiplier is less since the attention computations take the same number of FLOPs regardless of parameters.CoLoR-Filter.entirelyConditional Only.RHO-down.RHO-down + Prior.Domain Transfer: a Simple Testbedsec:booksSetupTraining.groeneveld2024olmowortsman2024smallscalesec:hyperparamssoldaini2024dolmaraffel2020exploringEvaluation.Baselines.Randomengstrom2024dsdmsec:algsCoLoR-FilterConditional OnlyRHO-downRHO-down + priorDSIRxie2023dataengstrom2024dsdmResults-1.5cmwidth=0.6\textwidthimages/books_tau.pdf-0.6cmScaling of final performance with  when targeting \textbf with 150m parameter models. CoLoR-Filter scales best with .fig:books_taufig:books_taufig:booksapp:curvessec:algsfig:1b-0.6cmwidth=0.5\textwidthimages/1b_all_books.pdf-0.6cmScaling CoLoR-Filter with  when training 1.2b models with data selected using smaller 150m models. Curves end when we exhaust the data in C4.fig:books_1b_tauScale generalization.fig:1bfig:books_1b_tauselecting fromapp:flopsDownstream Taskssec:downSetupTraining.groeneveld2024olmozellers2019hellaswagbisk2020piqaclark2018thinkmihaylov2018canwelbl2017crowdsourcingclark2019boolqsakaguchi2021winogrande-1.5cmwidth=0.6\textwidthimages/down_tau.pdf-0.7cmFinal performance versus  on the suite of downstream tasks for 150m models. CoLoR-Filter scales the best with .fig:down_tauEvaluation.groeneveld2024olmoeval-harnessbrown2020languageBaselines.sec:booksResultsfig:down_tauapp:curvesfig:heatmap-0.6cmwidth=0.5\textwidthimages/1b_all_down.pdf-0.6cmScaling CoLoR-Filter with  when training 1.2b models with data selected using smaller 150m models. Curves end when we exhaust the data in C4.fig:down_1b_tauScale generalization.fig:down_1b_tauapp:tableapp:flopsapp:idapp:batchwiseapp:finetuneapp:analysisDiscussionAcknowledgmentsplainnatreferencesLearning curves for 150m modelsapp:curvesTables of downstream resultsapp:tableData diversity and online vs. offline selectionapp:onlinemindermann2022prioritizedmindermann2022prioritizedeq:bayes      \approx \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | \Ddown, x_{<i}) + \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| x_{<i}) das2018Nemhauser1978AnAOash2021gone, mindermann2022prioritized      \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -&\log \int_\theta\Pr(x_i| \theta)\Pr(\theta | \Dprior, \Ddown, x_{<i}) \\ &+ \log \int_\theta \Pr(x_i | \theta)\Pr(\theta| \Dprior, x_{<i})

     \approx \min_{x_1, \dots, x_n \subset \Dtrain} \sum_{i=1}^n  -\log \Pr(x_i| \thetapriordownx) + \log  \Pr(x_i | \thetapriorx) mindermann2022prioritizedfig:onlineCompute cost for scale generalizationapp:flopsfig:costsfig:1bCan we do data selection in distribution?app:idfig:in_distGlobal vs. batchwise selectionapp:batchwisealg:color-filteralg:rhofig:batchFinetuning after targeted pre-trainingapp:finetunetab:150m-books-finetab:150m-down-finetab:1.2b-books-finetab:1.2b-down-fineHyperparameterssec:hyperparamsInspecting the selected dataapp:analysisDistribution of scoresRepresentative examplesfig:books_outliersfig:books_typicalfig:down_outliersfig:down_typical