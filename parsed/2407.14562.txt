Consider a LLM, denoted by , which is parameterized by . Given an instruction dataset  consisting of question and answer pairs , a highly effective method for aligning  with the target output  given  is to minimize the negative log-likelihood of  conditioned on  . This can be formulated autoregressively as:

where  is the number of tokens in .

To enhance reasoning performance on complex logical and algorithmic tasks, one effective approach involves using a specialized prompt, , to initiate a step-by-step reasoning trajectory prior to generating the final output: . Here,  represents a CoT reasoning process . Typically, this process decomposes complex, multi-step reasoning tasks into simpler, intermediate steps, thereby directing the LLM toward the correct answer. Nevertheless, several studies  have indicated that  may sometimes exhibit disorganized patterns that do not adhere strictly to logical reasoning processes. This inconsistency can adversely affect the LLM's reasoning performance across diverse tasks.

Prolog is a high-level symbolic language system designed for rule-based reasoning , fundamentally operating within a subset of predicate logic known as Horn Clause logic . It is renowned for its declarative programming paradigm, which contrasts with the imperative approach by emphasizing relations over the sequence of actions. In Prolog, the logic of computation is articulated through relations, denoted as facts  and rules . Typically, Prolog utilizes a depth-first search strategy  to trace a feasible logical inference path to the desired conclusion, yet it is also equipped to delineate all possible logical inference paths leading to the target results . The entire logical inference process within the Prolog engine, denoted as , can be represented by the following equation:

where  encompasses  valid logical inference trajectories toward the target .

Here, we introduce , a novel framework designed to enhance the general logical reasoning capabilities of LLMs across various logical tasks. As suggested by the name, our framework enables LLMs to imitate strictly logical reasoning trajectories that are generated and validated by the Prolog engine. The architecture of  is organized in a self-driven manner, eliminating the requirement for auxiliary services from other advanced LLMs, such as GPT-4  and Claude 2 . It solely relies on the utilization of efficient, open-source LLMs (i.e., Llama3-8B-Instruct ), fostering accessibility and ease of replication.

To employ the Prolog engine  for logical reasoning, we initiate the process by constructing a few-shot demonstration  (Listing ). This is designed to guide the model  in generating relevant rules , facts , and deriving the reasoning target :  Subsequently, the Prolog engine is employed to deduce the set of reasoning trajectories , which encompasses all logical paths leading to the target . This process is formalized as:  During this phase, we selectively utilize only those reasoning trajectories that conclusively reach the target , excluding any data that result from erroneous transformations or generation failures as delineated in Equation~. To ensure the acquisition of high-quality and interpretable reasoning trajectories via Prolog, we implement a meta-solver developed by , which retrieves all feasible reasoning paths.

After acquiring the set , we design a novel few-shot prompt,  (Listing ), to translate existing reasoning trajectories into CoT-like reasoning processes expressed in natural language. This transformation aids in constructing the new dataset :  It is important to note that this new dataset, , encompasses all  available reasoning trajectories for each instruction . Subsequently, we optimize the model using SFT as outlined in Equation~, aligning both the CoT-like reasoning trajectories and the target outputs conditioned on  autoregressively:

resulting in a trained model parameterized by . This entire learning process parallels imitation learning algorithms  that select ``optimal'' trajectories (here verified by the Prolog engine) and implement behavior cloning to directly imitate these trajectories. After training according to Equation~, the model  is tailored to perform domain-specific tasks present in . However, if the task distribution of  significantly diverges from that of , catastrophic forgetting (CF) may occur , potentially compromising the model's generality across other reasoning and general tasks. To address this, we employ a model averaging technique  to derive a new model  parameterized by : , where  is a hyper-parameter that balances specialization and generalization.

Throughout the entire process, we utilized two A800-80GB  GPUs for inference, fine-tuning, and evaluation tasks. We applied our  framework by continually fine-tuning the Llama3-8B-Instruct model which can be accessed in .

Our baseline was established by assessing the untrained Llama3-8B-Instruct model's performance on various datasets. This initial evaluation gave us a benchmark to demonstrate the enhancements our framework could provide. Additionally, we implemented the CARING  method to generate Prolog code with Llama3-8B-Instruct, assessing the correctness of solutions this code produced to problems. 

%  We evaluated  across seven prominent LLM benchmark datasets: two focused on mathematical reasoning (GSM8K and MATH), two on knowledge-based question answering (MMLU and GPQA), two on logical reasoning (ProofWriter and PrOntoQA), and one on code generation (HumanEval).

 GSM8K  comprises 8,500 high-quality, linguistically diverse grade school math word problems created by human experts. We randomly selected 2,000 entries from this dataset to generate reasoning trajectories and used the balance to gauge the effectiveness of our  framework. Given that the number of reasoning trajectories generated by the Prolog engine is variable, we capped the trajectories for each problem at 10 to mitigate data distribution bias. Consequently, we produced 20,000 pieces of training data for the LLM.

 MATH  encompasses a collection of 12,500 challenging competition-level mathematics problems, each accompanied by detailed step-by-step solutions. These solutions facilitate training models to generate complete derivations and explanations. We leveraged this difficult dataset to assess the OOD performance of our  framework, noting that the LLM was not previously trained on it.

 MMLU  benchmark is designed to gauge the knowledge acquired during pretraining by assessing models in exclusively zero-shot and few-shot scenarios, which closely aligns with methods used to evaluate human capabilities. It encompasses 57 subjects, spanning STEM, the humanities, and the social sciences, among others. Given its wide content range and varying difficulty, we utilize the entire dataset to evaluate the  framework's generalization abilities. Notably, within the  framework, the LLM does not have exposure to MMLU during the training phase.

 GPQA  represents a formidable dataset aimed at testing the capabilities of LLMs alongside scalable oversight mechanisms. The dataset comprises 448 multiple-choice questions crafted by domain experts in disciplines such as biology, physics, and chemistry. We employ this complete set to determine the generalization capacity of the  framework.

 ProofWriter  is a widely utilized logical reasoning dataset comprising many small-scale knowledge bases expressed in English, each containing facts and rules. Each knowledge base is paired with a set of questions, also in English, which can be definitively proven as true or false via proofs of varying depths, or the answer may be categorized as ``unknown'' under an open-world assumption (OWA) or presumed negative under a closed-world assumption (CWA). This dataset contains subsets of varying difficulty, we have selected the most challenging subset within the OWA setting, which includes 482 knowledge bases and 10,190 questions. We initially randomly sampled 100 knowledge bases and their corresponding questions to create reasoning trajectories. We restricted the number of allowed reasoning trajectories per question to five. Ultimately, we utilized all remaining data to evaluate the performance of our  framework.

 PrOntoQA  is a synthetic question-and-answer dataset designed to test the logical reasoning capabilities of LLMs. Each instance is structured to verify the validity of a statement within a specific context. We chose the most demanding subset , ``Fictional Characters'', with a depth level of 5, comprising 500 statement-context pairs. We randomly selected 100 statement-context pairs to generate reasoning trajectories and employed the remaining data for assessing the  framework's capabilities. The maximum number of reasoning trajectories per question was set to 5.

 HumanEval  comprises 164 unique programming problems designed to evaluate language comprehension, algorithmic proficiency, and elementary mathematics, some of which are akin to basic software interview questions. This dataset is employed to assess the functional correctness of programs synthesized from documentation strings. Given that HumanEval is unrelated to the two domains—mathematical reasoning and logical reasoning—used for training the LLM, we employ the entire dataset to assess the  framework's performance on general tasks following specialized training.

We perform typical continual supervised fine-tuning based on the checkpoint of Llama3-8B-Instruct. Our training framework follows the implementation of  using the LLaMA-Factory. we employed the Adam optimizer in the training process and selected a consistent learning rate of  for  epochs of training. To ensure comparability between the single trajectory and multiple trajectory datasets, despite their differing data volumes, we have adjusted the training regimen. Specifically, the model trained on the single trajectory dataset undergoes an equivalent number of training steps as the model trained on the multiple trajectory dataset for  epochs. During model inference and evaluation, we configured the sampling settings with a temperature of  and a top value of , based on the default recommended parameters from . For model averaging, we implement  for a balanced averaging between the base model and the fine-tuned model.