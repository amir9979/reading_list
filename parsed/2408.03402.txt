ULLME addresses the limitations of existing LLM-based dense retrieval frameworks by offering a flexible and comprehensive solution. The framework operates in three main stages. First, it enables bidirectional attention within LLMs by replacing the causal attention mask with a bidirectional one. This crucial modification extends the models' ability to consider both past and future context when generating embeddings, significantly enhancing its capacity for dense retrieval tasks. The transformed model is then returned as a PyTorch object, providing users with the flexibility to integrate it into various frameworks or pipelines. We will elaborate on this process in Section . Second, ULLME supports a diverse array of fine-tuning strategies, including Contrastive Learning, Supervised Fine-tuning (SFT), Direct Preference Optimization (DPO), and our novel Generation-augmented Representation Learning (GRL). This versatility allows for tailored optimization across a wide spectrum of retrieval tasks and domains, as detailed in Section . Finally, the framework streamlines the evaluation process by incorporating direct support for model validation using the Massive Text Embedding Benchmark (MTEB) library (Section ). This integration facilitates comprehensive assessment across numerous retrieval and embedding tasks. By seamlessly combining these elements, ULLME provides an extensive toolkit for leveraging LLMs in diverse dense retrieval tasks, encompassing everything from initial model adaptation to fine-tuning and evaluation. Our comprehensive approach aims to accelerate research and development for of LLM-based dense retrieval, offering researchers and practitioners a comprehensive platform for innovation and advancement.

To enable bidirectional attention in LLMs, ULLME requires only minimal code modifications, as illustrated in Listing 1. The framework's user-friendly design allows for easy initialization with various LLM backbones by simply specifying the  and  parameters. ULLME seamlessly integrates with Hugging Face Transformers, loading pre-trained LLMs directly from their repository. Additionally, our framework supports parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA) , offering flexibility in model adaptation. Once initialized, the model can be used to compute sequence representations. The  parameter plays a crucial role in controlling the attention mechanism: when set to , the model employs bidirectional attention, optimizing it for dense retrieval tasks, while  reverts the model to causal attention, mimicking the standard Hugging Face Transformer model output. This dual functionality allows ULLME to serve both as an advanced specialized embedding model and as a language model when needed, providing developers with a flexible tool that can conveniently transition between bidirectional and causal attention modes. ULLME provides various methods for extracting text embeddings from LLMs, such as using representations from the first token, last token, mean, or weighted mean pooling. However, it defaults to averaging the representation vectors from the final layers (mean) for better performance on our datasets.

%ULLME supports several approaches to obtain text embeddings from LLMs although it uses the average of representation vectors in the last layers as the default due to the greater performance.

Our ULLME framework supports multiple fine-tuning strategies, as illustrated in Listing 2.

 ULLME's Contrastive Learning objective utilizes in-batch negatives . The contrastive loss is formally defined as: .

Here,  represents a mini-batch,  is the input query,  denotes the positive (relevant) passage, and  represents negative (non-relevant) passages sampled from the current training mini-batch. The function  computes the relevance score between a query and a passage using cosine similarity of the induced representations for  and . To enhance the effectiveness of Contrastive Learning, especially under limited GPU memory constraints, ULLME incorporates advanced techniques such as GradCache  and cross-device contrastive loss computation. These optimizations allow for efficient training with larger batch sizes and more diverse negative samples, which are crucial for learning high-quality representations.

 In addition to contrastive learning, ULLME supports SFT, a strategy that enhances LLMs' ability to generate high-quality passages in response to queries. ULLME implements SFT using a next-word prediction objective: . Here,  is the length of the positive passage ,  is the -th token in , and  is the conditional likelihood of  given , computed by the LLM . Importantly, during SFT loss computation, ULLME reverts to using causal attention, mirroring standard LLM behavior.

ULLME incorporates Direct Preference Optimization (DPO)  as an advanced fine-tuning strategy, offering an alternative to traditional Supervised Fine-tuning (SFT). DPO has demonstrated superior effectiveness in LLM fine-tuning. Moreover, the DPO approach inherently accounts for both preferred and rejected outputs, making it intuitively more suitable for aligning models with text-ranking objectives compared to SFT. In ULLME's implementation, the ground-truth relevant passage  for a query  is treated as the preferred output, while negative and irrelevant passages  are considered dispreferred. The DPO loss function is designed to encourage the model to assign higher generation probabilities to  compared to any : . In this formulation,  represents the sigmoid function,  is a scaling factor, and  denotes the conditional likelihood computed by the original pre-trained LLM (the reference model). 

In addition to the standard DPO formulation, ULLME includes implementations of advanced variants such as Kahneman-Tversky Optimization (KTO)  and Contrastive Preference Optimization (CPO) . The modular architecture of ULLME facilitates the seamless integration of new preference optimization techniques as they emerge, ensuring that the framework remains at the forefront of LLM fine-tuning advancements. Finally, to maintain consistency with the model's pre-training paradigm, ULLME employs causal attention when computing the DPO loss, similar to the approach used in SFT.  

 ULLME further introduces a novel fine-tuning strategy GRL that explicitly aligns the LLMs' understanding of passage-query text relevance in embedding and generation spaces to boost representation learning. As such, GRL first computes a generation-based relevance score  utilizing the conditional generation likelihood of a passage candidate  given input query  from LLMs: , where  is the length of  and  is the -th token in . 

Next, we seek to recognize the consistency of the query-passage relevance scores obtained from the representations (i.e., ) and the generation likelihood (i.e., ). Particularly, let  be the set of  candidate passages for . For each candidate passage , we compute  and , then normalize these scores to obtain the representation and generation relevance distributions over :  and .

Afterward, we minimize the KL divergence between their distributions: , serving as a training signal to enrich representation learning for LLMs.

Finally, the overall training loss for GRL combines the contrastive loss , the direct preference optimization loss , and the KL-divergence loss : , where , , and  are weighting hyperparameters.

ULLME streamlines the evaluation process by integrating direct support for evaluating LLM-based text embedding models over MTEB, a widely-used Massive Text Embedding Benchmark with diverse tasks and datasets. This integration facilitates comprehensive model development with different methods and extensive assessment across numerous retrieval and embedding tasks in a single framework. ULLME wraps a fine-tuned model into a  instance, ensuring compatibility with MTEB's requirements for direct evaluation. In addition to supporting ULLME's fine-tuned models, our evaluation function is designed to perform seamlessly with most LLM models available in the Hugging Face ecosystem, including the latest LLM-Embedding models in the MTEB leaderboard. Users can easily specify the desired model through the  parameter, enabling effortless evaluation of various LLMs without the need for extensive configuration. ULLME allows users to select specific datasets and language subsets for evaluation. The evaluation results are reported using MTEB's predefined main scores of the corresponding dataset, ensuring standardized and comparable metrics across different models, as demonstrated in Listing 3.