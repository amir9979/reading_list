As depicted in Figure , temporal point cloud coordinates are encoded using Fourier basis functions to encapsulate periodic information as follows:

To enhance the modeling capability for periodic features, the original coordinate features are augmented by incorporating their sine and cosine values. Features from  are further encoded using a 4D neural field. The iterative Gaussian cloud soft clustering" module employs Gaussian statistics to extract characteristics such as mean, covariance and geometric features from the point cloud, offering a statistical description of internal dynamic changes. Furthermore, the proposed temporal radial basis function Gaussian residual (RBF-GR) module employs radial basis functions to interpolate these Gaussian parameters across different time steps. The designed 4D Gaussian deformation field module learns the temporal evolution of point cloud Gaussian parameters, generating a continuous spatiotemporal deformation field. Finally, an fast latent-geometric fusion module adaptively fusion features, enabling the generation of point clouds through a prediction head.

The raw point cloud coordinates lack topological structure and contain noise and redundancy. We propose an iterative Gaussian soft-clustering point cloud representation module (Algorithm ) that converts point clouds into structured Gaussian representations. The module introduces DGCNN  (Eq. ) to extract local geometric features, where  is the -th point,  is its k-nearest neighbor,  is the graph convolution operation.  is the convolution parameter.  represents tensor concatenation;  Self-Attention (Eq. ) to capture global contextual information, where , ,  are the weight parameters of the query, key, and value matrices, respectively, and  is the dimension of the key matrix.

Except for Gaussian cloud representation, we introduce 4D neural fields, parameterizing a continuous, latent spatio-temporal field function using multi-layer perceptron (MLP) networks, as shown in Fig. . This maps low-dimensional spatio-temporal coordinates  to a high-dimensional latent feature space, representing the motion and changes of point clouds in the spatio-temporal domain. Compared to Gaussian point cloud representations, deep networks can fit more complex non-linear spatio-temporal correlations.

Point cloud data from LiDAR includes noise, which, while not affecting subsequent perception tasks, impedes the learning of temporal consistency. We apply statistical outlier removal  during preprocessing to eliminate noise by calculating each point's average distance to its neighbors and identifying outliers using global mean and standard deviation. Experiments show (Table ) this method markedly decreases interference during network training.

This module accomplishes the interpolation and updating of Gaussian distribution parameters in the continuous time domain, which can be primarily divided into three parts (Fig. ):  The RBF Activator converts discrete time steps into continuous time representation by mapping scalar time  to radial basis function activation values . RBF networks encode time locally, with each kernel function centered at a specific time  and exponentially decaying activations. The RBF activation values  reflect the similarity between time  and center times , enabling smooth interpolation of Gaussian parameters.  The Gaussian Interpolator interpolates the mean  and rotation matrix  of the Gaussian distribution using RBF activation values. Gaussian parameters estimated at discrete time steps are interpolated to obtain parameter residuals  and  from time  to . The interpolation process exploits the linearity and closure property of Gaussian distributions.  The Feature Interpolator interpolates Gaussian features using RBF activation values to obtain a continuous feature representation in time. The module enables smooth updating of Gaussian distribution parameters in continuous time, allowing effective fitting and generation of time-varying 3D point cloud sequences.

% The Temporal-RBF-GR employs radial basis functions (RBFs) with learnable centers  and scales  to capture the temporal evolution of Gaussian parameters. % This module enables smooth and expressive modeling of the temporal dynamics of Gaussian distributions for accurate point cloud interpolation.% % % _i^{rbf}(t_2) = W_{rbf} \cdot {2s_i^2})}{\sum_{j=1}^{N}\exp(-{2s_j^2})},} \\% ^{(\theta)}, (\Delta R_{t_1\rightarrow t_2})^{(\theta)} = \sum_{i=1}^{M}_i^{rbf}(t_2) \cdot (\mu_i^{t_1}, (R_i^{t_1})),} \\%  = \Delta R_{t_1\rightarrow t_2}\Sigma_{t_1} \Delta R_{t_1\rightarrow t_2}^T,} \\% _{t_1\rightarrow t_2} = \sum_{i=1}^{M}_i^{rbf}(t_2) \cdot \Phi^{feat}_{i}}%  The temporal Gaussian graph convolutional (TG-GCN) deformation field plays a key role in capturing the spatiotemporal features from Gaussian point cloud representations (Fig. ). It takes as input the Gaussian means , covariances , and features  extracted from the time-series radial basis function Gaussian residual module, extracts the geometric topological features of local and global Gaussian clouds through the TG-GCN structure, effectively capturing geometric and temporal patterns.  The information on   aids in encoding the average positions () of points at different time steps, modeling overall motion and deformation trends;  The information on   describe the deformation degree and direction of each point over time, facilitating the modeling of complex dynamics; The   extract the geometric and temporal patterns in the point cloud, enhancing the model's expressive power and providing geometric features. Furthermore, the incorporation of temporal information aids in establishing the temporal consistency relationships within the 4D spatiotemporal point cloud. 

Based on the features extracted from the deformation field, the  and  respectively predict the motion flow and feature field (Fig. ), with the core principle being the learning of a continuous deformation field, utilizing the time information in the Gaussian point cloud representation and the spatial dependencies captured by graph convolution, to achieve smooth and realistic deformation of the point cloud, enabling accurate modeling of 4D point cloud sequences and interpolation between time steps.

The Gaussian representation pooling module first projects the original point cloud coordinates and latent features onto Gaussian spheres centered at each point, leveraging the previously established index mapping between the points and the Gaussian spheres (Sec. ). It then applies max-pooling to extract the most prominent features within each Gaussian sphere, enhancing the model's perception of deformation flow and features. However, this Gaussian sphere projection and max-pooling operation inevitably leads to a certain degree of information loss. To compensate for the potential information loss in the feature extraction process of the Gaussian representation pooling module, we further introduce a neural field module to learn and refine the point cloud deformation flow and feature expressions.

% % }, } \&= \phi(, , , ) \% ^, ^ \&= \psi(}, }) \% ,  \&= (^, ^)%  The proposed module fuses the complementary advantages of the latent features  from point cloud latent representations and the geometric features  from 4D Gaussian deformation fields via an attention-based mechanism.  serves as the query, while  serves as the key and value. Through linear projections (, , ), the features are mapped into a shared latent space. An attention score is computed between the projected query and key, which is then normalized via softmax to obtain attention weights. These weights adaptively assign importance to different feature components, enabling effective fusion. The weighted sum of the projected value features is then computed, integrating multi-modal information. Finally, a residual connection with the original query  yields the fused representation , preserving temporal information:

The module's efficiency stems from its simple linear operations and avoidance of iterative or recurrent mechanisms, enabling fast computation. Furthermore, the global attention mechanism provides a comprehensive receptive field, enhancing representational capacity compared to naive concatenation. By adaptively fusing geometric and temporal features, the module improves the model's expressiveness for capturing complex spatio-temporal patterns in large-scale dynamic LiDAR scenes.

The 3D Point Cloud Prediction head accurately predicts the point cloud at the target time step by fusing multi-source spatio-temporal information, including features  from the 4D neural field and Gaussian deformation field, time encoding  representing the target time step, and the initial prediction . This fusion provides rich spatio-temporal priors, enabling the model to reason about the target point cloud's structure and motion patterns. Through a lightweight MLP, the fused features are directly mapped to the residual flow , avoiding prediction from scratch and leveraging the initial interpolated flow field based on the Gaussian deformation field:

Adding  to the current frame accurately predicts the point cloud at the target time step. Finally, we use three self supervised loss functions: chamfer distance (CD) loss , earth mover's distance (EMD) loss , and smoothness consistency loss  to complete the self supervised learning. Specific details are shown in .

% CD is insensitive to point cloud density and distribution, focusing primarily on shape matching, whereas EMD is highly sensitive to these factors, accurately reflecting structural consistency. Dynamic Human Bodies (DHB) dataset  contains 14 sequences of non-rigid human motions. The NL-Drive  dataset captures large-scale motion scenes from KITTI , Argoverse 2 , and Nuscenes  for autonomous driving. KITTIs  and KITTIo  are two versions of the KITTI Scene Flow dataset with and without occlusion masks, respectively.  

Like existing methods , we use Chamfer Distance (CD) and Earth Mover's Distance (EMD) to measure the consistency between predicted and ground truth point clouds. For the 3D scene flow task, we employ two error metrics  and  and two accuracy metrics,  and , to quantify performance. 

We sample the input points to 1024 for object-level scenes and 8192 for autonomous driving scenes. NeuroGauss4D-PCI consists of 5 components, as shown in Table .  Refer to the  for the parameter settings of each component and more experimental details.

Table  demonstrates the superior performance of the proposed method, consistently outperforming others on CD and EMD metrics, achieving near-zero errors for sequences like Squat and Swing. The dynamic soft Gaussian representation effectively models non-rigid deformations in human motions by transforming raw point clouds into multiple Gaussian distributions, with radial basis functions learning their temporal residuals. Fusing 4D neural field features preserves per-point detail. The extremely low errors on the DHB dataset validate the model's excellence in non-rigid motion modeling and point cloud interpolation. Notably, the proposed method achieves the best performance with significantly fewer parameters (0.09M) compared to methods like PointINet  (1.30M) and NeuralPCI  (1.85M). This is attributed to the efficient Gaussian representation and deformation field design, enabling superior performance through a more compact model.

On large-scale autonomous driving LiDAR datasets , NeuroGauss4D-PCI demonstrates exceptional temporal point cloud prediction/interpolation performance, significantly outperforming uni-directional 3D flow  and neural field method , as shown in Table . It effectively handles challenges in temporal LiDAR scenes, such as large-scale non-linear motions, occlusions, and non-uniform data distributions (Fig. ). NeuroGauss4D-PCI models the 4D temporal point cloud as a Gaussian deformation field with continuous and differentiable Gaussian representations, ensuring smooth interpolation and robust predictions. Temporal Gaussian graph convolutions capture local and global spatio-temporal correlations, enabling fine-grained motion prediction. Notably, Gaussian representations offer parameter efficiency, expressing complex geometric shapes and non-linear motions smoothly with fewer parameters compared to neural field methods.

%  To demonstrate the effectiveness of the proposed point cloud interpolation model in capturing complex deformations and transient motion patterns between two frames, we evaluated its performance on 3D scene flow estimation using the Scene Flow KITTI dataset (both non-occluded  and occluded ), using error and accuracy metrics. As shown in Table , our method outperformed approaches based on feature pyramids (PointPWC , Bi-PointFlow ), complex 3D point cloud transformers (PT-FlowNet , GMSF ), and self-supervised learning (SPFlowNet , SCOOP ) across multiple metrics. Unlike other methods that require learning inter-frame correspondences, NeuroGauss4D-PCI only needs a single point cloud and target timestamp to predict the corresponding point cloud during inference. In the learning phase, NeuroGauss4D-PCI accurately models complex non-rigid deformations and details in temporal point clouds by integrating latent neural fields, iterative Gaussian representations, and 4D deformation fields. Its compact iterative Gaussian representations significantly reduce parameters while effectively capturing continuous point cloud changes by learning temporal residuals of Gaussian distribution parameters through radial basis functions, exhibiting superior capability in inter-frame point cloud matching.

% {1.0mm}% [t]% \centering% % %     % %     {!}{% % }% Table  shows the effect of each component on model performance. Using just the neural field, the Chamfer Distance (CD) measures 0.58 on the DHB dataset and 1.06 on the NL-Drive dataset, showcasing its capacity to capture spatio-temporal features. Adding the Gaussian point cloud representation slightly lowers the CD by 0.01 and 0.03 on DHB and NL-Drive, respectively, indicating limited benefits from this integration. Removing the neural field and relying solely on the Gaussian point cloud and T-RBF-GR module significantly worsens performance, emphasizing the neural field's critical role in modeling 4D spatio-temporal point clouds. However, integrating the 4D Gaussian Deformation Field with the Gaussian representation markedly enhances performance, with a 14\% and 19\% decrease in CD and a 17\% and 6.5\% reduction in Earth Mover's Distance (EMD) on the two datasets, respectively. This highlights the deformation field's effectiveness in accurately representing dynamic variations. Adding the T-RBF-GR module further improves performance, showcasing its utility in addressing dynamics and temporal correlations. By fusing the latent features and the explicit 4D deformation field features, we effectively reduce errors in point cloud prediction. The Fast-LG-Fusion module, in particular, achieves the best performance, attaining the lowest CD and EMD across datasets, except for a slight 0.02 increase in CD on the DHB dataset compared to direct concatenation.

% Table.  demonstrates the impact of each proposed component on the model's performance. When using only the neural field, the Chamfer Distance (CD) is 0.58 on the DHB dataset and 1.06 on the NL-Drive dataset, indicating its ability to capture spatio-temporal features and provide basic temporal encoding. However, incorporating the Gaussian point cloud representation yields minimal improvement, reducing the CD by 0.01 and 0.03 on DHB and NL-Drive, respectively, suggesting that directly incorporating Gaussian representations into the neural field does not fully leverage its potential.% Conversely, removing the neural field and using only the Gaussian point cloud representation and T-RBF-GR module leads to a significant performance degradation, highlighting the importance of the neural field in modeling 4D spatio-temporal point clouds. Notably, combining the 4D Gaussian Deformation Field with the Gaussian representation significantly improves the performance, with CD decreasing by approximately 14\% and 19\% on the two datasets, and EMD reducing by 17\% and 6.5\%, respectively. This demonstrates the effectiveness of the 4D Deformation Field in modeling the dynamic variations of point clouds, providing more accurate spatio-temporal feature representations crucial for complex dynamic scenes.% Further incorporating the T-RBF-GR module yields additional performance gains, indicating its potential in handling dynamics and temporal correlations, although its individual impact may be limited. Fusing the implicit features with the explicit 4D Deformation Field features, either through direct concatenation or our proposed Fast-LG-Fusion module, significantly improves the model's performance. Notably, the Fast-LG-Fusion module achieves the best overall performance across all datasets, with the lowest CD and EMD values (except for a 0.02 higher CD on the DHB dataset compared to direct concatenation).