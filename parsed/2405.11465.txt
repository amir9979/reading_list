The ICL scenario of LLMs can be regarded as a conditional text generation problem. Concretely, the probability of generating a target  is conditioned on the context , which includes  examples and the source . Therefore, the probability can be expressed as:

where LLM denotes the parameters of the large language model, and  is a context string concatenating  training instances. For example,  is concatenated with the special character, e.g., ``||'' or ``Sentence: ; Sentiment .'' which is denoted as . In this paper, we have different example sets  at different stages,  in the first stage, and  in the second stage, where  is a subset of .

%定义符号 explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning:

where ZSL denotes the zero-shot learning, which only contains the source ;  is the input representation of a query token , and  is the attention query vector;   denotes the input representations of the example's token;  are the projection matrices for computing the attention queries, keys, and values, respectively.~ regards  as some meta-gradients, which are used to compute the updated matrix . 

Investigating the data redundant problem not only helps to improve the training efficiency but also helps us understand the representation ability of small data and how many training samples are required and sufficient for a learning system.~ proposed to use the Influence Function to accurately and fast estimate the parameter change caused by weighting an example  for the training dataset. The influence of weighting  on the parameters is given by: 

where  is the Hessian and positive definite by assumption, ,  is the number of network parameters,  is the original dataset. After getting the weighting of each example , ~ propose generalization-guaranteed pruning or cardinality-guaranteed pruning to get the final compressed dataset .

%说明dataset pruning的意义%介绍iclr的方案,顺便介绍一下influence function的计算方法% retrieval relevant examples for the query text x

Given the training dataset  and the query source , we use BM25~ to retrieve a set of relevant examples  for . For each example  in , the BM25 score, denoted as , is computed. This score reflects the relevance of example  to the query . Specifically:

where  is the relevance score of example  with respect to query .

Subsequently, we form the set  which consists of the top-N examples with the highest relevance scores:

 where  is the number of examples we wish to recall for the given query .

For each  in , we calculate the input representation of tokens  as  and the meta-gradient . To compute  in Eq.~(), we require the Hessian of  for the parameter , which necessitates the computation of second-order derivatives. However, we only have access to first-order derivatives approximations of the parameters.  Considering that LLMs typically employ cross-entropy loss and maximum likelihood estimation (MLE) for fine-tuning, we have opted to employ the Fisher matrix as an approximation of the Hessian~. The key to the approximation process is as follows:

Then, combining the Eq.~() with  Eq.~(), the expression of the influence function for  is:

where . 

The score of  is determined by a combination of the influence score and the relevance score, represented as: 

Finally, the  in-context learning examples in  are chosen by:

This section introduces the detailed information about our experiments. 

%这里我们使用了GPT2系列的模型作为大模型的代表来验证我们方法的有效性,模型的介绍见表 We use the open source GPT2 model family~ (i.e., GPT2-Small, GPT2-Medium, GPT2-Large, GPT2-XL) as a representative of large models to verify the effectiveness of our method.  %Table  shows a brief description of the model.% %我们在五个数据集上进行实验,包括了3类NLP分类任务:推文分类,语言分析linguistic analysis, 仇恨言论检测,hate speech detection,和1个语义相似度数据集sick% We select five datasets for experimentation, including three types of NLP classification tasks: linguistic analysis, hate speech detection, and tweet classification, and one semantic similarity task. For the semantic analysis task, we choose the corpus of Linguistic Acceptability dataset (Cola)~, where each sentence is labeled as a grammatically compliant word sequence.  For the hate speech detection, we choose the online hate speech detection dataset (Ethos)~, and we detect hate speech about disability (Ethos-disability) here. For the tweet classification, we choose Tweet\_eval-stance\_feminist~ and Tweet\_eval\_stance\_hillary~, which are collected from Twitter. For semantic similarity task, we choose Sentences Involving Compositional Knowledge dataset (Sick)~, where each pair of sentences is labeled with a entailment relation. Table~ shows a brief description of the model. We use five datasets spanning four tasks: linguistic analysis, hate speech detection, tweet classification, and semantic similarity. Specifically, we employ ~, ~,  and ~ from Twitter, and ~.  We use Accuracy and  score as evaluation metrics.  Detailed dataset statistics and the prompt templates used can be found in Appendix~ and Appendix~.

% Sick~, Cola~, ethos-disability~, Tweet\_eval-stance\_feminist~, Tweet\_eval\_stance\_hillary~.% 在实验中,考虑到GPT2的输入长度限制,我们选择了K=3和k=4的示例个数来对比不同方法从训练数据集中选择出来的示例,%在计算影响分数时,为了方便计算,我们将所有样本的长度进行补齐、截断,具体的,我们将长度截断为所有长度的50%;% 此外,在考虑利用前向传播的几层来作为梯度时,我们这里只使用了一层,利用多层的前面传播结果可能会获得更好的结果;% 在第二阶段,利用BM25和影响函数进行融合时,我们这里将二者权重视为相同,针对不同模型,不同数据集,考虑自适应的权重或许可以获得更好的效果;% In the experiment, considering the input length limitation of GPT2, we select the number of =3 and =4 demonstrations to compare the examples selected from the training dataset by different methods.     To facilitate the calculation of the influence score, we truncate or supplement the length of all sentences to the same length. Specifically, we set the length uniformly to 50\% of the length of all sentences. For sentences that need to be truncated, we truncate the latter part of the sentence. For sentences that need to be supplemented, we repeat the sentence to the specified length. When considering using several layers of forward propagation as meta-gradients, we use only one layer here, although it may be better to use the results of the front propagation of multiple layers. In addition, in the second stage, when using BM25 and the influence score for fusion, we treat the weights of the two as the same. However, for different models and different datasets, it may be better to consider the adaptive weights. In the study, we chose  = 3 and  = 4 demonstrations to contrast example selection methods from training data. We set  = 100 for all models.  Sentences were either truncated or supplemented to have a uniform length at 50\% of the average sentence length. Although using multiple transformer layers' meta-gradient might be beneficial, considering the time efficiency, we used the first layer and obtained higher accuracy than baseline models. 

%random,BM25,sentbert% random随机结果反而比较好,szx: random 可以先不报了% 我们在数据集数量较多的数据集sick和cola上,和sentbert进行了对比   szx: 其他三个数据集没比是以为时间不够还是这个数据问题,或者 sentence-bert 的问题%之前的工作[]已经显示了BM25作为示例选择的强baseline,因此我们将我们的方法和BM25进行对比,BM25筛选出来示例后用于所有模型 Considering the model proposed in this paper is unsupervised and requires no training, it possesses a higher generalizability and operational efficiency compared to models that undergo supervised training. To ensure a fair comparison, our primary baseline is the unsupervised BM25-based In-Context Example Selection. Previous work~ has demonstrated that BM25 constitutes a robust baseline for demonstration selection, hence we juxtapose our methodology against BM25. The demonstrations selected by the BM25 are utilized across all GPT2 models.

% % 表4和表5分别展示了示例个数为3和4的结果。从表中的最后两行可以看到,在五个数据集上,在acc和 score上,对于4个模型模型,我们的方法均取得了一致的改进。在% 在示例个数为3的场景下,我们的方法在所有5个数据集上平均提升了5.34%,% 在示例个数为4的场景下,我们的方法在所有5个数据集上的平均性能均超过了BM25,平均提升了7.53%,其中在tweet_eval_stance_hillary数据集上提升了13.26%。在GPT2-XL模型上,所有数据集上的效果均超过了BM25。% Table  and Table  show the results of using 3 and 4  ICL examples, respectively. As can be seen from the last two rows of the tables, our method has been consistently improved for all four model models on the five datasets. With examples 3 and 4, our method improves BM25 by an average of 5.17\% and 6.64\% on all metrics, respectively. In particular, on accuracy, our method gets an improvement of 6.33\% and 7.80\% compared to BM25, respectively. These results demonstrate the superiority of our approach.

Tables  and  display results for three and four ICL examples, respectively. Observing the last two rows, our method consistently outperforms across all models and datasets. Using three and four examples, we surpass BM25 by averages of 5.17\% and 6.64\% in all metrics. Specifically, accuracy sees improvements of 6.33\% and 7.80\% over BM25. This underscores our approach's superiority. We found some higher model performance with three ICL examples compared to four, which can be explained by overfitting and example quality. Overfitting in few-shot learning means too many examples leads to adaptation to specific instances rather than general patterns, reducing accuracy on unseen data. Furthermore, if the additional fourth example is of lower quality or relevance, it can degrade model performance. 

In-context learning (ICL) has emerged as a fresh approach in natural language processing (NLP), where large models predict based solely on contexts supplemented by several examples~. Numerous studies have sought to modify, improve, and comprehend ICL, encompassing topics like prompt tuning~, intrinsic mechanism analysis~, evaluations~, and its use across various fields~, among others.

The goal of demonstration selection is to identify optimal examples for ICL.  demonstrated that choosing the nearest neighbors as in-context examples is an effective approach. The used distance measures include the pre-set L2 distance or the cosine similarity based on sentence embeddings. They introduced KATE, an unsupervised kNN retriever for in-context example selection.  suggested a two-phase retrieval process for demonstration selection. For a given input, it initially employs an unsupervised retriever (like BM25) to retrieve similar candidate examples and then uses a supervised retriever, EPR, to pick demonstrations from these candidates. Recent studies indicate that LLMs exhibit strong sensitivity to the examples chosen, resulting in significant performance variations~, dependency on example sequence~, and at times, an insensitivity to the actual labels~. Our research focuses on reducing training overhead and condensing crucial data from the training set into in-context examples, which in turn amplifies the ICL's effectiveness. 

% %  Given resource constraints, we provide limited validation in this paper. As our model is a model-agnostic approach that can be applied to various in-context learning selection models. In the future, we will validate the effectiveness of our model on more large-scale language models, baselines and datasets. In our experiments, we employ ~, ~,  and ~ from Twitter, and ~. Table~ shows more detailed statics of the datasets.

Based on , we employ minimal templates. For the GPT-2 series model, we distinguish between input and label using a space, and this is also applied between demonstration examples. Table  presents templates from five datasets, where blue represents the fixed section, and black varies depending on the example content.

 Equal contribution.\hspace Jun Xu is the corresponding author. \hspace  In-context learning has been extensively validated in large language models. However, the mechanism and selection strategy for in-context example selection, which is a crucial ingredient in this approach, lacks systematic and in-depth research. In this paper, we propose a data compression approach to the selection of in-context examples. We introduce a two-stage method that can effectively choose relevant examples and retain sufficient information about the training dataset within the in-context examples. Our method shows a significant improvement of an average of 5.90\% across five different real-world datasets using four language models. Introductiondai2022caninfluence_functionBackgroundIn-Context Learningdai2022can     \small              }_{}()         & =  W_{}  + \sum_i \left( W_{V} ^{\prime}_i \otimes \left( W_{K} ^{\prime}_i \right)^T\right)  \\         = & W_{}  + \Delta W_{}  \\         = & \left( W_{} + \Delta W_{} \right) ,     alignedequ:icl_opti_dualdai2022canDataset Pruningyang2022dataset     _{}(p) = _{\delta,p}}{\delta}\bigg|_{\delta=0} = -H_{}^{-1}\nabla_{\theta}L(p,), yang2022datasetMethodRecallxrobertson2009probabilisticC_1xp_jR(p_j, x)p_jx R_{j} = (p_j, x),  R_{j}p_jxC_1 C_1 = \{ p_j | j = 1, 2, \ldots, N \}, NxInfluence-Awared Rerankeq:influence_funcbarshan2020relatif \nabla^2 f() \approx \nabla f() \nabla f()^{\top} eq:influence_funceq:appro_hession     _{}(p) =  -_{}^{-1}G_{p},

todo choice one n这里可以使用C_近似也可以直接拿training set去做,不用近似了,好像更好,如果可以算的话

    =\left\{ \lVert_{}(p_1)\rVert_{F}^{2}+R_{1}, \ldots, \lVert_{}(p_N)\rVert_{F}^{2}+R_{N} \right\}.

     C_{2} = \left\{p_{i}|i \in I \right\}, ~I = \max_{}|\} \\ |I| = K}} . \linewidth! \renewcommand

Results of four ICL examples. The boldface represents the best performance.tab:k=3\linewidth! \renewcommand

Results of four ICL examples. The boldface represents the best performance.tab:k=4Experimentshttps://anonymous.4open.science/r/ICL-F302Experiments setupModels.radford2019languageDatasets.Linguistic Acceptability dataset (Cola)warstadt2019neuralonline hate speech detection dataset (Ethos and Ethos-disability)mollas2022ethosTweet\_eval-stance\_feministTweet\_eval\_stance\_hillarybarbieri2020tweetevalSentences Involving Compositional Knowledge dataset (Sick)marelli-etal-2014-sickapp:tempapp:datasetImplementation Details.Baselines.wang2023learning,gupta2023coverageOverall Performancetab:k=3tab:k=4Related WorkIn-context Learningdong2022survey,shin2022effect,zhang2023trained,bai2023transformerskim2022self,wang2022iteratively,mishra2022crosschan2022data,LiIPO23,garg2022cansrivastava2023beyond,WangMAKMNADASPK22sun2023shortDemonstration Selectionliu-etal-2022-makesrubin-etal-2022-learningnie2022improvinglu-etal-2022-fantasticallymin-etal-2022-rethinkingConclusionLimitation and Future WorkcustomAPPENDIX FOR REPRODUCIBILITYsec:appendixDatasetapp:datasetLinguistic Acceptability dataset (Cola)warstadt2019neuralonline hate speech detection dataset (Ethos and Ethos-disability)mollas2022ethosTweet\_eval-stance\_feministTweet\_eval\_stance\_hillarybarbieri2020tweetevalSentences Involving Compositional Knowledge dataset (Sick)marelli-etal-2014-sicktab:data_analysisTemplates.app:tempmin-etal-2022-rethinking,yoo2022groundtab:template\linewidth!       \renewcommand

  Template examples.tab:template