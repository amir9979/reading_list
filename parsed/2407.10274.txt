is a common strategy in Weakly Supervised Semantic Segmentation (WSSS). %It involves leveraging available label information to generate pseudo-masks, thereby enhancing model training.  In the field of histopathology imaging, pseudo-supervision is often tied with Class Activation Mapping (CAM)~. The basic idea is to train a classification network and use corresponding CAM images as pseudo-masks, assisting another segmentation network in fully supervised learning~. %. This approach was originally proposed by Chan et al.~ and applied to weakly-supervised tissue semantic segmentation tasks. However, methods based on CAM face a significant challenge. While the classification network tends to distinguish objects by the most discriminative features,  It should be noted that the segmentation task aims to identify the complete objects; but CAM usually highlights the most discriminative part, thereby compromising segmentation results. Moreover, due to the high homogeneity of histopathological images, CAM might confuse local activations caused by different representations within an object~. To address these challenges, SCCAM~ proposed to sub-categorize objects by feature-level clustering before the pseudo supervision procedure. %To address this challenge, SCCAM~ performs clustering at the feature level, categorizing objects into sub-categories, and uses this sub-category information to train the classification network, forcing it to learn better boundaries.  Han ~ introduced a Progressive Drop Attention (PDA) method to deactivate highlighted regions gradually. Alternatively, PistoSeg~ trained a Synthesized Dataset Generation Module, partially replacing the functionality of CAM.  C-CAM~ introduces a causality module to regularize CAM maps by addressing organ co-occurrence in medical images.

 is also widely adopted for various image segmentation~~~~ and classification tasks~~. It was initially proposed by Dietterich~, where training is done on sets of instances (referred to as "bags") instead of individual instances. In binary classification settings, a bag is labeled positive if it contains at least one positive instance, otherwise, it's labeled negative. With the rise of neural networks, particularly Convolutional Neural Networks (CNNs)~ for rich hierarchical feature learning, deep learning have been extensively combined with MIL methods, achieving outstanding results~. In the field of histopathological imaging, MIL was first applied to patch-level classification tasks within Whole Slide Images (WSIs)~. In such tasks, each WSI is treated as a bag, and each patch is regarded as an instance. This approach allowed histopathologists to annotate only at the image level, significantly reducing the labeling time and cost. Subsequently, Jia  further proposed treating each labeled histopathological image as a bag and referring image pixels as instances for pixel-level segmentation~. Since then, many emerging techniques have been applied to MIL. % to overcome some of its inherent flaws.  For instance, Hashimoto ~ incorporated multi-scale feature learning into MIL for WSI segmentation and Ilse ~ introduced attention mechanisms into MIL to address its blurred boundary issue. 

This research introduces a fusion-knowledge distillation framework designed to counteract potential degradation from noise in MIL outcomes, opening a new avenue of leveraging MIL in the pseudo-supervision paradigm. %This study, for the first time in literature, explores leveraging MIL-based segmentation results for pseudo-supervision. To address the noisy results by MIL, we innovate a fusion-knowledge self-distillation strategy to refine and stabilize the segmentation results. Knowledge distillation~ is a technique that relies on teacher-student (T-S) architecture and targets to transfer knowledge from one model (teacher model) to another (student model). %, where the teacher model is typically larger than the student model.  As shown in Fig.~, beyond labels, the student model can learn various feature representations from the teacher model, such as prediction probabilities, feature vectors, and more ~. Since the student model usually exhibits superior performance over the teacher model~, methods like momentum update have been widely used to further improve teacher's performance by updating it through the student model~. % When the student possesses capabilities similar to the teacher, the student model can exhibit superior performance~. Based on this property, methods like momentum update have been widely used in various tasks to further improve the teacher model's performance by updating it through the student model~. Kim et al.~ introduced Self-Knowledge Distillation. Self-Knowledge distillation extracts its knowledge to enhance generalization capabilities.  Particularly, in iterative knowledge distillation, the student itself acts as the teacher, leveraging its past predictions for more informative supervision during training for generalization capability enhancement~. To prevent the occurrence of model collapses in such self-supervised learning tasks, various strategies including contrastive loss~, clustering constraints~, predictor~ or batch normalizations~ are adopted in model optimization process.  %Specifically, in the t-th training epoch, the model uses the student model from the (t-1) epoch as the teacher model to provide the most valuable information. 

It's important to highlight that, in contrast to the ensemble teacher distillation approach for classification mentioned in~, our knowledge distillation method maintains the teacher model unchanged during the student's training phase. Once the student model outperforms the teacher, their roles are reversed, and a new cycle of knowledge distillation begins.

%Given this coarse-grained annotated training set, we have two options to generate the pseudo-masks for downstream self-distillation: (1) image classification plus CAM and (2) multiple instance learning taking image pixels as instances. As demonstrated in Fig.~, compared to the CAM-based methods whose outcomes are usually blurred, MIL-based segmentation has higher granularity and clearer boundaries~. Since the final segmentation accuracy is highly correlated with the precision of pseudo-masks, we opt for MIL in this study to train a rudimentary teacher model \(T\) for the downstream self-distillation. 

In our multiple instance learning, we take each image pixel as one instance and an image as a bag. Following the convention, the initial mask  of data  for teacher model training are naively set as follows: if , ; Otherwise, . This naively-initialized training set  is then exploited to train the segmentation model . Considering the cancerous regions can be any size, we follow the feature pyramid paradigm and let each block generate an independent segmentation probability map through a lightweight 1x1 convolution layer. Then, the output is upscaled to the original image size through bilinear interpolation. % and converted to probability maps through the sigmoid function.  By minimizing the difference between each of these probability maps and the actual labels, the system effectively discerns and learns the unique characteristics differentiating normal tissues from cancerous ones. 

Mathematically, given an image , let  be its multi-scale segmentation results by the model . We define their aggregated outcome  with weight parameter . To determine the optimal value of  and prevent overfitting, we set  as a learnable parameter and conducted a separate training for 10 epochs to ascertain its final value. Then, a compound loss is designed to optimize the teacher segmentation net:

where  refers to the dice loss. Notably, instead of directly using the naive segmentation mask  for model optimization, drawing on the optimization approach in~, we let ,  and , when ; otherwise, ,  and . The reason is that normal samples contain only normal tissue ; thus, the direct application of Dice loss would result in the Dice loss for all positive samples consistently being 1. Consequently, the model would tend to favor the class with a more substantial presence in the dataset, predicting all areas as cancerous regions.

%Notably, instead of directly using the naive segmentation mask \(l\) for model optimization, we apply an engineering trick in (). Specifically, due to the prevalence of normal samples with \(y = 0\) in the training set, the direct application of Dice loss could lead the model to a bias, predicting all areas as cancerous regions. This imbalance occurs because the Dice loss %, a measure used for the overlap between two samples, can%may inadvertently cause the model to favor the class with a more substantial presence in the dataset. Thus, to address this potential issue, we let ,  and , when \(y = 0\); otherwise, ,  and .

After the MIL training, the segmentation model  is used as the teacher in the second stage, i.e. knowledge distillation. 

Due to the uncertainty and label ambiguity in multiple instance learning, the pseudo-mask generated by the teacher model  may be noisy and inaccurate, which is likely to receive excessive focus during certain epochs of the training process~. Thus, the challenge of using the results of MIL-based segmentation as pseudo masks lies in how to prevent the segmentation model from deteriorating over time under pseudo supervision. To address this issue, we propose a novel knowledge distillation procedure which is composed by three basic elements: , (2) , and (3) .

%Note that the substantial amount of incorrect information in the initially MIL-based pseudo-labels is highly likely to receive excessive focus during certain epochs of the training process~, severely affecting the self-distillation's performance and may even lead to model collapse. To address this issue, we introduce (a) } and (2) } in the objective functions in student's optimization. distinguishes itself from conventional knowledge distillation approaches by targeting a distinct distillation objective. Its focus shifts from merely mimicking individual blocks in the teacher model to a more direct and holistic learning process. As illustrated in Fig.~, our fusion knowledge distillation method mandates that each block in the student model directly learns from the final output of the teacher model. This unique strategy involves guiding each block of the student model to acquire a balanced representation of both high-level and low-level features. This innovation empowers even the shallow layers of the student model to capture abstract features typically associated with deeper neural networks. Thereby providing the student network with the opportunity to achieve performance surpassing that of the teacher network.

%instead of mimicking the corresponding behavior of different blocks in the teacher model, our fusion knowledge distillation enforces each block of the student model to learn from the final output of the teacher model directly. By targeting the average weighted feature representation, each block in the student model is trained to accommodate both high-level and low-level representations. This strategy helps the shallow layers of the student grasp the more abstract features reserved for the deeper neural networks. Thereby providing the student network with the opportunity to achieve performance surpassing that of the teacher network. In the context of histopathology image segmentation, our fusion-knowledge distillation 带来的好处就是能够让both shallow and deep layers of the students对大的和小的region-of-interest (i.e.  cancerous region)都有比较强的表征作用。

In specific, for an input image , the teacher model and student model will each generate their multilevel results, denoted by  and  respectively. The student follows the same manner to calculate the weighted average . Then, the fusion-knowledge distillation loss can be formulated as %As we mentioned before, different from general T-S models where the student model mimics the corresponding outputs of the teacher model, the fusion knowledge distillation lets each block of the student model learn from the weighted average output () of the teacher model. By targeting the average weighted feature representation, each block in the student model is trained to accommodate both high-level and low-level representations. This strategy helps the shallow layers of the neural network grasp the more abstract features reserved for the deeper neural networks. Thereby providing the student network with the opportunity to achieve performance surpassing that of the teacher network. More precisely, the first loss function we used in stage 2 is similar to it in stage 1: Note, that since the normal patches only contain normal tissue, we use this ground-truth mask to optimize the student model. Following the process in teacher optimization, when , ,   and .

%When the performance of the student model surpasses that of the teacher model, we will completely swap the parameters of the two models, turning the original student model into the new teacher model, and the original teacher model into the new student model. %%\phi_T = \phi_S'  \; and\; \phi_S = \phi_T'\,, \quad t \;\%\; \eta = 0%%% and  are parameters of teacher and student models respectively.  and  are the best checkpoints for teacher and student models in pervious  epoch, and  is the current training epoch. The new student model will learn the teacher model's representations at different levels in the same way to achieve higher performance. Ideally, through this iterative process, the pseudo-labels generated by the teacher model will infinitely approximate the true labels, enabling the model to achieve performance similar to that of a fully supervised learning model.%%In human vision theory, image structural information describes the inter-dependency between pixels and these dependencies usually carry important information on objects and image semantic understanding~. Correspondingly, similar adjacent regions in histopathology images often correspond to the same tissue type. Thus, we propose to leverage image structural similarity information, quantified by %This implies that we can initially direct the network's attention to areas that are easily segmentable and calculate the similarity between these easily segmented areas and their adjacent regions using contrastive loss. This process reduces the confidence level in harder-to-segment areas, diminishing the misleading impact caused by misclassified regions within the pseudo-labels on the model.%the Local Saliency Coherence Loss \(L_{lsc}\), in our student model's training. \(L_{lsc}\) enforces similar pixels within the kernel to share consistent saliency scores, further propagating labeled points throughout the image during training~. Specifically, for the \(i\)th point  in the student's weighted map , we construct an  size kernel centered around it. For any other point  within the kernel, the similarity difference is calculated:%%D(i,j) = |f_{w,i}^s - f_{w,j}^s|%%%Concurrently, for the original input image , we calculate the similarity energy between the same points based on a Gaussian kernel bandwidth filter~ defined as:%%F(i,j) = {w_{n}} e^{-}{2\sigma_P^{2}} - }{2\sigma_I^{2}}},%%%where  is the normalized weight, and P(·) and I(·) represent the position and color of a pixel, respectively.  and  are hyperparameters of Gaussian kernels, and we always set  and . Then, the final  is%%L_{lsc} = \sum_{i,j}F(i,j)D(i,j).%% is introduced to prevent performance deterioration in the knowledge distillation process. As shown in Fig.~, MIL-based pseudo-masks invariably contain a substantial amount of noise. These noises may be intensified throughout the self-distillation, thereby imposing a detriment far greater than other methodologies. Specifically, we incorporate a weighted cross-entropy term  ~ into our distillation process.%we follow the study in ~ and introduce a weighted cross-entropy loss .%The self-knowledge distillation learning process perpetually extracts new knowledge from previous learning structures, intensifying these noises throughout the training progression, thereby imposing a detriment far greater than other methodologies. To reduce the model's focus on noise during the segmentation process, we introduce a normalization loss ~. where  is the pixel-level cross-entropy between segmentation predictions from the teacher and student models. This regularization term brings two benefits: (1) it prevents the student model from intensifying noise in pseudo masks during training, and (2) it encourages Teacher-student block-wise feature alignment statistically. %%CE(h,w) = -\sum_{k = 0}^1 f_{wk}(h,w)log(f'_{wk}(h,w))%%%%L_{normal} = mean(CE(h,w){mean(softmax(-CE(h,w)))})%%%Where  are height and width of input patch, and  is class number. In the cross-entropy(CE) function, pixels with high confidence but incorrect classification typically produce a higher loss. However, for pseudo-mask tasks, this classification error is likely caused by the inherent noise of the pseudo-mask itself. By applying the softmax function to the negative cross-entropy loss, the model can reduce the loss brought about by noise, making the model pay more attention to simpler samples.%	In summary, in the first stage, our overall segmentation loss function is:%%L = L_{dt}%%

In summary, our knowledge distillation objective function for student training is

where  is a scale factor for  . If  is small, it fails to significantly impact the model's learning process; conversely, when  is large, it causes the model to overlook smaller cancerous areas. %To determine the optimal value for , we conducted a simple experiment on the Camelyon16 dataset and confirmed the impact of 's magnitude on accuracy.  In this study, we used for  is 0.25. Ablation on this hyperparameter is shown in Table .

 is explored in our iterative knowledge distillation framework. Instead of the slow-paced momentum update on the teacher model during knowledge distillation, we initially freeze the model  as a static teacher for stable supervision. This effectively counters noise amplification during student model updates by providing a consistent reference, preventing error propagation in knowledge distillation and performance deterioration over time. With the training progress, the student gradually catches up, eventually outperforming the teacher model. Then, we completely swap the role of the two models, turning the original student model into a new teacher, and the original teacher model into the student for the next round of knowledge distillation. This iterative knowledge distillation cycle repeats several times to remove noise and imperfection in the evolving pseudo-masks. We show in the ablations that switching the roles of teacher and student models significantly boost the segmentation performance. 

We evaluate our approach on two public histopathology datasets as follows. %, Camelyon16~ and DigestPath 2019~.~ contains 398 annotated Hematoxylin and Eosin (H) stained slides for detecting breast cancer metastasis in sentinel lymph nodes, among which a total of 270 slides were allocated for the training set and the rest 128 slides were for testing. The training set comprised 111 tumor slides and 159 normal slides. The testing set included 80 normal slides and 48 tumor slides. %Among these slides, those containing cancerous tissues were referred to as "tumor slides", while those without were known as "normal slides".  These slides are stored in a multi-resolution pyramid. At level 0, each slide has a size of around 100,000 × 100,000 pixels, with x40 magnification (226 nm/pixel). 

To prepare the datasets for our weakly supervised task,  we crop image patches from slides into 1280 * 1280 pixels at level 1 (x20 magnification) in our experiment. Specifically, from the 111 tumor slides in the training set, we dropped those patches with white backgrounds exceeding 80\%, ultimately retaining 6,570 positive (tumor) patches in our training set. We then cropped the same number of image patches from normal slides and included them in our training set, herein referred to as neg-patches.%In consideration of data balance, we first cropped 8,000 image patches that contain tumor tissue from the 111 tumor slides in the training set. Subsequently, we dropped those with white backgrounds exceeding 80\%, ultimately retaining 6,570 tumor patches in our training set. For convenience, we refer to these patches as pos-patches. Following this, we cropped the same number of images from normal slides and included them in our training set, herein referred to as neg-patches.  Similarly, we cropped 985 pos-patches and an equivalent number of neg-patches from the test set. Notably, when selecting the pos-patches for the testing set, we also dropped all patches containing more than 90\% background areas. The rationale behind this is that including an excess of such patches in the test set could unintentionally skew the weakly supervised models' test results towards those of fully supervised models, obscuring the true measure of performance.

%~ is a Colonoscopy tissue segmentation dataset, which contains 250 images of tissue from 93 WSIs, with pixel-level annotation. All images were stained following the H protocol and scanned with a x20 magnification factor. %The average size of all images are of 5000 × 5000 pixels.  In our experiment, we selected 225 images for training, while the remaining 25 for testing. All images are randomly cropped into 1536 × 1536 pixels patches. Similar with the CAMELYON16 dataset, we cropped and filtered 9,000 pos-patches that contained cancerous tissues %and where the white background constituted less than 80\% of the area from the 225 training images,  and 18000 neg-patches, cumulatively forming the training set. From 25 testing slides, 726 pos-patches and neg-patches are included in the testing set. %All patches that contain more than 90\% cancerous areas are dropped to enhance the validity and reliability of our performance assessment. Table  shows the details about 2 curated datasets.% All the experiments were implemented in PyTorch environment on a server equipped with 2 NVIDIA GeForce RTX 3090 GPUs that have 24G Memory. All image patches are resized to 256 × 256 when feeding into our model. For both dataset, Adam optimizers are employed to train the model with initial learning rate 5e-5, a weight decay 5e-4, and a batch size 16. In stage 1, the backbone would be trained for 30 epochs to get the teacher model. In stage 2,  and the teacher and student model would switch their parameters every 30 epochs. All experiments are repeated three times to calculate the mean and standard deviation. Note, in our main experiment, the simplest VGG blocks is explored as the backbone. We will substitute VGG with ResNet and other more advanced MIL-based methods in our ablation study to demonstrate the generalization of the proposal knowledge distillation framework. %Due to differences in cancer types and image resolution, we used different hyperparameters for the two datasets. For Camelyon16 dataset, Adam optimizers are employed to train the model. In both stage 1 and stage 2, Adam optimizers have an initial learning rate 5e-5, a weight decay 5e-4, and a batch size 16. In stage 1, the backbone would be trained for 30 epochs to get the teacher model. In stage 2, the teacher and student model would switch their parameters every 30 epochs. For DigestPath 2019 the Adam optimizers with 1e-5 initial learning rate, 5e-4 weight decay, and 12 batch size are employed in both stage 1 and stage 2. In stage 1, the backbone would be trained for 30 epochs to get the teacher model. In stage 2, teacher and student model would switch their parameters every 30 epochs after 30 epochs. % Following convention, F1-score (F1), Intersection over Union (IOU) score~, and Hausdorff Distance(HD)~ are used to evaluate the lesion segmentation performance. F1 score is the harmonic mean between precision and recall. IOU is calculated as the area of overlap between the predicted segmentation and the ground truth, divided by the area of union of the predicted segmentation and the ground truth. % scoredefined as:%%%%where TP, FP, FN are true positive, false positive and false negative, respectively.  The Hausdorff Distance measures the spatial accuracy of the segmented regions, comparing the distance between the borders of the predicted segmentation and the ground truth. For each evaluation metric, we first calculate the value for each patch, and then the average of all patches is taken as the final score.

 We applied MIL on VGG as our baseline method. For comparison, the state-of-the-art weakly supervised methods: VGG-MIL~, Resnet-MIL~, SA-MIL~, Swin-MIL~, OAA~, PistoSeg~,OEEM~ are included. VGG-MIL, Resnet-MIL, SA-MIL and Swin-MIL are MIL based methods, and OAA, OEEM and PistoSeg are CAM based pseudo supervision approaches. In addition, we conduct the evaluation on a fully supervised method CAC-Unet~ for performance reference. For each method, we execute their officially-released codes on our datasets for a fair comparison.% Note, all results are reported on our datasets.   The quantitative evaluation results are presented in Table . Our approach surpasses SOTA weakly-supervised learning methods across both datasets. Particularly, compared to the VGG-MIL baseline, the proposed iterative knowledge distillation strategy brings in significant performance boosts. The qualitative evaluation are displayed in Fig. . Weakly-supervised learning methods based on Multiple Instance Learning tend to generate substantial noise in their predictions. In contrast, methods reliant on Class Activation Mapping (CAM) and pseudo-labeling struggle to produce sufficiently accurate segmentation maps. By integrating the strengths of both approaches, our model has achieved significant improvements in terms of accuracy and stability. %On the Camelyon16 dataset, our method achieved the best performance on all three evaluation metrics, and on DigestPath 2019, our method achieved the best performance on F1 score and HD score. % As we previously stated, the backbone of our model is not fixed and our iterative knowledge distillation strategy can be easily adapted to different MIL-based methods. To substantiate our claim, we substituted the model backbone with Resnet, SA-MIL and Swin-MIL and conducted tests on both datasets and report the experiment results in Table . Our iterative fusion-knowledge distillation method boosts the performance of all 4 MIL-based segmentation methods with considerable large margins.

Ablations are performed to assess the effectiveness of the building blocks in our solution. Each test was repeated for 3 rounds, and mean and standard deviation were reported. All ablation studies in this section takes VGG as the backbone.

% We compare our fusion-knowledge distillation to conventional knowledge distillation paradigms shown in Fig. 1 and report their performance over 450 epochs on Camelyon16 and Digetpath2019 in Fig.~ and , respectively. For basic knowledge distillation paradigms (a) and (b), the shallow layers of the student model are prone to learning local information from the teacher, while student's deeper layers can only learn global semantic information. Consequently, the improvement to the student model's performance is relatively limited. In contrast, with fusion knowledge distillation, each block of the student model learns from the multi-dimensional features extracted by the teacher model, which empowers student's segmentation capability. It is noteworthy that on the Digestpath2019 dataset, the performance of the knowledge distillation method (a) falls below the baseline of the VGG model. We speculate that the performance drop arises from a lack of constraints on the early layers of the student model within the knowledge distillation process.%to the extreme data imbalance present in the training samples, where a large amount of negative label training data and the extensive white background inherent in the Digestpath dataset cause method (a), which focuses only on global semantic information, to be more inclined to classify samples as negative.  This also highlights the necessity of fusing segmentation information from different dimensions.

%For basic knowledge distillation paradigm (b), the shallow layers of the student model can only learn local information from the teacher, while student's deeper layers can only learn global, semantic information. Consequently, the segmentation performance of the student network is slightly better than the teacher. In contrast, with fusion knowledge distillation, each block of the student model learns from the multi-dimensional features extracted by the teacher model, which empowers student's segmentation capability.%When the student model only learns the corresponding outputs of the teacher model, the feature representation obtained by each block is constrained. This means that the shallow layers of the student model can only learn local information from the teacher model, while the deeper layers can only learn global information. Thus, with the basic knowledge distillation method, the best performance that the student network can achieve is similar to the best performance achievable by the teacher network in weakly supervised learning. In contrast, with fusion knowledge distillation, each block of the student model learns from the multi-dimensional features extracted by the teacher model. Each test was repeated for 3 rounds, and the standard deviation is also shown in Fig. .% Our iterative fusion-knowledge distillation strategy proposes to iteratively switch the role between the teacher and student models. That is, the teacher model is also updated. In this ablation, we compare the segmentation performance with and without the teacher update and show the results in Fig.~ and Fig.~. With the same teacher-student framework, no teacher update tends to stabilize in performance around 200 epochs, but our iterative fusion-knowledge distillation continues to bolster overall performance by updating the teacher model. %, culminating in an increase of over 0.01 in F1 score compared to the standard model. % Our iterative fusion-knowledge distillation strategy is harnessed by two loss terms: ,  . Table  shows their impacts on segmentation results on both Camelyon16 and Digestpath2019.  substantially improves the network's segmentation capability and stability. The two loss terms are combined by a hyperparameter  in our method. Table  quantifies sensitivity of the solution to its numerical setting. It shows that the system is quite stable when . %%Our self-distillation strategy is harnessed by three loss terms: , , . Table  presents an ablation study on Camelyon16. On one hand,  enhances the network's stability without significantly compromising its intrinsic performance. On the other,  substantially improves the network's segmentation capability.%To demonstrate the necessity of these losses, we conducted ablation experiments on three types of loss functions, with the results presented in Table . Experiments 1 and 2 indicate that using  can enhance the network's stability without significantly compromising its intrinsic performance, while Experiment 3 demonstrates that  can substantially improve the network's segmentation capability. Therefore, we employ all three losses simultaneously to ensure that the network maintains a high degree of accuracy while also retaining stable performance.%%To determine the optimal parameter for , we conducted a simple experiment on the Camelyon16 dataset and confirmed the impact of 's magnitude on accuracy in Table .