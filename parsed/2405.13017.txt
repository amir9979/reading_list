Let us define  and  as the training and test splits of a dataset  for a single downstream task (e.g. sentiment classification), where each dataset contains pairs of a text input and associated labels. Importantly,  is taken from the period prior to  without any temporal overlap. Given such dataset with temporal split, we consider the following two settings of out-of-time (OOT) and in-time (IT).

In the first setting, we simply train the models on  and evaluate them on . Noticeably, models have no access to the instances from the test period at the training phase in this setting, so we refer the setting as  (OOT) as an analogy to the out-of-domain (OOD).

As a comparison to OOT, we consider the second experimental setting, which is designed to assess the effect of training instances from the test period.  The test set is randomly split into non-overlapped four chunks () for cross validation, where models trained on  are evaluated on . For each chunk of the test set, we downsample the IT training set to the same size as  with three random seeds and report the averaged metrics over the runs. To be precise, we consider a function  that randomly samples  instances from , and we independently train models on  for  . In contrast to OOT, we refer this setting as  (IT) setting.

Figure~ presents an example overview of the differences between IT and OOT settings from the perspective of data sampling periods (data from 2018 to 2022 in the example).

We consider the following five diverse social media NLP tasks: hate speech detection, topic classification, sentiment classification, named entity disambiguation (NED), and named entity recognition (NER). For each task, we employ a public dataset for English and leverage its original temporal splits, unless there is overlap between the periods of training and test sets.

Hate speech detection in Twitter consists of identifying whether a tweet contains hateful content. We use the dataset proposed by Waseem and Hovy~ framed as binary classification as the dataset to create the training and test splits based on the timestamp. The first half is used for training and the rest for test split. The training split is further randomly split into 2:8 for validation:training. We use accuracy to evaluate the hate speech detection models.

Topic classification is a task that consists of associating an input text with one or more labels from a fixed label set. For this evaluation, we rely on TweetTopic~, a multi-label topic classification dataset with 19 topics such as  or . As evaluation metric, we use micro-F1 score to measure the performance of topic classification models.

Sentiment analysis is a standard social media task consisting of associating each post with its sentiment. In particular, we use the dataset from the task 2: LongEval-Classification from CLEF-2023~ in which the task is framed as binary classification with positive and negative labels. The original training split contains around 50k instances while 1k test split, which is highly imbalance and the effect of the IT sample can be very limited. To balance the training and test splits, we randomly sample 2.5k instances from each label, amounting 5k new training instance. We use accuracy to evaluate the sentiment classification models.

NED is a a binary classification that consists of identifying if the meaning of a given target entity in context is the same as the one provided. We use the TweetNERD~  dataset and reformulated into NED following SuperTweetEval ~. Then, we create the train, validation, and test splits in the same way as the hate speech detection. We use accuracy to evaluate the NED models.

NER is a sequence labelling task to predict a single named-entity type on each token on the input text. We rely on TweetNER7~, a NER dataset on Twitter that contains seven named entity types. We use span F1 score to evaluate NER models.

Table~ shows the size and the period of the training and the test sets for each dataset, and Figure~ displays the number of tweets per quarter for each task. Topic classification and NER use the same tweets, which are sampled uniformly from each month, while NED and hate speech detection have the majority of the tweets in the latest quarter. Sentiment analysis covers the longest period in the dataset that spans over four years. Figures~ and  show the comparisons of the label distribution of the binary (i.e., hate speech, sentiment classification, and NED) and multi-classification tasks (i.e., NER and topic classification), respectively. % As can be observed, hate speech detection has fewer positive labels in OOT than in IT, while the other two tasks have the same ratio of the positive labels between OOT and IT. The same pattern can be observed for topic classification and NED, for which the label distribution does not substantially change.

% bernice:   12/2021% roberta:   02/2019% BERTWEET:  08/2019% TimeLM2019:12/2019% TimeLM2020:12/2020% TimeLM2021:12/2021% TimeLM2022:12/2022 We investigate an established general-purpose LM, RoBERTa~ as well as other LMs pre-trained on tweets including BERTweet~, TimeLM~, and BERNICE~. For RoBERTa and BERTweet, we consider the base and the large models, referred as RoBERTa (B), RoBERTa (L), BERTweet (B), and BERTweet (L). For TimeLM, we consider the base models trained on the tweets up to 2019, 2020, 2021 and 2022, referred as TimeLM2019 (B), TimeLM2020 (B), TimeLM2021 (B) and TimeLM2022 (B), and the large model trained upto 2022, referred as TimeLM2022 (L). The end date of the pre-trained corpus for each model is 2019-02 (RoBERTa), 2019-08 (BERTweet), 2019-12 (TimeLM2019), 2020-12 (TimeLM2020), 2021-12 (TimeLM2021 and BERNICE), and 2022-12 (TimeLM2022). All the model weights are taken from the transformers model hub~ and Table~ shows the details of models we used in the paper. Table~ shows the overlap between the period of the pre-trained corpus and the test set for each task, which will be relevant for the analysis on the effect of pre-training in Section . These models are then fine-tuned in the datasets presented in the previous section, in both OOT and IT settings. For model fine tuning, we run hyperparameter search with Optuna~ with the default search space. 

A possible direction to mitigate the temporal shift is to pre-train the LMs on the text from the test period, which does not require any labeling. Figure~ visualizes the performance and relative IT improvement of LMs with/without pre-training corpus covering the test period of each task for topic classification/NED/NER. At a glance, we cannot observe see any relationship between the pre-training corpus and the performance.  The averaged relative gains of the metrics from OOT within the LMs pre-trained on the test period and the others are 2.0 and 0.6 (topic classification), 3.5 and 3.8 (NER), and 2.1 and 1.9 (NED) respectively. Therefore, all models are affected by the temporal shift irrespective of the pre-training corpus date. This implies that the temporal shift cannot be robustly resolved by only adding data from the test period to the pre-training corpus, a conclusion that was also reached by  ().

In supervised machine learning label distribution, the distribution of the binary label over the test instances, shifts can affect a model's performance. In this section, we analyse this potential effect when it comes to temporal shifts. For this, we rely on hate speech detection, which presents the largest decrease in performance from IT to OOT, with a different label distribution between training and test (see Figure~). For the other tasks, the label distribution appears to be largely similar. To separate the effect of label distributional shift between IT and OOT from the temporal shift, we conduct a controlled experiment by balancing the label distribution of each IT training split to be the same as OOT training split. This is achieved by undersampling the size of the training set. Table~ shows the results, where the average relative gain is still positive, although it becomes less dominant in balanced experiment. This highlights how label distribution may change over time and this itself have an effect in model performance. A similar finding was already discussed by  ().

In this analysis, we have a closer look on the test instances that are incorrect in OOT, turning to be correct in IT. To be precise, we sort the test instance in a single task based on the number of models where the error in OOT setting has been corrected in IT setting over all the random seeds. In other words, given a test instance, we check whether a model prediction is incorrect in OOT, but correct in the IT setting. This particular instance is counted as a correction. In total, we have 10 models with 3 independent runs with different random seed to construct the training data, so 30 would be the maximum number of corrections. For sentiment classification, hate speech detection, topic classification and NED, we simply count instance-level corrections. Given the complex nature of NER evaluation, we decided to only focus on the entity type predictions for this analysis.

Table  shows the top instances in terms of IT corrections for each of the task. We can observed the marked differences across tasks, with NED and hate speech detection including instances which were corrected 100\% in the OOT setting. In fact, there are respectively 44 and 15 instances for which this is the case in these two tasks. Similarly for NER, the number of corrections is high. This is correlated with the main results of the paper (see Section ) which showed clear improvements for these tasks in the IT setting, but not for sentiment and and topic classification.

Finally, Table~ shows some of these instances for NED and hate speech detection. In the case of NED, the tweets relate to two new TV series that were on air at test time (Japanese  in the first example and Indian B in the second, both from 2021). This is similar to the hate speech detection in which the examples belong to the  show. This highlights the event-driven nature of social media, and the importance of acquiring the background context for the specific task.