The pruning problem for LLMs starts from a pre-trained dense model  and aims to find a sparse version of , where many channels and attention heads are discarded. The remaining weights  may be updated accordingly to preserve the performance. Consider a labeled dataset , where  is the number of samples, and a desired prune ratio  (i.e., the percentage of removed weights), our goal is to remove the weights that has the least impact on the model's prediction. Therefore, neural network pruning can be formulated as the following constrained optimization problem:

where  can be the standard loss function (e.g., cross-entropy loss) and  is the standard  norm. 

 To evaluate the significance of a specific weight , one common way is using its sensitive effect on the loss function, where  denotes the -th weight in the -th layer. Specifically, one can compare the difference in the loss function when  is included versus when it is excluded from the model (i.e., LLaMA-7B). Thus, the loss change can be formulated as~: 

where  and the Hessian matrix . Unlike previous work~, the pre-trained datasets of a large language model are inconsistent with the datasets of the downstream tasks, hence . This characteristic is advantageous for assessing the importance of weights through the gradient term in the context of LLMs, as calculating the Hessian matrix in the second term is impractical on LLMs with  complexity. Therefore,  can be approximated as:  As recently observed in LLMs larger than 6.7B (), a small set of hidden state features emerges with significantly larger magnitudes (outliers) than the remainders and zeroing out these features causes a significant degradation of performance. The vanilla scoring function Eq.~ does not highlight the unique characteristics of LLMs compared with smaller models. Given the th layer's input activation  (i.e., the output from the ()th layer of the network) and the th layer's weights , the pruning problem is also commonly treated as finding the subset  of  respecting a compression constraint , which most closely approximates the initial output as determined by the squared error metric. Assuming that the activation and weight matrices possess a suitable rectangular shape, the neural network pruning is defined as the following optimization problem :

To evaluate the significance of a specific weight , one can compare the difference in the layer-wise output when  is preserved versus when it is excluded from the model and write the formulation as:

where . 

 Eq.~ only considers the output changes in a single layer and does not take into account the global loss change across the entire network. We notice that, with the weight gradients, the global loss change can be quantified with weight change as shown in Eq.~. To calculate the salience of each weight relative to the change in global loss and layer-wise output, we measure  in Eq.~ through its global sensitivity  from Eq.~ and propose a heuristic hybrid sensitivity scoring function called Feature Map Sensitivity () as follows:

Compared to Eq.~ and Eq.~, our criterion Eq.~ integrates magnitude, activation, and gradient to optimally utilize the pivotal information from the three critical aspects, so as calculate the feature map sensitivity along with the loss changes. 

Let  denote the loss on a minibatch . The following Definition~ describes a classical ZO gradient estimation based on SPSA ().  where  is the gradient with backpropagation,  with  and  is the perturbation scale. The -ZO gradient estimate averages  over  randomly sampled . Malladi et al.~ found that  is the most efficient. Therefore, we choose  as the default. For each weight  in the model, the estimation of its gradient  (defined as ) is then

where  is the random corresponding to . In this way, the practical pruning score used in our MINI-LLM is defined as:  To maintain structural integrity, it is crucial for structured pruning to identify groups of interdependent structures within LLMs. Following Ma et al.~, we prune heads for Multi-Head Attention (MHA) and channels for Feed-Forward Network (FFN), respectively. We arrange the interconnected weights into groups and determine the sensitivity of each group (a set of coupled structures) defined as  by choosing the maximum sensitivity score of the structures in it, i.e., , where  is the number of interdependent structures in the group. Our structured pruning approach MINI-LLM is outlined in Algorithm~.

After pruning, we need a recovery stage to regain the performance. Due to the huge number of parameters, full fine-tuning becomes less feasible. LoRA~, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods~, has demonstrated strong capability for performance recovery, while significantly reducing GPU memory usage .

To facilitate this, we fine-tune the pruned models by employing LoRA which only updates two injected low-rank decomposition matrices that are attached to a frozen pre-trained weight matrix. Given two low-rank matrices  and  ( ), the forward computation can be written as:

where  denotes inputs. After adaption, the updated  can be re-parameterized as . 

. To verify the effectiveness and versatility of our MINI-LLM, we test it over three open-source LLMs with different structures: LLaMA-7B~, BLOOM-7B~, and OPT-6.7B~. All models undergo evaluation in a task-agnostic framework. We assess the zero-shot ability of pruned models on WikiText2~ and PTB~ for language generation with the perplexity (PPL) analysis, and smaller is better. Besides, we follow LLaMA to implement zero-shot task classification and multiple-choice on four common sense reasoning datasets: BoolQ~, PIQA~, HellaSwag~, and WinoGrande~. In addition to zero-shot evaluation, we conduct experiments on few-shot tasks to evaluate pruned LLMs' ability to learn in context. We choose the Massive Multitask Language Understanding benchmark ()  and conduct a 5-shot evaluation to remain consistent with the evaluation approach described by . In task classification and multiple-choice on common sense reasoning datasets, as well as on MMLU, classification accuracy is used as the performance metric.

. Our MINI-LLM conducts in a one-shot pruning framework. That is scoring only once and then pruning the network to the target prune ratio~. In the model pruning process, we use 10 randomly selected samples from Bookcorpus~ as the calibration data for evaluating the weight gradients and 128 samples for computing each layer's input (i.e., activation). Due to the varying sensitivity of each layer to pruning~, the first four layers and the last three layers are retained. During the recovery phase, we utilize the Alpaca-cleaned~ as the training dataset, which contains approximately 50k samples, to fine-tune the pruned models with a batch size of 64. Following , the learning rate is set to 1e-4 and a total of 2 epochs. Each pruned model is recovered by an Adam optimizer~ paired with a cosine decay schedule for the learning rate. We set LoRA , , and attach LoRA modules on all linear layers of the base model. In the inference stage, all the evaluations are implemented with a context length of 128.

%. We compare MINI-LLM with four one-shot structured pruning methods for LLMs. Magnitude-l1/l2: pruning based on the absolute values or the l2-norm of weights, respectively. LLM-Pruner~: pruning using criterion Eq.~ with backpropagation gradients. Wanda~: pruning based on the product of the magnitude of weights and their corresponding activations. Given that vanilla SparseGPT and Wanda are retraining-free unstructured methods, we adapt them for structured pruning with pruning and fine-tuning stages for a fair comparison while maintaining the same criterion. Except for LLM-pruner, which is a gradient-based method, the other methods are all gradient-free methods.

We prune LLaMA-7B with four prune ratios: from 20\% to 50\% and fine-tune the pruned models by using LoRA to restore model accuracy. The comparisons with the baselines are reported in Table~. From the results, we see that our MINI-LLM consistently surpasses all the gradient-free methods and closely matches or even outperforms the LLM-Pruner with backpropagation gradients across four prune ratios. For example, at a 20\% prune ratio, MINI-LLM achieves an average classification accuracy of 67.57\% across four inference datasets, better than other gradient-free methods, and obtains 92.71\% of the accuracy achieved by the original model. Although LLM-Pruner achieves better accuracy with 93.91\% of the accuracy attained by the dense model, the peak GPU memory required for pruning by LLM-Pruner is approximately twice that of MINI-LLM. Moreover, although both Wanda and MINI-LLM use weight magnitude and activation, MINI-LLM performs better, which indicates that estimated gradients are beneficial in guiding pruning. In addition, at a 40\% prune ratio, MINI-LLM achieves an average accuracy of 61.02\% on the four tasks, even better than LLM-Pruner's average accuracy of 60.96\%. 

However, similar to the observation in Ma et al.~, with a high prune ratio, such as 50\%, an obvious performance decline is observed, as shown in Table~. In this situation, our MINI-LLM and LLM-Pruner only retain 78.11\% and 78.92\% of the dense model's accuracy, respectively. Even for LLMs, structurally pruning under high prune ratios remains a major challenge.

 To validate MINI-LLM on other LLMs broadly, we prune both BLOOM-7B and OPT-6.7B with two prune ratios: 10\% and 30\%, and fine-tune the pruned models to restore model accuracy. The results in Table~ illustrate that our MINI-LLM steadily outperforms all gradient-free methods and exhibits performance comparable to, even surpasses at times, that of LLM-Pruner. For instance, at a 30\% compression rate on BLOOM-7B, MINI-LLM achieves a perplexity of 54.07 on the WikiText2 dataset, obviously outperforming LLM-Pruner's perplexity of 58.11. Similarly, at a 30\% compression rate on OPT-6.7B, MINI-LLM achieves a perplexity of 40.89 on the WikiText2 dataset and 57.44 on the PTB dataset, outperforming LLM-Pruner's perplexity of 42.94 and 65.09, respectively. In addition, at a 10\% prune ratio on OPT-6.7B, MINI-LLM achieves an average classification accuracy of 67.81\% across four datasets and obtains 98.60\% of the accuracy achieved by the original model, which is even better than LLM-Pruner' s 67.50\% and 98.15\%. This demonstration validates the effectiveness of MINI-LLM in efficiently compressing models of various structures to a specified size, while optimizing memory usage.

In addition, we observe that the pruning outcomes achieved by gradient-free methods such as Wanda and magnitude l1/l2 shown in Table~ significantly fell short in comparison to gradient-based pruning methods such as LLM-Pruner and MINI-LLM at a prune ratio of 30\% on the WikiText2 and PTB datasets for BLOOM and OPT. Using LLM-Pruner as a high-quality benchmark, we compare Wanda, representing gradient-free approaches, by assessing the similarity of their retained channels per layer against LLM-Pruner on the WikiText2 dataset. Similarly, we evaluate the similarity between LLM-Pruner and MINI-LLM. Specifically, the similarity is calculated by the formula: , where  and  denote the sets of the pruned channels obtained by LLM-Pruner and the examined method, respectively. The results are illustrated in Figure~. We can see that LLM-Pruner and MINI-LLM have more similar pruned channels compared to LLM-Pruner and Wanda. As a result, compared to gradient-free methods, the perplexity of MINI-LLM in Table~ is closer to the results of LLM-Pruner.

Due to the efficient approximation for the gradients of the pre-trained weights, MINI-LLM enables pruning on larger-scale LLMs, such as LLaMA-13B~. We prune LLaMA-13B with five pruning ratios: from 10\% to 50\% and present the zero-shot performance of the pruned LLaMA-13B without fine-tuning in Table~ and with fine-tuning in Figure~, respectively. Except for the model pruned with a ratio of 50\%, for which fine-tuning is conducted for two epochs, the compressed models obtained from all other pruning ratios are fine-tuned for just one epoch. The other fine-tuning settings, such as the learning rate and batch size, are the same as those for recovering LLaMA-7B. We follow Wanda~ to conduct inference with 2048 tokens. 

As shown in Table~, MINI-LLM outperforms the magnitude-based method (l2-norm) significantly when fine-tuning is not applied. For example, with a pruning ratio of 30\%, MINI-LLM achieves a perplexity of 11.04 compared to the magnitude-based method's 316.65 on the WikiText2 dataset. Similarly, as depicted in Figure~, MINI-LLM consistently maintains its substantial advantage over the magnitude-based method across a spectrum of pruning ratios when subjected to fine-tuning.  

In Table~, we report the mean accuracies for both dense LLMs and sparse LLMs with 20\% to 50\% sparsity. In the few-shot setting, MINI-LLM performs competitively with other methods, including backpropagation gradient-based LLM-Pruner. Specifically, at a 20\% prune ratio, MINI-LLM achieves an average accuracy of 26.60\%, which surpasses SparseGPT's 25.80\% and LLM-Pruner's 25.30\%. Notably, estimated gradient-based MINI-LLM consistently surpasses backpropagation gradient-based LLM-Pruner. This performance is not observed in zero-shot setting.

Table~ shows the number of parameters, MACs, GPU memory requirements, and total inference time for running the original model and the pruned LLaMA-7B models at different prune ratios. The results indicate that when the model is pruned by 50\%, the total inference time is reduced to 58\% and the GPU memory usage concurrently drops to 50\% of its original values, respectively. The evaluation is conducted in the inference mode and the sequence length is set to 64. The inference time is tested under the test dataset of WikiText2 on a single NVIDIA GeForce RTX 3090Ti (24GB).

 To enhance GPU memory efficiency over traditional backpropagation gradients, we utilize the classical ZO gradient estimation based on SPSA to approximately compute weight gradients with only forward passes for LLM pruning. Although SPSA-based ZO optimization is theoretically founded (), we especially reveal the effectiveness of the estimated gradients for guiding pruning LLMs in Figure~. As we can see, the results demonstrate that our score function FMS,  (represented by the red line in Figure~), consistently yields better performance compared to Wanda's pruning criterion,  (indicated by the green line), across four prune ratios ranging from 20\% to 50\% for LLaMA-7B on the WikiText2 dataset. On the PTB dataset, this improved performance is more evident, as shown in Fig~. Comparing these two criteria, our FMS includes an additional estimated gradient  compared to Wanda's. This indicates that the performance improvement over Wanda's comes from the estimated gradient information. This observation underscores the superior performance and effectiveness of gradient-based pruning methods in our experiments. However, as we previously mentioned, gradients based on backpropagation lead to substantial memory consumption and are less feasible. Therefore, gradient estimation based on the forward passes becomes valuable, allowing the criterion to incorporate guidance information from gradients. 

In the main body, we explored the effectiveness of the estimated gradients for guiding the pruning of LLaMA-7B. Here, we delve further into the effectiveness of the estimated gradients in guiding the pruning process for BLOOM-7B~ and OPT-6.7B~. The experimental results presented in Table~ indicate that our score function FMS, , consistently yields better performance compared to Wanda's pruning criterion, , for pruning BLOOM-7B and OPT-6.7B on the WikiText2 and PTB datasets. For example, at a 30\% pruning ratio,  achieves a perplexity of 54.07 on WikiText2 for pruning BLOOM-7B. This result shows a 30.82 improvement over . Similarly, at a 30\% pruning ratio,  achieves a perplexity of 40.89 on WikiText2 for pruning OPT-6.7B, which surpasses the perplexity of 82.93 obtained by using . 

 identified a distinct property of LLMs that a few hidden state features possess notably high magnitudes. Eliminating these features results in a considerable decline in performance. As argued in Section~, the vanilla pruning criterion  ( Eq.~) does not highlight this characteristic of LLMs. To validate that activation in FMS, i.e.,  (Eq.~), can bring improved performance, we compare the performance of the pruned models obtained by using the criteria Eq.~ and Eq.~ over four prune ratios on the LLM. The difference between these two criteria is that Eq.~ includes an additional activation term  compared to Eq.~. From the results shown in Table~, we can see that the activations enable an effective increase in performance on both the WikiText2 and PTB datasets. For example, at a 20\% prune ratio, the model pruned with  achieves a perplexity of 17.45 on the WikiText2 dataset. This result surpasses the 17.79 perplexity achieved by the model compressed by using . In contrast, on the PTB dataset, the performance enhancement provided by activation is generally more noticeable than on WikiText2. For instance, with 50\% parameters pruned, the model pruned with  achieves a perplexity of 62.84 on the PTB dataset. Compared to using , the perplexity result decreased by 3.53. It is worth noting that activations can be estimated using a small set of calibration data and executed in a single forward pass. Therefore, computing , as opposed to , almost does not require additional GPU memory overhead. 

In Table~, gradients in both pruning criteria are calculated by backpropagation. In contrast, the results in Table~ demonstrate the effectiveness of activation in estimated gradients-based pruning criteria. Similar to Table~, the difference between the two criteria in Table~ also lies in whether they include activation information or not. The results in Table~ indicate that using the pruning criterion with activation consistently yielded better results. For example, on the WikiText2 dataset, the model pruned with 50\% of its parameters using  achieves a perplexity of 44.69, reflecting a 8.54 improvement over the 53.23 perplexity by the model using . Switching to the PTB dataset on the same compression rate, also yielded positive results, with the model's perplexity dropping from 75.50 to 69.83, confirming the efficacy of the activation-inclusive pruning criterion across diverse data.

Consequently, the results in Table~ and Table~ demonstrate our pruning criterion FWS overall outperforms its counterpart without activation, whether the gradients are backpropagated or approximated.    

Table~, Table~ and Table~ show the zero-shot results of the pruned models with/without fine-tuning to discover the effectiveness of activations for guiding pruning LLMs. From the results in Table~, it is evident that incorporating activations  results in an overall enhancement of performance compared to the standard pruning criterion . For example, at a 10\% pruning ratio, the model pruned with  achieves a perplexity of 35.53 on the WikiText2 dataset for pruning BLOOM-7B. This result surpasses the 38.12 perplexity achieved by the model compressed by using . For OPT-6.7B, the performance enhancement provided by activation is also noticeable. For instance, with 30\% parameters pruned, the model pruned with  achieves a perplexity of 64.58 on PTB. Compared to using , the perplexity result decreased by 0.51. 

In Table~, gradients are computed by backpropagation. In contrast, the results presented in Table~ and  highlight the effectiveness of incorporating activations in estimated gradient-based pruning criteria when fine-tuning is applied or not. For instance, when fine-tuning is not applied, the model pruned with 30\% of BLOOM-7B's parameters using  achieves a perplexity of 91.43 on WikiText2, reflecting a 14.64 improvement over the 106.07 perplexity by the model using . In contrast, after undergoing fine-tuning, although the increase in performance becomes less pronounced as shown in Table~, it is still evident that activations play a significant role in enhancing performance.

Based on the findings in~ that the first and last layers significantly affect the model's performance, we investigate the impact of involving different ranges of layers in the pruning process on LLaMA-7B's performance. It includes analyzing the performance of models with pruning applied from the 1st to the 30th layer (represented by layer-1-30 in Figure~), from the 3rd to the 30th layer (layer-3-30), from the 4th to the 29th layer (layer-4-29), and from the 5th to the 28th layer (layer-5-28)~. From the results in Figure~, it is evident that layer-1-30 has the worst performance, while layer-3-30 and layer-4-29 have comparably better performance for both pruning methods. In contrast, the models derived from layer-5-28 pruning exhibit varying responses to different pruning methods. For instance, for MINI-LLM, their performance is similar to that of the layer-4-29 models, whereas, for LLM-Pruner~, their performance is somewhat inferior compared to both layer-4-29 and layer-3-30 models. Since the layer-4-29 pruning demonstrates consistent performance across various pruning methods, we conduct pruning for layer-4-29 in all LLaMA-7B experiments.

Considering the inferior performance of layer-1-31, in Figure~, we only display the proportions of layers involved in pruning for layer-3-30, layer-4-29, and layer-5-28. In addition, we present the average perplexity of LLM-Pruner and MINI-LLM for the three ranges of layers, marked with the orange or the blue five-pointed stars in Figure~, which is averaged over three pruning ratios: 30\%, 40\%, and 50 \% on WikiText2 and PTB. The average perplexity results in Figure~ illustrate that our MINI-LLM exhibits performance close to, even surpasses at times, that of LLM-Pruner. For instance, for layer-5-28, MINI-LLM achieves an average perplexity of 51.87 on PTB, outperforming LLM-Pruner's 53.73. These results demonstrate that MINI-LLM is a memory-efficient and effective method for gradient-based pruning.

Table~ shows the generation examples of the original and the pruned LLaMA-7B models achieved by MINI-LLM. The five experimental instructions encompass math, common sense, translation, and writing tasks. From the responses presented in Table~, it is evident that when pruning 20\% of the parameters, the pruned model maintains high performance in these tasks.