Research has revealed that many existing pre-training datasets contain a substantial number of duplicate samples . To explore the impact of duplicate data on model performance, numerous studies have been conducted on both general and domain-specific datasets . The results indicate that repetition at certain frequencies can significantly harm model performance . Although appropriate repetition under specific circumstances can be beneficial , this should result from careful selection rather than being an unintended consequence of data duplication.

Therefore, data deduplication is crucial for pretraining large language models. Exact deduplication is typically achieved through suffix arrays . MinHash  and SimHash  are widely used fuzzy deduplication methods. %, which have been applied in the training of models like the LLaMA  and the Falcon  series.  In recent research, some studies have shifted towards semantic-based deduplication.  and  utilize pre-trained embeddings for clustering to remove semantically redundant data.  combines both methods.  %Quality-based data selection is a common approach due to its significant impact on model performance . GPT-3  utilizes original WebText as a proxy of high-quality documents and a classifier is trained to identify high-quality data. Similar methods have also been employed in PaLM . The data selection methods based on diversity  and importance  has also been proven to be valuable. Furthermore, in order to improve the model's adaptability to the target domain, it is typically necessary to select domain-specific datasets for continuous pretraining . However, this approach often involves manual data curation. DSIR  introduces an automated method for selecting data that closely aligns with the target domain.%Additionally, in the CCNet pipeline, a fastText linear classifier and an n-gram model are utilized to obtain high-quality data.  Adjusting the significance of training samples through data reweighting has proven to be an effective strategy for enhancing model performance, either through modifying loss function weights or changing the sampling probabilities. Focal Loss, as introduced by , employs a soft weighting scheme to allocate higher weights to more challenging samples.  assign weights to training samples based on the direction of their gradients. In DSIR , sampling based on importance weights is utilized, allowing the training data to align with the distribution of high-quality corpora such as Wikipedia. DoReMi  explores an automated scheme for determining the sampling weights of different data sources. %%Pretrained language models are typically trained on datasets with complex compositions . The partitioning and mixing ratios of the data can have a significant impact on the performance of the models . PaLM  and GlaM  determine the proportion based on downstream task performance. DoReMi  can utilize group distributionally robust optimization to train a small proxy model in the domain to produce mixed proportions without prior knowledge of downstream tasks. Hard deduplication methods identify and remove duplicate samples. This process can be seen as partitioning the dataset  into numerous distinct subsets , such that . Each of these subsets contains samples deemed to be duplicates based on a specific similarity threshold. Within each subset , only one sample, denoted as , is retained, while the rest are discarded. If a subset consists of only one sample, it indicates that this sample has no duplicates within the dataset. %based on a similarity threshold. For each , a single sample is retained, denoted as , and all other samples in  are removed.

In the context of pre-training LLMs, the fundamental training goal is to maximize the log likelihood of the training data. Incorporating hard deduplication into this process can be formulated as:

where  denotes the log likelihood function,  represents the model parameters.  % is an indicator function that is active when  matches the retained sample .  Despite its utility, hard deduplication may inadvertently omit valuable data and fail to adequately consider the degree of redundancy.

To address the limitations of hard deduplication, we propose a soft deduplication method. This method employs sampling weights , allowing for a nuanced handling of data redundancy by adjusting the influence of each sample on the model based on its commonness:

We assume that the sampling weight of sample  can be represented as follows: %where  represents the current data distribution. The data commonness of the sample  is measured by its occurrence probability  within the current data distribution, reflecting its degree of duplication.  Here,  denotes the occurrence probability of sample , serving as a direct measure of its commonness. This probability-based measure effectively captures the degree of duplication of each sample. This approach ensures that samples with higher commonness are assigned lower weights, thus mitigating the impact of duplicates without discarding potentially valuable information. %Similar to DSIR, we assume that the sampling weight of the sample  can be represented as follows:% % W(x)=C\cdot{q(x)}% % where  represents the desired data distribution,  represents the current data distribution and  is a constant. The data commonness of the sample  is measured by its occurrence probability  within the current data distribution. Our desired data distribution is characterized by maximal diversity, which necessitates adhering to the principle of maximum entropy. This leads us to conclude that  ought to be a uniform distribution. Consequently, this premise simplifies the equation to: % % W(x)=C'\cdot{q(x)}.% %In practical implementation, we utilize an n-gram model to efficiently calculate the commonness of each data sample (Figure ). This process consists of 3 steps. In practical implementation, we leverage an n-gram model to process data, achieving high temporal efficiency in calculating the commonness of each data sample (Figure ). This process consists of three steps. %We can obtain the sampling weight of a given :% % q(x)=e^{{N}\sum\limits_{i=1}^{N}\log P(w_i |w_{i-1},\ldots,w_{i-n+1})}% % % % W(x)&=C'\cdot q(x)^{-{N}} \\% &=C'\cdot e^{-{N}\sum\limits_{i=1}^{N}\log P(w_i |w_{i-1},\ldots,w_{i-n+1})}.% % % A hyperparameter  is introduced to adjust the disparity between the maximum and minimum sampling weights.

Due to the vast volume of data, directly assigning individual sampling weights to each data point is impractical. To overcome this, we introduce an approximate method for data sampling that segments  samples into  categories. This process initiates by sorting the  samples in ascending order of data commonness, followed by dividing the dataset into  distinct segments according to  quantiles. For the -th segment, the sampling weight  is determined by the -th quantile, , as follows:

where  is a hyperparameter that adjusts the sampling weight and  is a normalization constant, which ensures that the sum of the weights across all segments equals one.  % % W_{Q_k}= W(x), x=Quartile_k.% % % W(x)=p(x)^{-T}.%  We conduct experiments on different versions of the Common Crawl dataset, which is a comprehensive and publicly accessible collection of data obtained through web crawling. %For each dataset, we randomly sample 40 billion tokens for training.%The first one, from RedPajama , involves the original Common Crawl data undergoing processing through the CCNet pipeline . This preprocessing includes a paragraph-level deduplication process and the deployment of a linear classifier aimed at identifying and selecting texts of superior quality. Additionally, we incorporate the SlimPajama Common Crawl dataset  into our analysis. The SlimPajama dataset represents a further refined iteration of the RedPajama corpus, boasting enhanced data cleansing procedures and the implementation of MinHashLSH  for more effective deduplication, thus resulting in a more streamlined dataset. The third dataset is the Falcon RefinedWeb , which similarly adopts rigorous deduplication and filtering methodologies. For each dataset, we randomly sample 40 billion tokens for training. is a subset of the RedPajama dataset . It involves the original Common Crawl data undergoing processing through the CCNet pipeline . This dataset has been subjected to paragraph-level deduplication; however, it has not undergone rigorous deduplication procedures. %This preprocessing includes a paragraph-level deduplication process and the deployment of a linear classifier aimed at identifying and selecting texts of superior quality.  is a subset of the SlimPajama dataset . The SlimPajama dataset represents a further refined iteration of the RedPajama corpus, boasting enhanced data cleansing procedures and the implementation of MinHashLSH  for more effective deduplication.

 is introduced as a pre-training dataset for the Falcon series . It undergoes rigorous deduplication processes using exact matching and MinHashLSH.

 In the experiments, we employ the same model architecture as the LLaMA  series. Our models are configured with 1.3B parameters, incorporating 16 attention heads and 24 layers. The hidden size is set to 2048, and the dimension of feed-forward network is 5504. Previous research has demonstrated the feasibility of pre-training validation on models of this scale . All models are trained from scratch to 40B tokens. The batch size is 512, and the training sequence length is 1024. The learning rate is decayed from 2e-4 to 2e-5. 

Our primary baseline is defined by directly training on a dataset that has been randomly sampled to encompass 40B tokens. In our study, we implement the SoftDedup method across all three datasets, facilitating a comparative analysis between our proposed technique and the established baseline for each dataset. Furthermore, for experiments conducted on the RedPajama CommonCrawl dataset, the SlimPajama CommonCrawl, which employs MinHashLSH for deduplication directly on it, is considered a hard deduplication baseline. %We propose two main baselines for our study. The first baseline involves training directly on the randomly sampled dataset of 40 billion tokens. Additionally, we consider clustering as a commonly used method for data partitioning. To obtain vector representations of the documents, we train a language model with 120M parameters on the same dataset. We then utilize the k-means algorithm for data clustering. Previous research suggests that the number of clusters should roughly correspond to the square root of the total dataset size . To improve the uniformity of the training data, we assign equal weights to the data after clustering and train models on this adjusted dataset. We evaluate the models by measuring their perplexity on the test sets and their few-shot performance on downstream tasks.

 Our test sets come from the Pile  and SlimPajama . The Pile test set consists of 22 subsets, including BookCorpus, DM Mathematics, and others. SlimPajama also includes 7 subsets, such as Common Crawl, C4, and GitHub. We measure the perplexity of the models on each sample and report the average for each subset. We investigate data leakage and remove the contaminated samples. Specifically, if a sample in the training set has more than 50 tokens of overlap with a sample in the test set, the former will be removed from the training set.

 In order to further evaluate the performance of the models, we measure their accuracy on 12 downstream tasks.  %We report the average few-shot accuracy on 12 downstream tasks.  The tasks cover the models' abilities in reading comprehension (SQuADv2 , Trivia QA ), commonsense reasoning (ARC easy and challenge , WinoGrande , HellaSwag , PIQA , Social IQa ), world knowledge (WebQuestions , NQ Open ), and contextual understanding (LAMBADA standard and openai ). The evaluation of downstream tasks is primarily accomplished through the utilization of the lm-evaluation-harness . %We adjust the number of data partitions and modify the weight allocation method to explore the effects of these different strategies. In exploring the impact of hyperparameters on our method, we focus on two key hyperparameters: the number of data partitions () and the weight parameter ().

Our experiments involves varying levels of data partition granularity by dividing the dataset into 10, 20, 50, and 100 segments. Regarding weight assignment, we modify the hyperparameter  within Equation  to alter weight disparities. We investigate three configurations that result in maximum-minimum weight differences of approximately 2-fold, 5-fold, and 10-fold, respectively. A larger disparity exerts a greater suppression on data with higher commonness. %Additionally, we conduct experiments using the DoReMi method to automatically determine the optimal weights, and the weights obtained from our approach are used as initial weights. To verify the effectiveness of our soft deduplication method, we conduct experiments on the RedPajama CommonCrawl dataset, which has not subjected to meticulous deduplication. Our findings indicate a significant improvement with our method compared to the direct training baseline, as illustrated in Figure . 

% Our approach consistently outperforms the baseline in terms of average perplexity across all evaluated datasets. Specifically, on the Pile test set, our method enables models to achieve baseline perplexity within 50,000 iterations, saving nearly 30,000 training steps. Furthermore, models continue to improve, ultimately reaching a lower perplexity, as shown in Figure . Similar advancements are observed in the SlimPajama test set, confirming our method's effectiveness (Figure ). Additionally, we report the average perplexity for each subset upon completion of training (Appendices  and ). Our method enables the models to yield performance improvements across the majority of the test subsets.

%  In our evaluation of downstream tasks, our method outperforms the baseline in accuracy. It accelerates learning on the RedPajama dataset, achieving baseline performance nearly 30,000 steps sooner and improving average accuracy by 1.77\% at the end of training, as shown in Figure . Detailed scores for each individual task at the final training checkpoint are delineated in Table . Our approach yields improvements in all evaluated tasks.

In summary, our experiments on the RedPajama CommonCrawl dataset substantiate that the soft deduplication method is capable of reducing perplexity and enhancing the accuracy of downstream tasks more efficiently compared to the baseline model. Such accelerated convergence is crucial for pre-training large language models, considering the significant costs associated with training duration and resource utilization. %%In Table , we compare the evaluation results of models trained on the RedPajama CommonCrawl and SlimPajama CommonCrawl datasets.%Comparative analysis reveals that deduplication significantly enhances model performance. Specifically, the baseline perplexity on the Pile test set decreases from 22.11 to 21.96, and on the SlimPajama test set, it is reduced from 38.09 to 34.58. Concurrently, there is an increase in average downstream task accuracy from 36.78\% to 37.38\%. These advancements underscore the critical role of high-quality data in model outcomes, demonstrating that deduplication can minimize noise and eliminate redundancies that might otherwise disrupt or mislead a language model during its training phase.%The implementation of our approach demonstrates a more pronounced performance enhancement in models trained on the RedPajama CommonCrawl dataset, which contains more duplicate data. Specifically, in the RedPajama CommonCrawl setting, there is a notable reduction in perplexity by 1.06 for the Pile test set and 2.44 for the SlimPajama test set, accompanied by a significant 1.77\% increase in average downstream task accuracy. In contrast, the improvements are less marked in the SlimPajama CommonCrawl dataset, with a perplexity reduction of 0.92 and 2.42 for the Pile and SlimPajama test sets, respectively, and an accuracy increase of 1.03\%. These findings suggest that our method may effectively address data imbalances, similar to the role of deduplication. By optimizing data distribution, it enhances the overall performance of the model.%Our method's impact on the RedPajama CommonCrawl indicates a potential as a superior replacement for the deduplication process. Notably, the application of our method on RedPajama CommonCrawl outperforms the baseline performance on SlimPajama CommonCrawl in terms of downstream accuracy, achieving 38.55\% over 37.38\%. This may indicate that our method not only eliminates the impact of redundant data by adjusting the data distribution but also retains as much useful information as possible. In the experiments carried out using the RedPajama CommonCrawl dataset, we also contrast the SoftDedup method against traditional hard deduplication techniques (refer to Table ). Considering that the SlimPajama dataset originates from the RedPajama dataset, refined through MinHashLSH deduplication, we employ models trained on the SlimPajama CommonCrawl dataset as the hard deduplication baseline.

The evaluation results of models on various downstream tasks reveal our method's superior performance over both the hard deduplication technique and the direct training baseline. In detail, while the hard deduplication method surpasses the direct training baseline in nine out of twelve tasks, showing an average increase in accuracy of 0.6\%, our SoftDedup method demonstrates more consistent and significant improvements. It outperforms the direct training baseline across all evaluated tasks, achieving an average accuracy enhancement of 1.77\%. These findings underscore the advantages over conventional deduplication methods in enhancing downstream task performance. %To facilitate a more effective comparison with existing methodologies, we conduct a comparative analysis of models trained on two datasets: the RedPajama CommonCrawl dataset and and its refined counterpart, the SlimPajama CommonCrawl dataset (refer to Table ). %The SlimPajama CommonCrawl dataset has undergone refinement via the MinHashLSH deduplication process, adhering to a Jaccard similarity threshold of 0.8. The results show a clear improvement in model performance after deduplication. Specifically, the baseline perplexity on the Pile test set decreases from 22.11 to 21.96, and on the SlimPajama test set, it is reduced from 38.09 to 34.58. Concurrently, there is an increase in average downstream task accuracy from 36.78\% to 37.38\%.%The results obtained from deploying our soft deduplication methodology on the RedPajama CommonCrawl dataset substantiate its efficacy, outperforming the existing deduplication techniques. Notably, the method achieves an enhanced downstream task accuracy rate of 38.55\%, which is a marked improvement over the 37.38\% accuracy rate obtained by the baseline model trained on the SlimPajama CommonCrawl dataset. This enhancement is also corroborated by the model's performance on the Pile test set. Our methodology mitigates the impact of data redundancy through judicious adjustments in data distribution and exhibits the potential to supplant current methodologies. To further assess the effectiveness of our method when applied in sequence with extant hard deduplication processes, we conduct experiments on the SlimPajama CommonCrawl and Falcon RefinedWeb datasets, which have undergone stringent deduplication processes (as shown in Figures  and ). 

% In the evaluations conducted on the Pile and SlimPajama test sets, our method exhibits consistent superiority over the baseline models. Notably, our models achieve equivalent baselines in perplexity with a reduction of 26\% to 39\% in the number of required training steps. Additionally, the ultimate performance of the models demonstrates a tangible enhancement, as evidenced by the results displayed in Figures , , , and . In terms of accuracy on downstream tasks, Figures  and  highlight the training efficiency achieved by our models. It is particularly noteworthy that our method reaches baseline performance with around 20,000 fewer training steps. We report the detailed scores in Appendices , , and .

In summary, even when applied to already deduplicated datasets, our method significantly enhances training efficiency and effectiveness. This underscores its capability to address the shortcomings of current deduplication techniques. Specifically, our approach reweights the data to reflect varying levels of duplication, thus avoiding one-size-fits-all solutions. This integration has the potential to become a standard practice in the pre-training of large language models. %In summary, when applied to already deduplicated datasets, our method still significantly enhances training efficiency and effectiveness, underscoring its ability to compensate for the shortcomings of current deduplication methods. Particularly, it reweights data to reflect varying duplication levels, avoiding blanket solutions. This integration could become a standard in large language model pre-training. In Figure , we illustrate the impact of different numbers of data partitions on model performance. We argue that investigating methods to further enhance the training effectiveness of higher-quality data is a more critical concern. Therefore, our experiments are conducted on the Falcon RefinedWeb dataset.

In evaluations conducted on both the Pile and SlimPajama test sets, models exhibit negligible variations in average perplexity across a range of data partition counts, specifically 10, 20, 50, and 100. This observation indicates that perplexity, as a metric, demonstrates relatively low sensitivity to changes in the granularity of data partitioning.

In contrast, we observe that as the granularity of data partitioning increases, the accuracy of the language model in downstream tasks also improves. As demonstrated in Figure , there is a clear correlation between the number of data partitions and the model's accuracy. This indicates that finer-grained data partitioning can make the training data more balanced, thereby enhancing performance in downstream tasks.

Figure  presents the outcomes of experimental investigations into the effects of varying disparities between maximum and minimum weights assigned to different data partitions. The methodology employed ensures a consistent ascending order in the allocation of weights, with greater disparities indicating a more pronounced suppression of data with high commonness. 

Experiments conducted utilizing disparities in the maximum to minimum weight ratios of 2-fold, 5-fold, and 10-fold reveal a consistent trend: increased disparities between the maximum and minimum weights lead to a reduction in average model perplexity. Although slight variations are observed in the performance outcomes for downstream tasks, the experiments demonstrate that the largest weight disparity consistently facilitates the most optimal model performance. %In the detailed series of experiments described in this work, our research has conclusively demonstrated that the SoftDedup methodology consistently improves training efficiency. Specifically, for models comprising 1.3 billion parameters, the implementation of SoftDedup has enabled the attainment of equivalent baseline performance while necessitating at least a 26\% decrease in the number of required training steps. This efficiency improvement is quantitatively substantial, resulting in a reduction of approximately 930 GPU hours in our experimental setup, utilizing exclusively 32G V100 GPUs. The computational processes of n-gram training and commonness calculation are executed solely on CPU resources. For a 40B token corpus, the n-gram training procedure (with ) requires 4 CPU cores for 5 hours, followed by computing data commonness using 4 CPU cores in 2 hours. Compared to the substantial costs of GPU conservation (at least 930 V100 GPU hours in our experiments), these expenses can be considered negligible. This underscores the efficiency of SoftDedup and the feasibility of its implementation in resource-constrained environments.

In Table , we provide a detailed report on the average perplexity for each subset within the Pile test set. For models trained on the RedPajama CommonCrawl dataset, our method results in improvements across 18 out of 22 subsets. For models trained on the SlimPajama CommonCrawl dataset, our method leads to improvements in 17 subsets. For models trained on the Falcon RefinedWeb, improvements are observed in 19 subsets. Due to the exceedingly small number of documents in the Ubuntu IRC subset, we exclude it from the calculation of the average perplexity on the Pile test set.

In Table , we provide a detailed report on the average perplexity for each subset within the SlimPajama test set. Our method has led to improvements across nearly all subsets.

In Table , we provide a detailed report on the accuracy for each downstream task. For models trained on the RedPajama CommonCrawl dataset, our method has led to improvements across all tasks. For models trained on the SlimPajama CommonCrawl and Falcon RefinedWeb datasets, our approach has also resulted in accuracy improvements on the majority of tasks. %Large language models are commonly pretrained using a corpus consisting of a weighted combination of various natural domains, such as CommonCrawls, Wikipedia, and GitHub. The significance of each domain is reflected through assigning different weights to the composition of the training corpus. Our preliminary investigations indicate that certain data within the same natural domain exhibit simplicity and local redundancy. Therefore, solely partitioning data and assigning weights based on natural domains may result in an overemphasis on such data during training. We propose that a more nuanced approach is necessary, involving further subdivision and differentiated weighting for data within each natural domain, to enhance model performance. In this study, we introduce a simple yet effective method for re-partitioning the data based on the perplexity of each training sample measured by a n-gram language model trained on the corpus. The experiments demonstrate that our proposed method speed up the training process of large language models, achieving the same perplexity value on the test dataset with xx\% less training steps. In addition, our proposed method improves average few-shot downstream accuracy by xx\% after training for the same number of steps.%The effectiveness of large language models (LLMs) is often hampered by duplicate data in their expansive pre-training datasets. Standard approaches to this challenge involve identifying and removing such redundancies. However, existing hard deduplication methods risk losing potentially valuable information. To address this issue, we introduce a novel soft deduplication technique. This approach retains the entirety of the dataset while strategically reducing the sampling weight of data with high commonness.Our experiments demonstrate that our proposed method speeds up the training process of large language models, achieving the same perplexity value on the test dataset with at least 26\% fewer training steps. Additionally, our proposed method improves the average few-shot downstream accuracy by 1.77\% after training for the same number of steps.Remarkably, when implemented on datasets that have already been stringently deduplicated, our method continues to yield significant performance enhancement. This signifies that our approach can serve as an effective complement to existing methods and become one of the standardized processes for pre-training LLMs.%The efficacy of large language models (LLMs) is frequently compromised by the presence of duplicate data within their expansive pre-training datasets. Traditional approaches to mitigate this issue primarily focus on the detection and removal of such duplicates. However, these hard deduplication methods risk discarding potentially valuable information and fail to account for the varying degrees of duplication. To address this challenge, we introduce a novel soft deduplication technique that preserves the integrity of the entire dataset while selectively reducing the sampling weight of data with high commonness. Our empirical analysis demonstrates that this method significantly enhances the training efficiency of LLMs, achieving comparable perplexity scores on the test dataset with at least a 26\% reduction in the number of required training steps. Additionally, this approach improves the average few-shot downstream accuracy by 1.77\% when trained for an equivalent duration. Remarkably, this method consistently yields substantial performance improvements even when applied to datasets that have been subject to rigorous deduplication. This indicates that our approach can effectively supplement existing methods and should be considered a standardized process in the pre-training of LLMs.%The effectiveness of large language models (LLMs) is often hindered by duplicate data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, risking the loss of valuable information and neglecting varying degrees of duplication. To address this, we propose a soft deduplication technique that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of "data commonness", a metric we introduce to quantify duplication degrees by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26\% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77\% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs. The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of "data commonness", a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26\% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77\% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs.

figure/deduplicationfigure/mainIntroductionraffel2023exploring,gao2020pile,penedo2023refinedwebhernandez2022scalingmuennighoff2023scalingleskovec2020miningcerebras2023slimpajama,penedo2023refinedweb666900penedo2023refinedwebfig:deduplicationRelated WorkData deduplication10.1145/3133908,bandy2021addressing,penedo2023refinedweballamanis2019adverse,lee2022deduplicating,biderman2023pythia,xue2023repeathernandez2022scalingmuennighoff2023scalingdoi:10.1137/0222058666900charikar2002similarityabbas2023semdedupsorscher2023neuraltirumala2023d4Data reweightinglin2018focalren2019learningxie2023dataxie2023doremiMethodHard deduplication

% = \sum_{x\in}\sum_{i=1}^{k} I[x=x_i]\log P(x|\Theta)  &= \sum_{x\in}I(x)\log P(x|\Theta),\\ I(x)&=

1, &x\in\{x_1,x_2,\cdots,x_k\} \\ 0, & \\ casessplitSoft deduplication  = \sum_{x \in } W(x) \cdot \log P(x | \Theta), \:W(x) \in (0,1).

W(x)\propto{p(x)}. Implementation of commonness calculationfig:main_method The n-gram model assumes that the appearance of a word is determined by the previous  words. The first step is to tokenize the original corpus. We use the same tokenizer as the pre-training models for consistency.   Tokenization. %In the training process of an n-gram model (), maximum likelihood estimation is used to calculate the probability of each n-gram. "" is the empirical choice made after our early experiments. To alleviate the issue of data sparsity, we employ the Kneser-Ney smoothing technique. The KenLM toolkit is utilized to accomplish this step.   In the training process of an n-gram model (where ), maximum likelihood estimation is used to calculate the probability of each n-gram. We empirically choose  after conducting early experiments. To alleviate the issue of data sparsity, we employ the Kneser-Ney smoothing technique . We utilize the KenLM toolkit to accomplish this step.   Train n-gram model.NEY19941https://kheafield.com/code/kenlm We utilize the obtained n-gram model to compute the commonness (measured by the occurrence probability) for each data sample. For a given  containing  tokens, 

  By employing the geometric mean, the influence of sample length can be eliminated.    %   %    % &PPL(x) = q(x)^{-{N}} \\   % &=e^{-{N}\sum\limits_{i=1}^{N}\log P(w_i |w_{i-1},\ldots,w_{i-n+1})}   %    %    %where  represents the number of tokens. Calculate commonness.   p(x) = \left(\prod_{i=1}^{N} P(w_i |w_{i-1},\ldots,w_{i-n+1})\right)^{{N}}.   Approximate sampling for large-scale data W_{k} = C \cdot \left({p_k}\right)^T %C = {\sum_{j=1}^{K} \left({p(q_j)}\right)^T} Experimental SetupDatasetsRedPajama CommonCrawltogether2023redpajamawenzek2019ccnetSlimPajama CommonCrawlcerebras2023slimpajamaleskovec2020miningFalcon RefinedWebpenedo2023refinedweb,almazrouei2023falconModel trainingtouvron2023llamatirumala2023d4,xie2023doremiBaselinesEvaluation metricsTest set perplexity.gao2020pilecerebras2023slimpajamaDownstream task accuracy.rajpurkar2018knowJoshiTriviaQA2017Clark2018ThinkYHsakaguchi2019winograndezellers2019hellaswagBisk2020sap2019socialberant-etal-2013-semanticlee-etal-2019-latentpaperno2016lambadaeval-harnessHyperparameter impact analysiseq:pplfigure/redpajama_main_resulttable/main_resultfigure/slimpajama_main_resultfigure/falcon_main_resultsfigure/partition_numfigure/weight_allocationResultsEnhanced performance and efficiency in language model pre-trainingfig:redfig:redcc_pilefig:redcc_slimpajamasec:appendix_pile_pplsec:appendix_slimpajma_pplfig:redcc_downstreamtab:mainSurpassing hard deduplication in effectivenesstab:mainA powerful complement to existing techniquesfig:slimfig:falconfig:slimcc_pilefig:slimcc_slimpajamafig:falcon_pilefig:falcon_slimpajamafig:slimcc_downstreamfig:falcon_downstreamsec:appendix_pile_pplsec:appendix_slimpajma_pplsec:appendix_downstreamFiner data partitioning for improved downstream task performancefig:partition_numfig:partition_num_downstreamEffects of sampling weight disparities on model performancefig:weightCost of data reweightingConclusionLimitationsanthology,customAppendixAverage perplexity for each subset in the Pile test setsec:appendix_pile_ppltable/ppl_piletab:ppl_pileAverage perplexity for each subset in the SlimPajama test setsec:appendix_slimpajma_ppltable/ppl_slimpajamatab:ppl_slimpajamaAccuracy for each downstream tasksec:appendix_downstreamtable/downstreamtab:downstream