The instruction following sample consists of an instruction , an optional input , and an output  in the same language.  Considering the vague boundary between instruction and input, the input  is ignored in this work for simplicity.  We define the -lingual  () sample  that the language  of instruction is different from the one of output ().  Taking the cross-lingual instruction sample in  as an example, the language of instruction is English, while the language of output is Urdu. 

To exploit the better generation performance in high-resource languages like English, we generate English instructions for a corpus in other languages.  Specifically, given seed cross-lingual instruction samples , where the  and  denote the high- and low-resource languages respectively, the language model is fine-tuned to generate the instruction  given the response  as input. Then, we use it to generate candidate cross-lingual instructions for the multilingual corpus, basing the assumption that some texts among the corpus are good responses in X-Instruction samples. 

After generating cross-lingual instruction samples, it is important to find and discard the inappropriate ones due to the quality of instruction following samples having a great influence on the performance of model aligned .  To achieve this, we design an iterative method named ``X-Instruction Refinement'', which is shown in . 

In the -th iteration, we first synthesize a pseudo-rating dataset  from a part of seed data , where , to train the evaluator that outputs three-level ratings.  Given the instruction , the best response () is the vanilla output , while the worst one () is selected from the mismatched output  ().  The reasonable but flawed output () is chosen from the modified output  with some parts deleted or duplicated in the vanilla output , or the output of the cross-lingual instruction-following model tuned on .  In the first iteration (), the seed data , which is not used in the pseudo-rating dataset , is adopted to fine-tune the cross-lingual instruction following model.  In the subsequent iteration (2),  consists of the best cross-lingual instruction samples found by the evaluator and . 

As the iteration proceeds, the quality of output from the instruction following model improves by tuning on higher quality X-Instruction samples, which reduces the gap between the second-level sample and the highest one in the pseudo-rating dataset.  Thus, the trained evaluator can find better cross-lingual instruction tuning samples.  The number of iterations is set to 3 by default, which is investigated in Section . 

In addition to the quality, the diversity of instruction tuning samples also has a great impact on the performance .  To diversify X-Instruction samples, we first obtain the embedding for each instruction  using a pre-trained sentence encoder.  After applying -means on these embeddings, the same amount of X-Instruction samples are sampled from each cluster ().  It aims to diversify cross-lingual instruction examples by avoiding the dominance of samples from a few domains in the final dataset.

For each language, we extract the first conversation turn of message trees in the Open Assistant dataset, and adopt the highest quality response as the output for each sample .  To construct the X-Instruction sample, the instruction of each sample is translated into English by Google Translate.  However, there are only a few or even no human-labeled samples for the most of languages involved in this work.  Thus, we translate the output of 3k English samples into the target language using Google Translate to supplement the seed data for each language. The statistics of seed data are reported in Appendix . 

The multilingual corpus for each language is extracted from the CulturaX dataset , which contains web texts in 167 languages and is filtered from mC4  and OSCAR .  We only sample 1M web texts for each language from this dataset due to the constraint on the computation budget. 

To investigate the diversity of X-Instruction, we parse English instructions and count the ones with verb-noun structure using the Berkeley Neural Parser .  Figure  illustrates the top 16 most common root verbs and their top direct noun objects.  % Consider to move the seed data into the appendix. We can find that the great diversity of X-Instruction constructed from web corpora.  The additional information of X-Instruction is reported in Table .

For each language, we randomly sample 200 instructions and corresponding texts to further evaluate the quality.  There are two questions designed to conduct this evaluation, and results are reported in Table .  It can be found that the quality of X-Instruction is good for more than 80\% of samples are valid.  The top-flows come from additional information in web text like navigation bar (11.3\%), incorrect URL and HTTP status codes (3.8\%).  % Check by human.% Yang wen LLaMA-2 models with 7B and 13B parameters are taken as base models and fine-tuned on cross-lingual samples in each language from the X-Instruction dataset.  Hyperparameters are reported in Appendix .  % Although LLaMA-2 models are pre-trained mainly on English corpus, it is found that good response in some unknown languages after tuning with large-scale instruction tuning data. used in this work are: % Datasets  To confirm the effectiveness of X-Instruction, we conduct the evaluation of open-end generation on Vicuna , WizardLM , LIMA , and Koala  datasets, which cover a variety of task categories.  Prompts in these datasets are translated into the 10 languages involved.  A comprehensive overview of these datasets is presented in Appendix . 

We take GPT-4  as a judge to conduct automatic evaluation, which is found a higher correlation with human judgements . Considering the excellent multilingual understanding ability of GPT-4 , it is reasonable and effective to employ GPT-4 to automatically evaluate the quality of responses in low-resource languages. 

Specifically, we adopt pair-wise evaluation and request GPT-4 to determine the better response between responses  from different models given the instruction .  In the evaluation, GPT-4 judge outputs a score, named GPT-4 score in this work, from 0 to 10 based on their helpfulness, relevance, and accuracy.  The details of GPT-4 evaluation prompt can be found in Appendix .

To alleviate the position bias in the evaluation by GPT-4 , we first request GPT-4 to evaluate , then switch the position of  and , which is , in the second evaluation.  The better response is the one that wins twice or wins once and draws once. 

Table  reports the win rates of models against ChatGPT on four benchmarks in three low-resource languages.   It can be found that X-Instruction exhibits a significant performance advantage over Alpaca-MT and Bactrian-X in all benchmarks.  The average improvement of X-Instruction models over Bactrian-M models, which distill outputs from ChatGPT, reaches 15.8\%.  Compared with ChatGPT,  demonstrates even better performance with a 67.66\% win rate. 

To evaluate the effectiveness of X-Instruction in diverse languages, we extend experiments to ten languages, including five medium-resource and five low-resource languages, on Vicuna and WizardLM datasets.  Table  reports the detailed results on ten languages across two benchmarks, which exhibit a certain level of uniformity across ten languages.  The X-Instruction models with only 3k labeled seed data on each language obtain the highest win rate across ten languages on average, indicating that our method can be extended to more languages. 

We further examine the generative quality by calculating the average GPT-4 score for each model from all comparison pairs, shown in Table .  The average generative quality of X-Instruction is the most pronounced, achieving an average score of 6.9 in all languages.  Notably, X-Instruction outperforms Bactrian-X by a large margin (1.3) and achieves better performance than Bactrian-M (5.6).  Although the response quality of  drops to 7.34 in five low-resource languages, the average win rate against ChatGPT increased by 14.9\% compared to the one in the other 5 medium-resource languages.  It comes from the worse performance of ChatGPT in low-resource languages.  % 13B 52.2% 7.62(med), 67.1% 7.34(5 low) Since X-Instruction models only learn how to reply in low-resource language, there is a conjecture naturally comes to mind: 

To validate our conjecture, we design a zero-shot evaluation using prompts in the output language instead of the English prompt used in training.  Table  shows the performance of zero-shot generation in ten languages.  Compared with the vanilla cross-lingual generation, the win rate of X-Instruction models under zero-shot evaluation only drops by 5.7\% on average.  The average quality of responses in these languages incurs a minor performance degradation (-0.7), which is 90.6\% of the vanilla one, indicating our model achieves exceptional zero-shot learning ability.

Figure  shows the detailed performance of models given prompts in different categories from the Vicuna dataset.  As shown in Figure  and , X-Instruction outperforms Bactrian-M in all categories, and surpasses ChatGPT in eight categories except fermi. The excellent performance in commonsense, writing, and knowledge categories may come from the native multilingual corpus used in the X-Instruction dataset. 

The primary reason for the inferior performance on code and math in Figure  is the lack of relevant corpora, which are filtered out by the language identification tool used in multilingual corpus cleaning.  It can be alleviated by adding language-agnostic instructions-following samples from code and math domains.  Figure  compares the outputs from  given English instructions or instructions in other languages (the zero-shot evaluation).  It can be found that the performance in the knowledge category declines the least when instructed in the output language. 

To evaluate models on instructions of different difficulties, we perform elaborate analysis on the WizardLM dataset, which contains labels of difficulty.  Following the evaluation of WizardLM , we split the test set into ``Easy'', ``Medium'', and ``Hard'' three parts with difficulty levels on , , and .  As shown in Figure , with the increase of difficulty, the win rate of  improves, while the one of  decreases.  It reflects that the quality of responses from ChatGPT decreases more than the one of X-Instruction models when given more difficult instructions.  It is noted that  under zero-shot evaluation surpasses ChatGPT in all difficulty skills. 

 We prompt GPT-4 to evaluate the responses in the following three views: helpfulness (0-10), relevance (0-10), and accuracy (0-10), and report the average results of 10 languages in Table . It can be found that the responses from  are uniformly better than the baseline model in the three dimensions, especially in the relevance dimension (+1.9).

To enhance the comprehensiveness and reliability of the evaluation, we further conduct the human evaluation in three low-resource languages, focusing on the general quality of responses on the Vicuna dataset.  Specifically, human evaluators are required to compare answers A and B generated by two models for each instruction, and choose an option from ``A wins'', ``B wins'', and ``Tie'' based on their judgment.  For the sake of fairness, we randomize the order of answers and eliminate the position bias. 

The results in Figure  demonstrate the better responses from our model compared with the ones of ChatGPT and , and indicate the consistency between GPT-4 and human evaluation.  Moreover, we provide examples in Appendix  for qualitative analysis of the responses from different models. 

% Li Chong To investigate the effect of X-Instruction refinement, we statistic the win rates of  against ChatGPT using 32k cross-lingual samples from different iterations of refinement on 4 benchmarks.  As shown in Figure , the win rate of models in Urdu and Bengali increases with more refinement iteration, which reflects the improvement in the quality of cross-lingual instruction tuning samples.  Thus, we set the number of iterations to 3 by default, where the improvement in the quality of response is almost saturated. 

We study the impact of data quantity and quality on the X-Instruction model using the Urdu samples from the third refinement iteration.  Figure  shows that the quality of the model responses will increase with more samples used and is close to saturation at 32k, which is similar to the findings of .  In addition, tuning on the samples with higher ratings brings better responses, which demonstrates that the evaluator trained does find higher quality instruction tuning samples. 

In addition to the generation ability in these languages, we further evaluate the performance of X-Instruction models on multilingual natural language inference  and commonsense reasoning tasks .  As shown in Table , the average improvement on the zero-shot in-context learning performance of the base model is 3.1\%, which is higher than the 1.8\% improvement from the Bactrian-X dataset.  It further confirms that cross-lingual instruction tuning on X-Instruction enhances the multilingual language understanding abilities of language models. 

The statistics of seed data used are reported in Table .  It is noted that all outputs of five low-resource languages are translated from English samples for none samples in these languages are found in the Open Assistant dataset . 

Table  shows the details of other multilingual instruction tuning datasets.  We also report the information of four open-end generation datasets in Table .  Notably, both Vicuna and WizardLM datasets have diverse categories, thereby ensuring the richness and diversity of the test sets.  It not only guarantees a thorough evaluation, but also minimizes the risk of evaluation bias since the test sets encompass various instruction categories.  

Figure  shows the detailed results of  vs. ChatGPT on four datasets.  Moreover, we report the evaluation results in all languages on two benchmarks from GPT-4 in Table . 

Figure  illustrates the impact of different numbers of clusters in the final diversification stage when sampling the same amount of data.  Given the amount of final data to 32k, the output quality of models tuned drops when the number of clusters reaches 2000, which may come from the smaller inter-cluster distance and less diversity in the sampled data.  Thus, the number of clusters is set to 1000 by default. 

To take a deep look into the detailed improvements brought by the cross-lingual instructions augmented in X-Instruction, we statistic the detailed performance on different categories in three languages (tr, sw, ur) for the seed model and  when compared with ChatGPT.  As shown in Table , the performance on prompts of all categories is improved, especially for the ``Counterfactual'', ``Common-sense'' and ``Roleplay'' categories.

We report two valid samples and invalid samples in Figure  and .  Although there are inappropriate instructions in the invalid samples, we can find that the semantics of the English instruction generated are related to the given text.  

To qualitatively analyze responses from different models, we report four cases in Figure , Figure , Figure , and Figure .  It can be found that  provides a more detailed and coherent response for the same instruction provided.  In some cases, e.g., Figure ,  provides suggestions in texts rather than codes needed for instructions about code generation, which may arise from the lack of code data in the multilingual corpus as responses. 

We conducted the quality evaluation of the X-Instruction dataset with five annotators.  We paid .2 for each annotation.  We illustrate the user interface for response comparison in Figure .