Contrastive Language-Image Pre-training (CLIP) from Radford~ has emerged as a highly successful approach for training vision-language models. CLIP is a dual encoder model with separate encoders  and  for extracting visual and textual features respectively. It also has two dedicated projection functions  and  that map the outputs of the encoders to a shared embedding space. Given a batch of   images and text pairs  in each training step, CLIP  computes the embeddings   and  where  represents the normalized features of image .   denotes the normalized features  of the corresponding caption . The loss is evaluated using InfoNCE~ whereby matching image-text pairs   constitute the positive samples and non-matching pairs  form the negative examples. A bidirectional loss is computed as 

where temperature  is typically a learnable parameter used to scale the logits.  is fixed in all of our ablation experiments as it has a noticeable impact on the model~ which makes comparisons across different experiments difficult.  is a similarity function measuring the distance between the features. In CLIP~ and our experiments,  is set as the dot product function.  The total loss is an average of the two losses in  and : 

In each training step, CLIP- samples a batch of examples of size , . Any given paired instance  is either the original example   or a composition of that example and another example , drawn from the dataset. Note that index  is taken with respect to the dataset size and not the batch size , , sample  may not be present in the current mini-batch. The proportion of composed samples in any mini-batch is controlled by a sampling rate hyper-parameter . The impact of this parameter is discussed in . 

In the case whereby  is a composite sample, the new caption  is a concatenation of the two original captions involved:  where  is a string concatenation function with the word ``and'' as a  conjunction. The positions of the captions on either side of this conjunction change, with  appearing first fifty percent of the time.

The new image is composed of the center half crops spanning either the height or the width of each image. For example, if the images have resolution , either  or   center crops are taken from both images and concatenated as illustrated in . We experiment with other forms of image augmentation methods such as MixUP and CutMix in . 

After assembling the mini-batch as described above, CLIP- proceeds to extract the image and text features as in CLIP:  and . With  and  computed, , , and  are used to compute the InfoNCE loss. 

The sampling strategy CLIP- employs exposes the model to a much higher diversity of images and their corresponding captions compared to the vanilla pretraining pipeline. As a result, we observe much more significant improvements in downstream transfer when the pretraining dataset is small. It is reasonably expected that relatively larger datasets such as RedCaps~ are already sufficiently diverse and, therefore, may not benefit from our method. Nonetheless, CLIP- still does better than CLIP on these large datasets. % albeit with reduced margins. %   Why will combining multiple different image-caption pairs into single instances during pretraining lead to improvements in downstream evaluations?  In other words, why will CLIP- work? To investigate this salient question, we examined the pretraining losses and cosine similarities of both the composite examples and plain examples as the model evolves. This fine-grained tracking of training mechanics provides insights into how the model handles plain simple examples versus composite examples, and whether there are any differences between the two groups.   

 Contrary to expectation that compound examples will be the more challenging to the model (since they are multiple examples condensed into single instances), we observed precisely the opposite: as shown in~, the loss on the composite examples is lower than the loss on plain examples especially in the early stages. Our hypothesis for this empirical observation is that the model more easily recognizes compound image-caption pairs because they tend to be structurally different from plain examples. The more interesting development arising from this phenomenon, however, is that the model is encouraged to dedicate more effort into learning the plain examples in CLIP- compared to CLIP as seen in~. We believe this elevated learning of plain examples together with the use of dynamic semantic compositions (See~) all contribute to the superior capabilities of our method as discussed in the next sections. 

 % This together with However, in~, we show that simply giving CLIP more training resources is not enough to bridge the performance gap with our method, suggesting the % Although, prior influential works in computer vision such as CutMix~ and MixUP~ have shown that linear combinations of different examples in input space produce good representations, there .  We conduct a thorough study of the transfer learning capabilities of our model in zero-shot image classification on many downstream benchmarks, including ImageNet~ in . Across different pretraining datasets, our method substantially improves over CLIP.  For ViT-S/16, CLIP- achieves a  top-1 improvement over the baseline CLIP model on ImageNet while outperforming CLIP on  out of  downstream datasets when pretraining on CC3M. Furthermore, these enhancements are maintained when we scale the vision encoder from ViT-S/16 to ViT-B/16 showing the continued effectiveness of our method over CLIP in a bigger model. When pretraining on RedCaps and CC12M, the gains of CLIP- over CLIP on ImageNet are respectively are  and . These results are remarkable, considering that our approach and CLIP both use the same number of parameters, memory, and computational resources during pretraining. Even in the relatively data-rich settings of CC12M and RedCaps, CLIP- still improves over CLIP on  out of   benchmarks for RedCaps and  of the  benchmarks for CC12M.  % All these suggest a continued Furthermore, these enhancements are maintained when we scale the vision encoder from ViT-S/16 to ViT-B/16 showing the continued effectiveness of our method over CLIP in a bigger model. % We observe effects of the pre-training datasets on different downstream benchmarks. For instance, pretraining on RedCaps substantially outperforms pre-training on CC12M on Food-101~{cite this} (31.9\% absolute top-1 accuracy improvement) and Flowers~{cite this as well} (24.2\% absolute top-1 accuracy improvement). However, % ####### Linear probe results table. Will likely move to a different location later In addition to the zero-shot transfer results as detailed in , we also provide analysis of the performance of CLIP- versus CLIP on zero-shot cross-modal retrieval in . For these evaluations, we use MS-COCO~ and Flickr30k~ as the downstream benchmarks. As in the zero-shot transfer setting, CLIP- yields significant improvements over the baseline model on both MS-COCO and Flickr30k across different pretraining datasets and model sizes. For example, when using CC3M as the pretraining dataset, our method outperforms CLIP by over  absolute top-1 retrieval accuracy in both image-to-text and text-to-image retrieval. The enhancement on MS-COCO is  on image-to-text and  on text-to-image retrievals. For both CLIP and our method, we noticed low retrieval results when pretraining on RedCaps, which we believe is related to the data distribution. We leave that analysis out for later works. 

 % We observe effects of the pre-training datasets on different downstream benchmarks. For instance, pretraining on RedCaps substantially outperforms pre-training on CC12M on Food-101~{cite this} (31.9\% absolute top-1 accuracy improvement) and Flowers~{cite this as well} (24.2\% absolute top-1 accuracy improvement). However, % compared to pretraining on CC12M% 

 Having verified the efficacy of our method using joint-embedding features in the zero-shot settings, we conduct several linear-probing evaluations in  to test the quality of the learned image features.  In these experiments, a randomly initialized linear layer is added on top of the pretrained image encoder  which is frozen. The text encoder , along with linear projections  and  are discarded. We train the linear layer for 50 epochs using a stochastic gradient descent optimizer with a weight decay of  and a momentum of . The linear probe learning rate and mini-batch size for each downstream dataset are provided in  in the Supplemental. 

 Using our proposed compositions, CLIP- surpasses CLIP in several linear probe experiments when the pretraining dataset is relatively small, , CC3M, indicating that our method also learns more discriminative image features than CLIP in that regime. These superior generalization capabilities come as a result of exposing the image encoder to a more diverse array of images through the compositions. We stress again that these gains are obtained using the same pretraining datasets (without any external augmentations), computational costs, and number of parameters as the CLIP baseline. The linear probing results are competitive on all downstream benchmarks when using larger pretraining datasets.

% {The supervision that the image encoder receives is also greatly increased in our pipeline due to the presence of hard negatives}.% To show the quality of our image features, we also provide a t-SNE~ visualization of the image features of CLIP and CLIP- in {figure caption for T-SNE}.% % % %   It could be argued that our method sees a lot more examples due to the compositions we employ, and that may be the reason for the observed improved performances. In , we show that a CLIP- model that uses a batch size of  examples outperforms the equivalent CLIP model trained with a batch-size of  by 1.6\% on ImageNet,  strongly indicating that our method is different from ---and more impactful than--- a technique to increase the CLIP batch size. Similarly, we show in~ that training CLIP for  times the number of epochs for the CLIP- model does not close the performance gap (compare CLIP-52 and CLIP--40 epochs in~). Indeed, the results in  highlight the   of CLIP- as its superiority over CLIP emerges towards the later stages of pretraining. CLIP- becomes even   All these empirical results point to concrete beneficial qualities of CLIP- as discussed in~ and not from any implicit amplified exposure to data.  

% and all examples in the mini-batch are the originals% for good performance% %  %% % as if they were semantically distinct examples% CLIP-'s advantages are not related to enhanced exposure to the data. Indeed, the results show that %%***** sampling probability rho The probability at which we create a composite sample as opposed to the original image-caption pair is an important parameter in our method which determines the percentage of the mini-batch that are compound instances. When , our method is identical to CLIP as no composition is performed. On the other extreme, when , all the examples in each mini-batch are instances of our composition method. As shown in , using a small non-zero sampling rate is more effective than CLIP. However, the performance deteriorates when more than fifty percent of the mini-batch are these compound image-text pairs. These results indicate that maintaining a reasonable percentage of the original examples is necessary likely because streamlined non-contradictory learning signal is significantly reduced when a majority of the batch are compositions. Also, since downstream evaluations do not involve such compositions, some exposure to examples with uniform semantic content during pretraining is important for effective transfer.  

% ******semantic compositions

We call CLIP- compositions semantic because the new instances are not just stylistically different from the constituent original examples, they are also semantically different.  Thus, it is fair to question whether or not this semantic differentiation is important in producing the observed favorable results over CLIP. After all, purely stylistic augmentations that use content from the same examples also increase data diversity and could yield the same outcomes as our semantic compositions. We investigate this prospect in this section. We train a model using two augmentations of the same example instead of two distinct examples as outlined in . On the image side, two random crops of the image are taken simulating two instances. For the text, we employ ``Easy Data Augmentation (EDA)''~ to generate a caption for the second crop while the first crop uses the original caption. These two stylistically generated examples are then combined using CLIP-. 

%

In , it is evident that such stylistic augmentations are sub-optimal compared to the semantic generations we employ in CLIP-.  On ImageNet, the CLIP- model achieves a  absolute top-1 accuracy than the stylistic augmentations model. This suggests that the content of the new instances is important as the model prefers the use of distinct examples in the composition. We also note that just increasing the diversity of examples is helpful as the stylistic augmentations method yields a \% zero-shot accuracy gain over CLIP on ImageNet. 

%****** Impact of RandomNess Whenever CLIP- composition is activated, the second example is usually chosen randomly from the dataset. This allows for every image-caption pair to be paired with any other image-caption pair in the dataset. Moreover, the pairings differ from one epoch to another, thus uncovering novel combinations of examples throughout pretraining.

We examine the impact of this dynamic nature of CLIP- versus using fixed pairs of examples. To do this, for every example , we allocate only one other example  that is fixed throughout training. Then, whenever  is involved in a CLIP- composition,  is used. The results in  suggest that dynamic compositions lead to better downstream results than fixed compositions. This makes intuitive sense because in the dynamic case, if a particular composition is unhelpful, there is a possibility of changing it in subsequent epochs. This possibility does not exist when the combinations are fixed.  in  of the Supplemental, we also investigate scenarios whereby we combine examples whose captions are either close or far apart in the feature space of a sentence embedding model~.  

% %******* Image composition function In this section, we compare our image mixing method with established systems such as CutMix~ and MixUP~. When activated, MixUP executes a weighted pixel-wise summation of the two images,  with the weighting factor,  sampled from the beta distribution . CutMix on the other hand takes a random crop from one of the images and pastes it at the same spatial location on the other image. The crop's dimensions are scaled by the value , . That is, ,  where  and  are the height and width of the image respectively.  %

 Unlike MixUP, our method as depicted in  preserves the integrity of each crop, and does not paste parts of one image on the other as in CutMix. Additionally, using the center-half crop of each image guarantees that substantial portions of both images are represented in the output image. We believe these characteristics of our method are important as demonstrated by its superior zero-shot results over MixUP and CutMix in . 

% Text composition, as described in , is used in these experiments.% % approximates a doubling of the effective batch-size by implementing a shared self-attention between some of the samples such that the actual batch-size remains unchanged.% This raises the question, is CLIP- just CLIP but under a higher batch-size?% We investigate that question in this section by comparing CLIP and CLIP- under different mini-batch sizes% % Due to computing resource constraints, we are unable to extend our current investigations into the billion-scale image-text datasets or to substantially large models. However,%  Since our inputs are of different modalities, visual and textual, it is important to examine whether compositions in each of these modalities produce similar effects. To that end, in , we conduct analysis where our method is applied on (1) only the captions, (2) only the images, and (3) both the captions and images. Of these three variations, executing the compositions on both the captions and images is the most effective, probably due to the symmetry of transforming both modalities. The second most effective is the captions-only approach. Option (2) is the least effective method likely because the images are naturally augmented (random cropping) in the baseline method whereas the captions are fixed.  These observations suggest that our method is more helpful in learning representations of the texts relative to those of the images. They also help elucidate why we obtain much bigger improvements over the baseline in zero-shot settings compared to linear evaluations. 

In CLIP-, the second example is chosen randomly from the dataset whenever the composition is active. In this section, we explore other ways of choosing that second image-caption pair. The configurations studied here include selecting a second instance whose caption is (1) the closest to, or (2) the farthest from the first caption of the anchor, with distance measured by the pair-wise cosine similarity between features obtained from a sentence embedding model~. We also consider the scenario where examples are paired randomly. However, the pairings are fixed once generated at the beginning of training. (CLIP- uses random assignments that change in every epoch.) 

Since computing the embeddings of all captions as well as the pair-wise cosine similarities of all examples is extraordinarily expensive computationally,  we randomly selected a subset of  examples from CC3M for these studies. We then computed the normalized features for all captions using the All-MiniLM-L6-v2 pretrained model from HuggingFace~. Afterward, we generated the  pair-wise cosine similarities for this experiment. We also run CLIP, our method as discussed in the paper, and the fixed random assigned described above on this subset. 

In zero-shot evaluations in this setting, we do not observe any significant differences between pairing examples whose captions are close or far in the embedding space. More importantly, CLIP- beats all other setups on a plurality of the downstream benchmarks including on ImageNet. This performance, along with its simplicity, efficiency, and scalability (since there is no need for pair-wise comparisons of examples) all further justify the adoption of dynamic random sampling in our method.  

As in  where we looked at different ways of composing the merged image, we study other ways of merging the captions in this section. In CLIP-, we adopt the very simple and highly flexible process of concatenating the two captions using ``and'' as a conjunction. Here, we compare that method against omitting the conjunction or rewriting the merged caption with a large language model (LLM). 

Similarly to the ablation in , we randomly selected  examples from CC3M for this ablation because of the costs involved in the LLM rewriting method. Each example in the subset is associated with a second example (drawn from the full dataset) at random. We generate these associations once and use them for all ablation methods in this section: when (1) concatenating , (2) concatenating , and (3) rewriting the output of (1) with an LLM. 

We employ the open-source 70B parameter model LLama2-chat~ for the LLM rewriting method. The temperature and top-p arguments of the model are set to  and , respectively. The maximum sequence length is set to 256. To rewrite a given caption, we (a) instruct the model to act as a text completion assistant, and (b) prompt the model as:  where merged caption is from option (1) above.  We use the first generated sentence as the new caption. The LLM-based captions are generated offline and saved to file before training begins. We emphasize that only  rewrites are undertaken, one for each entry in the sampled subset. Some examples of these rewritten captions are provided in . 

On zero-shot classification in this limited setting, our simple concatenation method with and without ``and'' as a conjunction, each obtained the highest accuracy on  out of  downstream datasets while the LLM rewrite option achieved the highest performance on the remaining  datasets as shown in . These results show that besides being highly efficient and flexible, our method is also comparable with the more resource-intensive large language model rewriting method. Additionally, while the LLM rewrite method may hallucinate non-grounded captions (See ), our method is devoid of such hallucinations. We use concatenation with the conjunction as the default caption composition method because of its coherence.

% [t]%     : Zero-shot results on all datasets for the ablations in ~ and . ViT-S/16 is the vision encoder. All models are are trained for 40 epochs on CC3M. Every CLIP- uses }%     %     \centering%     % % [!ht]%     : Retrieval results for the models in .}%           % * denotes the case where the image and text are added before being normalized.%      %      \centering%      %  % %     .  Retrieval results of all models in ~ in the main paper.}%      %      \centering%      %  Semantic Compositions Enhance Vision-Language Contrastive LearningCLIP-: CLIP CompositionsMaxwell Mbabilla Aladago\inst \and Lorenzo Torresani\inst \and Soroush Vosoughi\instM.~Aladago et al.Dartmouth College, Hanover NH 03755, USA \and Meta, FAIR\\ \email   In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non-matching pairs as negatives. This approach has led to remarkable outcomes in zero-shot image classification, cross-modal retrieval, and linear evaluation tasks. We show that the zero-shot classification and retrieval capabilities of CLIP-like models can be improved significantly through the introduction of semantically composite examples during pretraining. Inspired by CutMix in vision categorization, we create semantically composite image-caption pairs by merging elements from two distinct instances in the dataset via a novel procedure. Our method fuses the captions and blends 50\% of each image to form a new composite sample. This simple technique (termed CLIP- for CLIP Compositions), devoid of any additional computational overhead or increase in model parameters, significantly improves zero-shot image classification and cross-modal retrieval. The benefits of CLIP- are particularly pronounced in settings with relatively limited pretraining data.   multimodal learning\and semantic composition \and pretrainingIntroductionsec:introradford2021learning, JiaAlign2021, declipxu-etal-2021-videoclip, zhao2022lavilaMaMMUT, yu2022coca, naeem2023silcalexnet, resnetbert, gpt1, radford2019language, gpt3radford2021learningsimclr, SimSiam2021, mocooord2019representationmu2021slip, declipdeclipfan2023improving, lai2023scarcity, zhao2022lavilafig:main-methodyun2019cutmixfig:per-epoch-zero-shot-accfan2023improvingflickr30ms-cocoimagenetcc12mdesai2021redcapsRelated Workssec:related-worksQuattoni2007, devise2013, Joulin2016, Li2017, radford2021learningdevise2013radford2021learningJiaAlign2021mu2021slip, declip, xu2023metaclip, openclipoord2019representationradford2021learningJiaAlign2021declipSimSiam2021, bertmultiviewmu2021slipsimclrOttermu2021slip, declipdeclipyun2019cutmixzhang2018mixupjiang2023comclip, ITMix, lit2022, li2023blip2, yuksekgonul2023when% width=0.9\textwidthimages/main-diagram-v2.pdf\textbf:  We use the center half crops spanning the width (as in this illustration) or the height of the image. The captions are concatenated with the delimiter ``and''. We vary the positions of the captions on either side of the conjunction, \ie, the output caption can be either (a)  or (b) . We emphasize that only a fraction of the batch in each iteration constitute composite samples. The colored boxes and texts shown here are for illustrative purposes.fig:main-methodMethodsec:method-mainBackgroundradford2021learningoord2019representation     _{I_2T} &= -{B}\sum_{i = 1}^B\log{\tau}\left(z_I^{(i)}, z_T^{(i)}\right)\right)}{\sum_{j=1}^B\exp\left({\tau}\left(z_I^{(i)}, z_T^{(j)}\right)\right)}

    _{T_2I} &= -{B}\sum_{i = 1}^B\log{\tau}\left(z_I^{(i)}, z_T^{(i)}\right)\right)}{\sum_{k=1}^B\exp\left({\tau}\left(z_I^{(k)}, z_T^{(i)}\right)\right)} mindgapradford2021learningimage-to-text-losstext-to-image-loss      = (_{I_2T} + _{T_2I})/2 \ .   CLIP-\texorpdfstringsec:method-ourssubsec:composition-ratiofig:main-methodzhang2018mixupyun2019cutmixtab:zero-shot-image-mixupimage-to-text-losstext-to-image-lossfinal-lossdesai2021redcapsExperimental Setupsec: experimental-setup-mainPretraining Datasets. cc3mcc12mdesai2021redcapsModels. dosovitskiy2021anmu2021slipdeitdosovitskiy2021an, deitradford2021learningradford2021learningHyper-parameters.pytorchadamwEvaluationradford2021learning, mu2021slipimagenetcifarcifarcaltech101oxfordpetsradford2021learningdtdsun397coates2011stl10resisc45helber2017eurosatmu2021slip, fan2023improvingsec:retrievalsec:linear-proberesisc45helber2017eurosatResultsWhy is CLIP-\texorpdfstring an Effective Method?sec:effectivenessfig:per-epoch-lossesfig:per-epoch-cssec:ablations-main\textbf: CLIP- is our method. CLIP is a the model from~\cite trained in our setting. CC3M CLIP- models use  while CC12M and RedCaps models use .  Bold numbers are the best in each dataset and architecture comparison. tab:zero-shot-results-mainZero-shot Image Classificationsec: zero-shot-resultsimagenettab:zero-shot-results-mainZero-shot Cross-Modal Retrievalsec:retrievalsec: zero-shot-resultstab:retrieval-results-mainms-cocoflickr30Linear Probe Evaluationssec:linear-probetab:linear-probe-results-maintab:linear-probe-hyper-parameters-supAblationssec:ablations-mainsec:additional-ablations-supsubsec:semantic-vrs-stylisticsubsec:composition-ratiosubsec:stochasticity-vrs-fixed-main\intextsep0ptBoth CLIP and CLIP- are consistent across the three initializations. 0.1cmtab:means-standard-deviationtab:means-standard-deviationconsistentIs CLIP-\texorpdfstring a CLIP Model Exposed to More Data?sec:resource-exposuretab:zero-shot-batch-size-ablationfig:per-epoch-zero-shot-accfig:per-epoch-zero-shot-accfig:per-epoch-zero-shot-accstrong regularization effectmore superior to CLIP as training duration increases, extending the improvement from  zero-short accuracy on ImageNet when both models are trained for 40 epochs to over  when both are trained for 52 epochs.sec:effectivenessSampling Probability subsec:composition-ratio-0.09cmfig:sampling-probabilityWhy Semantic Compositions?subsec:semantic-vrs-stylisticsec:method-ourswei-zou-2019-eda\intextsep0pt\textbf: Using semantically distinct examples is better than stylistic augmentations. tab:zero-shot-stylistic0.2cm0.2cmtab:zero-shot-stylisticImpact of Stochasticity During Samplingsubsec:stochasticity-vrs-fixed-main% tab:zero-shot-stochasticitysec:sentece-embedding-supwang2020minilmImage Composition Functionsec:image-composition-mainyun2019cutmixzhang2018mixup\intextsep0pt\textbf: Our strategy outperforms CutMix and MixUP.  tab:zero-shot-image-mixup0.3cm0.1cmfig:main-methodtab:zero-shot-image-mixupConclusionsplncs04mainpage1section0\Alph\textbf: We use stochastic gradient descent optimizer with a decay of 0 and a momentum of 0.9 for all linear probe experiments. All linear probe experiments are tuned for 50 epochs. tab:linear-probe-hyper-parameters-supImplementation Detailssec:hyper-parameters-supsec: experimental-setup-mainadamwimagenettab:linear-probe-hyper-parameters-suppytorchDatasetssec:datasets-suptab:pre-trainingtab:downstream-datasets-supcc3mcc12mresisc45https://www.tensorflow.org/datasets/catalog/overviewhttps://pytorch.org/vision/stable/datasets.htmlAdditional Ablationssec:additional-ablations-supImpact of Modality Used in Compositionsec:modality-composition-suptab:zero-shot-modality-ablation\textbf: For each row, we pretrain a ViT-S/16 model on the 100k CC3M subset explained in \cref. \textbf stands for Largest Caption-Similarity while \textbf represents Smallest Caption-Similarity. \textbf is the case where we perform random assignments at the beginning and fix them for the rest of the training. We pretrain for 40 epochs using a reduced learning rate of , a batch-size of , and a warm-up period to  epochs. tab:zero-shot-other-pairings-supOther Ways of Pairing Examplessec:sentece-embedding-supwang2020minilmhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2wolf-etal-2020-transformers\textbf: For each row, we pretrain a ViT-S/16 model on the 100k CC3M subset detailed in \cref. CLIP- (default) represents the scenario where the captions are concatenated with ``and'' as a conjunction. CLIP- denotes the case where we omit the ``and''. Finally, CLIP- is the language model rewriting method.  We pretrain for 40 epochs using a reduced learning rate of , a batch-size of , and a warm-up period to  epochs. tab:text-composition-methodDifferent Methods of Merging Captionssec: caption-composition-supsec:image-composition-mainsec:sentece-embedding-supwith ``and''without ``and''touvron2023llamarephrase the sentence \{merged caption\}fig:example-captionstab:text-composition-methodfig:example-captionswidth=\linewidthimages/example-captions.pdfSample of captions generated using the LLM-based method and our CLIP- procedure.fig:example-captions