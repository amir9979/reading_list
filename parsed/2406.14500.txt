For our baseline approach, we adopt a prefixed zero-shot prompting strategy~, which prepended a brief instruction to the beginning of a standard null prompt. We use the instruction, ``You are an expert chest radiologist. Your task is to summarize the radiology report findings into an impression with minimal text''. This instruction provides the model with a fundamental context for the RRS task. Immediately following the instruction, we append the specific findings from the report and then prompt the model with ``IMPRESSION:'' to initiate the generation process. Additionally, we investigate the effectiveness of few-shot ICL prompts with up to 32 similar examples, using the same template as our Few-Shot prompting method, which is not incorporating the intermediate reasoning step (i.e., without the layperson summary). 

% In our experiments, we use two open-source state-of-the-art pre-trained large language models:% These models were state-of-the-art at the time of our experiments.  To select the best parameters in our study, we employed ROUGE-L and F1RadGraph metrics on the validation set. These metrics help determine the most effective parameter settings for the model. The ROUGE-L metric focuses on the longest common subsequence and is particularly suitable for evaluating the quality of text summaries. On the other hand, the F1RadGraph is specifically designed to assess the accuracy of extracting and summarizing key information from radiology reports by analyzing entity similarities. 

For optimizing our model's hyper-parameters, we employed a random search strategy. This involved experimenting with various settings: the number of prepended similar examples was varied across a set , and these examples were matched using different modality embeddings (text, image, or multimodal), all while employing the same template. We find that for the OpenChat-3.5-7B model and Meta-Llama-3-8B-Instruct, the best performance is achieved with 32 examples for . In contrast, the Starling-LM-7B model exhibits optimal performance with 32 examples when using the  prompt and 24 examples for the . Additionally, we experimented with temperature settings ranging from 0.1 to 0.9, top p values set between 0.1 and 0.6, and top k values of 10, 20, and 30. Through this exploratory process, we identified the most effective settings as a temperature of 0.2, a top p value of 0.5, and a top k setting of 20. We adopt the same hyperparameters for all experiments. These settings yielded the best results in our evaluations. It's significant to note the impact of the ``temperature'' parameter on the diversity of the model's outputs. Higher temperature values add more variation, introducing a greater level of randomness into the content generated. This aspect is especially valuable for adjusting the output to meet specific requirements for creativity or diversity.

To ensure compatibility with the model's capabilities, we restricted the length of the prompt (which includes the instruction, input, and output instance) to 7800 tokens. This limit was set to prevent exceeding the model's maximum sequence length of 8,192 tokens for OpenChat-3.5-7B, Starling-LM-7B model and Meta-Llama-3-8B-Instruct.  In cases where prompts exceeded this length, they were truncated from the beginning, ensuring that essential information and current findings were preserved. Moreover, we constrained the generated output to a maximum of 256 tokens to strike a balance between providing detailed content and adhering to the model's constraints. This approach was key in optimizing the effectiveness of summarization within the operational limits of the 7B models.

% ROUGE-L evaluates the longest common subsequence overlap and F1-RadGraph, a F-score style metric that measures the factual correctness, consistency and completeness of generated radiology reports compared to the reference.%, llama2-7b~, and GPT-NEO~ A natural question that arises is, ``Does integrating a larger number of examples in Few-Shot + Layperson prompting lead to better overall performance?''. To answer this question, we explore the relationship between performance and the number of examples integrated.  To better quantify the contributions of different components in our model, we conducted ablation studies focusing on various prompt types and modality embeddings for the radiology reports summarization task. Using the MIMIC-CXR validation dataset, we evaluated the performance of three models, OpenChat-3.5-7B, Starling-LM-7B, and Meta-Llama-3-8B-Instruc across a range of configurations. Our analysis focuses on understanding the effectiveness of embedding matches for different modalities (including image, text, and multimodal), as well as determining the optimal number of examples needed for effective summarization. The results of these ablations on the MIMIC-CXR validation set are presented in Figure~, Figure~, and Figure~. Specifically, we note that Few-Shot + Layperson prompting with multimodal embedding matched examples slightly outperforms the image and text embedding matched ones. For all OpenChat-3.5-7B, Starling-LM-7B, and Meta-Llama-3-8B-Instruc employing the LaypersonPrompt demonstrates performance enhancements compared to the original prompt.  

Furthermore, as we increase the number of examples, the performance continues to rise, which demonstrates that prompting the model with more in-context examples improves performance.  However, we can also observe a slight performance decrease for in some cases after reaching 24 examples. These findings suggest that while multimodal embeddings provide a robust framework for summarization, there is a complex relationship between the number of examples and performance gains. Our studies highlight the importance of multimodal context and suggest a diminishing return for additional examples in text and image modalities beyond a certain point. This insight is critical for optimizing the efficiency and accuracy of our summarization model when processing radiology data.

% In Table~, we can see the prompt length based on the different example numbers. We investigate the effect of prompt length. LLaMA-2-7B's maximum context length was 4,096 tokens and GPT-NEO's maximum context length was 2048 tokens, and Openchat and Starling's maximum context length were 8192. Due to the context length limitation, LLaMA-2-7B and GPT-NEO model are not perferm very well on the RRS task. We observed variations in performance among different language models. We found OpenChat and Starling are outperform than the Llama2 and GPT-Neo which maximum token context is much smaller than previous two models. 

Table~ shows the prompt lengths corresponding to various numbers of examples used in our study. We aim to explore how the length of prompts affects model performance. Initially, we explored LLaMA-2-7B and GPT-Neo-2.7B. However, given that LLaMA-2-7B and LLaMA-2-13B has a maximum context length of 4,096 tokens and GPT-Neo-2.7B is restricted to 2,048 tokens, such constraints on context length impact the performance of LLaMA-2-7B, LLaMA-2-13B and GPT-Neo-2.7B in the radiology reports summarization task compared to models capable of processing longer contexts like OpenChat-3.5-7B and Starling-LM-7B (up to 8,192 tokens). Specifically, these constraints significantly affect LLaMA-2-7B, LLaMA-2-13B and GPT-Neo-2.7B's ability to conduct in-context learning for summarizing radiology reports. The restricted context length can hinder these models from fully taking advantage of the extensive information required for accurate summarization in this domain. % radiology report summarization and for enhancing the overall performance of the models in this specific field.% %Our Fine-Tuning   &  & 25.87  & 47.86   & 64.74   & 77.93   &  51.84   \\ %\midrule% & LLaMA-2-7B & 3.89 & 21.20 & 38.75 & 65.81 & 19.41\\% & LLaMA-2-13B & 3.89 & 21.21 & 38.75 & 63.22 & 19.41  \\% & GPT-Neo-2.7B & 2.25 & 10.58 & 16.65 & 44.64 & 12.40\\% & LLaMA-2-7B & 3.67 & 16.12 & 35.48 & 64.61 & 20.87\\% & LLaMA-2-13B & 4.43 & 20.56 & 39.07 & 65.69 & 21.08 \\% & GPT-Neo-2.7B & 1.02 & 8.22 & 26.39 & 12.02 & 6.76 \\% & LLaMA-2-7B & 1.40 & 21.06 & 41.12 & 66.49 & 9.40 \\% & LLaMA-2-13B & 1.40 & 21.08 & 41.12 & 66.49 & 9.4 \\% & GPT-Neo-2.7B & 2.57 & 16.01 & 19.74 & 44.90 & 8.64\\% & LLaMA-2-7B & 3.27 & 21.00 & 40.85 & 69.29 & 11.87 \\% & LLaMA-2-13B & 4.38 & 24.22 & 43.42 & 69.37 & 12.47 \\% & GPT-Neo-2.7B & 0.46 & 9.26 & 27.98 & 14.79 & 2.46 \\%Our Fine-Tuning   &  & 16.05  & 34.41   & 57.08   &    & 36.31  \\ %\midrule% & LLaMA-2-7B & 4.31 & 19.74 & 41.49 & 50.63 & 21.91 \\% & LLaMA-2-13B & 4.69 & 19.59 & 41.90 & 49.77 & 20.38 \\% & GPT-Neo-2.7B & 4.33 & 13.60 & 18.07 & 35.03 & 15.40\\% & LLaMA-2-7B & 5.58 & 18.14 & 39.75 & 48.89 & 22.88 \\% & LLaMA-2-13B & 4.96 & 20.49 & 42.83 & 55.57 & 17.89 \\% & GPT-Neo-2.7B & .81 & 7.18 & 12.56 & 6.90 & 2.93 \\ Radiology report summarization (RRS) is crucial for patient care, requiring concise ``Impressions'' from detailed ``Findings.'' This paper introduces a novel prompting strategy to enhance RRS by first generating a layperson summary. This approach normalizes key observations and simplifies complex information using non-expert communication techniques inspired by doctor-patient interactions. Combined with few-shot in-context learning, this method improves the model's ability to link general terms to specific findings. We evaluate this approach on the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against 7B/8B parameter state-of-the-art open-source large language models (LLMs) like Meta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization accuracy and accessibility, particularly in out-of-domain tests, with improvements as high as 5\% for some metrics.

% \noindent  {This paper introduces a novel prompting approach that generalizes a simplified (layperson) summary before an expert summary, enhancing radiology report summarization with Large Language Models (LLMs) by combining this strategy with few-shot in-context learning. The core objective of this approach is to normalize key observations in findings using the layperson summary, enabling the identification of patterns linking simplified summaries to detailed expert impressions. This normalization process aids in connecting general terms to specific findings, facilitating an easy-to-hard progression in reasoning and guiding LLMs to progress from generating simple layperson summaries to composing complex expert-level summaries; this method improves LLM's reasoning ability.}% Radiology reports summarization (RRS) is crucial for patient care, requiring concise ``Impressions'' from detailed ``Findings.'' This paper introduces a novel prompting strategy to enhance RRS by first generating a layperson summary. This approach normalizes key observations and simplifies complex information using non-expert communication techniques inspired by doctor-patient interactions. Combined with few-shot in-context learning, this method improves the model's ability to link general terms to specific findings. We evaluate this approach on MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against 7B parameter state-of-the-art open-source Large Language Models (LLMs) like Meta-Llama-3-8B-Instruct, demonstrating improvements in summarization accuracy and accessibility.% % \noindent  Our prompting strategy was applied to 7B parameter state-of-the-art open-source LLMs, such as OpenChat-3.5-7B and Starling-LM-7B, tested on three radiology report datasets: MIMIC-CXR, CheXpert, and MIMIC-III. This method first translates radiology findings into layperson summaries and then uses these as intermediate steps to generate expert impressions.% % \noindent  Our prompting strategy outperformed traditional zero-shot and few-shot prompting methods in RRS. Across various datasets, it consistently surpassed baseline techniques, as notable improvements in ROUGE-L and F1-RadGraph scores indicated. It is particularly effective in summarizing both short and longer radiology impressions.% % \noindent  {The major finding of this study is that generating a layperson summary before the expert summary improves the expert summarization quality with LLMs. Future work will focus on enhancing model efficiency and the use of larger models for better performance. We also plan to integrate images directly into prompts for more accurate summarization.} Introductionvan2023radadaptsinghal2023largevan2023radadapt, van2023clinicalliu2022fewbrown2020language, dong2022surveylampinen2022canwang2023utsachen2022towardnori2023canzhang2022automaticyao2023knowledge, holmes2023evaluatingbrown2020language, dong2022surveygulich2003conversationalnori2023can, zhang2022automaticnormalizeyan2023stylepeter2024simplicitywidth=.75\linewidth./image/layperson-pipeline6.pdfOverview of the LaypersonPrompt Framework. First, we generate layperson summaries from the training corpus using LLMs prompting. Then, for a test input, we use multimodal retrieval to find relevant examples. Finally, we incorporate these layperson summaries into the prompt, applying patient-doctor communication techniques to improve the model's reasoning.fig:layperson-pipeline-1em-0.5em%[label=\bfseries (\arabic*.)]We introduce a novel prompting approach inspired by doctor-patient communication techniques that generate a simplified (layperson) summary before the expert summary. This strategy, combines with a few-shot ICL with the layperson summary, enhances RRS using non-expert LLMs. % We combine this strategy with few-shot ICL to improve radiology report summarization using non-expert LLMs.}

-0.5emWe evaluate LLM performance on three RRS datasets: MIMIC-CXR~, CheXpert~, and MIMIC-III~, and benchmark against open-source LLMs like Meta-Llama-3-8B-Instruct~ for comprehensive comparison. % We evaluate the performance of LLMs on three radiology report summarization datasets: MIMIC-CXR~, CheXpert~ and MIMIC-III~. {We also benchmark our results against well-known open-source LLMs, such as Meta-Llama-3-8B-Instruct~, to provide a comprehensive comparison.} % with 7B or fewer parameters  OpenChat-3.5-7B~, Starling-LM-7B~, Meta-Llama-3-8B-Instruct~, LLaMA-2-7B, LLaMA-2-13B~, and GPT-Neo-2.7B~,

johnson2019mimicirvin2019chexpertjohnson2016mimicllama3modelcard-0.5emWe conduct a comprehensive analysis to determine the optimal modality for ICL. We also examine the required number of examples and the impact of layperson summaries on impressions and evaluate model performance on inputs of different lengths. % We perform a comprehensive analysis of the optimal modality for ICL (i.e., should we use radiology images to find in-context examples or text alone), the required number of examples, and the impact of layperson summaries on impressions. Additionally, we assess model performance on inputs of varying lengths.

See the appendix for complete analysis.Related WorkLLMs for Medicine.singhal2023largesarkar2024identificationhernandez2023weluo2022biogptlu2022clinicalt5ouyang2022trainingagrawal-etal-2022-largeotmakhova-etal-2022-m3hu2023zero2mmRetrieval-Augmented LLMs.ma-etal-2023-queryshi2023replug,yao2023react,nori2023can,ma-etal-2023-queryshi2023replugyao2023react1.5mmCommunication Techniques for Laypersons.gulich2003conversational, leblanc2014patient, allen2023jargon, van2007patient, neiman2017cdco2017crowdsourcing, vanhoudnos2017malware, snow2008cheapjeblick2023chatgpt, lyu2023translating, li2023decodingMethodologyblackIn this section, we describe our prompting strategy. Figure~\ref shows a high-level overview of our approach. Our strategy has three main components: 1) layperson summarization of the training dataset used as in-context examples; 2) ``multimodal demonstration retrieval,'' which is how we generate embeddings to find relevant in-context examples; and 3) final expert summary prompt construction, which is how we integrate the layperson summaries and in-context examples to generate the final expert summary. We describe each component in the following subsections and how the three components are integrated into a unified prompt.2mmStep 1: Layperson Summarization of the Training Dataset.blackLayperson summarization involves converting complex medical texts into more straightforward language, enhancing accessibility and understanding for individuals without medical expertise~\cite. For instance, rephrasing ``pulmonary edema'' as ``fluid in the lungs'' makes it more comprehensible. This approach not only helps to bridge the knowledge gap for laypeople but also plays an important role in helping models better understand and summarize medical content. Intuitively, by generating simplified summaries as an intermediate step, models can more effectively capture the semantic meaning of the texts~\cite. In this context, we generate layperson summaries as an intermediate step for all training examples to enhance the generation of expert summaries.blackTo generate accurate layperson summaries, we use a zero-shot prompting strategy enhanced with metadata from an external tool. Specifically, we employ the CheXbert labeler~\cite to extract important medical observations from radiology impressions (e.g., ``No Finding'', ``Pneumonia'', ``Cardiomegaly'', etc.). Using CheXbert's output, we create additional instructions for the language model that include these key concepts. The exact form of the prompt is shown in Figure~\ref. This prompt integrates the Task Instruction, Findings, Impression, and the extracted CheXbert concepts. We then use this prompt to generate layperson summaries and store these summaries along with their corresponding Findings and Impressions as training triples, which are used as in-context examples.2mmStep 2: Multimodal Demonstration Retrievalwang2023utsadosovitskiy2020imagelehmanclinical\linewidth! Overall performance across the four prompts on the MIMIC CXR in-domain test.tab:test-results\linewidth! Overall performance across the four prompts on the Stanford Hospital (out-of-domain) test set. The in-context examples for this dataset are from the MIMIC-CXR dataset.-1emtab:hidden-results\linewidth! Overall performance across the four prompts on the MIMIC III.-1emtab:mimiciii-results2mmStep 3: Expert Summary Prompt Constructionfig:layperson-promptExperimental Results1mmDatasetsjohnson2016mimic, chen-etal-2023-towardjohnson2019mimicirvin2019chexpertwang2023openchatstarling2023llama3modelcard1mmEvaluation Metrics.papineni2002bleulin2004rougeZhang2020BERTScoredelbrouck-etal-2022-vilmedicdelbrouck-etal-2022-improvingsmit-etal-2020-combiningjain2021radgraph2mmOverall Results.tab:test-resultstab:hidden-resultstab:mimiciii-results2mmError Analysis.fig:error-.25em     colframe=black,     colback=white,     boxrule=0.5mm,     arc=3mm,     width=1\linewidth,     boxsep=5pt,     left=5pt,     right=5pt,     top=5pt,     bottom=5pt % : ``Lungs are clear without consolidation, effusion, or pneumothorax. The cardiomediastinal silhouette is within normal limits. No displaced fractures.''\\[.25em]: ``No acute cardiopulmonary process.''\\: ``The lungs look normal with no issues like infection or fluid build-up. The heart and nearby structures are normal, and there are no broken bones.''\\: ``No acute thoracic pathology.''\\: ``No acute cardiopulmonary process.'' Ground Truth Short ImpressionLayperson SummaryFew-ShotFew-Shot + Layperson (Our)-.25em-.25em     colframe=black,     colback=white,     boxrule=0.5mm,     arc=3mm,     width=1\linewidth,     boxsep=5pt,     left=5pt,     right=5pt,     top=5pt,     bottom=5pt % : ``Frontal and lateral views of the chest were obtained. New subtle opacity at the right lung base in the setting of similar lung volumes with increased opacity on the lateral view may be atelectasis, but could represent early or developing pneumonia in the appropriate clinical setting. Cardiac and mediastinal silhouettes are normal. No acute osseous abnormality is identified.''\\[.25em]: ``Right basilar opacity is probably atelectasis, but could represent early or developing pneumonia in the appropriate clinical setting.''\\: ``The right lower part of the lung has a subtle cloudy area, which might be a small lung collapse but could also be early or developing pneumonia, depending on the patient's condition.''\\: ``Normal cardiomegaly with subtle lung base opacity, possibly atelectasis or early pneumonia.''\\: ``1. Subtle opacity at the right lung base, possibly representing atelectasis, but could be early or developing pneumonia in the appropriate clinical setting.'' Ground Truth Long ImpressionLayperson SummaryFew-ShotFew-Shot + Layperson (Our)-.25emConclusionLimitationwang2024llmachiam2023gptteam2023geminiEthics Statementyildirim2024multimodalcustomAppendixsec:appendixBaseline and Implementation Detailsduan2019zero,zhao2021discreteblackWe conduct experiments with six open-source LLMs: OpenChat-3.5-7B~\cite, Starling-LM-7B~\cite,  Meta-Llama-3-8B-Instruct~\cite, LLaMA-2-7B~\cite,  LLaMA-2-13B~\cite, and GPT-Neo-2.7B~\cite. All experiments were conducted using two Nvidia A6000 GPUs. For the few-shot model, the average running time is around 2 hours. In contrast, the Few-Shot + Layperson models have an average running time of around 8 hours. Processing the MIMIC data with 24 examples takes approximately 36 hours. In our work, all of these models have been implemented using the Hugging Face framework~\cite. Specifically, for the OpenChat-3.5-7B, Starling-LM-7B, and Meta-Llama-3-8B-Instruct are reported to have strong performance in common sense reasoning and problem-solving ability~\cite. OpenChat-3.5-7B is built on the Mistral 7B with conditioned reinforcement learning fine-tuning, and Starling-LM-7B is built on OpenChat-3.5-7B with reinforcement learning from AI feedback. Moreover, OpenChat-3.5-7B, Starling-LM-7B and Meta-Llama-3-8B-Instruct had sufficient token maximums (8,192) compared to other 7B models (e.g., LLaMA-2-7B and  LLaMA-2-13B only has 4096 maximum tokens, GPT-Neo-2.7B has 2048 maximum tokens).blackboth Few-Shot and Few-Shot + Layperson prompting methodsblackFew-ShotblackFew-Shot + Layperson promptblackFor LLaMA-2-7B and LLaMA-2-13B models, we constrain the prompt length to 3800 tokens, and for GPT-Neo-2.7B, it is set to 1700 tokens.Discussion and Model Analysiswidth=\textwidthimage/openchat_result.pdf-0.5emValidation results vs. the number of in-context examples across various prompt types and modality embeddings on OpenChat-3.5-7B. \label%\columnwidthwidth=\textwidthimage/starling_result.pdf-0.5emValidation results vs. the number of in-context examples across various prompt types and modality embeddings on Starling-LM-7B. \label%\columnwidthwidth=\textwidthimage/llama_result.pdf-0.5emValidation results vs. the number of in-context examples across various prompt types and modality embeddings on Meta-Llama-3-8B-Instruct. \labelfig:openchat-plotfig:starling-plotfig:llama-plot0.8\linewidth! Average Token of Prompts.tab:tokenstab:tokensblackTherefore, LLaMA-2-7B, LLaMA-2-13B and GPT-Neo-2.7B do not perform very well, which may be due to limitations in their reasoning capabilities and the constrained number of examples in few-shot learning scenarios, restricted by the maximum token count. This means that even if 16 examples are provided, the models may truncate the initial examples to stay within the token limit, potentially losing valuable context.