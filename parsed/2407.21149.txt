is a publicly available dataset of 377,110 chest X-rays associated with 227,827 imaging studies from 65,379 patients. MIMIC-CXR is collected from the inpatient setting of Beth Israel Deaconess, a Boston hospital. (refer Table~) We also use the test-split of this dataset (Test-split MIMIC-CXR) as the hold-out set analysis, which consists of 5159 studies with 293 patients . 

 is a private dataset of 259,361 chest X-rays associated with 91,020 imaging studies from 35,771 patients. VA-CXR is collected in the outpatient setting of the Boston Veterans Healthcare Administration station (refer to Table~). The ground truth labels were extracted from the VA's corporate data warehouse (CDW) and joined with images using patient information from DICOM headers as published in Knight et al. .   

We used the CheXbert labelers  to expertly assign labels to 14 specific labels (Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, Support Devices) associated with different chest conditions from radiology reports. CheXbert is a BERT-based approach that automates the detection of these observations, effectively streamlining the process of annotating medical images and reports. The NLP label extraction outputs scores for four classes: positive, negative, blank, and uncertain, associated with each of the 14 labels. As the class names indicate, for example, for , positive class: radiology report indicates that the patient has ; negative class: radiology report indicates that the patient doesn't have  ; uncertain class: radiology report mentions   but NLP tool unable to determine if it is positive or negative; blank class: radiology report doesn't mention  .

Because ground truth assignment ultimately determines the accuracy of the imaging classifiers, we developed an image evaluation procedure in two steps. First, we evaluated the agreement between the NLP label extraction tool, CheXbert, and its precursor, CheXpert , a rule-based tool. We focused on positive class agreements for our evaluation, using only positive classes for classification, and have combined uncertain/negative class agreements. The disagreement between NLP label extraction tools indicates ambiguity/ less confidence on the labels, ultimately creating an unreliable ground truth. The agreement is measured for both MIMIC-CXR and VA-CXR datasets; Of note that neither of the datasets was used to train the NLP extraction tools. 

To validate the ground extracted from chexpert-labeler, we analyzed the relationship of specific ground truth labels to ICD codes in the patient's electronic health record (EHR) extracted from the VA's Corporate Data Warehouse (CDW).The assignment of ICD-9 and ICD-10 diagnosis codes associated with each condition was exploratory and not extensively optimized. 

Starting concepts were retrieved from the Chexpert-labeler github repository , where phrases they used to search notes can be found at .  These phrases, along with our own expertise, were used to identify diagnosis codes and cross-reference radiology reports with diagnoses.For example:   Pneumonia was identified by chexpert-labeler as indicated by , , , and ;  Edema was indicated by terms , , , , , , and ; Fracture was indicated solely by the word ; and Pneumothorax was identified by either  or .

This method enabled us to validate the ground truth labels by correlating them with the relevant ICD codes in the patients' EHRs, ensuring accurate cross-referencing of radiology reports with diagnoses.

For instance, the ICD-9 codes we used to indicate a pneumonia diagnosis in the outpatient diagnosis tables ranged from 480 to 486 and included 487.0. These codes encompass viral, bacterial, and other types of pneumonia, as well as pneumonia caused by unspecified pathogens. A similar approach was applied to ICD-10 codes for pneumonia and other conditions. The specific ICD codes used for each condition are detailed in Appendix Table~.

Using the 14 labels extracted from the corresponding radiology reports with CheXbert, we created a multi-label image classification model from X-ray images. We used a pre-trained DenseNet model as the core framework, removed the top classification layer, and integrated a custom classification layer for multi-label output.  Previous work shows the effectiveness of different resolutions of DenseNet-121 based multi-label classification chest X-ray model on MIMIC-CXR dataset  . This work uses the MIMIC-CXR trained DenseNet-121 Model on Chest X-ray pre-processed into 256x256 JPG images . 

We evaluated our models using the Area Under the Curve (AUC). The AUC score was calculated separately for each of the 14 labels, indicating the separability measure for a given chest X-ray label.  We also analyzed the difference in AUC scores between MIMIC-CXR and VA-CXR, and the prevalence for each label was calculated as the number of studies with positive results for a given label divided by the total number of labels, indicating the label's presence in the given cohort. 

We compared our source and target datasets along different dimensions: 1) Demographic Details: Age At Time of Imaging Study, Sex; 2) Imaging Study Details: Study Year, View  Point (Lateral view (Lat), erect anteroposterior (AP), posteroanterior (PA)) ; 3) Ground truth Labels: 14 labels. All the factors were analyzed against the accuracy of multi-label image classification.  The impact can be estimated by comparing the prevalence of the above-listed factors across source and target domains with the performance of the multi-label classification.  %  The subgroup analysis is performed on unseen datasets: Test Split MIMIC-CXR and VA-CXR. 

: We obtain the study year from VA-CXR studies date reported in DICOM headers.  The MIMIC-CXR dataset is unidentified; study years are replaced by anchor years. The comparison of performance across study years between the datasets isn't possible, so we don't present a study year analysis on the MIMIC-CXR dataset. 

: We obtain the sex of the patients in VA-CXR from the VA's corporate data warehouse (CDW) and the patients in MIMIC-CXR from the metadata. Sex is only classified into binary classes of male and female. We analyze the label-wise performance for each sex across both datasets. 

: We obtain the view position of the images from the DICOM metadata. We use four main viewpoints for analysis: Lateral view (Lat), erect anteroposterior (AP), posteroanterior (PA))  and Left Lateral view (LL). We compare the label-wise performance across both datasets

: We define the age of the patient as the age when the imaging study was performed. For VA-CXR, this age was calculated from the date of birth obtained from VA CDW and the date of imaging study from DICOM metadata. For MIMIC-CXR, we calculated age using the anchored date of birth and anchored imaging study date. The anchored dates were amended in a manner that the difference in years is constant, so we were able to approximate the age of the patient. The label-wise performance for both datasets was compared across six age groups: (0-50], (50-60], (60,70], (70,80], (80,90], and (90,100]. 

Table~ presents a comprehensive analysis of agreement and disagreement rates between Chexpert and Chexbert on the MIMIC-CXR and VA-CXR datasets across the 14 labels. The results shed light on the performance and consistency of Chexpert and Chexbert and the uncertainty of the ground truth labels. Notably, in MIMIC-CXR, Atelectasis was identified in 19.5\% of cases with a disagreement rate of 10.1\%, whereas in VA-CXR, the identification rate was lower at 9.8\% with a disagreement rate of only 0.7\%. This discrepancy in positive identification and disagreement rates is further exemplified in conditions like Cardiomegaly, where MIMIC-CXR reported positive identification in 15.7\% of cases with a significant disagreement rate of 39.1\%, contrasting with VA-CXR's 9.3\%  positive identification rate and 13.7\% disagreement rate.

While the X-ray classification accuracy was evaluated against the label extracted using NLP of the appropriate radiology report, it is possible also to compare directly against diagnosis codes in the clinical record.  However, interpreting this comparison is challenging due to two main issues: first, diagnoses are based on a broader range of information beyond just the X-ray, and second, patients may have multiple conditions, making it difficult to determine the specific reason each X-ray was ordered. Despite these challenges, we have conducted two useful comparisons of X-ray findings against diagnosis codes for nine categories within our dataset.

The top of Figure  shows the fraction where a diagnosis related to the label was recorded within a week (either before or after) of the X-ray, compared to the total number of patients, including those who never had the diagnosis. Patients who had the diagnosis but not within the one-week window were excluded from this calculation. This plot, labeled 'Sensitivity,' reflects the proportion of cases where a diagnosis occurs within a week of noting the condition in the X-ray.   We observe a positive finding on an X-ray, as extracted from the radiology report, which is mostly associated with a specific diagnosis of pneumonia and edema and frequently associated with pleural effusion and pneumothorax.  We also see that the ChexBert model is more frequently associated with a corresponding diagnosis than the ChexPert model.

The bottom of Figure  compares the factor by which the enrichment ratio of positive finding in the radiology report increases with a concurrent (within one week) diagnosis.  Pleural effusion, for example, is seen 10 times more frequently in the radiology report when a concurrent diagnosis code is noted.  It is also true, however, that 2/3 of the pleural effusion diagnoses are not accompanied by a positive finding in a radiology report, as shown in the top bar chart, and also that 90\% of the time a negative finding is made, no diagnosis is found.  We see that the conditions with the highest enrichment ratios differ from those with the highest sensitivities, but the trend that the ChexBert model generally outperforms the ChexPert model by a few to 20\% remains.

Several factors were observed to contribute to the quantitative comparison of labels extracted from the radiology reports to the observed diagnoses, including the extent to which diagnosis codes could be found that correspond well to the radiology report finding, whether the X-ray is a screening or confirmatory test, the prevalence of the condition, and the chronic vs. acute nature of the condition.  We provide this figure as a survey across these factors and present the complete set of counts in Appendix Table~.  Specific ICD 9 and 10 codes used for each condition are provided in the Appendix Table~.

Table~ compares ground truth extraction techniques, specifically focusing on the label-wise performance metrics on the VA-CXR dataset. Two techniques, ChexPert and ChexBert, are evaluated based on their Area Under the Curve (AUC), prevalence, and count across 14 labels. Across the findings, both techniques generally demonstrate similar AUC values, indicating comparable performance in distinguishing positive cases. For instance, in identifying Atelectasis, both ChexPert and ChexBert exhibit AUC values around 0.80, with similar prevalence and count numbers. Notably, in the case of Cardiomegaly, ChexBert shows a notably higher AUC of 0.862 compared to ChexPert's 0.753, suggesting potentially superior performance in this specific finding. However, the prevalence and count metrics vary across the findings and between the two techniques. For example, ChexPert tends to have higher prevalence and count values for several findings like ECM (Enlarged Cardiomediastinum), while ChexBert shows higher values for others such as Pleural Effusion. 

Table~ shows the comparison of MIMIC-CXR (Source dataset), Test Split MIMIC-CXR (Hold-out Source Dataset), and VA-CXR based on AUC on the 14 labels.  The Hold-out Source dataset is the test split of MIMIC-CXR dataset,  DenseNet-121 Model. The Test Split of MIMIC-CXR gives us a fair comparison to VA-CXR, the unseen target dataset. This can be observed based on the AUC drop from the overall MIMIC-CXR to test-split. In Table~, the difference in AUC between Hold-out and Target indicates the performance variation between VA-CXR and Test Split of MIMIC. The Negative value of the difference in AUC indicates that the VA-CXR performs better than the Test Split MIMIC-CXR, and the positive value indicates that the Test Split performs better. The Enlarged Cardiomediastinum (ECM) label has the highest difference in AUC, indicating a huge performance drop in VA-CXR. This could directly impact the lack of a large number of image studies in ECM in the source dataset compared to the target. 

Table~ shows the label-wise distribution of the study years. The AUC systematically drops as for study year  for all labels except  and , which peaks at . The prevalence increases over the years in VA-CXR for , , and . 

Figure~ compares the label-wise AUC and prevalence of the two sexes across the Test Split MIMIC-CXR and VA-CXR. This comparison is essential as the female-male patient ratio in VA-CXR is higher than that of MIMIC-CXR; dashed lines in the figure~ can observe this. 

Figure~ compares labels across Test-Split MIMIC-CXR and VA-CXR. The VA-CXR doesn't contain any  images; the figure shows only MIMIC-CXR performance on the . The prevalence and AUC of viewpoints vary based on the label, with  having the most difference between the datasets.  has a drop in performance in VA-CXR in  view position, potentially due to low prevalence in both datasets. 

Figure~ shows the VA-CXR prevalence increase as the age increases across all labels except , , and .  The highest performance drops of 0.15 to 0.2 AUC  in VA-CXR compared to the Test Split of MIMIC-CXR can be observed in  and . In VA-CXR, it is interesting that the AUC performance across the age groups is more stable, i.e., there is not much change in AUC, with the exception of .  

As seen from the results, the ground truth validation and multi-label classification performance across the NLP extraction tools showed that though the VA-CXR dataset has lesser disagreement rates than the MIMIC-CXR datasets, and there were AUC differences between models when using ChexPert and ChexBert (potentially propagated by distribution differences). When comparing the multi-label classification performance on different datasets, the unseen datasets didn't show domain shift other than a few labels such as . Among the different subgroup analyses, the study year had the most drastic differences in the performance of the multi-label classification model. These differences indicate that the domain shift is definitely of concern in study years.

\textwidth%      \centering       Advanced Computing for Health Sciences, Computational Sciences and Engineering Division,\\ Oak Ridge National Laboratory \\       IT Services Division, Oak Ridge National Laboratory \\       Veteran Affair's Association \\       Los Alamos National Laboratory \\       Yale University \\       Stanford University \\       Department of Biomedical Informatics, Vanderbilt University Medical Center \\       Corresponding Author Email: chandrashekm@ornl.gov      : This study aims to assess the impact of domain shift on chest X-ray classification accuracy and to analyze the influence of ground truth label quality and demographic factors such as age group, sex, and study year.

: We used a DenseNet121 model pretrained MIMIC-CXR dataset for deep learning-based multilabel classification using ground truth labels from radiology reports extracted using the CheXpert and CheXbert Labeler. We compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR dataset comprises over 259k chest X-ray images spanning between the years 2010 and 2022. 

: The validation of ground truth and the assessment of multi-label classification performance across various NLP extraction tools revealed that the VA-CXR dataset exhibited lower disagreement rates than the MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores between models utilizing CheXpert and CheXbert. When evaluating multi-label classification performance across different datasets, minimal domain shift was observed in unseen datasets, except for the label "Enlarged Cardiomediastinum." The study year's subgroup analyses exhibited the most significant variations in multi-label classification model performance. These findings underscore the importance of considering domain shift in chest X-ray classification tasks, particularly concerning study years.

: Our study reveals the significant impact of domain shift and demographic factors on chest X-ray classification, emphasizing the need for improved transfer learning and equitable model development. Addressing these challenges is crucial for advancing medical imaging and enhancing patient care.

ObjectivesMaterials and MethodsResultsConclusionIntroductionMethodswidth=\linewidthFigures/DomainShift-Architecture.jpegDomain Shift Analysis based on Image Classification Modelfig:archDatasetMIMIC-CXRtab:datasetsjohnson2019mimicjpgVA-CXRtab:datasetsknight2024vision\arraystretch1.2Source and Target Datasettab:datasetsGround truth Label Extractionsmit2020chexbertPneumoniaPneumoniaPneumoniaPneumoniaPneumoniaLabel Validationirvin2019chexpertRelation to Diagnoses Codeshttps://github.com/stanfordmlgrouppneumoniainfectioninfected processinfectiousedemaheart failurechfvascular congestionpulmonary congestionindistinctnessvascular prominencefracturepneumothoraxpneumothoracestab:ground_icdMulti-label Image Classificationhuang2017denselyhaque2023effectjohnson2019mimicjpg, johnson2019mimicMetricsDomain Shift AnalysisStudy Year AnalysisPerformance Analysis based on SexView Position AnalysisAge Group AnalysisResultsGround truth Label Validationtab:GroundtruthComparison with Diagnosis Codesfig:sensDx.pngwidth=.7\linewidthFigures/Xray.sens.enrich.png\centering Characterization of the concurrence of positive findings on chest X-rays with associated diagnoses for our VA data set.  The top panel shows the sensitivity with which a related diagnosis is given within a week before or after a positive x-ray finding, which we labeled 'Sensitivity'.  The bottom panel shows the factor by which the ratio of positive to negative X-ray findings increases when a diagnosis code is present.  Patients with a diagnosis code not within one week of the assessed X-ray are excluded from the calculation for both plots.      fig:sensDx.pngfig:sensDx.pngtab:ground_counttab:ground_icdMulti-label Image Classification on VA-CXR across NLP toolstab:GroudtruthPerformanceMulti-label Image Classification Performance across multiple datasetstab:datasetAUCtab:datasetAUCStudy Year-wise Performance on VA-CXR fig:studyYear2020 to 2022ConsolidationPleural Other2020AtelectasisEnlarged CardiomediastinumPleural Effusionwidth=\linewidthFigures/StudyYearLabelwise_600.jpg Accuracy and Prevalence of Labels Study Year Wise \\ fig:studyYearPerformance across datasets based on Sexfig:genderAccfig:genderAccView Position-wise across datasetsfig:viewPositionLateralLateralECMPleural OtherAPAge Group-wise comparison across datasetsfig:age-groupNo FindingLung LesionPneumothoraxEnlarged CardiomediastinumSupport DevicesAtelectasisSummaryEnlarged CardiomediastinumDiscussionQuality of Ground Truthseyyed2020chexclusion,rajpurkar2023currentPerformance Comparison:enlarged cardio mediastinumSubgroup analysisgichoya2022aiirmici2023chestsoin2022chexstrayClinical relevancekhan2023drawbacksPotential Workflowset al.tiu22moor23fig:sensDx.pngLimitationsConclusionAcknowledgmentsfigure0\thefigureA.\arabictable0\thetableA.\arabicAppendix:Age When Image was Taken versus Study Year