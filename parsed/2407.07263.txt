Our pretraining dataset consists of three different domains of data:  English natural language data, multilingual natural language data, and source code data. Table  highlights the data sources that compose the pretraining set along with their respective token counts. In our English corpus, the Web Crawl data is sourced from Common Crawl (CC) snapshots while the remaining categories are comprised of high-quality sets. For instance, the miscellaneous category consists of BigScience ROOTS , Reddit, and Pile-Stories , the encyclopedia category contains Wikipedia and Stack Exchange, and scientific papers includes ArXiv and PubMed. 

The multilingual dataset consists of 53 languages with the majority of examples being drawn from CC snapshots, although a small portion comes from machine translation parallel corpora . Lastly, our source code data is drawn from permissively licensed GitHub repositories and totals over 43 languages. 

%Table  highlights the composition of data sources used in full detail. We pretrain the model for 8T tokens. Given that current state of the art LMs are pretrained for trillions of tokens, we want to experiment on top of a pretrained model that is emblematic of the type of models which the continued pretraining recipe would be used for.

%Utilizing this set of data sources, we pretrain the model for a total of 8T tokens. As current state of the art LM are being trained for ever larger token counts, we wanted to build on top of a pretrained model that would most aptly characterize the setting and type of model in which continued pretraining of models would be used in. 

As the most likely scenario in continued pretraining is that the available datasets are exactly those which made up the pretraining set, the vast majority of our continued training data blend is comprised of the pretraining data sources. The only new additional source of data is a set of question and answer (QA), alignment style examples. Such examples have been shown to better extract stored knowledge within LMs . This set of QA data totals 2.8B tokens and Table  highlights the categories of types of QA examples. 

We experiment using a 15B parameter decoder-only transformer  LM with causal attention masks. It has 3.2 billion embedding parameters and 12.5 billion non-embedding parameters. Additional architectural specifications include: 32 transformer layers, a hidden size of 6144, 48 attention heads, Rotary Position Embeddings (RoPE) , squared ReLU activations in the MLP layers, a SentencePiece  tokenizer with a vocabulary size of 256k, no bias terms, and untied input-output embeddings. Additionally, we use grouped query attention (GQA)  with 8 KV heads.

%Table  highlights the exact configurations which contribute to the total parameter count.%

The model is pretrained with a sequence length of 4,096 and uses batch size rampup over the first 5\% of pretraining tokens, starting from a batch size of 384 and building up to one of 1,152. We use a cosine learning rate schedule, with warmup of 16B tokens, to decay from a  maximum learning rate (LR) of  to . We train using the AdamW  optimizer with , , and a weight decay of 0.1. In continued pretraining, the only hyperparameter that is altered is the learning rate schedule.

%During continued pretraining, all architectural details remain the same. The only hyperparameter that is altered is the learning rate schedule whose final values are shared in the experiments.%In continued pretraining, the only hyperparameter that is altered is the learning rate schedule.%The only axis along which we make %During continued pretraining, all architectural details and most hyperparameter values for the model stay the same as during pretraining. The only axis along which we make alterations is the learning rate schedule and our experimental results highlight the settings we considered as well as the final chosen values.%learning rate warmup over the %as is standard across most recently developed LMs ,%During pretraining, we warmup the learning rate over the first 16 billion pretraining tokens and it is then annealed using a cosine learning rate decay schedule, as is standard across most recently developed LMs , from a max learning rate of  to a minimum of . %including high school mathematics, US history, economics, medicine, and more

We evaluate the model using a representative set of tasks to test its change in abilities across the English, multilingual, and coding domains. To assess English capabilities, we evaluate on the widely-used MMLU  and Hellaswag  benchmarks. MMLU measures the model's world knowledge across 57 domains while Hellaswag assesses commonsense reasoning ability within natural language inference. For our multilingual evaluations, we use the Multilingual Grade School Mathematics (MGSM)  benchmark and specifically report the average accuracy across the language subset of Spanish, Japanese, and Thai, as they represent a high, medium, and low resource language respectively. Lastly, to assess the model's coding capabilities we utilize the Python code generation task of HumanEval  with evaluations reported in the pass@1  setting. In our results below, we report the average score across all four of these tasks with fully detailed evaluation scores shared in the Appendix. 

A crucial component of any training run is the data distribution -- it defines the information which a model sees and directly impacts the model's capabilities. As continuous pretraining builds on top of a model which has already seen a given pretraining distribution, it is important to define a data distribution which allows the model to learn new concepts without also deviating too far from the pretraining distribution such that the model begins to experience training instability and accuracy regression. Through a series of runs which tackle what compositions of data distributions best improve the abilities of a pretrained model, we identify general characteristics that can be applied across most continuous pretraining scenarios. In these experiments, we use a learning rate schedule that starts from  and decays to 0 with cosine annealing.

First, we examine if the inclusion of QA data, which improves the ability of a model to extract stored knowledge , improves model accuracy. Coupled with this question is another on how to best incorporate the QA data, or more generally any dataset which is not contained within the pretraining data distribution, into the continued training run: immediately at the beginning and throughout the entirety of continued training, or rather reserved till the end of continued training following a curriculum learning setup . We hypothesize that inclusion of new data sources at the beginning of continued pretraining allows for the model to best learn the new information, but may cause learning instabilities that could be mitigated by showing the new dataset at the end of the run when the learning rate is less aggressive. To answer these questions, we compare continued training entirely with the pretraining data blend, entirely with a QA data blend, and with a mix of the pretraining and QA data blends where we start with the pretraining blend and switch to the QA data blend late in the training run. The QA data blend in this scenario adds the QA dataset to the pretraining data distribution with a weight of 10\%. 

Table  illustrates that the incorporation of QA data markedly outperforms solely using existing data from the pretraining set. Additionally, first using the pretraining data blend for the majority of training tokens before transitioning to the QA data blend at the end of continued pretraining exhibits improved accuracy compared to using the QA blend throughout the entirety of training. This indicates that continued pretraining runs should begin with a data distribution which more closely aligns to the pretraining one followed by a blend that then introduces new data. Moving forward, we refer to the initial blend as the general blend, GB, and the latter blend as the QA blend, QB, and discuss how they can be refined to realize further improvements. 

We hypothesize that the optimal GB will be one which places greater emphasis on high quality data sources and areas of model weakness, without deviating too far from the pretraining distribution. Such a blend will enhance knowledge in needed areas and prime the model for the QB blend without worry of experiencing large training instabilities. Figure  illustrates the various GB distributions we consider; in addition to upweighting sources of interest, we either subset web crawl to just high quality documents, as identified by being in the bottom quartile of perplexity scores from a KenLM model  trained on Wikipedia, or remove web crawl altogether. Experimenting with the various GB distributions for all 300B tokens of continued training, Table  shows that each improves upon the pretraining distribution. Even though it does not achieve the highest average accuracy, we choose Upweight Non Web with High Quality Web as the GB moving forward, because compared to others, it most consistently achieves high scores across all considered tasks as shown in Table . 

With a GB distribution in place, we now look to define the QB distribution by first refining the weights placed on the sources within the QA data and then optimizing the QB distribution as a whole. In the initial QB distribution, the QA data was added as is, and this weighting is shown as QA blend 1 in Figure . Given that the pretrained model struggles on STEM tasks, we create two additional blends that both upweight the QA STEM data while either maintaining the original weight of QA world knowledge, blend 2, or QA chat, blend 3, data as seen in Figure . We choose to maintain the weight in world knowledge and chat information as such examples cover a broad range of topics and help better align model responses to questions respectively. Table  highlights that upon adding each of the QA blends to the initial QB distribution following 250B tokens of the identified GB, QA data that emphasizes both STEM and chat information leads to the best results.

%As chat information aids in the model's resposne, we believe it blahs thorugh this.%We find that the model particularly struggles on tasks that assess STEM based reasoning capabilities.We now incorporate the QA data within the overall QB distribution. In previous runs, the QB distribution, aside from the QA dataset, exactly mirrored the pretraining set. We define a new series of distributions based on more aggressive upweighting of sources in areas of model weakness and amount of weight placed on the QA dataset as seen in Figure . Table  details that the aggressive weighting in the QB is beneficial, and we use the QB termed QA blend moving forward. With refined GB and QB distributions, the average evaluation accuracy has improved from 48.9 for the pretrained model to 55.4, a 13\% improvement.

The learning rate schedule greatly impacts the training dynamics and efficacy of continued pretraining . 

In our above continued pretraining experiments, the learning rate schedule starts at a maximum LR of , which is equal to , and decays to a minimum LR of 0 using cosine annealing. As seen in Figure , a minimum LR of 0 facilitates a steep slope of decay but the magnitude of LR is severely impacted, especially over the tokens where the QB is used which may impact the model's ability to extract full utility from the QA data. To understand the trade-off between these two characteristics of the learning rate schedule in continued pretraining runs, we experiment with two additional minimum learning rate values:  and . 

Table  highlights that it is in fact best to strike a middle ground between magnitude of LR and slope of decay, as a minimum LR of  achieves the best accuracy. Such a minimum LR value allows for a learning rate schedule that has reasonable decay over the QB tokens, unlike when using a minimum LR of , without severely sacrificing on magnitude of LR, as was the case with a minimum LR of 0.

Experiments with varying learning rate warmup and maximum LR values led to accuracy regressions compared to the schedule detailed above. In addition, we ran ablations with a different annealing schedule, WSD , however the results were not competitive to cosine annealing. Full details and results for both studies are shared in Appendix .

% 0 is what we were using previously%Additionally, warmup is a crucial factor. We consider three different warmup levels, stating from max to 1.5x max, starting from 0.5 amx to max, and starting from 0 to the lr at that point in the scheule had it been consistntent. We find that the results here are not up to par as seen in Appnedix blah Table blah. 

Until this point, we have been switching between the GB and the QB after 250B tokens of continued pretraining. We believe this to be sub-optimal, as it is unclear how switching between distributions after a fixed number of tokens can be easily translated to continued training runs of different token horizons. We hypothesize that the optimal point for switching between the data distributions depends upon the learning rate schedule. Figure  highlights how both the number of tokens and learning rate values for the QB blend would differ if the distribution switch occurred at progressively smaller fractions of the maximum LR. As the fraction goes to 0, both the slope of decay and magnitude of the learning rate shrink, meaning that there likely is an optimal point in the learning rate curve where both of these characteristics are still conducive to enable learning but also not too aggressive to the point where the data shift in the QB distribution causes training instability. 

Table  highlights that switching between the GB and QB at  achieves the best accuracy and improves upon the heuristically chosen switch point by 0.4 points on average. Wanting to confirm this distribution switch point holds at differing amounts of continued pretraining tokens, we ran an ablation on a scale of 100B tokens and found that  again maximized the results as seen in Table . 

This finalizes our continued pretraining recipe. We highlight the utility of this recipe as it allows the model to achieve an average accuracy of 56.1, which improves upon the natural baseline of continued training on the pretraining distribution, as shared in Table , by 9\%. 

 We show the efficacy of the identified continued pretraining recipe when used at varying numbers of continued training tokens. Table   illustrates that on continued training horizons from 100B to 1T tokens, the identified recipe consistently achieves improved evaluation results -- realizing a 16\% gain over the pretrained model when using 1T tokens of continued training. We do note that the slope in accuracy improvement from 300B to 1T tokens is lower than that from 100B to 300B tokens, we hypothesize that as we are mainly reusing documents from the pretraining set when doing a large number of continued training tokens the repeated number of epochs on the same data sources have decreasing marginal utility.  %Additionally, the recipe improves upon our earlier experiment of simply continued pretraining on the pretraining distribution for 300B tokens, which achieved an average accuracy of 51.5, by nearly 9\%.%%% all with relatively the same data available in pretraining, as we see too much of it we would expect gains to slow down, which we do see but we see positive benefit with our schedule throughout%In an effort to mitigate some of the blah we blah

In an effort to improve the utility of the data sources that are seen for multiple epochs in long horizon continued pretraining runs, we aim to find a subset of examples that are most helpful for model improvement. As the QA dataset was shown to significantly boost model accuracies, we hypothesize that restricting each pretraining data source to the set of documents which are most similar to the QA examples would be beneficial. To do so, we use the E5-large-v2  text embedding model to obtain an embedding for each document in our pretraining and QA sets. Using the Faiss library , we efficiently perform a 50-nearest neighbor search across all these embeddings to obtain the 50 most similar, non-QA documents to each example in the QA set. The identified subset of examples constitutes 60B tokens, and we term this approach document mining.

Table  shows a training run where we replace all non-QA data sources in the QB distribution solely with the examples identified via document mining. We find that these documents substantially improve the performance of the continued pretraining run and believe that document mining is a viable approach at extracting further utility from existing data sources. 

%% showing we can improve in further, just adding to QB

The 53 multilingual languages contained within the pretraining set are: AR, AZ, BG, BN, CA, CS, DA, DE, EL, ES, ET, FA, FI, FR, GL, HE, HI, HR, HU, HY, ID, IS, IT, JA, KA, KK, KN,	KO, LT, LV, MK, ML, MR, NE, NL, NO, PL, PT, RO, RU, SK, SL, SQ, SR, SV, TA, TE, TH, TR, UK, UR, VI, and ZH.

%Table  lists the multilingual languages contained within our pretraining set.

The 43 programming languags contained within our pretraining set are: assembly, c, c-sharp, common-lisp, cpp, css, cuda, dart, dockerfile, fortran, go, haskell, html, java, javascript, json, julia, jupyter-scripts, lua, makefile, markdown, mathematica, omniverse, pascal, perl, php, python, R, restructuredtext, ruby, rust, scala, shell, sql, swift, systemverilog, tex, typescript, verilog, vhdl, visual-basic, xml, and yaml.

Table  shares the results across all tasks for each experiment mentioned within Section .

In identifying a learning rate schedule for continued pretraining, we experiment with various degrees of warmup and values of . The combinations we consider are: warmup from  to ,  warmup from  to , and warmup from 0 to what the expected learning rate value would be had the pretraining learning rate schedule been extended to incorporate the continued training tokens (i.e., from 8T to 8.3T). We use  to specify the minimum learning rate value of the pretrained model, which is . Figure  highlights each of these schedules, and we note that these combinations were chosen to quantify different degrees of aggressiveness when using warmup in a continued pretraining learning rate schedule. 

As highlighted in Table , we find that including any level of warmup within the continued training learning rate schedule causes regressions in evaluation accuracies, indicating that it is best to decay directly from .

%We additionally experiment with various degrees of warmup within our learning rate schedule for continued pretraining. Decaying till , %After having identified appropriate values for  within the learning rate decay schedule, we experimetned with various scheduels of warmup and . These schedules are displayed in Figure balh. In particular we tried doing the following. As highlighted in Table blah we see that including warmup within contieud training learning rate scheduels causes regressions in evaluation accuracies, likely as because blah and blah. Hence, we move forward with the recommended min to blah. %this one likely will go to the appendix

In addition to cosine annealing, we experiment with the WSD learning rate scheduler . Table  compares the best found setting of WSD with cosine annealing. The WSD schedule produces significantly lower evaluation accuracies than cosine annealing. We hypothesize that in continued pretraining, switching the decay schedule from the one used during pretraining is harmful. Hence, for models pretrained with cosine annealing, the learning rate schedule in continued training should also use cosine annealing. 

%cite appendix -- move this table there %

Table  highlights that the findings of our experiments in Section  also hold at the continued training token horizon of 100B tokens. This indicates that regardless of the number of continued training tokens, transitioning between the GB and QB distributions at  is optimal. 

When extending the number of continued pretraining tokens to 1T, we found that our existing QB distribution would cause the small QA dataset to be trained on for a large number of epochs. To correct for this, we reduce the weight on the QA datset so that it would be trained on for no more than 4 epochs. Figure  demonstrates the distribution of the QB when used at the scale of 1T continued pretraining tokens. 

%% As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important to be able to reuse a model after it has completed pretraining; allowing for a model's abilities to further improve without needing to train from scratch. In this work, we detail a set of guidelines that cover how to design efficacious data distributions and learning rate schedules for continued pretraining of language models. When applying these findings within a continued pretraining run on top of a well-trained 15B parameter model, we show an improvement of 9\% in average model accuracy compared to the baseline of continued training on the pretraining set. The resulting recipe provides a practical starting point with which to begin developing language models through reuse rather than retraining. Introductionbrown2020language, chowdhery2022palm, openai2024gpt4, geminiteam2024geminiopenai2024gpt4, claude32024, geminiteam2024geminirekateam2024reka, deepseekai2024deepseektouvron2023llama2, gemma24, parmar2024nemotron4ibrahim2024simple, jang2022continual, ke2023continual, yıldız2024investigatingparmar2024nemotron4Related Worksrolnick2019experience, caccia2021online, lesort2022understanding, gupta2023continual, lin2024rho1Robins1995CatastrophicFR, FRENCH1999128gupta2023continual, ibrahim2024simple, winata2023overcoming, scialom2022finetunedjin2022lifelong, jang2022continual, jang2023temporalwiki, loureiro2022timelms, qin2022ellechaudhry2019tinyhinton2015distillingke2023continual, gururangan-etal-2020-dont, wu2024llamawu2024llamazan2022cert, yadav2023exploring, ma2023ecomgptct, yang2024pllama, labrak2024biomistralibrahim2024simpleibrahim2024simpleExperimental SetupData SourcesPretrainingtab:data_high_levellachaux2020unsupervisedpile-dataset-2020schwenk2019ccmatrix, el2019ccalignedContinued Pretrainingallenzhu2023physicstab:qa-dataModel Architecture and HyperparametersVaswani+2017su2023roformerkudo2018sentencepieceainslie2023gqaloshchilov2019decoupledEvaluationhendrycks2020measuringZellers2019HellaSwagCAshi2022languagechen2021evaluatingkulal2019spocContinued Pretraining RecipeStart with a data distribution that is similar to the pretraining set but places larger weight on high quality sources before transitioning to a second distribution that incorporates QA data and upweights sources in areas of model weakness.

  The learning rate schedule should start from  of the pretrained model and decay with cosine annealing to .

  The switch between data distribution should occur at  in the learning rate schedule.

Experimentstab:base_model_resultsData Distributionsec:data_distributionswidth=\linewidthacl-style-files/figures/GB_distrs_big_name.pngBreakdown of the various distributions considered for the General Blend (GB). We use Upweight Non Web w/ High Quality Web as the GB moving forward given its strong performance across all evaluation areas.fig:gb_distrsallenzhu2023physicssoviany2022curriculum, blakeney2024doestab:initial_exps_resultsfig:gb_distrsheafield2011kenlmtab:general_blend_resultstab:data_distr_exps_results_detailedfig:qb_qa_distrfig:qb_qa_distrtab:qa_blends_resultsfig:qb_distrstab:qa_blends_full_resultswidth=\linewidthacl-style-files/figures/QB_distrs.pngBreakdown of the various distributions considered for the QB. e refers to  epochs of the QA data. The final chosen distribution is shown as QA Blend which used 2 epochs of QA data.fig:qb_distrsLearning Rate Schedulegupta2023continual, ibrahim2024simple, winata2023overcomingfig:decay_lrstab:lr_decay_resultshu2024minicpmsec:appendix_experiments_learning_ratesSwitch of Data Distributionssec:experiments_distribution_switchfig:lr_distribution_shifttab:distribution_switchtab:distribution_switch_100Btab:initial_exps_resultsAblationsVarying Token Horizonsct_varying_tokens_resultsDocument Miningwang2022textjohnson2017billionscalect_mined_documents_resultsConclusionLimitationscustomDatasec:data_sourcesMultilingual Datasec:appendix_data_sources_multilingualCode Datasec:appendix_data_sources_code Table  lists the programming languages contained within our pretraining set.

tab:code_lang_summaryExperimentssec:appendix_experimentsData Distributionsec:appendix_experiments_data_distributionstab:data_distr_exps_results_detailedsec:data_distributionsPer-task evaluation results of each experiment mentioned within Section \ref on defining data distributions for continued pretraining.tab:data_distr_exps_results_detailed% all into oneFIXtab:initial_exps_results_detailedFIXtab:general_blend_results_detailedFIXtab:qa_blends_results_detailedFIXtab:qa_blends_full_results_detailedLearning Rate Schedulesec:appendix_experiments_learning_ratesPer-task evaluation results of the experiments mentioned in Table \ref on identifying an appropriate learning rate decay schedule for continued pretraining. tab:lr_decay_results_detailedfig:warmup_lrstab:warmup_results_detailedComparison of including warmup within learning rate schedules for continued pretraining. No warmup achieves the best evaluation results. tab:warmup_results_detailedhu2024minicpmtab:lr_decay_results_detailed_wsdWe find that WSD causes significant regression in evaluation accuracy compared to cosine annealing. Both learning rate schedules were decayed till .tab:lr_decay_results_detailed_wsdSwitch of Data Distributionssec:appendix_experiments_distribution_switchtab:distribution_switch_100Bsec:experiments_distribution_switchPer-task evaluation results of the experiments mentioned in Table \ref on how to switch between data distributions in continued pretraining.tab:distribution_switch_results_detailedAblation of the data distribution switch experiments at a continued pretraining scale of 100B tokens. As found for the 300B token continued training horizon, switching distributions at  achieves the highest accuracy.  tab:distribution_switch_100BAblationsVarying Token Horizonssec:appendix_continued_training_varying_amountsfig:qb_distr_lengthwidth=\linewidthacl-style-files/figures/QB_lengths.pngDistribution of the QB blend when extending the number of continued pretraining tokens to 1T.fig:qb_distr_lengthPer-task evaluation results of the experiments mentioned in Table \ref on how the identified continued pretraining recipe performs at varying amounts of continued training tokens.tab:ct_varying_tokens_results_detailedPer-task evaluation results of the experiments mentioned in Table \ref on how document mining increases the utility of existing data sources in continued pretraining.tab:ct_mined_documents_results_detailed