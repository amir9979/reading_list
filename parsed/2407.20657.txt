The objective of Phase 1 is to train learnable context words used in , facilitating the extraction of generalizable text features in Phase 2. To fully harness the capabilities of CLIP~ model, recent studies~ have shown the effectiveness of leveraging context optimization. These methods have demonstrated enhanced performance of the CLIP model across various distribution-shifting scenarios. We adhere to the recent protocol outlined in~ and integrate it to enhance our attack framework. We utilize the same training dataset (, ImageNet-1K~ which contains diverse distribution shifts) as in Phase 2.

We model the context words used in  via learnable continuous vectors  as follows:

where each  () is a trainable 512-dimensional float vector with the same word embedding size of CLIP~, and  is the number of context words. The training is exclusively focused on these word vectors, whereas the weights of the CLIP image encoder  and text encoder  remain unchanged. The training objective is to optimize  by minimizing the cross-entropy loss computed using pairs of input images and GT class labels through few-shot learning. When the input images  are passed through , the produced image features can be represented as . With  class labels, we can obtain text features  by feeding the class labels into . Then, the prediction probability can be computed as:

where  is the temperature parameter learned by CLIP~, and  denotes the standard cosine similarity.

Using the prediction probability and a one-hot encoded label vector , the context training loss can be computed as follows:

where the trained  along with  is utilized in Phase 2.

This phase aims to train a perturbation generator model  capable of crafting transferable perturbations as described in Algorithm~. We randomly initialize the generator . Given a batch of clean images  with a batch size of ,  generates unbounded adversarial examples. Subsequently, these are constrained by the perturbation projector  within a predefined budget of .

 . The generated adversarial images  and clean images  are passed through the surrogate model , where we extract -th mid-layer features. We define the surrogate model loss  as follows:  where  denotes the standard cosine similarity.

 . As CLIP~ employs contrastive learning to learn representations in a joint vision-language embedding space, we design a contrastive learning~ based attack loss for effectively harnessing CLIP features. Our loss design differs from GAMA~ since we separate heterogeneous surrogate and CLIP features, and utilizes representative text features extracted from ground-truth (GT) class labels of input images. Additionally, our method leverages CLIP's perturbed image features, which can provide effective loss gradients. Given clean images  of batch size , we first feed  into the CLIP image encoder  for acquiring clean image features . We input  into  followed by  and craft adversarial images by . We then pass  through , and CLIP's adversarial image features  can be obtained as follows:

Since we utilize a large-scale ImageNet-1K~ dataset for training, we have corresponding GT class labels  for each . If there are  classes in the training dataset (,  for ImageNet-1K), we can obtain the corresponding set of  text features using the CLIP text encoder . In contrast to GAMA~, we pass  as input to the pre-trained , which yields more effective text-driven prompt inputs for . Then, text features  extracted from the GT class labels of input images can be computed as follows:

With adversarial image features  as the anchor point, we employ adversarial text features  as the positives, which are computed by identifying the least similar text features compared to the input image features . Specifically, we compute a set of text features denoted as  using the class names of all  classes, where each  corresponds to . For each batch, we randomly select  candidates from  for computational efficiency and identify the least similar candidate, using its label as . We define the adversarial text features  as follows:

We use  from Eq.~() as the anchor,  from Eq.~() as the negatives, and  from Eq.~() as the positives to constitute our contrastive loss. Finally, our prompt-driven contrastive loss can be formulated as follows:

where  denotes the desired margin between  and . All the features are -normalized before the loss calculation. As a result,  facilitates more robust training of the generator  by leveraging the prototypical semantic characteristics of text-driven features, boosted by the frozen robust .

 . Our generator  is trained by minimizing  as follows:

where the total loss  facilitates  to be trained towards more robust regime via effective prompt-driven feature guidance.

Since our perturbation generator  has been trained robustly, we freeze it for the final inference stage of crafting adversarial examples. With the frozen , we can generate adversarial examples by applying it to arbitrary image inputs, even if they belong to target data distributions different from the source domain. We assess the performance of the trained  in black-box scenarios across diverse domains and model architectures. The generated adversarial examples are expected to exhibit superior cross-domain and cross-model transferability.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Since our goal is to generate adversarial examples which show high transferability across various domains and models, we carry out experiments in challenging black-box scenarios, namely  and  settings. Building upon a recent work~ that demonstrated remarkable attack transferability by employing a large-scale training dataset (, ImageNet-1K~), we also leverage ImageNet-1K to train our perturbation generator and prompter. The efficacy of the trained generator is assessed by conducting evaluations on three additional datasets: CUB-201-2011~, Stanford Cars~, and FGVC Aircraft~. Specifically for the  setting, we evaluate our method on unknown target domains (, CUB-201-2011, Stanford Cars, FGVC Aircraft) and victim models distinct from both the source domain (, ImageNet-1K) and the surrogate model. For the  setting, we evaluate our method against black-box models with varying architectures, maintaining a white-box domain setup using ImageNet-1K.

The perturbation generator is trained on ImageNet-1K against a pre-trained surrogate model of VGG-16~. For the  setting, we employ fine-grained classification victim models trained by DCL framework~. These models are based on three different backbones: ResNet50 (Res-50)~, SENet154, and SE-ResNet101 (SE-Res101)~. For the  setting, we explore various different model architectures such as Res-50, ResNet152 (Res-152)~, DenseNet121 (Dense-121), DenseNet169 (Dense-169)~, Inception-v3 (Inc-v3)~, MNasNet~, and ViT~.

We adhere closely to the implementations employed in recent generative model-based attacks~ to ensure a fair comparison. The mid-layer from which we extract features from the surrogate model (, VGG-16~) is 3. For the CLIP~ model, we use ViT-B/16~ for the image encoder and Transformer~ for the text encoder, consistent with the configurations used in GAMA~. We train the perturbation generator using Adam optimizer~ with  and . The learning rate is set to , and we use a batch size of  for a single epoch training. The perturbation budget for crafting the adversarial image is constrained to . We use a contrastive loss margin of . Regarding the prompt context training of Phase 1, we follow the standard protocol~. Learnable word vectors are randomly initialized by a zero-mean Gaussian distribution with standard deviation of . We use a SGD optimizer with a learning rate of , and train the word vectors with a maximum epoch of  by 16-shot learning on ImageNet-1K~. The number of context words  is set to  and related analyses are shown in Table~. We also employ distribution-shifted versions of ImageNet (-V2~, -Sketch~, -A~, -R~) to identify the robustness of the learned context words. More details are provided in the Supplementary Material.

Our primary evaluation metric for assessing the attack effectiveness is the top-1 classification accuracy. The competitors include state-of-the-art generative model-based attacks, such as GAP~, CDA~, LTP~, BIA~, and GAMA~. We train all the baselines~ on the same ImageNet-1K dataset for fair comparison.

We compare our method with the state-of-the-art generative model-based attacks~ on various black-box domains with black-box models. In the training stage, we utilize ImageNet-1K~ as the source domain to train a perturbation generator model against a pre-trained surrogate model of VGG-16~. In the inference stage, we evaluate the trained perturbation generator on various unknown target domains, namely CUB-200-2011~, Stanford Cars~, and FGVC Aircraft~, using different victim model architectures. Specifically, we employ several fine-grained classification models that have been trained using the DCL framework~. These victim models are based on three different backbones: Res-50~, SENet154 and SE-ResNet101 (SE-Res101)~. We assess the effectiveness of our trained perturbation generator in this challenging black-box attack scenario.

As shown in Table~, our method exhibits superior attack effectiveness with significant margins on most cross-domain benchmarks, which are also cross-model. This highlights the robust and potent transferability of our crafted adversarial examples, enabled by prompt-driven feature guidance and prompt learning to maximize its effectiveness. We conjecture that the remarkable generalization ability of PDCL-Attack might be attributed to the synergy between our two proposed methods: harnessing the superior representation power of CLIP's text features and improving it further with a prompt learning method to produce generalizable text features. In essence, our approach indeed enhances the perturbation generator's capability to generalize across various black-box domains and state-of-the-art model architectures.

While our method demonstrates the effectiveness in enhancing the attack transferability in the strict black-box scenario as shown in Table~, we further conducted investigations in a controlled white-box domain scenario, specifically within ImageNet-1K~. We train a generator against a surrogate model of VGG-16~, and subsequently assess its effectiveness on victim models with various architectures such as ResNet50 (Res-50), ResNet152 (Res-152)~, DenseNet121 (Dense-121), DenseNet169 (Dense-169)~, Inception-v3 (Inc-v3)~, MNasNet~, and ViT~.

As shown in Table~, our method mostly achieves competitive performance even in the white-box domain scenario. We conjecture that our CLIP-driven guidance, coupled with optimized prompt context, can improve the training of the generator in crafting more resilient perturbations, ultimately showcasing enhanced generalization capabilities in unknown feature spaces of victim models. It is noteworthy that incorporating the pre-trained CLIP ViT-B/16~ image encoder alongside the surrogate model might be one of the factors which could enhance the ViT-based transferability. Nonetheless, compared to GAMA~ which also utilizes the same CLIP encoder backbone, our method surpasses it particularly on ViT-based model transferability. We posit that incorporating text feature guidance from GT labels and adversarial image-driven loss gradients further improves the training of the perturbation generator.

In transfer-based attack scenarios, the adversary leverages surrogate models to simulate potential unknown victim models and craft adversarial examples. In this work, we additionally leverage a vision-language foundation model (, CLIP~) and formulate effective loss functions to enhance the training. In Table~, we evaluate the effect of our proposed losses used for improving the transferability of adversarial examples in both cross-domain and cross-model scenarios. Remarkably, in , our approach achieves state-of-the-art results even without employing prompt learning. This highlights the effectiveness of our proposed CLIP-driven attack loss  compared to GAMA~. This demonstrates the effectiveness of the text-driven semantic feature guidance from GT labels and back-propagated loss gradients from adversarial CLIP image features. Moreover, incorporating prompt learning  into the framework further boosts the transferability as demonstrated in . When employing  trained with , the text-driven guidance facilitated by  is strengthened by the extraction of more generalizable features. Compared to the baselines, our method consistently improves both cross-domain and cross-model attack transferability.

In Table~, we compare our prompt learning method with various hand-crafted text prompts in a cross-domain attack scenario. Using domain-specific heuristic prompts, such as ``a  of a '' or ``a  style of a '', CLIP's textual guidance might fall into sub-optimal regime due to the domain-specific overfitting. Considering this, we investigated a domain randomization approach by randomly initializing a word vector, , ``a  style of a ''. In this trial, we randomly initialize  for each training iteration. However, the effectiveness is comparable to that of the na heuristic prompts, indicating the necessity for a meticulously crafted prompt to attain better performance. We conjecture that our superior results might be attributed to the enhanced representational power of generalizable text features attained through learned context words. This also aligns with recent findings~ that prompt learning not only enhances the downstream task performance, it can even improve the robustness of the trained model in distribution-shifted scenarios. Remarkably, increasing the capacity of learnable context words () further enhances the attack effectiveness.

While our work primarily focuses on crafting more effective perturbations, it is also crucial to carefully examine the image quality of the adversarial examples for real-world deployment. Therefore, we investigate how the perturbation budget affects the transferability of cross-domain attacks in Table~. We compare the top-1 classification accuracy after attacks using the generator trained with the perturbation budget of . Across each test-time perturbation budget, our method consistently demonstrates superior attack performance. In other words, ours can achieve higher attack transferability with lower perturbation power and better image quality, which are significant advantages in real-world scenarios. PDCL-Attack can generate effective and high-quality adversarial images as shown in Fig.~.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Discussion% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%