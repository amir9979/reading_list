Let us consider an image classification problem, where a dataset , for which every pair  represents an input image and its associated semantic target label, is used to train a model  to output  as the correct class of an unseen sample  by minimizing an ERM objective. If the training set  presents a bias and the model is naively trained,  likely learns shortcuts based on spurious correlations between samples and target classes instead of the actual semantic attributes . In a biased dataset, most samples present these spurious correlations (i.e., are biased-aligned), while only a small portion does not (i.e., bias conflicting), by definition. Consequently, the model naively trained on a biased dataset shows poor generalization performance when tested on new data drawn from an  distribution, as it memorizes the few bias-conflicting samples present in the training set instead of properly learning their semantics.  

In this scenario, we can expect that the bias-aligned samples of a certain training class are compact, showing high density in the model's feature space, as they share bias attributes. Intuitively, the distribution  of a biased training class will be skewed, with the bias-aligned samples being concentrated in the main mode (a wide peak) and the bias-conflicting belonging to the (potentially long) tails of such distribution. This is the typical setting for Anomaly Detection, defined as the task of revealing uncharacteristic samples in a data set, defined as anomalies or outliers. Anomaly Detection methods can effectively learn a margin separating the samples in the high-density region (bias-aligned) from the samples belonging to the distribution's tails, which will fall outside of it, as they are in lower-density regions (bias-conflicting). 

Hence, we claim it is possible to identify bias-conflicting samples in a biased dataset as the samples inducing  features in a biased model. To this end, we exploit and customize a well-established anomaly detection algorithm, One-Class Support Vector Machine (OCSVM) , to design a  procedure capable of identifying conflicting samples with high accuracy. The use of OCSVM stems from its effectiveness, efficiency, and controllability, while we leave to future work investigating Deep Anomaly Detection approaches for this task.

Several Anomaly Detection (AD) methods have been proposed, either relying on deep learning   or traditional machine learning approaches . In this work, we aim to frame the problem of unknown bias identification in the context of anomaly detection, and we opt to exploit the One-Class Support Vector Machine (OCSVM). It can be regarded as an unsupervised technique that learns a hyperplane separating the training samples from the origin while maximizing the hyperplane distance from it, and which can learn meaningful margins without the need for manual annotations . In the context of dataset bias, its associated decision function would ideally return  for in-class (bias-aligned) samples and  for samples detected as anomalous (bias-conflicting). 

Such decision function is outlined in Eq. , where  and  are the learned Lagrange multipliers and hyperplane's offset respectively, while  is the Gram matrix of distances between sample points in kernel space. The higher the argument of the , the higher the (signed) distance of a sample from the learned margin, with a positive distance indicating that a sample should be regarded as in-class (positive sign), or out-of-class (negative sign). As such, the argument of the , can be regarded as an .

In our bias identification approach, we modify Eq.  to shift the argument of the  by a threshold  (which we automatically compute, see Algorithm ), so that when the anomaly score is higher than  the corresponding sample is classified as in-class, and anomalous otherwise. Such a decision threshold can be used to restrict anomaly predictions with respect to the original formulation. In our approach, we train one OCSVM per target class, allowing us to tailor bias detection specifically to individual categories.

Naively training a model on a biased dataset produces a biased classifier, learning the spurious correlations between samples and target classes, representative of bias.  However, after just a few epochs () the model typically overfits the training samples, memorizing bias-conflicting samples as well.  Consequently, the actual distribution shift between bias-conflicting and bias-aligned samples in the biased model's feature space disappears.  To overcome this issue while being robust with respect to the number of training iterations, we employ the Generalized Cross Entropy (GCE)  as our loss function for the bias identification step. As shown in , GCE loss makes the model to  onto bias, weighting the samples that the model classifies with high confidence (i.e., bias-aligned) more than the uncertain ones (i.e., bias-conflicting). However, differently from , we use GCE to further amplify the distribution shift between bias-aligned and bias-conflicting samples, avoiding bias-conflicting sample memorization and favoring their identification with anomaly detection algorithms. 

In this work, we design a debiasing scheme based on  (see Fig. ). Starting from a model trained using the biased dataset with vanilla ERM and hence likely biased, we employ a  with replacement, whose weights are set depending on the prediction  of the bias identification step, as the inverse of the two estimated populations. 

With this sampler, during the debiasing training process,  we build each mini-batch such that the ratio of bias-aligned and bias-conflicting samples is roughly balanced. We further perform upsampling adding three augmented images for each bias-conflicting sample in each mini-batch, so that the number of bias-conflicting samples in the batch results 4 times higher than the number of bias-aligned ones. Exploiting this batch construction scheme, a classic CE loss function is then used for debiasing a model. Differently from other two-step methods (e.g., JTT ), MoDAD performs the debiasing process on an already biased model, facing a realistic scenario when bias is unknown. 

To validate our approach, we consider a  (CIFAR-10) and three  (BAR, BFFHQ, Waterbirds) datasets. The synthetic dataset allows the test of the algorithm under controlled varying conditions, specifically the knowledge of the bias and in which percentage it affects the data. In realistic datasets, instead, prior information regarding bias could not be available, while potentially presenting additional challenges including unbalanced per-class populations, and erroneous annotations, to name a few. 

 is the first investigated dataset  in the version made available from . This dataset is derived from the original 10-class CIFAR-10 , composed of 60,000 images altered by 10 different types of corruptions following the protocol introduced by . Such image corruptions are   , , , , , , , , ,  , each one being  highly correlated with the ten semantic labels. We consider three degrees of correlation  corresponding to . The train and validation sets have the same correlation  between bias-attribute and semantic label, while in the test set 90\% are bias-conflicting and only 10\% have spurious correlations with the label. Figure  shows some examples from this dataset.

 (Biased Action Recognition) is an action recognition image dataset with a strong correlation between the performed action and the setting in which it is carried out. It was introduced in  and consists of six different semantic classes (, , , , , ), for a total of 2,595 images. The samples are further divided into a training set of 1,941 images and a test set of 654 images with no pre-defined validation set. For example, the  class contains a great majority of images in the training set where the subject is performing rock climbing. In contrast, in the test set, we can find many images of people climbing on snow or ice (see Figure ).  Additionally, this dataset does not provide ground-truth annotation regarding whether a sample is bias-aligned or bias-conflicting.

 is a visually perceived gender-biased image dataset of human faces, obtained from the Flickr-Faces-HQ dataset . Introduced in , we refer to the version made publicly available from . It is characterized by two target attributes related to age ( and ),  where the great majority of training samples are either young female faces or old male faces (bias-aligned), while a small minority are old females or young males (bias-conflicting).  This correlation is broken in the validation and test sets, where the two attributes are uniformly distributed. It totals 21,200 images where 19,200 images are used as training samples, 1,000 samples for validation and 1000 samples for testing. Samples of  this dataset are shown in Figure .

 is an image dataset introduced in  that combines  subjects from the Caltech-UCSD Birds-200-2011 (CUB) dataset  and image backgrounds from the Places dataset . Images are divided according to a target attribute regarding birds being Waterbirds or Landbirds, while the background being land or water represents the bias attribute. In this setting, images depicting Landbirds on land and Waterbirds on water are bias-aligned samples, whereas pairing a Landbird with a water background constitutes a bias-conflicting sample (and vice-versa for Waterbirds on land background).

We employ two quantitative metrics in our results:  and . Average Accuracy is the accuracy over the whole test set, which typically contains a balanced number of bias-aligned and conflicting samples, except otherwise stated. Conflicting Accuracy refers to the average accuracy computed only on the bias-conflicting data. We provide the mean and standard deviation of three independent runs for each experiment. For the comparative analysis, we follow the same evaluation protocol of Lee  for Corrupted CIFAR-10, as we employ the very same version of this dataset, thus reporting the Average Accuracy of the whole test set. The test sets of BFFHQ and Waterbirds are balanced, containing an equal percentage of bias-aligned and bias-conflicting samples. For the BAR dataset, we report just the Average Accuracy due to the absence of bias annotations preventing us from computing separate metrics. Finally, as a baseline, we also report the performance of a model trained without any debiasing strategy, i.e. through Empirical Risk Minimization  with standard cross-entropy loss.  For the sake of comparison, we employ a ResNet-18  for Corrupted CIFAR-10, BAR, and BFFHQ, like in . In the experiments involving Waterbirds, we opt to use a ResNet-50 as in . For the real-world datasets, we start from a model whose weights are pre-trained on ImageNet , like in .  For each model, we plug on top of the last backbone layer an MLP with one hidden layer followed by a ReLU. About the OCSVM, we set the contamination parameter to  and adopt a Radial Basis Function (RBF) kernel. For all the other technical details, please refer to the Supplementary Material. Table  provides MoDAD's results on three different  values of Corrupted CIFAR-10, benchmarking with relevant existing approaches. Despite the basic debiasing step, our method is competitive with respect to state-of-the-art approaches for the three different bias ratios, reaching similar performances of DFA  and LfF , while being slightly lower for . Please, note that our performance result is more stable considering the standard deviation, which is an order of magnitude lower than that of the other methods. Moreover, we significantly outperform JTT , whose debiasing approach is similar to the one employed in MoDAD, for all the bias ratios, further suggesting the effectiveness of our bias identification mechanism.  Regarding the accuracy of our anomaly detection algorithm in bias identification, the average and standard deviation for the F1-score over the ten corrupted CIFAR-10 classes, correspond to ,  and , for  respectively (see Table ). Finally, in the last entry of Table  we report the oracle represented by using the ground truth bias labels and our proposed debiasing method. The reported oracle accuracy provides the best performances among the referenced methods, confirming our intuition regarding the importance of precise bias identification for model debiasing. 

Table  reports the performance of MoDAD and a comparison with relevant existing works on the three real-world datasets. First, we evaluate the accuracy of our bias-identification approaches on Waterbirds and BFFHQ, obtaining an average f1-score of  and , respectively (see Table ). Regarding the debiasing performance, MoDAD surpasses other unsupervised debiasing works, apart from the Average Accuracy on Waterbirds, where LfF  reports the highest among all the works in this category. At the same time, our method reaches higher Conflicting Accuracy, almost matching the supervised state-of-the-art on this dataset (GroupDRO ). Regarding BAR, we match the performance of DebiAN  () on average, but with a significantly lower standard deviation ( for DebiAN,  for MoDAD). BFFHQ has a bias-correlation , thus capturing the contribution of the few bias-conflicting training is much harder. However, in BFFHQ we report the most significant improvement over existing works, with a Conflicting Accuracy of , corresponding to an improvement of  over the second-best approach (DFA ). 

In this section, we report ablation studies to evaluate the impact of our design choices on the proposed methodology. Additional analyses can be found in the Supplementary.

 To further support the effectiveness of our anomaly detection-based bias identification procedure, we perform an ablation study on our bias identification method. Specifically, we run our debiasing step using bias-aligned and bias-conflicting predictions obtained with the original implementation of JTT . The last entry of Table  summarizes the obtained results. On three runs, we obtain an average decrease in the debiased model performance corresponding to   and  for the Conflicting accuracy in Waterbirds and BFFHQ, respectively, and  in Average accuracy for BAR. The obtained results further support the impact of the anomaly detection method for identifying bias in the performance of the final debiased model.

 To show the soundness of the proposed approach regardless of the anomaly detection method adopted, we perform several tests by using other readily available anomaly detectors, on the Corrupted CIFAR-10 dataset (with ). Specifically, we consider the  (LOF) , the  (IFO) , and the  (COV) . Table  reports the average accuracy on Corrupted CIFAR-10 with . OCSVM is indeed the best option among the considered ones. However, the obtained results confirm that changing the detector has only a modest impact on the results, demonstrating that the intuition regarding bias-conflicting samples as anomalies does not indeed depend on our specific design choice of the anomaly detection method.