To filter the data we hired a total of 165 annotators through the Prolific platform. We first asked participants to specify their country of origin, location, and their cultural background. Then, we asked them to retrieve images from the VizWiz dataset visualizer related to their cultural background, provide the image name, the reason they think the image is culture-related, and their preferred caption from the dataset (VizWiz provides five different image captions per image). We also gave them the option to suggest a better caption that includes cultural aspects. After collecting all the culture-specific candidate images, we proceeded to a second step of verification. In this step, we retained only those images that had received consensus agreement from at least two individuals. We collected a total of  images and  captions spanning 60 different identified cultures. It should also be noted that more than 96\% of the annotators suggested a cultural revision of the original captions. We refer to Appendix~ for further information about the annotation guidelines and data filtering approach and results. 

We conducted experiments on the image captioning task in the zero-shot setting, in which a pretrained model is queried to produce a textual description for an image without finetuning on the same dataset. We relied on four commonly used open-access models: BLIP-2 6.7B  with OPT as LLM backbone , InstructBLIP 7B  with Vicuna backbone , Idefics2 8B , and LLaVa-1.6 7B  with Mistral backbone . We also used two state-of-the-art closed-access models: GPT-4o  and Gemini Pro 1.5 . For all of these models, we experimented with two different prompt types including a culture-specific prompt following  and a default captioning prompt taken from . The exact prompts can be found in App~.  % While some of these models have multilingual capabilities, we only prompt the models in English We evaluated the model-generated captions in two ways: (1) via the COCO evaluation suite and (2) through human evaluation. The COCO evaluation suite was first introduced by  as a framework to assess image captions using numerous automatic metrics, including BLEU , CIDEr , METEOR , and SPICE . For consistency with our culture-specific re-annotations (two captions per image), we also used two reference captions per image to score models on the original annotations. Since each image has five original captions, we report aggregate results over all ten two-caption combinations. Our human evaluation had two stages. In the first stage, we asked 60 participants to determine if a caption is accurate (on a binary scale) given the corresponding image. In the second stage, we asked the same participants to rank all captions (human-generated, and model-generated) according to their preference. We did not make the annotators aware that one caption was model-generated to minimize bias. We provide further details on the human evaluation in Appendix~.