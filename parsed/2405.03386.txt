multi-annotator classification approaches approximate true class labels by aggregating multiple noisy class labels per instance. Subsequently, the aggregated class labels and their associated instances serve as the training dataset for the downstream task. The simplest aggregation approach is majority voting, which outputs the class label with the most annotator votes per instance. By doing so, majority voting naively assumes all annotators have the same accuracy~. More advanced approaches~ overcome this issue by estimating each annotator's performance (e.g., via a confusion matrix) when aggregating class labels.  However, such approaches typically expect multiple class labels for each instance~.

 multi-annotator classification approaches do not need multiple class labels per instance because they train the classification model without any detached stage for aggregating class labels. A common training principle is to leverage the expectation-maximization (EM) algorithm that iteratively updates the classification model's parameters and annotators' performance estimates (M-step) to accurately estimate the latent true class labels (E-step)~. Such EM algorithms come at the price of high computational complexity and the need to plan when to switch between E- and M-steps~. Therefore, several approaches have been proposed to overcome these issues when training NNs. A common approach is to extend an NN-based classification model by a noise adaption layer~, whose parameters encode annotators' performances on top of the classification layer. Alternatively, a separate so-called annotator performance model is jointly trained with the classification model~. In this case, both models' outputs are combined when optimizing the target loss. Alongside the training algorithm, the underlying assumptions regarding the modeling of annotator performance play a crucial role. Here, simplifications of the probabilistic graphical model in Fig.~ are often made, for example, by ignoring the instance dependency of the annotator performance~. Our work follows recent one-stage approaches~, which model annotator performance as a function of the latent true class and instance's features. 

Regularization reduces NNs' overfitting on instances with false class labels. However, common regularization approaches, such as weight decay~ and dropout~, are often insufficient for tasks with severe class label noise~. Data augmentation via  is a more robust regularization approach~. Given two randomly drawn instances  with their true class labels , a new instance-label pair for training is generated via convex combination:

where the scalar  is sampled from a symmetrical beta distribution  with the concentration parameter . This way,  expands the training dataset by utilizing the idea that interpolating feature vectors linearly should result in linear interpolations of their targets, requiring minimal implementation and computational overhead. Meanwhile, various extensions of  have been proposed, including an extension mixing hidden states of NNs~ and an extension specifically tailored for image~ or text data~.  To the best of our knowledge, the only  extension~ for multiple annotators is proposed for opinion expression identification tasks~. Its idea is to make predictions by combining learned annotator embeddings for the same instance to simulate the annotation of an expert. Beyond the task type, our approach differs substantially from this extension by mixing class labels across different instances and annotators while explicitly modeling each annotator's performance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Assuming the probabilistic graphical model of Fig.~, the joint distribution of the true class label  and noisy class label  given the instance  and the annotator  factors into a product of two categorical distributions:

For predicting instances' class labels, we aim to estimate an instance's class-membership probability distribution . Therefor, we employ a classification model in the form of an NN with parameters , defined through the function

where  are the estimated probabilities for the true class label of instance~. Accordingly, the estimated Bayes optimal prediction for our objective in Eq.~ is given by the class with the maximum probability estimate:

For estimating annotators' performances, we aim to approximate the probability distribution  for each possible class label . Therefor, we employ an annotator model in the form of an NN with parameters , defined through the function

where  is the estimated confusion matrix of annotator~ for instance~, represented through  as output of the classification model's penultimate layer.

Since the true class labels  in the complete likelihood function  are latent, we optimize both models' parameters  by maximizing the marginal likelihood of the observed noisy class labels :

where  comprises the annotators who provided a class label for instance . The marginalization (summation) of the latent true class label is shown in Eq.~. The function  in Eq.~ outputs the estimated the probabilities for estimating which class label an annotator will assign to an instance. Thus, the function  outputting the class label with the highest estimated probability is given by:

Converting the marginal likelihood function in Eq.~ into a negative log-likelihood function yields the cross-entropy as the loss function:

By default, the optimal predictions  and  are not identifiable~ because there are multiple combinations to produce the same output . Therefore, we resort to a common solution proposed in literature~ and initialize the annotator model's parameters  to approximately satisfy:

with  as the probability of obtaining a correct class label,  as an identity matrix, and  as an all-one matrix. We set , implying that solutions with diagonally dominant confusion matrices are preferred at training start. 

Optimizing the loss function in Eq.~ corresponds to empirical risk minimization (ERM)~ because the classification and annotator models are forced to fit the observed noisy class labels perfectly. Although the annotator model attempts to separate the noise in the class labels during training, overfitting is still an issue (cf. Section~). Therefore, we extend ~ for robust regularization and improved generalization of . Our idea for the extension of  to multi-annotator classification tasks lies in shifting the perspective from mixing tuples of instances and class labels to mixing triples of instances, annotators, and class labels. Concretely, we propose the following extension: Applying the above formulation allows us to handle varying numbers of noisy class labels per instance while considering which class label originates from which annotator. Moreover, we can natively manage even datasets with only one class label for each instance. Fig.~ illustrates our  extension as data augmentation performed in the instance and annotator feature space. Intuitively, this has two main effects. On the one hand, we simultaneously regularize the classification and annotator model. This is because mixing class labels from different annotators across instances makes it more difficult to memorize which class label an annotator provides for an instance. On the other hand, we improve the generalization by not only linearly interpolating the instance but also the annotator feature space. We demonstrate both effects in our ablation study (cf. Section~), which includes an analysis of the  hyperparameter controlling the degree of regularization. For example, defining  recovers the ERM solution.

Fig.~ summarizes the training with  as a Python code snippet. The design of the classification model's architecture depends on the underlying data modality and task. For example, one may employ a residual network (ResNet)~ for image data. In contrast, the annotator model must process two vectors as inputs. For this purpose, we use a simple multi-layer perception (MLP) with input concatenation. Our repository provides more implementation details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% We design experiments according to the problem setup of Section~. Table~ overviews our setup, which we detail in the following.

 We select our datasets to cover a wide range of real-world settings for obtaining meaningful assessments of the approaches' robustness and performances. Concretely, experiments are performed on eleven real-world datasets across three data modalities: image, tabular, and text. The number of classes ranges from  to . Five datasets contain noisy class labels from humans, while we simulate the annotators providing noisy class labels for the remaining six datasets. The number of annotators ranges from  to . As labels are costly, the average number of provided class labels per instance (approximately ranging from one to four) is considerably lower than the number of annotators. The fraction of false class labels ranges from low noise levels (ca. ) to medium noise levels (ca. ) to high noise levels (ca. ).

: Since the number of datasets annotated by multiple error-prone humans is limited, we include datasets with simulated annotators. Ideally, the noisy class labels of simulated annotators are close to human class labels. To do so, we follow related work~ and train an individual NN for each annotator. These NNs differ in their training hyperparameters and in the training data they use. Specifically, we train  NNs with different parameter initializations, numbers of training epochs, learning rates, and ratios of randomly sampled training instance-label pairs per class. Then, each NN's predictions serve as the noisy class labels. This way, we mimic annotators with different expertise regarding certain classes and feature space regions. Obviously, we have access to the noisy class label of each simulated annotator for each instance. Yet, we set the average number of class labels per instance to a much lower number (three or one) to account for the limited annotation budget in real applications. Moreover, it is also common that some annotators provide many labels while other annotators provide very few labels. We account for this by adopting an existing method~, where each annotator is assigned an individual probability for annotating an instance.

 For benchmarking the performance of , we compare it to eight one-stage multi-annotator classification approaches, which are ~, trace-regularized estimation of annotator confusion ()~, common noise adaption layers ()~, learning from multiple annotators as a union ()~, multi-annotator deep learning ()~, geometry-regularized crowdsourcing networks (, )~, and learning from crowds with annotation reliability ()~. These one-stage approaches mainly differ regarding their training algorithms and estimation of annotators' performance. While prioritizing one-stage approaches for their reported performance gains~, we still include basic two-stage approaches to better contextualize the results. Specifically, we employ majority voting () as a lower baseline and its combination with vanilla ~ (). Further, we show the results for training with the true class labels () as an upper baseline.

 According to our objective in Eq.~, we assess a classification model with parameters  through its empirical classification accuracy on a separate test set :

where the instance-label pairs in  are independently sampled from the joint distribution . Going beyond the standard classification setting, we additionally assess the annotator model with parameters . For this purpose, we adopt the idea of evaluating how well the model can predict whether an annotator provides a wrong or correct class label for a certain instance~. In case of , we define a function , which outputs the estimated probability of obtaining a correct class label from an annotator  for a given instance :

where  denotes the diagonal of the confusion matrix as a column vector. Other one-stage multi-annotator classification approaches similarly provide performance estimates of annotators. Since this estimation task can be interpreted as a binary classification task, we compute the area under the receiver operating characteristic~ () to assess how well the different approaches can predict annotators' performances. Another evaluation score of interest is the accuracy in predicting the noisy class labels provided by the annotators for the training data:

which allows us to identify overfitting by comparing it to .

: We specify architectures to meet the requirements of the respective datasets. For the three tabular datasets , , and , we train a simple MLP with two hidden layers of parameters. For the datasets , , and , which consist of  images, we employ a ResNet18~. The other three image datasets , , and  contain higher-resolution images, so we use DINOv2~ as a pre-trained vision transformer (ViT). More concretely, we freeze the feature extraction layers of the ViT-S/14 and train the classification head implemented through an MLP with one hidden layer of parameters. Typical image data augmentations are performed for the six image datasets. An analog procedure is applied to the text datasets  and , with the difference that we use bidirectional encoder representations from transformers (BERT)~ as a pre-trained architecture. 

 For all datasets, we employ RAdam~ as the optimizer and cosine annealing~ as the learning rate scheduler. Training hyperparameters (cf. Table~), such as the initial learning rate, the batch size, the number of training epochs, and weight decay, are empirically specified to ensure proper learning and convergence of the . We set further hyperparameters specific to a multi-annotator classification approach according to the recommendations of the respective authors. This way, we ensure meaningful and fair comparisons. Moreover, a training, validation, and test set is given for each dataset. If no validation set is provided by the respective data creators, we define a small validation set with true class labels. In this case, the validation size is set either to , , or , depending on the number  of training instances. Following related works~, we use such a validation set to select the model parameters with the highest validation accuracy throughout the training epochs. However, acquiring a validation set with true class labels may be costly in settings with noisy class labels~. Thus, we also report the results for the models obtained after the last training epoch. Each experiment is repeated ten times with different parameter initializations. Accordingly, all results refer to means and standard deviations over these ten repetitions. 

This study ablates the regularization and generalization effect of our  extension as a part of . Further, we study the gain of mixing annotators and class labels across different instances.

 Figure~ exemplarily depicts the learning curves of  with (, solid line) and without (, dashed line) our  extension for the datasets  and . The colors distinguish the two evaluation scores  (test set) and  (training set). The observation that the greenish dashed learning curves surpass the greenish solid learning curves demonstrates that training  with our  extension diminishes the accuracy of predicting noisy class labels assigned by annotators within the training set. In other words, our  extension makes memorizing the training data more difficult. Yet, the observation that hat the purplish dashed learning curves fall short of the purplish solid learning curves demonstrates that our  extension boosts the test accuracy. Together, these observations verify our  extension reduces overfitting to noisy labels. 

 Table~ ablates the generalization effect and robustness regarding the hyperparameter , used for sampling the mixing coefficient  in Eq.~. We present results for a subset of four datasets encompassing different data modalities, numbers of classes, false label fractions, and numbers of noisy class labels per instance. Further, three of these datasets contain noisy class labels from simulated and one from human annotators. A key observation is that for all tested  values and datasets, integrating our  extension into the training of  improves its generalization performance. Consequently, these performance gains are also robust regarding the choice of the hyperparameter~. Yet, certain -values lead to larger improvements, e.g.,  seems to perform best across these four datasets. However, we set  for all subsequent experiments to enable a fair comparison with other approaches.

 Inspired by the idea of Zhang et al.~, we modify our  extension to only combine two triples , if and only if both instances are equal, i.e., . Evaluating this modified  extension while keeping the rest of  unchanged allows us to study the benefit of mixing triples containing different instances. Table~ presents the corresponding results in its last column. For the two datasets  and , each with one class label per instance, the results are identical to the training w/o , as mixing only happens if multiple class labels per instance are available. No substantial improvements can be seen for the dataset  compared to training w/o , whereas the performance gains are noteworthy for the dataset .  Despite this, the performance results across all four tested datasets fall short of those achieved with our original  extension as part of . These results highlight the importance and broader applicability of mixing different triples containing different instances.

 Table~ presents the results for comparing the classification models' performances trained by the different approaches per dataset. As expected, training with the true class labels, i.e., , leads to the best results across all datasets. Comparing the performances of this upper baseline to  as the lower baseline, we clearly observe the negative impact of the noisy class labels. For example, the performance gap between the lower and upper baseline is about  for the dataset , which contains the highest fraction of false class labels. The approach  strongly reduces this performance gap for almost all datasets and thus confirms the benefit of vanilla  in combination with two-stage approaches. If we now also consider the results of the one-stage approaches, we recognize that  is the only approach outperforming the two-stage approaches for each data set. The other one-stage approaches often perform inferiorly on datasets with many classes. For example, except , all one-stage approaches are worse than  for the data set  with  classes. Further,  outperforms its competitors by considerable margins for four of the five datasets annotated by humans. Comparing the classification models' performances after the last epoch and after the best epoch, selected via a validation set during training, we observe inconsistent improvements in this model selection. This is mainly due to the small size of the validation sets. However, this also shows another advantage of , which performs better than its competitors on ten out of eleven data sets when no expensive validation sets with clean labels are available. 

For a more compact presentation of the results in Table~, we further compute each approach's rank per dataset and report their means in Fig.~. Moreover, we evaluate statistical significance at the level of  by following a common test protocol~. Concretely, we perform a Friedman test~ as an omnibus test with the null hypothesis that all approaches perform the same and observed performance differences are due to randomness. If this null hypothesis is rejected, we proceed with Dunn's post-hoc test~ for pairwise multiple comparisons between  and each of its competitors. Thereby, we employ Holm's step-down procedure~ to control for the family-wise error rate. This test protocol is applied to the classification model's performances after the last and the best epoch. The results demonstrate that  significantly outperforms each competitor.

 Table~ presents the results for comparing the annotator models' performances. Intuitively, a high score implies that the corresponding annotator model can accurately predict whether an annotator will provide a correct or false class label for a given instance. Here, we include only the results for the datasets with simulated annotators because the test sets of the other datasets have not been annotated by humans. Further, only the related approaches, training an annotator model, are considered. For the results after the last and best epoch, we observe that  performs best on four of the six datasets while providing competitive results for the other two datasets. As a result, our approach has the potential to be used in applications where it is important to obtain accurate predictions of the annotators' performances, e.g., when selecting the best annotator to provide class labels in an active learning setting~.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%