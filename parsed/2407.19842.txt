The GPT-2 Small model , is a decoder-only transformer architecture with 117 million parameters. It consists of 12 transformer blocks, each containing 12 attention heads, followed by a Multi-Layer Perceptron (MLP). Layer Normalization  that is applied to each component. The model takes an input sequence of  consecutive tokens, which are embedded into  using a learned embedding matrix , where  represents the vocabulary size. Additionally, positional embeddings are added to .

As shown by , interpreting a transformer architecture is easier when thinking of it as a , whose initial value is , where all the components sequentially read from and write to, modifying the initial vector. Finally, the final residual vector is unembedded using a matrix , which is tied to the embedding matrix (i.e., ) in the case of GPT-2. This unembedding process yields a vector , where  represents the logits for the th token in the vocabulary, following the prediction of the th token in the sequence.

As previously-seen, the logits are obtained by linearly mapping the final residual stream vector by using the unembedding matrix. As layer normalization can also be approximated as a linear map, it implies that the logits can be decomposed as a sum of contributions of the different components. Essentially, this can be used to analyze which components contribute the most to the correct (or incorrect) prediction. More formally, if  is the output of the th attention head of the th layer, the logit attribution on the th token can be expressed as:

where  is the th column of the unembedding matrix.

Activation patching, first presented in , consists on  (i.e. replacing) the activations of a given component with the activations obtained by running the model on a . If patching the activation of a given component causes a large drop of performance, it implies that such component is relevant to the task of study, hence enabling us to locate the circuit. 

When it comes to corrupting the prompt, many ways have been used across the literature. For example, it is common to perform , which consists on simply setting the activations of a given component to zero, or , which adds noise to the activations sampled from a Gaussian distribution. However, these methods modify the activations such that the model goes off-distribution, which can give misleading results. In our approach, we use , which simply replaces the activations with other activations from different prompts of the dataset. This process is more principled as the others, as it uses activations that are in-distribution. 

First, it is crucial to clearly define the task of study and build a dataset that elicits such behavior. For example, when studying the task of 3-letter acronym prediction, one could build a synthetic dataset composed of three words together with its corresponding acronym. It is important to remark that this dataset is  used for training, but to perform the corresponding patching experiments that will be used to both locate the circuit and its possible vulnerabilities.

Then, we should define a metric that quantifies the ability of the model to perform the task of study. It is common among the MI literature to use the  or similar. In our case, we decide to use the following metric, which can be applied to any general task: 

where  is the set of possible answers (e.g. the set of capital letters on the task of acronym prediction),  is the correct answer and  is the logit associated to the token . Essentially, this metric enables us to quantify the ability of the model to perform the task of study: the higher the logit difference, the better it performs. 

Once that the dataset is curated and the metric is clearly defined, we will apply a series of activation patching experiments to systematically identify the underlying circuit associated to the task of study. As previously-mentioned, activation patching consists on  (i.e. replacing) the activations of a given component with the activations obtained by running the model on a . If patching the activation of a given component causes a large drop of performance as specified by the previously-defined metric (e.g the logit difference), it implies that such component is relevant to the task of study, hence enabling us to locate the circuit. The aim of this step is to locate a subset of the model that is responsible for the task of study, hence narrowing our focus when it comes to detecting the possible vulnerabilities. 

In addition to the activation patching experiments, different MI techniques could be used to get a basic understanding of the circuit, which could be helpful for our objective. This includes looking at the attention patterns, inspecting the weight matrices, or performing logit attribution techniques, among others . 

Once we have identified the underlying circuit and have a basic understanding of how it works, we proceed to identify its possible vulnerabilities by first generating adversarial samples. More formally, given a neural network , and a sample  that is correctly predicted by the model , an adversarial sample  is defined as a sample that is imperceptibly close to  (, where  is a similarity metric such as the L1 norm) but it is misclassified by the model ().

The process of generating adversarial samples can be formulated as an optimization problem by first defining an  that encourages the generation of samples that are misclassified by the model. For example, it is common to use the margin loss :

Once that the loss is defined, the generation process can be stated as:

where  is a similarity metric such as the L1 norm, which is commonly used on images. 

Even though the previous method is widely applied on the image and speech domains, it is not directly applicable to the text domain because the data space is discrete, therefore gradient-based methods cannot be applied. In order to overcome this, we will use a technique similar to the one that  used for prompt tuning and discovery. Briefly, the main idea is to optimize on the continuous embedding space, instead of the discrete text space. 

More formally, let  be an initial sample from the dataset, where  is the set containing all the tokens from the vocabulary and  is the number of tokens of the sample. As  lies in a discrete space, it is not possible to directly optimize  so that a loss  is minimized. However, the sample can be embedded into the continuous embedding space , where  is the dimension of the space. As it lies on the continuous space,  can be optimized to minimize the previously-defined loss and then performing the inverse operation to return back to the token space, . Therefore, our method will be based on this technique to automatically generate adversarial samples that will be used to detect and understand vulnerabilities in the underlying circuit.

The pseudocode of our approach for adversarial sample generation is presented in Algorithm . First, an initial sample  is extracted from the dataset and it is embedded to obtain . Then,  is iteratively updated so that  is minimized by (i) projecting  into the nearest embedding vectors (i.e. each row of the embedding matrix) (ii) computing the gradient of  w.r.t the projected embeddings  and (iii) using the masked gradients  to update the continuous embeddings . Note that the gradients are multiplied by a binary mask  that is specified by the user to control which parts of the sample are changed or remain constant.  

In our approach, the previous algorithm will be used to generate adversarial samples. For example, when looking for vulnerabilities on a palindrome classifier circuit, we can try generating non-palindromes that are incorrectly classified as palindromes by defining a loss that encourages classification error and using the algorithm to optimize different parts of the starting prompts. Notice that this process will be typically iterative, where we slowly refine the initial prompts and parts to optimize, with the objective of (i) obtaining a preliminary understanding about the possible vulnerabilities that the circuit may have (i.e. a certain position of the sequence) and (ii) prepare a dataset of adversarial samples that will be used on the next step to locate the vulnerabilities.

Finally, it is important to remark that the proposed algorithm is general and can be applied to any differentiable model that uses an embedding matrix, which is common across almost all language models. Also, our method supposes a better approach versus using a brute-force approach to generate adversarial samples (for example, by naively replacing tokens of the initial sample until it is misclassified), as the complexity of this methods grows exponentially as the sample length and vocabulary grows. 

On the previous step, Algorithm  was used to iteratively obtain adversarial samples and get an preliminary understanding about the possible vulnerabilities that the circuit of study may have. Now, these generated samples will be used to locate the components affected by the possible vulnerabilities by performing a series of logit attribution experiments. Specifically, we will compute the logit attribution of the different components of the circuit to see their individual contribution to the final prediction. If a component has a large negative logit attribution, it implies that it contributes to the misclassification of the sample, eventually enabling us to further narrow down the scope of where the vulnerability is.

% Now, another round of activation patching experiments will be performed to identify the parts of the circuit that are affected by the possible vulnerabilities. Similar to the previous experiments, we will patch the initial activations obtained by the ones obtained on a corrupted prompt. However, in this case, the corrupted prompt will be the associated generated adversarial sample. % For example, in the case of palindrome classification, one may have different pairs of initial non-palindromic samples, and adversarial non-palindromic samples obtained with Algorithm  that are misclassified. Hence, replacing the activations of the original samples with the corrupted adversarial samples will enable us to locate the concrete vulnerability of the circuit.

Finally, the last task is to understand the identified components by applying different MI techniques, such as inspecting the attention patterns with different prompts, analyzing the weight matrices, etc. 

We will study the task of acronym prediction on GPT-2 Small . Specifically, given three words and the first two letters of its corresponding acronym  (e.g. ), the task of the model will be to predict the  of the acronym (e.g.  ). We selected this task because it is both complex enough to showcase our proposal and serves as an illustrative example without becoming overly extensive. Following the same concept, GPT-2 Small is sufficiently large to show that our approach can be used on a real environment without the analysis becoming too extensive. It is also important to note that we could also study the task of predicting the first/second letters, but we omit them to avoid redundancy, as it does not add any extra relevant information to the case study.

The first step consists on building a dataset that elicits the task or behavior of study. Hence, we built a dataset composed by three-letter acronyms. These acronyms were built by sampling from a public list of  nouns . In order to properly isolate the behavior of choice and provide a clearer analysis, we selected the samples whose words and respective letters of the acronym were individually tokenized, e.g. , where  delimits the different tokens. A more detailed explanation of the dataset building process can be found in the supplementary materials. %Appendix A

We also need a metric to quantitatively assess the performance of the model at the task of study. In this case, we will use the logit difference, as defined in Equation , where the set of possible answers is composed by the capital letters, i.e. .

Once that the task of study has been clearly defined and we have a dataset that elicits the behavior as well as a metric to assess the performance of the model regarding that task, we need to identify the underlying circuit, i.e. the components that are relevant to the task. As stated in the previous section, we will perform a series of activation patching experiments to identify such components. Specifically, as we are analyzing the task of predicting the third letter of the acronym, we will corrupt the third word of the initial samples by resampling them with a different word from the dataset and use the activations of this run to patch the original ones. If patching the activations of a given component causes a large drop in logit difference, it implies that such component is relevant for the task of study. 

Figure  shows the variation in logit difference obtained by individually patching every head of GPT-2 Small. A lower value means that patching that specific component decreases the ability of the model to perform the task, hence the component is important and forms part of the underlying circuit. The results show that heads ,  and   cause most of the impact, so we will narrow our focus on these components and try to detect the possible vulnerabilities that may be present. 

Before getting into the next step, it is also interesting to get a basic understanding of how the selected components work. We performed a set of experiments that suggest that these heads work by attending to the word corresponding to the letter of the acronym to be predicted (i.e. the third word) and copies the capital letter of the word. These experiments can be found in the supplementary material. %Appendix B.

Once that we have identified the circuit and narrowed down our scope to a subset of components, we will generate adversarial samples with the objective of (i) discovering the possible capital letters that might be more vulnerable and (ii) using them to locate the components that are affected by the possible vulnerabilities and performing experiments to understand the source of such vulnerabilities. In order to do so, we apply Algorithm , setting the mask  so that it only optimizes the third word of the initial samples. The vocabulary embedding  will be composed by every possible 1-token noun that we have in our dataset. Hence, the output of this algorithm will be an adversarial sample, i.e. an acronym whose third letter is misclassified by GPT-2 Small. We repeat this procedure several times with a batch size of 128 until we obtain 1000 adversarial samples. Notice that we could gather a larger number, but the previous already had enough variety for our purposes. 

Then, we perform an analysis to study which letters are more prone to vulnerabilities. Specifically, we will compare the original probability distribution against the new generated adversarial distribution. In order to do so, if  is the probability of sampling an acronym whose third letter is the th capital letter in the adversarial distribution and  is the same for the original distribution, we define the following:

A large value of  implies that the th capital letter is more present in the adversarial distribution as compared to the original, hence we will use it as a gauge to analyze which letters may present a vulnerability. Figure  shows the obtained value of  for each letter of the vocabulary. It can clearly be seen that letters  and  are 6 and 3 times more likely to appear on the generated adversarial distribution as compared to the original one, respectively. In other words, the results obtained show that these two letters are much more likely to be misclassified. 

As previously shown, letters  and  are considerably more prone to be misclassified, suggesting that there might be one or more components in the model that may have a vulnerability which can be exploited. 

In order to locate these components, we apply the logit attribution technique as described on Section . Specifically, we will cache the output of every attention head and project these vectors into the direction of the logit difference, which essentially gives us the amount that every component writes into the correct direction. Therefore, if a head outputs a negative value, it implies that it contributes to misclassifying the sample. 

Figure  shows the logit attribution obtained on the adversarial samples with the letter , which was the one that showed the largest  on the previous experiment. These logit attributions reveal that the three components of the circuit that we have previously discovered contribute negatively to the output, but the contribution of head  is considerably larger than the other two. Hence, this implies that head  contributes the most when misclassifying samples with the letter , which we found to be a source of vulnerability. Repeating the results for letter  gave us similar results which led to the same conclusion, which can be seen in the supplementary materials. %Appendix C. Finally, we performed a set of logit attribution experiments to try to understand the source of such vulnerability. Specifically, we gathered the output of head  as before, but project it into the directions of the different capital letter directions. In essence, this gives us information about what this component is trying to predict. 

Figure  shows the results obtained for the adversarial samples with the letter . The results clearly shows that head  consistently misclassifies adversarial samples with the letter  by trying to predict the letter . Interestingly, repeating the experiments with the letter  also shows the same results. The rest of results are also included in the supplementary materials. %Appendix C.