Vision-language models have garnered significant attention in recent years due to their ability to understand and integrate vision and language instructions. By jointly training on a vast dataset of images paired with corresponding textual descriptions, CLIP  learns to map images and text into a shared embedding space. Different from this straightforward learning protocol, Flamingo  and BLIP2  utilized frozen image encoders to obtain visual representations and integrated them with textual instructions to form the input. In particular, Flamingo achieved this by utilizing cross-attention layers to integrate visual and linguistic features. In contrast, BLIP2 introduced a Q-former to bridge the frozen image encoder with the frozen language model, ensuring that the most relevant visual features are transferred to the LLM decoder to produce the desired text. Additionally, Liu et al.  proposed an automatic scheme to construct 158K language-image instruction-following data with the LVLM LLaVa. To further improve the quality of visual representations, Chen et al. scaled up the vision encoder to 6 billion parameters, integrating it into the InternVL , aiming to align the representation of the enhanced vision encoder with the LLM. 

Inspired by these achievements, attention has been shifted to the clinical domain. Building on OpenFlamingo-9B, Med-Flamingo  pre-trained using paired and interleaved medical image-text data collected from publications and online textbooks. Similarly, LLaVA-Med  constructed a biomedical VQA dataset by sampling image-text pairs from PMC-15M  and utilized GPT4 to produce multi-round questions and answers. In addition to 2D image inputs, RadFM  introduced an architecture that supports visually conditioned generative pre-training. It integrated text input with 2D or 3D medical scans, enabling the generation of responses for diverse radiologic tasks. To alleviate the inherent data noise, Chen et al.  refined medical image-text pairs from PubMed and constructed PubMedVision. With this dataset, HuatuoGPT-Vision  was proposed by incorporating Qwen as vision encoder  and Yi  as the LLM part. 

Despite growing attention, there remains a lack of comprehensive evaluation of generalised LVLMs in the medical domain, especially for commercial ones (e.g., GPT4o, Gemini, etc.). To address this gap, this study conducts a thorough evaluation of highly competitive LVLMs.

To evaluate the capabilities of medical LVLMs, researchers have established several Medical VQA benchmarks (Table I), with VQA-RAD , Path-VQA , SLAKE, OmniMedVQA, and CAREs  being the most representative ones. For example, SLAKE built knowledge-driven QA pairs on a semantic-aware dataset to evaluate the LVLMs' capabilities, and CAREs creatively evaluated more social characteristics of LVLMs such as confidence, safety, fairness, and privacy. Although efforts have been made to Medical VQA benchmarks, most studies focused on homologous aspects. In particular, recent benchmarks (e.g., CAREs and OmniMedVQA) were eager to enhance the diversity of test data (with 16 and 12 modalities, respectively) and the number of test samples, while overlooking the essence capabilities of LVLMs. For instance, although all the benchmarks assessed anatomical understanding and abnormalities, few of them investigated quantitative reasoning and spatial reasoning, while none of them explored multimodal comprehension capability. Interestingly, the common conclusion indicates that general LVLMs significantly outperform their medical-specific counterparts in terms of overall capabilities. This finding highlights the need for a deeper investigation into the specific strengths and weaknesses of LVLMs in the medical domain.

In this study, we further investigate the strengths and weaknesses of generalized LVLMs and their medical counterparts by assessing their in-depth properties when dealing with medical queries. We creatively introduce the multimodal comprehensive assessment, which evaluates the capability of LVLMs given the prior knowledge from both linguistic instructions (text prompts) and visual guidance (visual box prompts). Additionally, we carefully designed multiple-choice questions to better estimate the spatial reasoning of various LVLMs, which enables us to explore whether the LVLMs have learned knowledge in a systematic, clinicians-like manner or are merely relying on representation clipping.

 RadVUQA was developed using multi-source, multi-anatomical public datasets, resulting in the subsets RadVUQA-CT, RadVUQA-MRI, and RadVUQA-OOD. Specifically, 2D CT(MR) images were sampled to ensure a diverse representation of body parts, encompassing 117(56) classes such as the spleen, heart, and kidney for RadVUQA-CT (RadVUQA-MRI). The dataset also varied in terms of prompts, including purely visual prompts, visual prompts with text descriptions, and prompts incorporating both text and spatial instructions. Additionally, the question types were categorized into open-ended and close-ended formats to provide a comprehensive evaluation framework.

 was collected from TotalSegmentator, one of the largest publicly available full-body CT datasets . The dataset comprises 1,204 3D CT scans, with annotations for 117 distinct anatomical human structures. Scans with an axial plane width or height smaller than 200 pixels were excluded. From each scan, we extracted fifteen 2D slices from the transverse, sagittal, and coronal planes, specifically at 25\%, 40\%, 55\%, 70\%, and 85\% of the total number of layers along each axis. This extraction process resulted in a total of 11,448 2D CT images, each accompanied by corresponding mask labels.

 was derived from TotalSegmentator-MRI , which consists of a random sample of MRI scans performed at the University Hospital Basel PACS between 2011 and 2023. The original dataset comprises 298 3D MRI scans, each annotated with 56 anatomical human structures. Scans with an axial plane width or height smaller than 200 pixels were excluded. We extracted 1,021 2D axial slices and their corresponding masks, taken at 10\%, 25\%, 40\%, 55\%, 70\%, and 85\% of the total number of slices. 

 was collected from multiple resources, including 250 synthetic 2D chest CT images from , 250 real 2D chest CT images from , and 63 2D animal CT scans from embodi3D. Additionally, the real images (from ) were augmented to simulate unharmonised data by introducing motion-blur, window-shift, noisy, sharpness, and low-resolution variants, with 1250 images (250 for each) in total.

In TotalSegmentator, the raw semantic labels are instance-based, assigning distinct labels to different instances within the same semantic category (e.g., different ribs). However, this labelling criteria becomes over-detailed for designing VQA datasets, which is beyond the scope of the capabilities of existing LVLMs. Therefore, we assigned four types of labels to each instance. (1) spatial category label (e.g., left lung, right lung, etc.); (2) category label (lung, heart, gut, rib, etc.); (3) anatomical location (abdominal cavity, thoracic cavity, pelvis, etc.); and (4) general category (e.g., organ, gland, bone, muscle, etc.). Details of our mapping criteria can be found in the supplementary materials. These labels enable us to set up QA pairs based on prior knowledge instead of manual labelling. 

 Unlike most VQA datasets that were typically organized based on classification data, RadVUQA was developed using publicly available segmentation datasets (Fig. ). This ensures that the QA pairs in RadVUQA can be more accurately generated and have richer semantic meaning than existing counterparts.   Each question in RadVUQA comprised a  and a . Initially, we set up a basic or an advanced prompt for each context prompt of the given image. The basic prompt briefly introduces the context of the input data, while the advanced prompt gives more details by telling LVLMs about the existing structures. It is of note that in all the QA examples, the content within the '\{\}' represents the semantic labels specific to each image. 

Following the context prompt (basic or advanced), the specific query assesses the capabilities of LVLMs by introducing open-ended questions (OEQs) and close-ended questions (CEQs) across the following aspects.

 evaluates whether the model can identify organs/structures from the image. We set up two OEQs to test the models' performance regarding different prompts. These queries were integrated with basic or advanced prompts.

It could be found that OEQ1.1 has less prior knowledge than OEQ1.2, which was designed to test the effectiveness of Prompt CoT for different LVLMs.

 explores the capabilities of interpreting linguistic and visual instructions. We achieved this by providing prior knowledge of the RoI (Region of Interest). In particular, we labelled the Region of Interest (RoI) with a green bounding box in the input image and asked LVLMs to recognize its anatomical structure. This task requires the model to not only understand linguistic instructions but also accurately localize the prompt bounding box (the green one). Additionally, CEQ2 refers to multiple-choice questions, further testing the model's comprehension ability.

 test LVLMs' capabilities in quantitative analysis and spatial perception. OEQ3.1 simultaneously investigates the model's quantitative capability and anatomical knowledge, requiring the model to identify the largest object and distinguish its anatomical labels.

Subsequently, OEQ3.2 and OEQ3.3 gradually reduce the complexity of the spatial perception assessment, allowing us to evaluate the model's ability to understand and interpret spatial relationships with decreasing difficulty.

In such scenarios, LVLMs were first asked to visually localize the targets, then recognize their categories, followed by illustrating the spatial relationships between the RoIs. Although OEQ3.3 introduced extra prompts to allocate an object, it is still challenging for existing LVLMs.

 investigates the model's capability to know the mechanisms and comprehend the functional roles of organs or structures. 

This evaluation extends beyond mere recognition of visual anomalies, requiring the model to contextualize these observations within a broader clinical framework. 

 Although there have been several benchmark studies for medical LVLMs, most of them ignore the effects of imaging variety and overly focus on increasing the scale of evaluation sets rather than more insights. Here comes the question: how do the LVLMs (Large Vision-Language Models) perform across different imaging settings, quality and characteristics? Unfortunately, despite the few existing studies only assessing noisy interference , this question remains largely unexplored. 

Initially, CT and MRI heavily rely on image preprocessing techniques. For instance, the influence of acquisition protocols (such as different reconstruction kernels, normalisation strategies, and patient positioning) on model performance has not been thoroughly investigated. These factors are critical in real-world clinical environments where imaging conditions are far from standardized and harmonised. Guided by , the OOD subsets were designed to assess the models' capability against these different scenarios, including noise, diverse contrast, sharpness, motion blur, low-dose scanning, etc. Specifically, we assess LVLMs in the following aspects:

 simulate various scenarios including motion blur, biased imaging protocol (different reconstruction kernels with different contrast and sharpness), low-resolution scanning, and noisy data. For fair comparisons, the QA pair for the unharmonised data remains the same as those for OEQ1.1, OEQ1.2, and CEQ1.

 estimates the capacity of LVLMs against adversarial attacks or malicious instructions. For instance, replacing the query image with synthetic scans or nonhuman scans. Therefore, we designed two QA pairs to test this phenomenon 

In general, RadVUQA comprises 10,759 images and 193,662 question-answer pairs (different context prompts combined with OEQs/CEQs), focusing on evaluating five fundamental characteristics of LVLMs. The dataset includes a diverse range of anatomical structures, with 117 from CT scans and 56 from MRI scans. Despite the fewer modalities and number of images compared to OmniMedVQA , RadVUQA comprises more QA pairs due to its in-depth and thorough QA framework, ensuring data diversity through its extensive coverage of anatomical structures. We want to emphasize that although anatomical attributes may seem intuitive and straightforward, they are essential for evaluating the core capabilities of LVLMs in the medical domain. These attributes are critical for assessing whether these models possess a genuine understanding of human physiological knowledge.