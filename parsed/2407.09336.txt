In time series analysis, where data often lacks explicit labels and exhibits complex temporal dependencies, contrastive learning offers a promising avenue for extracting informative features~. By leveraging the sequential nature of time series data, self-supervised contrastive learning methods can capture long-range dependencies, temporal patterns, and underlying dynamics, facilitating various downstream tasks such as classification. Unlike supervised learning approaches that rely on labeled examples, contrastive learning aims to learn representations by maximizing the similarity between similar instances (positive pairs) while minimizing the similarity between dissimilar instances (negative pairs)~. This process encourages the model to capture underlying structures and patterns present in the data without explicit supervision.

Augmentations, a significant component within the contrastive learning framework, are designed to introduce distortions and variations to the time series data, thereby enriching the learning process and promoting the discovery of invariant features~.  The selection of the  method is crucial for enhancing the quality and diversity of the learned representations, which largely affect the performance of contrastive learning algorithms~. 

Differ from the fields of images and languages where intuitive guidelines can guide the creation of suitable augmented samples based on well-understood principles~, the process of manually selecting augmentations for time series data faces significant obstacles. This challenge arises from the complex and often imperceptible nature of the temporal structures in time series data, which makes it difficult to apply straightforward rules or human intuition effectively~.

The prevalent approach to selecting augmentations in time series analysis typically involves manual strategies, encompassing both direct and indirect empirical methods. Direct empirical selection entails conducting initial experiments to assess the effectiveness of widely used augmentations (Section~), a process that, despite its thoroughness, proves to be both time-consuming and resource-intensive. On the other hand, the indirect method relies on existing literature, adopting augmentations that are frequently utilized~. However, the efficacy of such augmentations can vary significantly across different time series datasets, often rendering literature-based approaches less effective.

% The mainstream of augmentation selection in time series analysis remains manual choice including direct empirical selection and indirect empirical selection. The direct empirical selection means the users conduct preliminary experiments to compare the effectiveness of commonly used augmentations (Section~) and select the most effective one, this manner is time consuming and computationally expensive. The indirect manner is mainly based on literature support that using the most commonly-used augmentations in literature and simply use it in their project. % However, the effectiveness of augmentation is dramatically varying depending on the properties of time series datasets, so the literature-based results often don't work well. % In this work, we benchmark the augmentation selection for time series datasets and provide recommendations on augmentations to an arbitrary dataset. The key challenge is how to keep the results generalizable to a general dataset. Our solution is to generate 12 synthetic datasets based on the signal decomposition principle, draw conclusions on the effectiveness of augmentations, then build connection between the trend and seasonality patterns with the efficiency, respectively. We evaluated the conclusion of the effectiveness of augmentations using 6 real-world datasets. 

In this study, we benchmark the effectiveness of augmentations in time series datasets, offering guidance for applying augmentations to diverse datasets. A critical challenge we address is ensuring the applicability of our findings across a broad range of datasets. Our approach creates 12 synthetic datasets, leveraging signal decomposition to quantitatively evaluate the impact of eight augmentations.  This process enables us to establish a bridge between  and the . We further validate our findings by applying these insights to six real-world datasets, thereby offering a more generalized and practical framework for augmentation selection in time series analysis.

% % Self-supervised contrastive learning is a key paradigm in deep learning that aims to learn effective representations by emphasizing similarities and differences between samples. Essentially, this approach focuses more on the inherent characteristic of the data itself through the distinction between positive and negative pairs. The basic goal of contrastive learning is to encourage the model to map similar samples more closely while pushing away dissimilar samples, thereby forming a more discriminative feature space.  Self-supervised contrastive learning learns effective representations by discriminating similarities and differences between samples. It aims to map similar samples closely while pushing dissimilar ones apart, based on augmented views~. Contrastive learning is crucial for various reasons. First, contrastive learning eliminates the need to label data, relieving the burden of extensive annotation efforts, especially in some expertise-demand areas like healthcare~. Additionally, the approach of leveraging the intrinsic structure of data offers powerful and semantically meaningful embeddings that can remain stable and general across different tasks.  

% 2. Framework of contrastive learning (insert one figure, may use the one in your survey) The framework of self-supervised contrastive learning (Appendix Figure~) typically includes the pre-training, fine-tuning, and testing stages. In the pre-training stage, a self-supervised encoder takes the positive pairs and negative pairs and learns to project the input samples into embeddings. Based on the learned embeddings, we calculate and minimize the contrastive loss, making positive pairs close to each other and negative pairs far away from each other in the embedding space. After pre-training, the well-trained encoder and a downstream classifier (depending on the fine-tuning mode) are used to make predictions for the input samples. In the fine-tuning stage, only a small amount of labeled samples are utilized to optimize the encoder and/or downstream classifier. Typically, the training set in fine-tuning stage is a subset of the training set in per-taining stage. The ratio between these sets is called , ranging from 0 to 1. Lastly, the testing stage applies the fine-tuned encoder and classifier for new data.  

% {insert one contrastive framework figure. add the one from the survey or ijcal workshop, or have a new one while showing what we have done benchmarking on (like on what element in the framework?)}% 3. Introduce how contrastive works, especially in pertaining. The key idea behind it: augmented sample should have close embedding with the original sample.  The principle behind effective contrastive learning is to  in the data. During pre-training, the unlabeled data are augmented, or in other words, transformed into variants of the original samples, which we call positive pairs~. Instead, negative pairs are samples from different classes or subjects, depending on the experimental design.  The adopted encoder then projects the original sample and its variants (augmented identical samples) into feature representations. At this stage, the role of the contrastive loss function is to minimize the distance between positive pairs while maximizing the distance between negative pairs. Through training and optimization, the encoder learns to correctly distinguish between positive and negative pairs, which means that the augmented samples should have closer embedding with the original sample. 

% 4. The most important components in contrastive learning: augmentations, contrastive pair construction.  Therefore, data augmentation methods (i.e., how to augment data) and the construction of contrastive pairs are the most important components of contrastive learning, as they will directly affect the performance of the encoder and limit the performance improvement. 

In self-supervised contrastive learning for time series, the augmentations can be seen as transformations that slightly alter the original sample to create a contrastive pair (along with the original sample) during the phase of pertaining. 

Suppose a time series sample  contains  timestamps while there is an observation at each timestamp. This work focuses on univariate time series (i.e.,  is a scalar), but our experimental design and conclusions can easily extend to multivariate time series (i.e.,  is a vector).  % Although there is a wide range of time series augmentations were proposed in the last three years since the emergence of self-supervised contrastive learning framework~, some are borrowed bluntly from image processing (like rotation) or have limited usage (such as R-peak masking is limited to ECG signals, channel-wise neighboring only applies to multivariate time series). Therefore, in this work, we investigate 8 types of augmentations that are most commonly used in time series studies: jittering, scaling, flipping, permutation, resizing, time masking, and frequency masking. For better intuition, we visualize the augmentations in Appendix Figure~ (adopted from ). The 8 augmentations we investigate in this work are elaborated in Appendix Section~.

% We originally summarized 16 types of augmentations commonly used in studies in the scope of contrastive learning for medical time series, which fall into three categories including transforming, masking, and neighboring augmentations~. % However, % In this benchmarking study, we focus on 8 most common time series augmentations that are suitable for almost all types of time series data, as we aim to obtain more general results. The 8 augmentations we benchmark in this paper are Jittering, Scaling, Flipping, Permutation, Resizing, Time masking, Frequency masking, and Time-wise neighboring. % {Ziyu: do we need to explain why we remove some augmentations? like filtering and channel-wise neighboring?} Signal decomposition is a fundamental technique in time series analysis, where the goal is to dissect a time series into several interpretable components, typically including trend, seasonality, and residual component~. In math, a time series sample  can be decomposed into:

where  denotes the timestamp. The trend , seasonality , and residual component  are functions of timestamp. The , , and  are coefficients to adjust the scale of each component in the whole signal. 

The  component reflects the underlying long-term progression of the dataset, showing how the data evolves over time, irrespective of cyclical or irregular patterns.   shows the pattern that repeats over a known, fixed period, such as daily, weekly, monthly, or quarterly seasonality. Identifying seasonality helps in understanding regular variations in the time series. The  component, also called noise in some studies, encompasses the random variation in the time series. These irregularities and fluctuations cannot be attributed to the trend or seasonality components.

The decomposition allows us to understand the underlying structure of the time series signal, facilitating better representation learning and downstream classification.

STL is the most popular and effective method for decomposing a time series~. It employs Loess, a local regression technique, to extract the trend and seasonality components, allowing STL to handle any type of seasonality pattern, not just fixed periodicity. STL's flexibility comes from its non-parametric nature, meaning it does not assume a specific statistical model for the time series data. This makes STL particularly useful for complex data with varying patterns of trend and seasonality.

In practice, STL decomposition iteratively fits loess smoother to the observed time series to separate the data into trend, seasonal, and residual components. This iterative approach ensures that the decomposition accurately reflects the underlying patterns of the time series, even in the presence of noise and outliers.

% STL is an acronym for "Seasonal and Trend decomposition using Loess",% STL is the most common and effective one. In principle, an arbitrary time series signal can be decomposed into the sum of a trend, a seasonality, and a residual component, as shown in Eq.~. The difference between the variety of time series datasets is the different functions of trend, seasonality, and residual parts, along with the weights when they integrate together. 

To increase the generalization of our results, therefore, we establish a series of synthetic datasets to cover as broad as time series patterns. To generate synthetic datasets, we need to consider 3 factors: Next, we answer the questions.

% Trend: linear, non-linear We investigate a linear trend  and a non-linear trend  as two representatives in time series data (Section~):

where  is a coefficient to adjust the details of the trend. 

 Any time series signal can be regarded as the combination of a series of trigonometric functions, according to the Fourier series theorem~. This theorem states that any periodic signal can be represented as a sum of sines and cosines (trigonometric functions) with different frequencies, amplitudes, and phases. Thus, we employ a typical trigonometric function as :

which is the weighted average of sine and cosine functions with phrase shift, where , , and  are parameters for amplitude, frequency, and phrase, respectively. 

Moreover, the Morlet Wavelet is a popular function for time-frequency analysis, particularly effective for analyzing non-stationary signals. We define  as: % where  is a coefficient contributes to the morelet shape.  This is a common form of simplified Morlet wavelet, which combines a complex exponential (sine wave) with a Gaussian window. In Eq~,  where  denotes the central frequency. Our implementation follows Scipy. % }.

For the text below, we omit the  in trend and seasonality functions for simplicity. 

To simplify matters, we use a standard Normal Gaussian distribution  as the residual component and keep it consistent across all synthetic datasets. 

Considering 2 trends and 2 seasonalities, we generate 4 groups of datasets, naming Dataset groups A, B, C, and D, respectively. % For each dataset, we consider three situations: 

For each synthetic dataset group, we build 3 datasets corresponding to the three situations above, denoted by a suffix `1', `2', or `3'. Thus, we have 12 synthetic datasets in total, named A1, A2, ...., and D3.  For example, dataset A1 is composed of linear trend and trigonometric seasonality, with . We provide the workflow in Figure~. See Table  for more details. 

In this work, we focus on trend and seasonality, so keep  constant across all synthetic datasets. % For simplify, we keep  (we get 0.3 from preliminary studies) across all datasets,  We get 0.3 from preliminary studies: a larger residual component will overwhelm the synthetic dataset, making it too noisy; otherwise, a too small  will make the datasets too simple, and all augmentations can achieve great performance, leading to less discriminative in benchmarking.

 Within each synthetic datasets, we use different coefficients in the trend and seasonality functions to simulate different classes, forming a 6-class classification task. In detail, we have two options for the  in trend  (no matter  or ):  or .  For Seasonality, the coefficient in seasonality has a larger effect on signals, for better generalization, we choose three options for the coefficient  in :, , or . For , we use , , or .

By considering two options in  and three options in , we have 6 classes in each of 12 datasets.

Within each class of each dataset, we need a large amount of samples to train deep learning models. In other words, we need intra-class sample variations.  In principle, this variation can be synthesized by changing residual component, or changing the trends and seasonalities. While changing the residual component, the instinct properties of samples within a class are still obvious, making the classification performance very high, losing discriminative. Thus, we add perturbations to the trend and seasonality to formulate the sample variations. % in this work. 

In practice, we randomly select the alpha from two uniform distributions: , and . The range  is calculated from the value 0.2 with a variation range 0.1, in other words,  or . % Simialrity, we randomly select beta from , , and . % To have enough training power, we sampled 1000 samples for each class. 

In summary, we generated 12 synthetic datasets, each dataset contains 6 classes while each class has 1000 samples. The whole dataset cohort has 72,000 samples. We'll release the code for dataset synthetic, the users are free to increase the datasize, or change the setting to get their own datasets. 

% [ht]% \centering% % % {!}{%% }} & {c}{} & {c}{} & {l|}{{*}{}} & {l}{{*}{}} & {*}{} & {c}{} & {c}{} & {l}{{*}{}} \\ \cmidrule(lr){3-6} \cmidrule(lr){10-13}%  &  & {c|}{} & {c|}{} & {c|}{} & {c}{} & {l|}{} & {l}{} &  & {c|}{} & {c|}{} & {c|}{} &  & {l}{} \\ \midrule% {*}{A1} & {*}{0.1} & {*}{0.1  0.3} & {l}{{*}{}} & 0.05  0.15 & - - & 0 & {*}{C1} & {*}{0.1} & {l}{{*}{}} & {*}{0.1  0.3} &  & 3.5  4.5 & 0 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 1 &  &  & {l}{} &  &  & 4.5  5.5 & 1 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 2 &  &  & {l}{} &  &  & 5.5  6.5 & 2 \\%  &  & {*}{-0.3  -0.1} & {l}{{*}{}} & 0.05  0.15 & - - & 3 &  &  & {l}{{*}{}} & {*}{-0.3  -0.1} &  & 3.5  4.5 & 3 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 4 &  &  & {l}{} &  &  & 4.5  5.5 & 4 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 5 &  &  & {l}{} &  &  & 5.5  6.5 & 5 \\ \midrule% {*}{A2} & {*}{0.5} & {*}{0.1  0.3} & {l}{{*}{}} & 0.05  0.15 & - - & 0 & {*}{C2} & {*}{0.5} & {l}{{*}{}} & {*}{0.1  0.3} &  & 3.5  4.5 & 0 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 1 &  &  & {l}{} &  &  & 4.5  5.5 & 1 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 2 &  &  & {l}{} &  &  & 5.5  6.5 & 2 \\%  &  & {*}{-0.3  -0.1} & {l}{{*}{}} & 0.05  0.15 & - - & 3 &  &  & {l}{{*}{}} & {*}{-0.3  -0.1} &  & 3.5  4.5 & 3 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 4 &  &  & {l}{} &  &  & 4.5  5.5 & 4 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 5 &  &  & {l}{} &  &  & 5.5  6.5 & 5 \\ \midrule% {*}{A3} & {*}{0.9} & {*}{0.1  0.3} & {l}{{*}{}} & 0.05  0.15 & - - & 0 & {*}{C3} & {*}{0.9} & {l}{{*}{}} & {*}{0.1  0.3} &  & 3.5  4.5 & 0 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 1 &  &  & {l}{} &  &  & 4.5  5.5 & 1 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 2 &  &  & {l}{} &  &  & 5.5  6.5 & 2 \\%  &  & {*}{-0.3  -0.1} & {l}{{*}{}} & 0.05  0.15 & - - & 3 &  &  & {l}{{*}{}} & {*}{-0.3  -0.1} &  & 3.5  4.5 & 3 \\%  &  &  & {l}{} & 0.45  0.55 & - - & 4 &  &  & {l}{} &  &  & 4.5  5.5 & 4 \\%  &  &  & {l}{} & 0.85  0.95 & - - & 5 &  &  & {l}{} &  &  & 5.5  6.5 & 5 \\ \midrule% {*}{B1} & {*}{0.1} & {l}{{*}{}} & {*}{0.1  0.3} & 0.05  0.15 & - - & 0 & {*}{D1} & {*}{0.1} & {*}{0.1  0.3} & {l}{{*}{}} &  & 3.5  4.5 & 0 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 1 &  &  &  & {l}{} &  & 4.5  5.5 & 1 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 2 &  &  &  & {l}{} &  & 5.5  6.5 & 2 \\%  &  & {l}{{*}{}} & {*}{-0.3  -0.1} & 0.05  0.15 & - - & 3 &  &  & {*}{-0.3  -0.1} & {l}{{*}{}} &  & 3.5  4.5 & 3 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 4 &  &  &  & {l}{} &  & 4.5  5.5 & 4 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 5 &  &  &  & {l}{} &  & 5.5  6.5 & 5 \\ \midrule% {*}{B2} & {*}{0.5} & {l}{{*}{}} & {*}{0.1  0.3} & 0.05  0.15 & - - & 0 & {*}{D2} & {*}{0.5} & {*}{0.1  0.3} & {l}{{*}{}} &  & 3.5  4.5 & 0 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 1 &  &  &  & {l}{} &  & 4.5  5.5 & 1 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 2 &  &  &  & {l}{} &  & 5.5  6.5 & 2 \\%  &  & {l}{{*}{}} & {*}{-0.3  -0.1} & 0.05  0.15 & - - & 3 &  &  & {*}{-0.3  -0.1} & {l}{{*}{}} &  & 3.5  4.5 & 3 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 4 &  &  &  & {l}{} &  & 4.5  5.5 & 4 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 5 &  &  &  & {l}{} &  & 5.5  6.5 & 5 \\ \midrule% {*}{B3} & {*}{0.9} & {l}{{*}{}} & {*}{0.1  0.3} & 0.05  0.15 & - - & 0 & {*}{D3} & {*}{0.9} & {*}{0.1  0.3} & {l}{{*}{}} &  & 3.5  4.5 & 0 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 1 &  &  &  & {l}{} &  & 4.5  5.5 & 1 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 2 &  &  &  & {l}{} &  & 5.5  6.5 & 2 \\%  &  & {l}{{*}{}} & {*}{-0.3  -0.1} & 0.05  0.15 & - - & 3 &  &  & {*}{-0.3  -0.1} & {l}{{*}{}} &  & 3.5  4.5 & 3 \\%  &  & {l}{} &  & 0.45  0.55 & - - & 4 &  &  &  & {l}{} &  & 4.5  5.5 & 4 \\%  &  & {l}{} &  & 0.85  0.95 & - - & 5 &  &  &  & {l}{} &  & 5.5  6.5 & 5 \\ \bottomrule% % }%  Based on the results from 12 synthetic datasets, we can draw comprehensive conclusions about the effectiveness of various augmentations across a broad range of time series datasets. These datasets include those with linear or non-linear trends, those with trigonometric or Morlet seasonalities, and those that combine trends and seasonalities with varying weights.

To evaluate the correctness of the conclusions summarized on synthetic datasets, we evaluate the conclusions on 6 real-world datasets. The 6 datasets cover different classes (from 2 classes to 10 classes), from single channel to 12 channels, from short series (24) to long series (1280), and from diverse scenarios including human activity recognition, heart disease diagnosis, mechanical faulty detection, traffic pedestrian monitoring, electronic demand, and financial market prediction. For datasets with unbalanced classes, we employ upsampling by randomly duplicating instances of the minority class until it matches the number of instances in the majority class, thereby balancing the sample amount in different classes in the training set. We present the statistics of the dataset in Table~ and provide more details in Appendix Section~.

% 

We first evaluate the effectiveness of single-view augmentations. For each of the 8 commonly-used time series augmentations introduced in Section~, we conduct 5 independent training (including pre-training and fine-tuning) in a single augmentation setting and report the average performance. % Note that in this work, we do not discuss combinational augmentations, such as jittering + flipping.

SimCLR and the follow-up studies use double-viewed augmentation in visual representation learning. However, in time series, single-view and double-view augmentations are both widely used and proven empirically helpful.  Here we provide a comprehensive comparison between single-view and double-view augmentation.

In each dataset, we select the three most useful augmentations based on the results from single-view augmentation benchmarking (Section~), then conduct double-view augmentations. There are 6 situations in total (select 2 from 3 augmentations with replacement). Note, the top 3 augmentations and consequenced 6 situations are dataset-specific. 

Random Recommendation refers to randomly selecting an augmentation from the augmentation pool without prior knowledge about the query dataset.

We first rank all the augmentations within each dataset, from Rank-1 to Rank-9 (there are 9 methods in total, including 8 augmentations plus a no-pretraining method). Then, we accumulate these rankings across all 12 synthetic datasets. As a result, augmentations with higher rankings are deemed more popular. Here, a higher ranking indicates a better performance or a smaller numerical rank (e.g., Top 1 is better than Top 2). % For any given query time series dataset, we recommend the top-K augmentations based on their popularity.  Note that the recommended augmentations from popularity-based recommendations remain the same across any query dataset.

% % -------------------------End Methods----------------------------% In this section, we report the experiment results of single augmentation benchmarking on synthetic datasets.  We present the results for the 12 synthetic datasets by categorical groups represented by the prefixes A, B, C, and D.  The datasets with the same prefix exhibit consistent trends and seasonality components, although the weights  and  differ.  % Due to space constraints in the main manuscript, we provide a detailed discussion exclusively only on the  ranked augmentations. A comprehensive table containing results on all augmentations can be found in Table~. % ------------------Results, new version------------------------% % % % 

In this part, we present the findings regarding the trends and seasonalities in the synthetic datasets and their relation to the top augmentations. We first report seasonality then trend. % for the importance% Season is more important Based on the experimental results from dataset groups with the same seasonality component, we can conclude that in these time series synthetic datasets, seasonalities hold greater importance compared to trends. In other words,  More detailed observations are shown below: % Both Dataset Groups A and C rank resizing as the top augmentation, underscoring the importance of the seasonality component as they both share the same compound trigonometric seasonality, though they differ in their trend components (linear and non-linear). % The increased margins (from A1 to A2 to A3, and similarly for group C) exhibit a consistent downward trend, aligning with the decreasing influence of the compound trigonometric seasonality.% On the other hand, in dataset groups B and D (both with Morlet), disregarding the sub-datasets with suffix 3 (very low proportions of seasonality component), for the two sub-datasets with suffix 1, the highest ranked three augmentations are consistently time masking, jittering, and frequency masking. Likewise, for the two sub-datasets with suffix 2, the top three augmentations remain the same as time masking, jittering, and flipping despite the differences in their trend component. % In contrast to the situation observed in dataset groups A and C, these sub-datasets show consistency where the increased margin tends to increase as the weight of Morlet seasonality decreases.% For dataset groups A and B, both characterized by linear trends, we observe . This indicates that % In dataset groups (C and D) with non-linear trend component, no consistent correlations can be drawn from the experimental results. However, a similar observation to the dataset groups with linear trends is noted: 

Next, we report the experimental results of single augmentation benchmarking on real-world datasets. The summarized detailed results can be found in Table~.

Time masking and jittering share the top augmentations, with frequency masking occupying the third-highest rank. The increased margin of top-1 augmentation compared to the no-pretraining baseline is 3.36\%.

Five augmentations have similar performance, with the best augmentation achieving an increase of 1.30\% compared to the baseline performance. This dataset is not sensitive to augmentations.

Resizing, permutation, and time masking rank the top three augmentations, respectively. The resizing achieved an increased margin of 21.64\%, which is the highest margin across all real-world datasets, indicating this dataset is sensitive to augmentations. However, we didn't find reasonable causality for why FD is so sensitive to augmentations. 

Frequency masking, scaling, and flipping are the top three augmentations. The top augmentation improves the baseline by 18.09\%, making it the second-highest increased margin among the six real-world datasets. Moreover, it's worth mentioning that for the MP dataset, all eight benchmarked augmentations demonstrate improved performance, except permutation.

The top augmentation is resizing, providing a performance increase of 3.02\%, while time neighboring and jittering share the second rank. Although the increased margin may appear low and not noteworthy, similar to the situation with the MP dataset, for the ElecD dataset, all augmentations except permutation outperform the no-pretraining baseline.

Flipping, resizing, time masking, and frequency masking are all identified as the top augmentation (with similar performance) for this dataset, with a increased margin of 1.54\%. Furthermore, all eight augmentation methods achieve better performance compared to the no-pretraining baseline.

In this section, we compare the single-view augmentation method with double-view augmentations. Our goal is to assess whether a smaller distance within a positive pair (i.e., augmented view versus original sample) or a larger distance (i.e., augmented view versus another augmented view) is more effective for contrastive learning in time series classification.

 In practice, for each dataset, we first identify the three best augmentations, as shown in Figure~ and Table~. Let's call them Top-1, Top-2, and Top-3, respectively.  For , the best performance is achieved by Top-1. For , the results are symmetrical for the two views, leading to six possible combinations: (Top-1, Top-1), (Top-1, Top-2), (Top-1, Top-3), (Top-2, Top-2), (Top-2, Top-3), and (Top-3, Top-3). Given that Top-1 outperforms Top-2 and Top-3, we reasonably assume that the (Top-1, Top-1) combination is more effective than (Top-2, Top-2) and (Top-3, Top-3), allowing us to disregard the latter scenarios. In summary, for double-view augmentations, we conduct four combinations: (Top-1, Top-1), (Top-1, Top-2), (Top-1, Top-3), and (Top-2, Top-3).

We report the comparison results in Tables~-. We describe  by taking the first row in Table~ for dataset A1 as an example: the no-pretraining (i.e., without any augmentation) F1 is 0.841; the best single-view augmentation boosts the F1 to 0.9715, claiming the increased margin of 13.05\%. We use the (Resizing, Resizing) combination for double-view augmentation (both  and  are generated by resizing but with different parameters), the double-view archives the F1 of 0.918, claiming 7.70\% margin over the no-pretraining baseline. So we know the single-view augmentation (0.9715) works better than the  (Resizing, Resizing) double-view augmentation (0.918). 

In dataset group A, the results of all datasets show that . Both A1 and A2 show some improvement with double-view augmentations over no-pretraining baseline, but pairing the top one augmentation with itself does not consistently yield better performance compared to when it is paired with the second or third best augmentation method.  For Dataset A3, only the (time masking, jittering) combination results in a slight improvement over the no-pretraining baseline, while other pairs present worse performance. This indicates that .

% For the results of dataset group B, we observe the same pattern of performance in B1 and B2 as in A1 and A2. However, for dataset B3, only permutation is better than no-pretraining, so we only consider one double-view of (permutation, permutation). The results show the double-view augmentation obtains better performance than the single-view agumentation (0.3314). % The performance of B3 around 0.33 is not good, probably due to the dominate of linear trend is less discriminative across classes, but still better than random classification (0.167 for a 6-class classification).

The results for Dataset Group B follow a similar pattern as observed in Groups A1 and A2 for B1 and B2. For Dataset B3, permutation is the only augmentation that outperforms the no-pretraining baseline, leading us to focus solely on the double-view of (permutation, permutation). This double-view augmentation obtains better performance (0.3391) than its single-view counterpart (0.3314). Although the performance of B3, at approximately 0.33, is not particularly high (likely due to the less discriminative nature of the linear trend across classes), it still surpasses that of random classification, which stands at 0.167 for a 6-class classification.

In Dataset C1, the double-view combination of (resizing, time-neighboring) performs equally to the single-view of resizing, with both achieving a margin of 25.55\%. For C2, single-view augmentation outperforms all double-view combinations. In the case of C3, the combination of (time-masking, time-masking) shows better performance than the top-ranked single-view. However, in all the datasets, the single-view and the best double-view augmentations perform similarly.

Dataset group D continues to follow the same pattern observed in dataset groups A and B, where single-view augmentation outperforms double-view augmentations.

In summary, in most cases, , while top-to-top dual-view enhancement fails to provide additional performance gains, not even surpassing single-view augmentation in most instances.

We evaluate the effectiveness of recommended augmentations by  which is borrowed from recommendation system studies~.  Recall@K measures the proportion of relevant items found in the top-K recommendations provided by the model. For example, Recall@3 in this work means that: how many among the recommended 3 best augmentations are truly in the 3 best augmentations for the query dataset.

where  denotes the set of recommended K best augmentations while  denotes the set of truly K best augmentations (Section~). % (got by experiments in Section~).

In this study, we consider 8 augmentations, plus the no-pretraining baseline, there are 9 items for recommendation. In the main paper, we report Recall@1, Recall@2, and Recall@3, while presenting all the results (K=9) in the Appendix Table~.

%  We compare three recommendation methods: our trend-seasonality-based, popularity-based, and random recommendations, as detailed in Section~.  Each method generates a list, , of the  top K augmentations.

On the six real-world datasets, we identify the  top K augmentations, denoted as , based on the results in Section~. We then calculate Recall@K using the formula in Eq~. % We report the results in Table~. We provide a concrete example below for better interpretation. Recall@3 = 0.667 = 2/3 means that: 2 out of 3 recommended augmentations fall within the true 3 best augmentations. 

Please note that while we use the popularity-based recommendation as a baseline, it is also one of our proposed methods because the popularity is calculated based on the experiment results we obtained from the synthetic datasets.

% In this section, we present the recall at 3 obtained from three methods of augmentation recommendation on each real-world dataset (Table~). % Recall@K denotes the ratio of recommended Top K augmentations over the true Top K augmentations. The true augmentations are identified through our benchmarking experiments on real-world datasets. 

Before discussing the results in detail, we walk through the Trend-Season-based recommendation process again, using the real-world dataset  as an example, to help readers better understand the method.  We will clean and organize our implementation thoroughly and then release it to the public.  The  denotes Time Series Augmentation Recommendation Method. 

The key idea is to determine which synthetic dataset ElecD is most similar to. To this end, we follow the six steps designed in Section~.

 Decompose each time series sample in dataset ElecD independently using STL. Then, we take the average of the decomposed trend and seasonality to get the overall trend  and seasonality .

% to obtain its trend  and seasonality . We decompose all the samples independently, then take the average of the decomposed trend and seasonality.  Calculate the similarity between  and , , as well as the similarity between  and , . The similarity values are presented in Table~, which are 0.5339 and 0.4566 for , , along with 0.2179 and 0.1944 for , .

 Calculate the divergence score based on the similarity scores obtained in the previous step. We obtain a trend divergence of 0.1562 and a seasonality divergence of 0.1136, both exceeding the threshold of 0.05, which indicates that the similarities between the two trends/seasons are significant and cannot be ignored.

 Based on the calculations in Steps 2-3,  is similar to  and  is similar to .  Therefore, we can identify that  is the most similar synthetic dataset to dataset ElecD.

 Calculate the power of the trend and seasonality components of dataset ElecD, and we obtain  and . % to be 0.1431 and 0.5752 respectively.  Further, , we determine the decomposition weights of trend and season components to be  and .

 Based on the decomposition weights determined in Step 5, in dataset group A, we select  as the synthetic twin dataset, and use augmentations from A1 (from Table~) to make recommendations for dataset ElecD.  % The 3 best augmentations for A1 are Resizing, Jittering, and Time masking . % A1: Resizing, Jittering, Time masking% ElecD: Resizing, Jittering, Time-neighboring, %   Next, we detail our recommendation process and the calculation of Recall@K as follows: % If the user need only the best augmentation, we reccommend the top augmentation from dataset A1, which is resizing. The resizing is consistent with the true top augmentation for dataset ElecD (see Table~), resulting in Recall@1 = 1. % %% If the user requires to get two augmentations, we suggest the 2 best augmentations from A1: resizing and jittering. They also match the top two augmentations in ElecD, resulting in recall@2 = 1.% %% Further, if it is necessary to recommend three augmentations, we recommend resizing, jittering, and time masking. Two out of three among them fall in the ElecD's truly best three augmentations, resulting in recall@3 = 0.667. % From this example, we show how we calculate the Recall@K for all other real-world datasets through a similar process. 

Next, let's extend to all real-world datasets. Among the six datasets, as shown in Table~, HAR and FD have smaller trend divergence scores than the empirical threshold (0.05), so we only focus on seasonality. For MP, both the trend and seasonality divergence scores are smaller than the threshold, which means that MP is not similar to any of our trends or seasonalities: indicating the guidelines provided in this paper do not apply to MP. Thus, we calculate Recall@K for five datasets. 

As shown in Table~, our Trend-Season-based recommendation significantly outperforms both random and popularity-based methods. Although Popularity-based is obviously better than Random recommendation,  by a great margin: 40\% absolute improvement in Recall@2 and 13.4\% in Recall@3. % In detail, when recommending the top augmentation, our method accurately matches the best augmentation in 3 out of 5 real-world datasets. In comparison, the popularity-based method achieves 1 match, and the random method has no matches. Moreover, when recommending two augmentations, our method outperforms the other two baselines on 4 out of 5 datasets, with 3 achieving an exact match with the top two augmentations. When recommending the three best augmentations, our trend-season-based method outperforms the random method, and is comparable with or exceeds the popularity-based recommendation across all datasets.

For completeness, we add the evaluation on MP in Appendix Table~. We observe that both popularity-based methods and our approach do not yield results in the top three recommendations, affirming the effectiveness of our divergence score. This points out a limitation: when the query dataset does not closely resemble any of the 12 synthetic datasets we used, the current guidelines become inapplicable. One promising future direction is to increase the coverage and generalizability. Further details are discussed in Section~.

Overall, all experimental results robustly support the effectiveness of our Trend-Seasonality-based method in recommending suitable augmentations for a diverse range of time series datasets for contrastive learning.