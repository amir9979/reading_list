Radiology report generation is an image-to-text cross-modal task, as medical images and radiology reports are in two different feature spaces. Existing methods are more tend to improve the overall performance by extracting refined image features~ or improving the network structure of the text decoder~ and ignoring the gap between modalities. Therefore, we propose textual inversion to reconstruct image representation within the text embedding space to eliminate the spatial gap. In this module, we map image embeddings  to pseudo words . via feeding image features into a three-layered full-connected network, which can be formulated as:

After obtaining the pseudo words, we input them into the text decoder after a series of operations to obtain . We explicitly constrain that the generated pseudo words should be able to represent the image features sufficiently by calculating the contrastive loss between  and the image feature . This optimization process guides the network to generate reports that are faithful to images.

We first perform cross-modal interaction by employing cross attention mechanism~ between pseudo words  and text embeddings . Since pseudo words are derived directly from image features, it is beneficial to align visual and linguistic features through this interaction. This process can be expressed as:

We assume that the pseudo words can compensate for the missing information or correct the redundant information for the text feature . Based on this intuition, we concatenate aligned text feature  with aligned pseudo words . The concatenated features can be fused well through multi-layer perceptron (MLP), and thus we obtained the processed pseudo words , where . It can be expressed using a formula:

The log probability  is obtained by decoding . We then implement self-supervised refinement by calculating contrastive loss between text embeddings  and image features . By minimizing the contrastive loss, we encourage the network to generate  that closely resemble the expression of . After continuous back-propagation and optimization, the generated pseudo words  can adequately represent the image semantics, which is beneficial for the generation of reports faithful to the original images. 

We utilize  to quantify the difference between the generated report and the ground truth~, thus guiding the model to generate reports that are close to the ground truth. The formulation of  is as follows:

The log probability of the output against the target sequence  is obtained by  for the position  of the -th sample. To ensure consistent input sequence length, all sequences are filled to the same length. The mask  indicates whether a real word exists at the position: if present, it is 1; otherwise, it is 0. The log probability of the filled part is set to 0 by multiplying with the mask  to prevent the filled part from affecting . Finally, we normalize  by dividing it with the sum of all mask values  to ensure that the loss value is not affected by changes in sequence length.

In addition to optimizing the network to generate more accurate reports through , we also constrain the textual inversion to generate pseudo words that are close to the image representation through . We obtained the score matrix  by calculating the correlation between image features and text features via dot product, which can be expressed as , where  denotes matrix multiplication. we evaluate the cosine similarity between image features and text features and get the score matrix  of size . We optimize the network by constructing a symmetric cross-entropy loss to maximize the cosine similarity between  real image-text pairs while minimizing the cosine similarity between  unpaired image-text pairs~.

 is a matrix of size , where the elements on the diagonal are 1, indicating positive samples, while the off-diagonal elements are 0, indicating negative samples. The overall loss function  of our network is defined as: . Instead of relying on manually labeled datasets, we leverage contrastive learning to measure the similarity between text and image, guiding the network to optimize parameters for generating reports faithful to the visual content.

 We conducted experiments on two widely-used datasets: the small dataset IU X-ray~ (containing 7,470 chest X-ray images and 3,955 corresponding reports) and the large dataset MIMIC-CXR~ (containing 377,110 images and 227,835 corresponding reports). To ensure consistency and fairness in comparisons, we followed the data processing methods utilized by the three baselines~. After excluding samples without corresponding radiology reports, IU X-ray is divided into training, validation and testing sets with a proportion of 7:1:2~ while MIMIC-CXR is divided according to the official splits~.

 We evaluate  on natural language generation (NLG) metrics including BLEU~, METEOR~ and ROUGE-L~, which are widely used to assess the fluency and accuracy of generated reports. We not only focus on the quality of the generated reports but also on their ability to accurately capture lesions in the images. Therefore, we employ clinical efficacy (CE) metrics to evaluate the detection accuracy of generated reports. CheXbert~ is applied to extract labels of 14 medical observations from reports. Precision, recall and F1 are calculated by comparing the labels of the generated reports with ground truth. 

 To verify the generalization and effectiveness of , we use R2Gen~, R2GenCMN~ and VLCI~ as the baseline models in our experiments. These baseline models are improved with , and the results are compared with the original baselines, as shown in Table~. Experimental results show that all metrics are enhanced by improving networks with , which indicates that  can eliminate the gap between modalities and generate more accurate reports. It is remarkable that our approach does not require additional data and can seamlessly integrate into these baselines~, which is of great importance for network migration and practical applications. What's more, we can recognize from the results that prior methods have overlooked the impact of the gap between modalities on radiology report generation. Hence, future research should focus more on cross-modal interactions.

 To explore the effectiveness of each component in  and the rationality of the network structure, we conducted various ablation experiments. First of all, we explored the effectiveness of textual inversion and self-supervised refinement, as shown in Table~. The significance of textual inversion is investigated by calculating the contrastive loss between image features  and text embeddings , while the role of self-supervised refinement is explored through the calculation of the contrastive loss between image features  and pseudo words .

Secondly, we carried out experiments to explore the structure of . We replace the three-layer linear structure with a three-layer transformer encoder to explore the structure of the textual inversion module. It's easy to see that the result is worse than MLP with the same dimension of the hidden layer from Table~. We hypothesize that this is because the medical image features are not complex, so using a transformer may lead to overfitting, and it will also increase computational costs. We investigate the significance of cross-modal interaction and MLP by removing cross attention and MLP from the complete self-supervised refinement network respectively. Pseudo words' significance is investigated by directly incorporating  into the self-supervised refinement network. Furthermore, the importance of decoding text embeddings is explored by computing the contrastive loss between  and . We can speculate that each module plays an important role in generating more refined pseudo words from Table~ since removing any one of them the performance of the network is degraded.

 We draw attention maps to explore the region of the medical image that the word of the generated report is interested in. Fig.~ illustrates that the model improved by  is more sensitive to the correct regions and can generate reports that are closer to the ground truth. This demonstrates that our model can eliminate the cross-modal gap and thus generate reports faithful to images.