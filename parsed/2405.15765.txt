Our customer support dataset consists of tens of billions of tokens of customer support transcripts collected using an in-app messaging interface over the course of Cash App's operational history. These transcripts have been processed to remove PII using an industry standard redaction pipeline. The cases are initiated by a message from a customer and contain a combination of automated system responses and human responses as the support advocate works to resolve the customer's query. Examples of how the dataset is formatted in each phase of our training pipeline are shown in the Appendix.

Even though we redact our data, PII leakage is a common risk with LLMs in deployed applications but it is not a risk for our system as designed for two reasons. The first is that we are using our LLM as a classifier to select the correct template to respond to a customer with; and the second is that all responses are routed through a customer service advocate. 

We annotate each message in the aforementioned customer support transcripts with a "<CUSTOMER>", "<SYSTEM>", or "<ADVOCATE>" prefix to indicate the participant and join the annotated messages together with newlines. For tokenization, we employ the same BPE tokenizer used in the pre-trained Pythia model opposed to any domain-specific tokenization such as in ~. Documents shorter than 2048 tokens are packed together into full length sequences to increase throughput. Shorter documents within the sequence are separated with a special end-of-text token to give the model indication that these documents are unrelated.

During domain adaptation, we generally follow the original pre-training configuration for the model sizes reported in ~ including the use of the AdamW optimizer with  and  values of 0.9 and 0.95, and weight decay of 0.01, learning rate based on the model size, and batch size of 2 million tokens. We employ a linear warmup for 1\% of our total training steps followed by cosine decay to zero. Our complete hyperparameters are shown in Table~. Additionally, we use the Zero Redundancy Optimizer (ZeRO) ~ to efficiently scale training to multiple GPUs.

We pre-train for up to 14500 steps of 2M tokens, reserving 1B tokens for evaluation although due to computational constraints and time considerations, we interrupted the larger models late in training. We save checkpoints every 1450 steps (2.9B tokens) which we use for discriminative fine-tuning.

In this step we fine-tune the domain-adapted Pythia models from the previous step for a sequence classification task. We select 250K random messages on which a customer support advocate chose to use a template reply and consider that as our label. We use a 50\% train/test split at the support case level to prevent leakage if selecting multiple messages from the same support case. The label set comprises 640 unique template responses.

We consider the support case up to the labeled message as the context. We are interested in comparing our domain adapted models with BERT-large which has a maximum context of 512 tokens, so we use a rolling window with an earliest-first truncation strategy to select the most recent whole messages up to the maximum of 512 tokens. We omit the "<CUSTOMER>", "<SYSTEM>", and "<ADVOCATE>" annotations in this case to give more room for support case context based on previous unpublished work that shows these annotations provide minimal information in this task.

We initialize a new linear layer of 640 classes and select the right-most token of each sequence in the batch to pool as input as in ~ and implemented in the transformers library . We fine-tune for one epoch as we have observed additional fine-tuning quickly overfits and evaluate the result with Top-1, Top-3, and Top-5 accuracy.

Fine-tuning hyperparameters are shown in Table~ Our model training pipeline is composed of two phases -- domain adaptation and discriminative fine tuning. The domain adaptation phase requires billions of tokens and can take multiple days to train while discriminative fine tuning requires orders of magnitude fewer tokens and only takes hours. Because domain adaptation is trained using the standard causal autoregressive objective, we find that we are able to reuse domain adapted models across different discriminative fine tuning jobs. We anticipate performing further domain adaptation after 10 billion additional tokens have been acquired, but otherwise update models via discriminative fine tuning to accommodate changes to template usage.

As a baseline, we fine-tune BERT-large as well as customer support domain-adapted variant using the approach described in ~ that is used in several of our other online systems. For details of our domain adaptation using BERT's masked language modeling objective, we refer readers to ~. Given BERT-large generally takes several epochs to converge ~, we fine-tune for a maximum of 10 epochs and report the epoch with the highest test set performance which which in practice, was epoch 9.

% %Comparison between different models%% Top5 Accuracy vs Domain Adoptation Validation Loss% 

Given our two-stage training pipeline, our goal in studying scaling laws is to predict how well a model adapted to a domain using a language modeling objective (stage 1) will perform when discriminatively fine-tuned as a classifier (stage 2). We evaluate this scaling behavior using various metrics that offer insights into performance, efficiency, and behavior across different scales, including

 The number of floating point operations executed during the language modeling stage. It provides insights into the computational complexity of the LLM and how it scales with model and dataset size.

 The cross entropy between the model's predicted next token and the actual next token in training data, directly optimized by the language modeling objective. Monitoring the loss ensures that the model is converging toward an optimal language model of the data.

 The cross entropy between the model's predicted class probabilities and the ground truth label, directly optimized by the discriminative fine-tuning objective. It provides a measure of how well the classifier can differentiate between classes.

Figure~ depicts properties of the scaling laws observed in our experiments. In Figure~ (a) we plot classification loss as a function of language modeling FLOPs for each of the model sizes we adapted. We observe overlap across the three model sizes tested such that for a given compute budget such as  FLOPs,  larger models exhibit lower training loss, suggesting their ability to learn complex patterns and structure within the training data. While we do not fit a formal scaling law here, the observation is consistent with compute optimal language modeling ~ applied to discriminative fine-tuning.

In Figure~ (b), we plot the classification loss as a function of number of tokens seen during our domain adaptation stage. We observe scaling as a function of number of training tokens with larger models exhibiting lower classification loss for the same number of training tokens. We also observe a linear relationship between supervised classification loss and the number of training tokens across model sizes. These results suggest that even the largest model we trained will still benefit from more data. We find this encouraging because it means we are able to produce better classifiers when discriminatively fine-tuned, despite our practical parameter-limitations~.

We relate language modeling loss to classification loss in Figure~ (c). We observe a linear relationship between language modeling loss during domain adaptation and classification loss during discriminative fine-tuning across model sizes. This means despite the different training objectives in these two stages of our pipeline, we can easily predict how amenable a particular LLM is to discriminative fine-tuning from its language modeling abilities. One reason for this may be due to our choice of pooling during discriminative fine-tuning using the right-most token of the input sequence. This makes our fine-tuning objective to minimize . If we consider class  to be a special "class token", the fine-tuning objective reduces to the standard language modeling objective assuming all other regular subword tokens  are masked.

In Table~ we compare the classification accuracy for different model sizes with BERT-large serving as a baseline. The results span a wide accuracy range of more than 10\% between the weakest model (BERT-large) and the strongest model (Domain Adapted Pythia-2.8b), highlighting the impact of scaling data used for language modeling as well as increasing model size. Domain adaptation regularly improves the accuracy of a model by approximately 4-5\%, which is a larger uplift than that which results from increasing the model size at the scales tested here. We observe that the Pythia models outperform BERT in the roughly parameter equivalent case of BERT-large (310m) compared to Pythia 410m with Pythia 1.4b outperforming BERT-large with domain adaptation. This comparison not only underscores the impact of model and dataset size on accuracy, but may also point to the nature of the language modeling objective in determining discriminative fine-tuning performance. BERT-based models are pre-trained with a masked language modeling objective that only covers 15\% of tokens while the causal language modeling objective used in the Pythia models predicts every token in the sequence which may make language modeling more sample efficient ~ and allow use of larger datasets. Finally, while we limit the sequence length to 512 tokens during discriminative fine-tuning to ensure a fair comparison between BERT-large and Pythia models, the Pythia models can make use of up to 2048 tokens of context during language modeling to learn longer-range dependencies.

% TODO: perhaps add params to this table (?)  -- could also compare with deberta 1.5b?% vanilla runs% 410m {'eval_loss': 2.365234375, 'eval_top_1_accuracy': 0.454328, 'eval_top_3_accuracy': 0.669576, 'eval_top_5_accuracy': 0.750192, 'eval_runtime': 187.839, 'eval_samples_per_second': 665.464, 'eval_steps_per_second': 5.201, 'epoch': 1.0}                                                % 1.4b {'eval_loss': 2.111328125, 'eval_top_1_accuracy': 0.486392, 'eval_top_3_accuracy': 0.708704, 'eval_top_5_accuracy': 0.788888, 'eval_runtime': 522.5615, 'eval_samples_per_second': 239.206, 'eval_steps_per_second': 1.87, 'epoch': 1.0}  % 2.8b {'eval_loss': 2.025390625, 'eval_top_1_accuracy': 0.499608, 'eval_top_3_accuracy': 0.723272, 'eval_top_5_accuracy': 0.803088, 'eval_runtime': 1019.7304, 'eval_samples_per_second': 122.581, 'eval_steps_per_second': 3.831, 'epoch': 1.0} % Prediction Time Histogram -- TODO: convert x-axis to milliseconds%% Prediction Time Histogram -- TODO: convert x-axis to milliseconds

We also quantify the latency of a typical forward pass for the model sizes we consider which is important for deployed use cases that contain a human in the loop like customer support. While transformer latencies are well-described by the model architecture itself including number of parameters ~, we compute them empirically for our observed sequences via load test in our production environment. We optimize each model using TensorRT on an NVIDIA A10 GPU with FP16 support and then conduct a load test for five minutes sampling from a distribution of 10000 input sequences. The load test makes requests of batch size 1 at a given rate to our end-to-end inference service that includes truncation to a maximum length of 512 tokens. Table~ lists the peak 1-minute average, P99, and max latencies in milliseconds observed during the load test. We observe that Pythia-2.8b quickly saturates a single GPU at higher levels of concurrency leading to increased tail latency. Because our production system contains a human in the loop that is shown the model's predictions, we establish a latency budget as to not negatively impact their workflow. For a latency budget of 100 ms, Pythia-2.8b will need to be scaled to an additional GPU for each additional request per second, which we view as impractical for most industrial use cases. In contrast, Pythia-1.4b on a single GPU can handle 5-10 requests per second and Pythia-410m fails to saturate a single GPU at the request rates considered here. While larger models may thus appear to be less well-suited for use cases with low latency budgets, we see promise in the relatively smooth linear scaling of classification loss with number of training tokens (Figure 3, for example Pythia-1.4b trained over approximately 27B tokens has approximately the same classification loss as Pythia-2.8 trained over approximately 3B tokens). Therefore domains with a large amount of unlabeled data may be able to utilize smaller models trained for longer or even by repeating data~.

We now turn to experiments that we have performed in our online system that uses the models described in preceding sections to surface the most appropriate templates to customer support advocates. Whenever a new message is sent during a customer support case, the model considers the most recent conversation context (up to its maximum context length) and returns the top-5 highest probability templates that are part of its training set as candidate classifications. To evaluate the effectiveness of the system, we remove these predictions from a randomly selected 2\% of advocate-support case interactions which allows us to determine how often an advocate would choose one of our predicted templates when they are working on a given support case as well as the effects of the system on other business metrics.

Our primary metrics of interest are related to customer support efficiency that do not affect our customer-facing support experience in any noticeable way. A simple first order metric to optimize for is the amount of time it takes an advocate to choose the correct template response based on their training. We find that we have reduced the selection time by 7.38 seconds on average over the lifetime of the system and in general, are able to continually improve on these savings (Figure ). These selection time savings correspond to a 3.56\% total time reduction over the course of an entire support case, a substantial savings when considering the support footprint required for Cash App's tens of millions of active users. During the initial launch of the system, we conducted an internal audit and found no evidence that this time reduction affected customer support standards.

New response templates are frequently introduced and existing ones are deprecated, so we need to retrain our model in a discriminative manner. To date, we have retrained our model four time and before releasing the model, we A/B test over a shorter two-week period to determine the effect on our efficiency metrics. Over the lifetime of each model, we find that we are consistently able to keep the time it takes to select the correct template with our predictions at approximately 13 seconds, in contrast to 19 seconds without templates (Figure ). A/B tests indicate that we are able to significantly decrease selection time as a function of retraining (Table ). Together these results are consistent with the overall increase in selection time savings over the total system lifetime (Figure ).

% v1 vs v2: selection time decrease -0.4532838, t = -6.227002835, df=3236252, scipy.stats.t.sf(6.227002835, 3236252) * 2 = 4.755005641820159e-10% v2 vs v3: selection time decrease -0.4532838, t = -6.124634679, df=7443204, scipy.stats.t.sf(6.124634679, 7443204) * 2 = 9.089642730273674e-10% v3 vs v4 selection time decrease -0.733725084, t =-10.79425063, df=5698811, scipy.stats.t.sf(10.79425063, 5698811) * 2 = 3.666616088481634e-27 During the training phase of our model, we prioritize accuracy as an offline evaluation metric. However, in the online phase, we aim to optimize response selection time. Therefore, we seek a simple relationship between the two to inform model selection. To investigate this, we computed the accuracy of our model in our holdout that removes model predictions and compare it to the selection time savings between treatment and holdout groups for the (top ) response templates by volume. We observe a clear positive relationship validated by Mann-Kendall test such that as the prediction accuracy improves, the average time saved on selection tends to increase (Figure ). Accurately predicting the most frequently occuring response templates is expected to significantly reduce our selection time. However, accurately predicting new response templates shortly after their introduction, as customer support advocates familiarize themselves with them, is also important for optimizing selection time.