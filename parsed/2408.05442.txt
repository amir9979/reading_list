The document  contains a substantial amount of irrelevant information, so the first task is to filter this out and identify all relevant conditions . By leveraging the powerful language comprehension capabilities of LLMs, we can instruct the model to directly identify the locations of conditions in the document.

In addition to identifying the conditions, it is crucial to arrange them in a particular logical order according to the document. We address this by instructing models to construct condition groups , where  is the total number of condition groups in the document. The -th condition group  is defined as , where  are the conditions in the -th group,  is the logical operator connecting them and  represents the fulfillment of condition  that will be determined in the next step. We parse the model's output to obtain , ultimately forming a compositional logical expression of conditions , where . The solving result of  and  are in , where  denote the  answer and   answer.

Besides, the conditions in the document are usually free-form, making it difficult to thoroughly separate a condition from other irrelevant context. Simply truncating or slicing the document may result in incomplete conditions. For example, in Figure~, the condition  lacks a subject, which needs to be extracted from the previous sentence, . This incompleteness could prevent the model from correctly understanding the meaning of the condition, consequently hindering its ability to accurately verify the fulfillment of the condition.

To address this problem, we take context-based augmentation after identifying condition's locations. This approach allows us to obtain a short paragraph for each condition, containing all the necessary additional information. Specifically, we employ two augmentation methods: leveraging the structural information of the document or using transcription. When the document has a certain structure, such as HTML tags for each paragraph, we use this to find the relevant context for augmentation. We take the entire subsection where the condition appears as the augmentation paragraph, ensuring it contains enough background information while being much shorter than the entire document. When there is no structural information available, we instruct the model to directly transcribe the condition based on the context.

After acquiring all condition groups and augmenting each condition, we instruct the models to verify the fulfillment of each condition sequentially. This involves taking the question, scenario, and augmented condition as input, and leveraging the powerful reasoning capabilities of LLMs to determine the status of each condition. For each condition , the verification process can be formalized as determining the value of function . Here,  means the condition is satisfied by the user,  means it is contradicted, and  means the condition is not mentioned. Conditions that are either satisfied or contradicted lead to a  answer, while conditions that are not mentioned result in a  answer. Therefore, the solving process of the expression  can be seen as boolean operations on true/false values .

After obtaining the verification result  for each condition , we need to recompose these results into groups  and logical expression , and follow the document to determine whether the answer is  or . And if the answer is , all missing conditions should be listed along with the answer.

Traditionally, this is done by prompting models to implicitly reason and resolve the logical expression. However, recent studies have shown that even large language models struggle with logical or mathematical reasoning tasks~. Therefore, a better solution is to offload the computation process to an external symbolic interpreter~. In chain of condition, we use a Python interpreter to solve the logical expression . For  answers, we also identify all missing conditions  by . This approach reduces model inference costs, improves precision, and enhances interpretability.

After obtaining the complete result for the conditions, we instruct the models to generate the answer. Since we have already verified each condition's fulfillment, we can leverage this information for more accurate answer generation. Specifically, we add these conditions  along with their fulfillment  into the prompt. This provides the model with straightforward information about the conditions, reducing the need to repeatedly infer their fulfillment from the document. Additionally, these conditions help the model locate relevant paragraphs about the question in the document.

Throughout our experiments, we use two conditional question answering datasets: ConditionalQA~ and ShARC~. More information about these datasets are in Appendix~.

 is a dataset features at long and complex documents, and has many different types of question. The document in ConditionalQA is well-structured, because it is directly crawled from websites and contains HTML tags for each paragraph. This brings the convenience for condition identification and augmentation. 

We use the metrics from the original paper~ for evaluation, which includes two sets of metrics: EM/F1 and conditional EM/F1 (abbreviated as ). EM measures the exact match of predicted answer spans with gold ones, while F1 is the harmonic mean of token-level precision and recall. Conditional EM/F1 jointly measures the correctness of answer spans and the predicted conditions, providing a more comprehensive assessment of a model's performance on the CQA task. The exact metric computation functions are in Appendix~.

 is a conversational QA dataset, and the original task is to answer the question if the information in the dialog history is enough, or to generate a new question to acquire missing information. We follow the previous work~ to isolate the QA task from the conversational setting to form a benchmark of the CQA task, resulting in a dataset that the model only needs to answer "yes", "no" or "not enough information". Additionally, we discard all irrelevant questions from the dataset for better measurement. 

We evaluating model's accuracy on the ternary classification about the answer. Since there are no human annotated conditions in the dataset, so it is not possible to further measure the accuracy of missing conditions predicted by the model, and we leave the more precise evaluation of ShARC as future work.

We compare our approach, chain of condition, with 4 different prompting baselines in total. 

~ is the only existing approach for prompting models for the CQA task as far as we know. This method extend the original text prompt with additional LLM-generated codes, which elicits the model's conditional reasoning abilities for CQA tasks.

~ is a recently proposed, well-performing prompting method, and we adapt it for the CQA task. This method decompose the question by explicitly asking and answering intermediate questions until reaching the final answer and missing conditions.

Additionally, we use  prompting and  prompting~  as our baselines.

The supervised baselines for ConditionalQA include ~, ~, ~, ~, ~, and ~.

For ShARC, since we follow previous work to modify the dataset's output format and discard all irrelevant instances, no available supervised baselines exist. Therefore, we only compare chain of condition with other prompting baselines mentioned above.

We conduct our experiments on four different large language models (LLMs) to investigate whether chain of condition performs consistently better across various settings. We use a commercial model, GPT-3.5-Turbo, and three open-source models, Llama-2-70B-chat, Llama-2-13B-chat, and Mistral-7B. Additionally, we leverage GPT-4~ for limited experiments exclusively on ConditionalQA due to cost constraints. For all models, we set the temperature to 0.0 to ensure reproducibility of the results, while using default settings for others.

The original documents in ConditionalQA can be up to 9320 tokens long, exceeding the context limitations of many LLMs, posing a challenge for all prompting methods. This could be solved by introducing a retriever to retrieve only relevant paragraphs of the document. Therefore, to address this issue and eliminate the interference from retriever performance in our experiments, we use an oracle retriever to select relevant passages for the question. We follow the methodology of previous work~ by retaining all sections that include at least one human-annotated gold evidence and concatenating them to form the input.

And for comparison with supervised methods, we employ two approaches: (1) Using a retriever to retrieve relevant paragraphs from the document, and (2) Using a long-context version of an LLM as our backbone model. The results of these approaches will be discussed in Section~.

See more setup details in Appendix~.

We report the performance of chain of condition and all baselines on two benchmark datasets. Table  presents the performance of all prompting methods on ConditionalQA, while Table  shows the performance on ShARC. Table  compares the results of chain of condition with all supervised baselines on ConditionalQA.

The original evaluation script of ConditionalQA provides not only the overall result but also a detailed breakdown by question type in the dev set. We report the overall results here, with more detailed results available in Appendix~.

 It surpasses all other baselines on both datasets, establishing a new state-of-the-art. Additionally, Self-Ask also performs relatively well on ShARC, which can be attributed to the dataset's features. The conversational format of ShARC is naturally suitable for leveraging Self-Ask, making it reasonable for it to perform better than Zero-Shot or Chain of Thought prompting.

 With backbone models like GPT-3.5-Turbo or GPT-4, it surpasses all supervised baselines with few-shot settings. This result highlights the promising future of prompting methods for the CQA task, not only achieving better performance but also reducing the costs for fine-tuning.

Moreover, the performance of prompting GPT-3.5 with retrieved passages is much lower than prompting GPT-3.5 with gold evidence or prompting GPT-3.5-16k with the full document. This indicates that the performance of the retriever is a bottleneck that limits the effectiveness of chain of condition. It also suggests the substantial potential of it when augmented with a better retriever.

In this section, we first conduct ablation studies with GPT-3.5 on the chain of condition framework to demonstrate the necessity of each step. Next, we show that chain of condition consistently outperforms all baselines in more challenging task settings, and finally analyze the reasons for its superior performance.

 The importance of this step lies in two aspects. First, it ensures the model identifies all possible conditions and  can explicitly solve the logical expressions by external tools, improving performance on conditions. Secondly, it allows us to generate condition-aided answer, which is only feasible if all conditions are explicitly identified. 

To prove the first hypothese, we conduct an ablation study on ConditionalQA because it has gold-labeled missing conditions. In this study, we prompted GPT-3.5 to first identify all conditions, then check their fulfillment, and finally indicate all unmentioned conditions implicitly through reasoning. As shown in Table~, this ablation results in a drop of 3.2 EM score and 2.8 F1 score for answer measurement, as well as 17.4 EM score and 19.5 F1 score for joint answer and condition measurement. The performance drop is much greater when measuring both the answer and condition compared to measuring the answer alone, which indicates that removing the condition identification step leads to a much larger decrease in accuracy for conditions. Further investigation into the model's output reveals that the average number of predicted missing conditions for  answers by the model increases from 1.27 to 1.67, suggesting that the model tends to judge conditions as not mentioned by the user more frequently when the condition identification step is omitted.

We also conduct an ablation for the necessity of using both logical operators "and" and "or". We remove each of them and prompt the model using chain of condition. The results in Table~ indicate removing either operator reduces the performance.

The discussion of the second hypothesis is covered in the ablation study of the answer generation step in the following paragraphs.

 The removal of contextual information can hinder the model's ability to correctly understand the meaning of a condition. To prove this, we leverage the structured document of ConditionalQA to conduct an ablation study. We remove all other paragraphs of the condition's subsection, keeping only the original condition as input for verification. The result of conditional EM drops by 2.4 from 52.9 to 50.5, and the conditional F1 drops by 2.5 from 61.0 to 58.5 for this setting on GPT-3.5, indicating that the performance of condition prediction decreases due to reduced verification accuracy. Thus, condition augmentation would improve verification precision.

In this ablation, we remove the verification results of conditions from the input of answer generation. The results are shown in Table~. The performance drops by 3.2 EM score and 2.8 F1 score for answers, and by 2.4 EM score and 1.9 F1 score when jointly measuring answers and conditions on ConditionalQA. Additionally, the accuracy drops by 2.7 on ShARC.

Furthermore, we find that the performance drop on ConditionalQA is mostly attributed to the yes/no type questions, with a drop of 7.0 EM/F1 score and 5.3 conditional EM/F1 score. The likely reason for this phenomenon lies in the answer determination procedure: an extracted span-type answer can be found directly in the document even without verifying any condition. However, a yes/no answer must be inferred from the document along with each condition's fulfillment. Therefore, including the conditions' fulfillment in the prompt helps the model by reducing the need to repeatedly infer their fulfillment, allowing it to directly synthesize the information to generate the final answer. %'EM': 0.8741258741258742, 'EM_with_conditions': 0.6712731712731713 -> 80.4, 61.8 Most questions in these CQA datasets involve identifying and solving conditions, but only a small portion of them are truly . This is because, in many cases, the conditions for the answer are all satisfied by the user's scenario, so the model only needs to give a correct judgement on whether the answer is . However, when we consider only the  answers in the dataset, correctly addressing them becomes more challenging. This is because the model not only needs to properly generate the answer and determine it is , but also precisely indicate the missing conditions.

The experimental results support this intuition. The performance of all methods on ConditionalQA greatly drops when considering only the  answers, as shown in Table~. Besides, chain of condition consistently outperforms other prompting baselines in this setting, demonstrating its effectiveness in indicating missing conditions. 

Furthermore, in order to analyze the reasons behind chain of condition's superior performance on the CQA task, we divide the dev set of ConditionalQA based on the total number of gold conditions for each question in the document, resulting in two question groups. The first group contains data with at most one conditions, while the second group has at least three conditions, indicating a more complex set of conditions for solving.

We report the performance of GPT-3.5 with all prompting methods on these two groups in Table~. Since there isn't a metric that directly measures the quality of predicted conditions, we additionally report the F1 score of the predicted conditions. The results highlight the increased difficulty of questions involving complex conditions, and chain of condition shows much less performance degradation in this more complex group. This indicates its superior ability to handle complex conditions. We attribute this to the explicit identification of conditions and the use of a code interpreter to resolve the logical relationships between conditions.