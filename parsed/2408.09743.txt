% X-ray medical report generation has attracted a great deal of attention in recent years, especially as Large Language Models (LLMs) are booming in the field of Natural Language Processing (NLP). The combination of medical report generation and LLMs has become the most popular research direction in report generation. % %% % Existing X-ray medical report generation models can be divided into CNN (Convolutional Neural Networks)-based, RNN (Recurrent Neural Networks)-based, and Transformer-based frameworks. To be specific, Li et al.~ propose a model that combines CNNs and RNNs to generate medical reports from chest X-ray images. Jing et al.~ develop a medical report generation model based on the LSTM~ (Long Short-Term Memory) framework. They first predict the possible diseases using LSTM and then generate medical reports based on those predictions. Chen et al.~ demonstrate the effectiveness of generating detailed and accurate radiological reports from X-ray images using the Transformer model, leveraging visual and text data to improve performance. Wang et al.~ pre-train a ViT model on the high-resolution X-ray images using masked auto-encoder for medical report generation. % As large language models (LLMs) achieve great success in language understanding, generation, and reasoning, current researchers are inclined to integrate medical report generation with LLMs. Specifically, R2Gen-GPT~ proposes a medical report generation method based on LLM which combines the image and text and fed into the decoder Llama2-7B~ for report generation. % RGRG~ applies a practice similar to object detection tasks to medical report generation by using GPT-2~ to generate separate sentences for the detected areas and then reconnect the sentences. % %% % Different from existing works, in this paper, we propose a novel context sample retrieval-guided LLM framework for X-ray medical report generation, thus, our model becomes more sensitive and accurate in predicting diseases. In addition, we adopt the vision Mamba as the encoder and achieve similar performance with the computationally expensive Transformer based vision models. %%  Existing X-ray medical report generation models can be divided into CNN (Convolutional Neural Networks)-based, RNN (Recurrent Neural Networks)-based, and Transformer-based frameworks. To be specific, Li et al.~ propose a model that combines CNNs and RNNs to generate medical reports from chest X-ray images. Jing et al.~ develop a medical report generation model based on the LSTM~ (Long Short-Term Memory) framework. They first predict the possible diseases using LSTM and then generate medical reports based on those predictions. Chen et al.~ demonstrate the effectiveness of generating detailed and accurate radiological reports from X-ray images using the Transformer model, leveraging visual and text data to improve performance. Wang et al.~ pre-train a ViT model on the high-resolution X-ray images using masked auto-encoder for medical report generation. 

% The combination of medical report generation and Large Language Models (LLMs) has become the most popular research direction in report generation. LLMs are the focus of current research which can be divided into two categories: base LLMs and professional LLMs. The most famous early LLM is Google's BERT~, which understands text via a bidirectional encoder, significantly improving the performance of natural language processing tasks by pre-training on large amounts of unlabeled text and then fine-tuning on specific tasks. Meta develops a foundational large language model, LLaMA~, with several billion to several hundred billion parameters. LLaMA significantly reduces computing resources and energy requirements while maintaining high performance, demonstrating broad potential in practical applications. GPT~ (Generative Pre-trained Transformer) series consists of large-scale language models developed by OpenAI for natural language generation and understanding. GPT-3~ is renowned for its 175 billion parameters and its powerful capabilities in both generating and understanding language. % R2Gen-GPT~ proposes a medical report generation method based on LLM which combines the image and text and fed into the decoder Llama2-7B~ for report generation. % RGRG~ applies a practice similar to object detection tasks to medical report generation by using GPT-2~ to generate separate sentences for the detected areas and then reconnect the sentences. % For the LLMs developed for medical domains, MedicalGPT~ is a healthcare language modeling project based on the ChatGPT training process that includes incremental pre-training, supervised fine-tuning, RLHF (Reinforcement Learning from Human Feedback), and DPO (Direct Preference Optimization). Developed by Google, Med-PaLM2~ is a medical language model that combines an improved foundational language model (PaLM 2~), domain-specific fine-tuning for the medical field, and new prompting strategies, including new integrated refinement methods. Med-PaLM2 improves performance by over 19\% compared to Med-PaLM~ and has reached a new state-of-the-art level. MedVersa~ is capable of handling multimodal medical inputs and outputs, supporting real-time task specification. It leverages large language models as learnable collaborators, learning simultaneously from both visual and language supervision, and performs multifaceted medical image analysis across various clinical scenarios. LLM in conjunction with medicine is very promising. Inspired by these LLMs, we propose to further improve the quality of X-ray medical reports via large language models in this paper. 

The combination of medical report generation and Large Language Models (LLMs) has become the most popular research direction in report generation. LLMs are the focus of current research which can be divided into two categories: base LLMs and professional LLMs. The most famous early LLM is Google's BERT~, which understands text via a bidirectional encoder, significantly improving the performance of natural language processing tasks by pre-training on large amounts of unlabeled text and then fine-tuning on specific tasks. Meta develops a foundational large language model, LLaMA~, with several billion to several hundred billion parameters. LLaMA significantly reduces computing resources and energy requirements while maintaining high performance, demonstrating broad potential in practical applications. GPT~ (Generative Pre-trained Transformer) series consists of large-scale language models developed by OpenAI for natural language generation and understanding. GPT-3~ is renowned for its 175 billion parameters and its powerful capabilities in both generating and understanding language. 

For the LLMs developed for medical domains, R2Gen-GPT~ proposes a medical report generation method based on LLM which combines the image and text and fed into the decoder Llama2-7B~ for report generation.  RGRG~ applies a practice similar to object detection tasks to medical report generation by using GPT-2~ to generate separate sentences for the detected areas and then reconnect the sentences. MedicalGPT~ is a healthcare language modeling project based on the ChatGPT training process. Developed by Google, Med-PaLM2~ is a medical language model that combines an improved foundational language model (PaLM 2~), domain-specific fine-tuning for the medical field. Med-PaLM2 improves performance by over 19\% compared to Med-PaLM~ and has reached a new state-of-the-art level. MedVersa~ is capable of handling multimodal medical inputs and outputs, supporting real-time task specification. Inspired by these work, we propose to further improve the quality of X-ray medical reports via large language models in this paper. 

Due to the high computational cost in the widely used Transformer networks, the State Space Model (SSM)~ is proposed to achieve linear complexity. To be specific, the S4~ model introduces a new structured state space approach through a layered and modular design. This improves the modeling ability of long sequence dependencies and significantly enhances the efficiency and accuracy of sequence modeling. S5~ is an in-depth improvement and simplification of the S4 model, aiming to enhance computational efficiency and ease of use while retaining powerful sequence modeling capabilities. Mamba~ proposes a time-varying state-space model based on a selection mechanism to efficiently model long sequences. With the success of Mamba, researchers have applied it to a variety of research fields. In the field of vision related to medical report generation, the SSM-only approach Vim~ (Vision Mamba) has achieved good results in terms of performance and efficiency, especially for processing high-resolution images. This is accomplished by adding positional embeddings to image sequences and utilizing bi-directional SSMs to compress visual representations. VMamba~ proposes a visual Mamba model with a global receptive field and linear complexity. Its success comes from the Cross-Scan Module (CSM), which scans simultaneously from all four corners of the feature map, ensuring that each element in the feature map integrates information from all other locations in different directions. Mamba-2~ is an improved version based on the Mamba architecture. By incorporating SSD theory and structural attention mechanisms, enhances performance and efficiency while maintaining the advantages of Mamba.  %%%%  Inspired by the linear computational cost, in this work, we propose to encode the X-ray image using a half-precision vision Mamba network and achieve similar performance on three X-ray medical report generation benchmark datasets. 

Retrieval-Augmented Generation (RAG) is a hybrid approach that combines retrieval and generation to enhance the performance of NLP tasks, particularly those requiring extensive knowledge and contextual information.  %%%%  BGFormer~ (Batch-Graph Transformer) introduced a new Transformer architecture called SSA (Structure-constrained Self-attention), which deeply mines the relationships between samples to provide a new method for robust and differentiated data representation and learning.  CricaVPR~ generates more robust image features by correlating multiple images in a batch through an attention mechanism, using cross-image differences like perspective and illumination as cues for feature learning.  CRAG~ introduces a search evaluator to assess the quality of retrieved documents and enhance search results through large-scale web searches. At the core of EgoInstructor~ is the retrieval-augmented module, which utilizes existing third-person video resources to help the model better understand and describe first-person perspective video content.  Retrieval augmentation can significantly improve generation quality and reduce dependence on the size of the training dataset. RALF~ (Retrieval-Augmented Layout Transformer), proposed by Horita et al., enhances the generation process by retrieving layout examples most similar to the input image, thereby overcoming the challenge existing methods face in capturing high-dimensional layout structures when training data is limited. EVCAP~ is a retrieval-augmented image captioning method based on external visual-name memory.  It constructs external memory using object images and names, and generates image captions through a retrieval-augmented model.   %%%%  In the field of medical report generation, RAG can also serve as a clinical decision support tool by combining medical databases and research papers, helping physicians quickly access the latest research on disease diagnosis, treatment options, and drug information. Our extensive experiments on three benchmark datasets support the effectiveness of context samples for the medical report generation task. 

Current widely used Mamba networks are developed based on the continuous State Space Model (SSM). It maps a one-dimensional function or sequence   to   through a hidden state  .  The computing procedure can be summarized as follows: 

where , ,  denotes the state matrix, input matrix, and output matrix. 

As the image and text we processed are discrete data, the aforementioned continuous SSMs needed to be transformed into discrete ones. For example, the S4~ and Mamba model adopts the Zero-Order Hold (ZOH) to realize this goal, i.e., 

where the  is a timescale parameter (also called step size).  Thus, we can reformulate the discrete version of SSM as:  To further strengthen the SSM, Gu et al. propose the Mamba~ which makes the model varying from time-invariant to dependent. And also speed the training and inference using a couple of hardware-aware algorithms. Inspired by the success of Mamba in natural language processing, researchers also adapt it to the computer vision community, e.g., the VMamaba~ used in this paper, and vision Mamba~. We prefer the readers to check the reference~ for more details. 

In this paper, we propose a novel contextual sample retrieval guided large language model framework for efficient X-ray medical report generation. As shown in Fig.~, it can be divided into three main parts, i.e., the Mamba vision backbone, context retrieval module, and large language model (LLM) for report generation. Given the X-ray image, we first extract its visual tokens using the Mamba backbone. Meanwhile, we retrieve context samples (X-ray samples with and without disease) from the training subset based on the input image and embed them into visual and text tokens. Then, the residual tokens which measure the difference between the input and context samples can be obtained via the subtract operator. Finally, we feed the vision tokens, context residual tokens, and prompt statements into the LLM to generate a high-quality medical report. One can note that the proposed framework stands out from existing methods by  and , which enhances feature representation and discriminative learning while maintaining computational efficiency.

In this subsection, we will introduce the R2GenCSR framework from the perspective of , , , and . 

% \noindent    Assume the dataset contains  X-ray images, , where each  represents a X-ray image with  channels, height , and width . For each X-ray image , the corresponding feature map  can be obtained after feeding it into the VMamba backbone, where  and  are the spatial dimensions of the feature map, and  is the number of feature channels. The reason our framework adopts VMamba instead of conventional visual Transformer models, such as ViT and Swin-Transformer, is because the computational complexity of this model is linear (), requiring lower computational resources. As shown in Fig.~, the basic VMamba block consists of Layer Normalization (LN), Linear Layer, DW-Conv layer, SiLU activation layer, SS2D module, and also the skip connections. 

Then, two distinct types of representations are generated based on feature map , i.e., global features  and sequential tokens . Specifically, a 2D global average pooling is applied to  over the spatial dimensions to get the global features . The sequential tokens  are obtained by flattening the  along the spatial dimension, then, processed by projection layer (Proj) and layer norm (LN) operations.  Mathematically speaking, 

Here, global features  capture the global information of the X-ray image, meanwhile, the sequential tokens  learns the channel representations. 

% \noindent  Given the visual tokens of the input X-ray images, one can directly feed them into the large language model to generate the medical reports, clearly, this approach will still reach its performance bottleneck. The context learning methods~ suggest that other samples in the training set may still play a positive role in each round of model optimization. Therefore, we consider designing a method to mine samples that are positively and negatively correlated with the current sample to enhance discriminative feature learning, thereby better guiding the LLM to generate accurate medical reports.

For each sample in a mini-batch, we can retrieve its context samples based on keywords in the medical reports. In our implementation, we exploit two different approaches: 1). We adopt the CheXbert~ to find the possible 14 kinds of diseases from the annotated medical report. If the sample is annotated as , we treat it as a negative context sample (without disease), otherwise positive (with disease).  2). We find that the medical report with and without  symbol can be roughly divided based on visual features, as illustrated in Fig.~ (e). Similarly, we can treat the context samples with/without  as the positive/negative samples, respectively.  In addition, we also randomly select context samples to augment the training of our proposed R2GenCSR model. 

After the context samples are retrieved, we extract the global visual features of X-ray images as  using the Mamba backbone. These features are then projected into the language space of the LLM using a learnable projection layer, resulting in . This projection aligns the visual features with the text embeddings used by the LLM, facilitating seamless integration of visual and textual information. 

% Note that the Mamba vision backbone is frozen when extracting feature of context samples in each mini-batch. After that, the Mamba vision backbone is unfrozen. Freezing the network to extract context image features at the beginning of each epoch offers several benefits: % 1). It ensures stable feature extraction by maintaining consistent context image features and enhancing training efficiency and model performance. % 2). It reduces computational overhead by avoiding backpropagation during context feature extraction, saving training time and computational resources. This approach mirrors human cognitive processes of stable memory retrieval and difference analysis, making the model's behavior more human-like.  To guide the large language model to generate more accurate medical reports, in this paper, we measure the difference between the current input X-ray sample and context samples, and term the difference as . For each context sample, we assign a disease prompt ``" or ``" and also take the visual-prompt difference into consideration, i.e., 

where  and  denote the projected and tokenized features of the -th positive and negative image, respectively.  denotes the concatenate operation.  and  represent the residual tokens for the positive and negative examples, while  are the residual tokens for the disease prompt.  is the  -th tokenized and projected text token.

% As illustrated in Eq.~, subtracting disease prompts in conjunction with visual tokens can significantly enhance the generation of medical reports: % Firstly, this method bridges the semantic gap between the input image and contextual samples, offering a comprehensive account of the discrepancies. By capturing all contextual differences, it deepens the model's comprehension of the input data. % Secondly, it fortifies the correlation between information by intricately associating visual attributes with textual narratives. This integration not only breaks down information silos but also filters out irrelevant or superfluous details. % Additionally, it serves as a noise reduction technique by accentuating key features and discrepancies, thereby minimizing noise interference and sharpening the focus on critical aspects of the medical reports.   All operations are conducted on high-dimensional language space. Each token, including those in the residuals and the original text, is converted into an embedding vector by the previous step, which ensures all elements of the prompt are represented in a manner that the LLM can effectively understand and process. Therefore, the final prompt for the LLM is constructed by concatenating the residuals and tokenized features as follows:

Here,  and  are the tokenized text and vision token representations of the current image, respectively. This structured input is fed into the LLM to generate the final medical report, incorporating both visual and textual context in a coherent and informative manner. 

% Indeed, the concept of residual information as a "reward signal" is a powerful paradigm in the context of medical report generation. By emphasizing the unique characteristics and differences of the input image, the residual information effectively guides the model to refine its output, thereby elevating the quality of the reports. In the realm of information theory, this residual information can be likened to "compressed information," which distills the essence of the data by retaining only the critical features while discarding the redundancy. This selective retention not only optimizes the model's processing efficiency but also enhances its accuracy by ensuring that the most pertinent information is highlighted.% Moreover, from a cognitive science perspective, the model's focus on detecting changes and differences mirrors the human cognitive strategy for managing complex information. By emulating this aspect of human thought processes, the model becomes more adept at discrimination and adaptability. This alignment with human cognitive strategies not only enhances the model's discriminative ability but also fosters a closer alignment between the model's performance and the expectations of human users, thereby increasing the relevance and utility of the generated medical reports.% The composite prompt effectively combines the contextual differences from both positive and negative samples with the input text, enriches the contextual information available to LLM and enabling the LLM to generate comprehensive and accurate medical reports. By leveraging both visual and textual residuals from context samples, our retrieval mechanism provides the LLM with detailed and nuanced information, improving the quality and relevance of the generated medical reports.% \noindent     The Large Language Model plays a central role in generating detailed and accurate medical reports from X-ray images. As described in the previous sections, the input to the LLM consists of a composite prompt that includes both visual and textual residuals derived from positive and negative context samples, along with the tokenized text corresponding to the input X-ray image. Once the contextual information has been integrated, the LLM generates the medical report. The generation process involves decoding the embedded and contextually enriched prompt into a coherent and comprehensive text.  %%%%  In our experiments, various LLMs are evaluated to achieve higher performance, including lightweight LLM Qwen1.5 (0.5B, 1.8B, 4B, 7B)~, the medium-sized Llama2 (7B)~, Llama3 (8B)~, and large-scale MedicalGPT (13B)~. For even larger LLMs, considering the computational cost, this paper will not consider them for the time being. 

To optimize our R2GenCSR framework, we adopt the cross-entropy loss function to measure the difference between the generated medical reports and the ground truth annotations. Specifically, we apply instruction-tuning to the LLM to generate medical reports, maintaining its original auto-regressive training objective. During this process, the LLM is fine-tuned specifically on the tokens of the medical report, guided by the instruction prompt that encapsulates the visual and textual residuals. Our loss function is defined as the negative log-likelihood of the sequence of report tokens. This can be formulated as:

where  denotes the trainable parameters, and  is the length of the whole medical report.  is the token being predicted at the current step ,  is the instruction prompt that includes the residuals and tokenized features, and  is the sequence of report tokens before the current prediction token . The instruction-tuning ensures that the LLM generates a report that aligns with the provided instructions and context, thus producing a coherent and informative medical report. 

% To integrate these visual features with textual data, we perform the following steps:% % Prior to training, a fixed set of  positive and negative image pairs are sampled from the training set. For each of these  images, we extract the global visual features of these  example images as  using the frozen Mamba vision backbone. These features are then projected into the language space of the Large Language Model  using a learned projection layer, resulting in ,  is a feature space of LLM  language space dimensionality. During training, the Mamba vision backbone is unfrozen. % % The residuals for the positive and negative samples are computed as follows:% % _v &=  - ' \\% _t &=  - ' \\% _v' &= ' - '' \\% _t' &= ' - '' % % where  and  are the projected and tokenized features of the current image respectively, and ,  are the projections and tokens of the positive and negative examples.% % The final prompt for the Large Language Model (LLM) is constructed by concatenating the residuals and tokenized features as follows:% %  &= [_t, _v, _t, _t', _v', _t', , , ]% % Here,  and  represent the text residuals for positive and negative samples,  and  are the corresponding visual residuals, and  and  are the tokenized text and vision token representations of the current image, respectively.% This structured input is fed into the LLM to generate the final medical report, incorporating both visual and textual context in a coherent and informative manner.  We evaluate the performance of our model on three datasets, including ~, ~, and ~ dataset. A brief introduction to these datasets is given below. 

~ is one of the most widely used publicly accessible medical image datasets in the field of medical report generation, which was released on year 2016. It contains 7,470 images and 3,955 radiology reports, each consisting of four parts: indication, comparison, , and . For a fair comparison, we have used the same dataset partition protocols as R2GenGPT and set the training/test/val for the dataset to 7:1:2.

~ is a large publicly available dataset of chest radiographs with free-text radiology reports. These records, comprising 377, 110 radiographic images and 227, 835 radiology reports collected from 65, 379 individuals, span the years 2011-2016 and originate from the Beth Israel Deaconess Medical Center Emergency Department in Boston, MA. For a fair comparison, we used the same dataset partition protocols as R2GenGPT, where 270,790 samples were used to train the model, and another 2,130 and 3,858 samples were used as validation and test sets, respectively.

~ is a large, newly released, organized med dataset with 223K radiology report-X-ray pairs from 64.7K patients, with each report detailed into 11 sections and X-rays in DICOM format with 47 metadata elements. It's annotated for 14 chest conditions and patient metadata. We utilize the  as our ground truth and randomly partition the dataset into a ratio of 7:1:2, which consists of training, testing, and validation sets with 40,463, 5,780, and 11,562 samples respectively. The split protocols for this dataset will be released for other researchers to reproduce our experiments. 

%  For the X-ray medical report generation, we adopt the widely used four metrics for the evaluation, including ~,  ~,  ~, and  ~.  % More details about these metrics can be found in the supplementary material.  Specifically, CIDEr measures the consensus between the generated captions and multiple reference captions. It evaluates the quality of image captioning by computing the cosine similarity between n-grams in the generated caption and those in the reference captions.  BLEU evaluates the quality of machine-generated translations or text summaries by comparing them against reference translations or summaries. It measures the precision of n-grams (usually up to 4-grams) in the generated text compared to the reference texts. ROUGE-L assesses the quality of text summaries or translations by comparing them to reference texts. It focuses on the longest common subsequences between the generated and reference texts, emphasizing recall. METEOR evaluates machine-generated translations or summaries by considering both unigram precision and recall, as well as the alignment between the generated and reference texts. It also incorporates stemming and synonymy matching.

% To assess the efficacy of our method, we employ the Llama2-7B and Qwen1.5-1.8B models as our large language models for the MIMIC-CXR and IU-Xray datasets, respectively. The Vmamba model served as the visual encoder for both datasets. 

In our experiments, the input X-ray image is default resized as , and the beam search is adopted for the report generation. The beam width is set as 5 and 3 for the IU-Xray and MIMIC-CXR datasets, respectively. The training procedure is conducted on a server with NVIDIA A800 80GB GPUs using a mixed precision. We train the proposed R2GenCSR for 20 and 25 epochs on the MIMIC-CXR and IU-Xray dataset. Mini-batch sizes are 36 and 32 for the MIMIC-CXR and IU-Xray datasets, respectively, both trained at a learning rate of 1e-4. The CheXpert Plus dataset adopts the same training protocol as MIMIC-CXR. More details can be found in our source code. 

% To assess the efficacy of our method, we employed the LLAMA2-7B and Qwen1.5 1.8B models as our large language models for the MIMIC-CXR and IU-Xray datasets, respectively. The Vmamba model served as the visual encoder for both datasets. During experimentation, the IU-Xray dataset utilized a beam search size of 5, in contrast to the MIMIC-CXR dataset's size of 3. Training occurred on NVIDIA A800 80GB GPUs with mixed precision, with the MIMIC-CXR dataset undergoing 20 epochs and the IU-Xray dataset 25 epochs. Mini-batch sizes were 36 and 32 for the MIMIC-CXR and IU-Xray datasets, respectively, both trained at a learning rate of 1e-4. The Chexpert Plus dataset adopted the same training protocol as MIMIC-CXR.  As shown in Table~, we compare our results to state-of-the-art (SOTA) methods on the IU X-Ray datasets. It is important to note that the R2GenGPT method's reported results were based on a concatenation of  and  as the testing ground truth, which we believe is not representative of general scenarios so we re-trained their method using only with . Our method demonstrates competitive performance, achieving a BLEU-4 score of 0.206, which surpasses existing methods, highlighting the effectiveness of our context-guided efficient X-ray medical report generation framework. Additionally, our approach attains in the precision of ROUGE-L, METEOR, and CIDEr scores, which are currently at 0.401, 0.412, and 0.579, respectively. These results, albeit not optimal, still affirm the superiority of our approach in generating precise and coherent medical reports compared to current SOTA methods.

As shown in Table~, we report our results on the large-scale MIMIC-CXR dataset. Our method achieved a BLEU-1 score of 0.420, a BLEU-4 score of 0.136, and a ROUGE-L score of 0.291, indicating its ability to generate precise and contextually relevant medical reports. The CIDEr score of 0.267 suggests that the method produces descriptive reports that closely match the reference summaries, highlighting its practical application in clinical settings. Note that our approach employs a dataset-specific strategy, with R2GenCSR-Llama2 being the model variant optimized for the MIMIC-CXR dataset, ensuring that the model is well-suited to the medical report it processes.

% Please add the following required packages to your document preamble:%   As shown in Table~, we re-train three R2Gen series medical report generation models on the recently released CheXpert Plus dataset, including R2Gen~, R2GenCMN~, and R2Gen-GPT~. We can find that the large language model (Llama2) based R2Gen-GPT achieves better performance on all four evaluation metrics. However, our newly proposed R2GenCSR-Llama2 model still beats the R2Gen-GPT and improves 0.001, 0.005, 0.006, and 0.014 on Bleu-4, ROUGE-L, METEOR, and CIDEr metrics, respectively. These experimental results fully validated the effectiveness of our proposed modules for the X-ray medical report generation task. 

% % []%  & 0.416 & 0.263 & 0.181 & 0.132 & 0.250 & 0.290 & 0.165 & 0.135/0.144 \\ \hline% IU-Xray & R2GenGPT-paper & 0.488 & 0.316 & 0.228 & 0.173 & 0.438 & 0.377 & 0.211 & - \\%  & Vmamba(18/90) & 0.476 & 0.313 & 0.227 & 0.173 & {\ul 0.553} & 0.373 & 0.211 & 0.093/0.167 \\%  & Swin(11/15) & 0.465 & 0.299 & 0.214 & 0.161 & 0.542 & 0.376 & {\ul 0.219} & 0.127/0.153 \\%  & Qwen-0.5B(7/15) & {\ul 0.494} & {\ul 0.330} &  &  &  &  & 0.203 & 0.093/0.164 \\  %  &  &  &  & {\ul 0.240} & {\ul 0.177} & 0.463 & {\ul 0.395} &  & 0.142/0.159 \\ \hline% % % %   In the general scenario, we conduct a detailed component analysis of our proposed framework on the IU-Xray dataset to investigate the impact of each key module, including VMamba, the utilization of context information, fixed pair strategy, and the performance of different Language Models (Qwen1.5 and Llama2), as shown in the Table~. The results  and  indicate that with all the component existent can significantly improve the performance across various evaluation metrics (Bleu-1, Bleu-4, ROUGE-L, and METEOR). Specifically, we use Swin Transformer instead while VMamba absent, by comparing  with  and  with  that the VMamba module outperforms the Swin Transformer in Bleu-1, Bleu-4, ROUGE-L, it confirm that Vmamba extracting efficient visual features from X-ray images. When comparing  with  and  with , the utilization of context is shown to facilitate the model in producing high-quality reports. Furthermore, compared  with  and  with , the fixed pair strategy resulting in improved Bleu-4 and METEOR metrics for both Llama2 and Qwen1.5 backends, is contribute to effectively utilizing both positively and negatively samples. The  with  comparison between Qwen1.5 and Llama2 language models reveals that the choice of language model does indeed influence the final performance, with Qwen1.5 generally yielding superior results. Additionally, our method is shown to be versatile and capable of generalizing even when used with other language models. Based on these comprehensive experiments, we can conclude that our method is effective and robust in generating X-ray medical reports.

In this section, we conduct a series of ablation studies to evaluate the impact of various components within our proposed context-guided efficient X-ray medical report generation framework. 

As shown in Table~, we evaluate the Tiny, Small, and Base versions of VMamba on the IU-Xray dataset. As the scale of the VMamba backbone increases, the performance metrics show a consistent improvement. The Base version of VMamba demonstrates the best overall performance, with improvements over the Small version of 0.009, 0.002, 0.001 and 0.003 in Bleu-1, Bleu-4, ROUGE-L, and METEOR, respectively. It confirms that a larger VMamba backbone can capture more intricate visual features from X-ray images, thereby enhancing the quality of the generated medical reports.

 As shown in Table~, we assess the performance of three distinct methods: Chexbert~ is utilized to extract 14 medical observations, we define the images corresponding to reports labeled "No Finding" serving as negative samples, while all other images are classified as positive. Despite its moderate performance, Chexbert provides a baseline for comparison. The Random method randomly retrieves context samples and could slightly improve the generation results. The Keyword method retrieves samples based on keyword matching and is the most effective approach, surpassing the other two methods on all evaluated metrics.

%   As illustrated in Table~,  we observe that conducting visual subtraction alone in the large language model embedding space yields better results than subtraction outside of it, with a 0.004 improvement in Bleu-4. When both visual and context-instructed text residuals are considered, there is a slight improvement in the metrics after the LLM projection space, with increases of 0.008, 0.006, 0.009 and 0.002 in Bleu-1, Bleu-4, ROUGE-L, and METEOR, respectively. This analysis demonstrates the importance of feature subtraction at different stages and confirms that our approach effectively enhances the feature representation.

As shown in Table~, we conducted experiments on the IU-Xray dataset using three image resolutions: 512  512, 448  448, and 224  224. The performance metrics, including Bleu-1, Bleu-4, ROUGE-L, and METEOR, improved by 0.014, 0.010, 0.013 and 0.010 as the resolution decreased from 512  512 to 224  224, respectively. This unexpected trend can be partially explained by the fact that we utilized the Vmamba model which was pre-trained on 224  224 resolution and it is adept at processing and extracting meaningful features from images of this specific resolution. % However, it is crucial to note that while lower resolutions may offer computational benefits, they also pose a risk of omitting critical diagnostic details. % % [1]{\ImgWarpLeft#1\ImgWarpRight}% []{}  As illustrated in Figure , we present the performance among models of varying sizes and architectures, including the Qwen and Llama series models, Specifically, the Qwen1.5-1.8B model demonstrates a notable improvement in metrics such as Bleu-4 and ROUGE-L by 0.206 and 0.401, respectively. However, the Llama3-8B model, despite its larger size, underperforms compared to the Qwen1.5-4B model, even worse than Llama2-7B. The specialized MedicalGPT (Llama-13B) model exhibits competitive performance within the Llama series, showcasing the potential benefits of domain-specific fine-tuning for large language models in medical report generation tasks. Therefore, selecting an appropriate LLM that balances size, architecture, and domain specialization is necessary for optimal report generation performance.

 The Table~ illustrates the performance across different numbers of context sample pairs. The results reveal an optimal point at 3 context sample pairs, which achieves higher scores than 10 pairs, with improvements for Bleu-1, Bleu-4, ROUGE-L, and METEOR of 0.023, 0.024, 0.018, and 0.010, respectively. As the number of context samples increases (from 3 to 10), there is a noticeable decline in performance across all metrics. We suggest that while additional context samples can contribute to a richer feature representation, an excessive number may introduce noise, detracting from the model's ability to generate accurate and coherent reports. The use of a single context sample pair also yields competitive results, but the slight improvement seen with three pairs indicates that a moderate amount of context can enhance the learning process. 

% % \noindent   % % % DarkRed% SeaGreen4 In Table~, we compare the training and testing efficiency of VMamba and Swin Transformer as vision backbones on a single A800 80G GPU. Although R2GenCSR-VMamba has slightly more trainable parameters (91.7 M vs. 90.9 M) and consumes more memory (75723 MB vs. 70203 MB), its training time requires only 3.98 hours per epoch, which is less than the 5.85 hours per epoch needed for R2GenCSR-Swin. Despite similar testing times for both models, VMamba has a lower FLOPs count (1852.35 G) than Swin Transformer (1855.02 G) and the overall efficiency of R2GenCSR-VMamba is superior.

As shown in Table~, we examine the influence of positive-negative context-instructed text and different instruction prompts on our model's report generation quality. For instance, by modifying the context-instructed text from  to , the Bleu-4 score decreased by 0.010. Similarly, by changing the instruction prompt from  to , the Bleu-4 score decreased by 0.007. Although the differences in performance between the prompts are relatively small, optimizing prompts can subtly affect the model's output.

% [h]% % % % % \centering% % % % {!}{%  &0.514&0.206&0.401&0.215 \\ \hline% Note:  normal. Note:  with disease.  &0.514&0.206&0.401&0.215 \\ \hline% Observation:  appears to be normal and healthy. Observation:  shows clear signs of disease.   & 0.503 & 0.196 & 0.384 & 0.209 \\ \hline% Indication:  shows no signs of pathology. Indication:  exhibits symptoms of a medical condition.   & 0.502 & 0.200 & 0.393 & 0.211 \\ \hline% Findings:  reveals a lack of abnormalities. Findings:  reveals the presence of a pathology.   & 0.499 & 0.202 & 0.393 & 0.207 \\ \hline% % Note:  normal. Note:  with disease. & 0.517 & 0.206 & 0.402 & 0.219 \\% Note:  normal. Note:  with disease.  & 0.497 & 0.199 & 0.392 & 0.209 \\ \hline% Note:  normal. Note:  with disease.  & 0.504 & 0.196 & 0.393 & 0.217 \\ \hline% Note:  normal. Note:  with disease.  & 0.502 & 0.203 & 0.388 & 0.213 \\  % %  &  &  &  &  &  &  &  \\ \hline% % Note: \ImgWarp  normal. Note: \ImgWarp  with disease. & 0.496 & 0.337 & 0.250 & 0.196 & 0.379 & 0.209 & 0.557 \\% % Observation: \ImgWarp  appears to be normal and healthy. Observation: \ImgWarp  shows clear signs of disease.  & 0.503 & 0.337 & 0.251 & 0.196 & 0.384 & 0.209 & 0.589 \\% % Indication: \ImgWarp  shows no signs of pathology. Indication: \ImgWarp  exhibits symptoms of a medical condition.  & 0.502 & 0.340 & 0.255 & 0.200 & 0.393 & 0.211 & 0.638 \\% % Findings: \ImgWarp  reveals a lack of abnormalities. Findings: \ImgWarp  reveals the presence of a pathology.  & 0.499 & 0.338 & 0.255 & 0.202 & 0.393 & 0.207 & 0.631 \\% % % Analysis: \ImgWarp  indicates an abnormality associated with a disease.  Analysis: \ImgWarp  indicates a state of good health. &  &  &  &  &  &  &  \\% % % Diagnosis: \ImgWarp  confirms the existence of a medical issue.  Diagnosis: \ImgWarp  confirms the absence of medical issues. &  &  &  &  &  &  &  \\% % Note: \ImgWarp  normal. Note: \ImgWarp  with disease. & 0.517 & 0.355 & 0.265 & 0.206 & 0.402 & 0.219 & 0.565 \\% % -\textgreater Construct a full and methodical diagnostic summary for the chest X-ray displayed. & 0.497 & 0.341 & 0.256 & 0.199 & 0.392 & 0.209 & 0.501 \\% % -\textgreater Develop a detailed and professional medical assessment from this chest X-ray image. & 0.504 & 0.343 & 0.255 & 0.196 & 0.393 & 0.217 & 0.500 \\% % -\textgreater Analyze and generate a detailed report on the findings of this chest X-ray. & 0.502 & 0.343 & 0.258 & 0.203 & 0.388 & 0.213 & 0.639 \\ % \hline% % % }% % % %   The X-ray image and its corresponding feature map are displayed side by side in Figure . It is evident from the feature map that the VMamba vision backbone effectively extracts discriminative visual features from the X-ray image, emphasizing regions of interest such as lesions, organs, and other anomalies. These extracted features supply detailed information for subsequent processing. % Moreover, the introduction of context retrieval further enhances the feature representation, allowing the model to focus on relevant visual cues and ignore irrelevant information. This refined feature extraction process is a key factor contributing to the superior performance of our proposed framework on the IU-Xray, MIMIC-CXR, and CheXpert Plus datasets.  Figure~ illustrates a side-by-side comparison of X-ray images alongside their respective ground truths and the reports generated by our model. It is clear that our method is capable of producing reports that closely align with the ground truth, with only minor discrepancies highlighted in mismatching terms but the overall performance of our framework on the MIMIC-CXR dataset is promising. 

% % } %set default line width to 0.75pt        % [x=0.75pt,y=0.75pt,yscale=-1,xscale=1]% %uncomment if require: \path (0,460); %set diagram left start at 0, and has height of 460% %Image [id:dp9886432403263177] % \draw (50,70) node  {\includegraphics[width=75pt,height=75pt]{1.png}};% % Text Node% \draw (200,85) node   [align=left] {[lt]{136pt}\setlength% % Ground Truth% % Pa and lateral views of the chest provided. there is no focal consolidation effusion or pneumothorax. The cardiomediastinal silhouette is normal. Imaged osseous structures are intact. No free air below the right hemidiaphragm is seen.% };% % Text Node% \draw (400,85.5) node   [align=left] {[lt]{136pt}\setlength% % Baseline% % Pa and lateral views of the chest provided. There is no focal consolidation effusion or pneumothorax. The cardiomediastinal silhouette is normal. Imaged osseous structures are intact. No free air below the right hemidiaphragm is seen. Clips are noted in the right upper quadrant compatible with prior cholecystectomy.% };% % Text Node% \draw (600,85) node   [align=left] {[lt]{136pt}\setlength% % Ours% % Pa and lateral views of the chest provided. There is no focal consolidation effusion or pneumothorax. The cardiomediastinal silhouette is normal. Imaged osseous structures are intact. No free air below the right hemidiaphragm is seen. Surgical clips project over the right upper quadrant of the abdomen.% };% % Text Node% \draw (50.5,9.78) node   [align=left] {[lt]{67.32pt}\setlength% % Original Image% % };% % % %     \centering%     [t]{0.2\textwidth}%         \centering%         \includegraphics[width=\linewidth]{figures/Positive-Negative-3d-view0.png}%     %     \hfill%     [t]{0.2\textwidth}%         \centering%         \includegraphics[width=\linewidth]{figures/Positive-Negative-3d-view1.png}%     %     \hfill%     [t]{0.2\textwidth}%         \centering%         \includegraphics[width=\linewidth]{figures/Positive-Negative-3d-view2.png}%     %     \hfill%     [t]{0.2\textwidth}%         \centering%         \includegraphics[width=\linewidth]{figures/Positive-Negative-3d-view3.png}%     %     \hfill%     [t]{0.2\textwidth}%         \centering%         \includegraphics[width=\linewidth]{figures/Positive-Negative-each-3d-view.png}%     %     %     %    Although our proposed R2GenCSR achieves better performance on three large-scale report generation benchmark datasets, however, our model may still limited by the following issues:  1). We adopt a simple retrieval strategy for context sample mining, more advanced retrieval techniques can be exploited to achieve better performance;  2). The knowledge about the disease is ignored in our R2GenCSR framework, this information may be useful to guide the X-ray image report generation task.  In our future works, we will consider to improve the proposed framework from the aforementioned two aspects.