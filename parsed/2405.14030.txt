In this section, we first try to investigate the existence of spurious features within VLMs through a comparative analysis of zero-shot classification performance on the Waterbirds dataset. To show that spurious correlation impairs the performance of VLMs, we conduct two experiments: zero-shot classification using the original Waterbirds dataset (with natural background) and using a modified version of the Waterbirds dataset from which the background have been erased based on mask.

Figure~ presents the group accuracy changes across these two scenarios. In the first scenario (using original data), the models exhibit uneven accuracies across majority and minority groups, reflecting the unbalanced group robustness across the dataset. When the backgrounds are removed, the accuracy in recognizing  and  (minority groups) boosts, as the confounding elements are no longer able to mislead the model. Despite the relatively small change on majority , we see consistent accuracy drop in majority , across three different architectures. The disparity in performance and drastic accuracy change in both minority and majority group, confirms our hypothesis that visual representations in current models are entangled with spurious features that significantly impair classification performance. This raises an foundational question: does this imply that we are unable to achieve flawless task execution on CLIP representations when faced with spurious features? If not, how?

In order to see the upper limit of CLIP visual representation with linear transformation, we applied deep feature reweighting (DFR)~ to 29 attribute classification challenges (see Table~ for details) on the CelebA dataset, where each attribute demonstrated a gender-biased distribution. Likewise, they also involve four groups based on gender and attribute presence: female without  (), male without  (), female with  (), and male with  (). Following DFR's implementation, a linear layer is attached to the CLIP image encoder to facilitate binary classification, with updates restricted solely to the weights of the linear layer. As a supervised method, DFR usually signifies the peak performance that a linear layer can attain, by strategically adjusting sample weights based on their group frequency to enhance accuracy, particularly for underperforming groups, and reduce the impact of spurious attributes without altering the primary network. High accuracy in these groups suggests that the standard CLIP image encoder successfully captures essential task-related features, not merely relying on these features for predictions under ERM. 

Figure~ showcases the outcomes for various attribute classifications on CLIP ViT-L/14 model, sorted by ascending average group accuracy.  The spectrum of attributes ranged from subtle features like straight hair and narrow eyes to more overt characteristics such as eyeglasses, baldness, and hats. Notably, the DFR approach strategy enabled a majority of the attributes --- over 25 out of 29 --- to achieve more than 75\% worst group accuracy (WGA), with more than 23 attribute classification tasks surpassing 80\% average group accuracy. The five attributes with the highest accuracies exceeded 95\% in both the worst and average group accuracy measures. These results underscore how fine-grained the CLIP's visual representations are, capturing a comprehensive spectrum of visual information, including subtle features that are typically challenging for human perception. Hence, we believe that visual representations learned by CLIP are adept at extracting nuanced features within images for various tasks by linear transformation.

VLMs are trained to align the representations of images with their corresponding captions via cosine similarities. Ideally, one might expect the representation of ``a photo of a dog'' to solely encapsulate the dog's key features without incorporating ambient elements like lawns. However, the examination of real-world data reveals a spurious correlation where dog images are typically associated with outdoor environments, and cat images are often taken indoors. We hypothesize that these contextual features are inevitably embedded in the CLIP text representations.

To test this hypothesis, we examined various prompts and corresponding image pairs, calculating the cosine similarity between them. For instance, we evaluated pairs like (``a photo of a camel''/``a photo of a cow'', desert/pasture images), (``a photo of a polar bear''/``a photo of a panda'', glacier/bamboo forest images), and conversely (``a photo of a pasture''/``a photo of a desert'', camel/cow images), (``a photo of bamboo forest''/``a photo of glacier'', polar bear/panda images), with ensuring the images tested here did not contain the objects mentioned in the prompts. This methodology helps quantify the extent of spurious features embedded in text representations.

Figure~ shows the result. Notably, the cosine similarity distributions, indicated by the  and , reveal strong correlations—for example, the prompt ``a photo of a camel'' with camel-free desert images and the prompt ``a photo of a cow'' with cow-free pasture images. This pattern is consistent across various tested pairs, underscoring the substantial presence of context-related features in CLIP text representations that are not explicitly present in the prompts. For the prompt and image pair with less pronounced correlations, like ``a photo of a dog'' to forest and desert, we did not observe the same level of disparity in mean and median value, see Figure~ in Appendix for more details.

These results suggest that using text representations for zero-shot classification or debiasing with the representations from spurious attribute prompts~ could lead to unexpected outcomes, due to the embedded non-target features.

The results illustrated in Figure~ show that leveraging DFR can recover an optimal task matrix for binary classification. This raises the question: is it possible to identify a text prompt whose representation closely approximates the DFR-trained linear probe? If achievable, this may give us clue of how to reach equivalent performance through zero-shot classification using textual prompting in the future.

In this section, we introduce a text recovery workflow named , a framework designed to identify a text prompt whose representation aligns with a specified target vector, sourced from either CLIP's image/text encoder or even the weights of a linear layer, as depicted in Figure~. The entire framework focuses on optimizing the token embeddings, , which is the only learnable tensor in the framework. We initiate the process with a initial text such as ``a photo of dog'', anticipating that the final recovered text will follow the format ``a photo of '', thereby simplifying our optimization approach by starting with a prompt close to the desired outcome. After passing the  initialized by initial text through the frozen CLIP text encoder, we extract the end-of-text vector, , as our resultant text representation. The similarity between  and the target vector  is measured using the designated loss function

which guides the backward propagation to refine . Upon convergence of the loss,  is mapped back to the tokens most similar in terms of cosine similarity. Finally, the recovered tokens are decoded into a human-readable text prompt using the CLIP token decoder. Reader can refer to Algorithm  and Figure~ for better understanding.

To validate the effectiveness of , we performed 4,537 experiments and show the results in Figure~. For each experiment, the target vectors  are from using CLIP to encode the target text (e.g., ``a photo of a cat''). We initialize the learnable embedding  with the token embedding of various initial text (e.g., ``a photo of a dog''), as random initialization may add extra difficulty to the recovery. Although the final  might not exactly match , we considered a trial successful if the text decoded from the learned token embeddings  contained the target text, and unsuccessful otherwise.

In Figure~, every cell's color in the matshow suggests whether the experiment on the pair (initial text, target text) succeed. The cell filled with  color means that the recovery is successfully performed on the text pair. In the left matshow, a total of 1,936 animal related text pairs were tested, with 82\% of the experiment succeed. 100\% of the animal text can be recovered from at least 26 initial text. In the right matshow, a total of 2,601 food related text pairs were tested, with 80\% of the experiment succeed. 100\% of the food text can be recovered from at least 21 initial text. This demonstrates the efficacy of our  framework in reconstructing text from vectors, robust to different start point or end point in the feature space. 

Since we posit that the linear probe, trained via DFR, precisely captures the core features pertinent to our task, we employed the DFR-trained linear probe as  and utilized  to approximate the ideal human-readable text prompt. We showcase the recovered text from DFR linear probe (landbird vector) in Table~ with various EOT position. Despite these efforts, the results below suggest that isolating core from spurious features using an optimal human-readable text prompt still remains a challenging endeavor. For more result, please refer to Table~ and Table~ in Appendix.

 We describe the dataset details used in our study here:  Our dataset preprocessing steps are consistent across all datasets and models. Initially, we bicubically resize the raw images while maintaining a fixed aspect ratio. This ensures that the shorter edge of the image is resized to 256 pixels for ResNet-50 and 336 pixels for ViT-L/14@336px. Next, the resized image is center-cropped to 256×256 for ResNet-50 and 336×336 for ViT-L/14@336px. Following this, the RGB image is normalized by subtracting the mean pixel value  and dividing by the standard deviation , in line with CLIP's procedure. No additional data augmentation is applied after these steps.

 We utilize CLIP~ as the visual-language model in our study, consisting of two components: a vision branch and a language branch. For the vision branch, we test two popular architectures, ResNet~ and Vision Transformers (ViT)~, specifically focusing on ResNet-50 and ViT-L/14@336px, in line with the setup in~. For the language branch, CLIP incorporates the pre-trained masked language model, BERT~. Following established protocols from prior work~, our experiments are consistently performed with frozen language and image encoder weights, with only the attached linear layer being trainable.

 For DFR, we use the Adam optimizer~ with a weight decay of 0 and a learning rate of 0.01. The ReduceLROnPlateau scheduler is adopted with a factor of 0.5 and patience of 3. The models are trained for 20 epochs with a batch size of 256. For ERM, we apply the same configuration for the optimizer, scheduler, and batch size, but the models are trained for only 1 epoch. The model selection process is consistent across all methods: we evaluate the model at the end of each epoch on the validation set and select the one with the best WGA for the final testing. All accuracy metrics reported in this paper are based on the test set.

 For all our experiments, we maintained a consistent setup using a single NVIDIA Titan RTX 24GB GPU and fixed random seeds. The experiments were conducted using PyTorch 2.0.1+cu117 and Python 3.8.13.

 Worst-Group Accuracy (WGA) represents the lowest model accuracy among different groups  in the testing set, as defined in Section~. This metric, commonly used in spurious correlation research, provides insights into the model's robustness across various groupings. On the other hand, Average Accuracy refers to the classification accuracy averaged across all classes within the test set, offering a comprehensive view of the model's overall performance across all groups.

Let  be a subspace of  and let  be a vector in . We denote the closest vector to  on  by . Let  be the orthogonal decomposition with respect to .

By definition  lies in , where  is the base of subspace  and so there exist a vector  in  such that 

We know that  lies in , we thus have  and so

Suppose that  Then  so  by the previous proof. But  (the orthogonal decomposition of the zero vector is just ), so , and therefore  is in .

Since the columns of  are linearly independent, we have , so , as desired. Let  be a vector in  and let  be a solution of  Then  so 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \newpage% % % \item {\bf Claims}%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The claims made match experimental results, and systematically shows the better expressiveness of the visual representation in CLIP than the text representation% \item {\bf Limitations}%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Section~.% \item {\bf Theory Assumptions and Proofs}%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: This paper does not include theoretical results. %     \item {\bf Experimental Result Reproducibility}%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Figure~, Figure~, and Algorithm~.% \item {\bf Open access to data and code}%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See link in Abstract, Figure~, Figure~, and Algorithm~.% \item {\bf Experimental Setting/Details}%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Appendix~.% \item {\bf Experiment Statistical Significance}%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Figure~ and Figure~.% \item {\bf Experiments Compute Resources}%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Appendix~.% \item {\bf Code Of Ethics}%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The authors have reviewed the NeurIPS Code of Ethics.% \item {\bf Broader Impacts}%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Section~.% \item {\bf Safeguards}%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The paper poses no such risks.% \item {\bf Licenses for existing assets}%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See Sections  and Appendix~.% \item {\bf New Assets}%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: See the link in Abstract and Appendix.% \item {\bf Crowdsourcing and Research with Human Subjects}%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? %     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.% \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.%  Large vision-language models (VLMs), such as CLIP, have become foundational, demonstrating remarkable success across a variety of downstream tasks. Despite their advantages, these models, akin to other foundational systems, inherit biases from the disproportionate  distribution of real-world data, leading to misconceptions about the actual environment. Prevalent datasets like ImageNet are often riddled with non-causal, spurious correlations that can diminish VLM performance in scenarios where these contextual elements are absent. This study presents an investigation into how a simple linear probe can effectively distill task-specific core features from CLIP's embedding for downstream applications. Our analysis reveals that the CLIP text representations are often tainted by spurious correlations, inherited in the biased pre-training dataset. Empirical evidence suggests that relying on visual representations from CLIP, as opposed to text embedding, is more practical to refine the skewed perceptions in VLMs, emphasizing the superior utility of visual representations in overcoming embedded biases. Our codes will be available in . Introductionsec:introradford2021learningdeng2009imagenetschuhmann2021laionbeery2018recognitiondevlin2018bertniven2019probingsagawa2019distributionallyliu2018largePromptCraftVisualDistiller--We show that VLMs like CLIP rely on non-causal spurious features for decision-making, yet linear probing is sufficient to extract key features for various downstream tasks.     --We develop a simple yet effective text recovery framework called  that recovers text from vector embeddings. We find that CLIP's text embeddings are contaminated by diverse elements, making text embeddings impractical for debiasing the model.     PromptCraft--We demonstrate that using visual embeddings from CLIP to distill visual representations is highly effective. The debiased features achieve excellent performance in group accuracy comparable to supervised methods like DFR, which offers a more comprehensive understanding of the distinct capabilities and limitations of CLIP's visual and textual representations. Related WorkMitigating Spurious Correlations in Uni-modality Models.sagawa2019distributionally,geirhos2020shortcutgeirhos2020shortcut,shah2020pitfalls,hermann2020shapeshe2009learning,cui2019classshimodaira2000improving,byrd2019effectsagawa2019distributionally,kirichenko2022last,izmailov2022featuretaghanaki2021robustliu2021just,nam2020learningzhang2022correct,zhang2022contrastive,yang2023mitigatingEnhancing Group Robustness in VLMs.petryk2022guidingzhang2023diagnosingyang2023mitigating, zhang2022contrastivezhang2022contrastiveyang2023mitigatingchuang2023debiasingPreliminariessec:preNotations.sagawa2019distributionallyliu2018largeObjective.     _{}(f_{\theta})=E_{(,,a,g)\sim P} [l(f_{\theta}(), )],

    _{}(f_{\theta})=\max_{g\in }E_{(,,a,g)\sim P_g}[l(f_{\theta}(), )]. Does there exist a task matrix that can achieve optimal task performance?Unraveling Spurious Correlations in Vision-Language Modelsfig:group_accuracy_change_wbAssessing the Expressiveness of CLIP's Visual Representations under Linear Probingkirichenko2022lasttab:celeba_attr_distributionfig:celeba-other-attrCan language unveil the path to the optimal task matrix?Language Representations are not as Pristine as One Might Thoughtfig:cosine-similaritymean_greenmean (green triangle)median_orangemedian (orange line)fig:cosine-similarity-appendixchuang2023debiasingCan we optimize text prompts through inverse problem solving?fig:celeba-other-attrPromptCraftfig:prompt_craft     (_{}, _{})=\|_{}-_{}\|^2_2-\lambda\cdot_{}, _{}\rangle}{\|_{}\|\cdot\|_{}\|},       algo1fig:prompt_craftPromptCraftfig:matshowfig:matshowmygreengreenPromptCraftPromptCrafttab:recovered_texttab:recovered_text_landbird_alltab:recovered_text_waterbird_all\IIf1\State\algorithmicif\ #1\ \algorithmicthen\EndIIf\unskip\ \algorithmicend\ \algorithmicif%using PyTorch\textttalgo1\texttt; \texttt; target vector ; initial text \texttt\texttt\texttt\texttt\texttt\texttt\textttExtracting vector at the end of the token position from the embedding\textttThe loss function follows Eq. (\ref)\textttUpdating the input embedding \texttt\textttFind the closest tokens to the embedding \textttCan image unveil the path to the optimal task matrix?fig:visual_distillerVisualDistiller     _{W^\perp} = (I - B(B^T B)^{-1} B^T) _{}, proofzhou2016placesWahCUB_200_2011tab:waterbirdstab:waterbirdsVisualDistillerVisualDistillertab:celebatab:celeba_other_attrVisualDistillerConclusionssec:conclusionsPromptCraftVisualDistillerLimitations and Broader Impacts.plainnatbibliographyAppendix / supplemental materialImplementation Detailssec:implementation_detailsDatasets overview.The  dataset~ is a popular benchmark for binary classification tasks, specifically designed to examine spurious correlations. By combining the Caltech-UCSD Birds-200-2011 (CUB) dataset~ with backgrounds from the Places dataset~, this dataset challenges models to classify birds as either landbirds or waterbirds, with the background attribute (land or water) potentially influencing the classification. We follow the standard training, validation, and testing splits as described in~.     Waterbirdssagawa2019distributionallyWahCUB_200_2011zhou2016placessagawa2019distributionallyThe ~ dataset consists of over 200,000 celebrity images, primarily used for binary classification tasks. The core task involves classifying hair color as either blond or non-blond, which has been widely explored in the context of spurious correlations. Interestingly, gender emerges as a spurious attribute in this dataset. We adhere to the standard dataset splits as described in~, and this dataset is licensed under the Creative Commons Attribution 4.0 International license. In addtion to hair color, in Figure~, we also test the other attributes spuriously correlated with gender. See Table~ for more details. CelebAliu2018largesagawa2019distributionallyfig:celeba-other-attrtab:celeba_attr_distributionDataset Preprocessing.Model Architecture.radford2021learninghe2009learningdosovitskiy2020imageyang2023mitigatingdevlin2018bertyang2023mitigatingTraining Details.kingma2014adamComputational Resources.Evaluation metrics.sec:preCosine Similarities Distribution on non-spurious correlated prompt image pair\texttt workflowProof of Eq.~(\ref)proof\texttt Performance on CelebA: Non-blond/blond hair\texttt Performance on CelebA: Other AttributesZero-shot Classification Performance by Different PromptText Recovered from DRF Linear Probe by Setting Different EOT Index