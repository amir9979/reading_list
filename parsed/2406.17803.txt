We present the performance improvements with different augmentations compared to scenarios with no user profiles in ~. 

Considering only semantically similar context ( Non-Personalization Retrieval-Augmentation) improves performance on LaMP-2 and LaMP-5 but decreases performance on the remaining four tasks. When compared to considering personalization information only ( Personalization Augmentation), the semantic information approach shows a performance gap in most tasks. These observations indicate that semantic information ( context similar to the input) has a limited impact on the effectiveness of user profiles in the personalization of LLMs.

When the candidate set is limited to user profiles from the current user ( context similar to the user), retrieval augmentation ( Personalization Retrieval Augmentation) shows performance improvements across almost all tasks. It outperforms strategies that consider either semantic information or personalization alone. This evidence suggests that semantic information contributes to the effectiveness of user profiles only when built on top of personalization information.

Our results validate that semantic information has less impact on the effectiveness of user profiles compared to personalization information, and only contributes when combined with personalization. Additional experiments in ~ confirm that our conclusions hold across different quantities of user profiles and even with larger non-fine-tuned models (-2).

We identify three key parts within user profiles that potentially affect LLM personalization: () the previous input from users, () the response produced or endorsed by users, and  () the accurate mapping between these inputs and responses.

% To investigate the impact of three parts,  we employ two distinct strategies to sample or retrieve two lists of user profiles, which are then merged to form a final profile list.  Specifically, user profiles are randomly selected from the entire pool of user profiles, denoted as , or chosen either randomly  or semantically based on the retrieval model  from the profiles of the current user .  The previous section has demonstrated that the effectiveness of the selected user profile for personalization is ordered by . We only use the input part from the first list and the output part from the second list from two strategies,  , where .  Note that if the employed sampling strategies for input and output are the same, , it would maintain the correct mapping within the user profiles due to the same list of the selected user profiles. We report the results on four tasks ( LaMP-2, LaMP-3, LaMP-4 and LaMP-5) that have the complete user profiles in ~.

In ~, diagonal cells from the bottom-left to the up-right ( ) maintain the correct mapping, whereas other cells do not.  However, except for the bottom-left cells, a consistent increase is not evident in centric and up-right cells compared to their respective row (incorrect input) and column (incorrect output) counterparts.  For instance, in LaMP-5, using personalized w/o retrieval for both input and output performs worse than its row neighbor (0.421 vs.~0.428/0.428) and its column neighbor (0.421 vs.~0.428/0.445). These observations suggest that the correct mapping between previous input and response is not necessary for LLM personalization.  An exception is observed in LaMP-3, where the centric and up-right cell performs better, possibly because LaMP-3 uses virtual labels (scores from 1 to 5) without semantic information, thereby necessitating correct mapping to guide LLMs in interpreting the scores accurately.

 Due to the more powerful sampling strategy from left to right, the right cell uses user profiles with a more effective input for personalization than the left cell. % However, no substantial increase is observed across all four tasks.  In LaMP-4, with a more powerful input part within user profiles, performance on centric and bottom-centric cells with  for the input part is better than their right neighborhood with  for the input part (0.155 vs. 0.153 and 0.144 vs. 0.142 for ROUGE-L metric).  Our results indicate that the improvement does not result from the complete user profile, suggesting that personalization within the previous input has a limited impact on LLM personalization.

Cell-related strategies from bottom to top use user profiles with a more effective output for personalization.  A noticeable change in color from  to  ( to  for LaMP-3) indicates that a more effective output markedly enhances performance.  Even with incorrect mapping, the up-centric cell with a more effective output outperforms the centric cell with correct mapping but a less powerful output.  Our results underscore that, compared to mapping and the input part of user profiles, personalization within the response produced or endorsed by users plays a crucial role in LLM personalization.

It is the response produced or endorsed by users that enhances LLM personalization, rather than correct mapping or previous input from users. Correct mapping is deemed necessary only for non-semantic output space.

Previous findings indicate that the correct mapping between input and output does not necessarily contribute to LLM personalization.  To further investigate the role of its input and output parts, we conduct experiments where we focus solely on either the input or output part while ignoring the mapping.  We modify the template part of the prompt with minimal changes (see ~).  It is important to note that using the incomplete user profile helps to shorten the input context, thereby enabling the incorporation of more user profiles for personalizing LLMs within the limited input length.  We expand the number of utilized user profiles, ranging from  to  of the most relevant profiles. 

Our results in ~ reveal that, except for LaMP-3, using only the output part of user profiles achieves comparable (LaMP-4) or even superior performance (LaMP-2 and LaMP-5) compared to using complete user profiles.  In contrast, utilizing only the input part leads to noticeable performance degradation across all tasks.  This supports our earlier findings, emphasizing the importance of user profiles with a more powerful output for LLM personalization.  It further underscores that responses produced or endorsed by users play a pivotal role in effective personalization, particularly when contrasted with correct mapping and previous input considerations.  In the case of LaMP-3, where tasks involve non-semantic label spaces, utilizing incomplete user profiles (only output or input) leads to a noticeable decrease, supporting the notion that correct mapping is essential for such tasks.

As depicted in ~, our analysis of various proportions of user profiles reveals that using complete user profiles often exceeds the maximum input length, necessitating truncation that leads to substantial performance losses. In contrast, focusing solely on personalized responses improves performance across all tasks compared to using complete user profiles. Notably, there is a substantial increase in LaMP-2 (from 0.571 to 0.626 for the F score) and LaMP-4 (from 0.182 to 0.188 for ROUGE-1).  For LaMP-3 and LaMP-5, where using complete user profiles suffers from severe degradation, exclusively leveraging the output of user profiles proves to be a robust strategy. This not only affirms the efficacy of the output part for personalization but also highlights that using only the output part extends the capacity of LLMs to leverage more user profiles, leading to greater performance improvement.

The findings underscore the vital contribution of the output part of user profiles to LLM personalization.  This finding supports the strategy of extending the LLMs' capacity to utilize more user profiles, especially when focusing on the output, which may result in notable performance gains.

The order in which multiple user profiles are arranged within the input context may largely affect the personalization capabilities of LLMs.  Previous work in RAG  has indicated that LLMs may overlook the document containing the correct answer when placed in the middle position of the input context.  Considering that user profiles differ fundamentally from the documents typically retrieved in traditional RAG setups, it's crucial to assess how the order of user profiles influences personalization.

To explore the effect of varying orders of user profiles in the input context, we select the top  relevant user profiles and arrange them in the input context using different approaches (seen ~):

Following conventional retrieval practices , this method places profiles with higher semantic similarity earlier in the input context.

This order is the inverse of the More Relevant First, positioning the most relevant profiles nearer to the end of the context.

 Unlike the above orders and the ListInMiddle Ranker in RAG , this method places the more relevant user profiles in the more central position.

Our results in ~ reveal substantial variations in performance across tasks when the same subset of user profiles is arranged in different orders within the input context.  For instance, on LaMP-2,  achieves a MAE of 0.298, whereas  records 0.291. Similarly, on LaMP-5,  yields a ROUGH-1 score of 0.152, while  achieves 0.161. This observation emphasizes that the user profiles in different positions of the input context do not contribute equally to performance gains.

Before we analyze the difference in the position of input context,  the findings in Figure  highlight the importance of semantic relevance, showing that user profiles with higher semantic similarity have a stronger impact on personalization.  ~ further supports that  achieves the best result on all tasks except for LaMP-1 and LaMP-2.  exhibits improvement ranging from  to nearly  across the last four tasks, with a slight decrease on LaMP-2 due to the effectiveness of most user profiles within the current user. Additionally, both  and  consistently outperform  on all tasks, emphasizing that user profiles closer to the beginning contribute more to personalization.

We further examine if the conclusion holds when only using the output part.  We analyze performances with three different orders only using the output part of different proportions of the top relevant user profiles, ranging from  to  in ~.  Our results demonstrate that  outperforms on all tasks except LaMP-2 when only using the output part, and the performance gap widens as more user profiles are utilized.  For LaMP-2,  performs better than , suggesting that profiles placed at the start position (even if they are not the most relevant) substantially contribute to personalization. This underscores that even when only using the output part of user profiles, the user profile closer to the start of the input context contributes more to the personalization of LLMs.

We also show the performance of the augmentation methods with different numbers  of the used user profiles in ~.

 The reported results show that compared to non-personalization augmentation methods, the personalization augmentation methods achieve a constant improvement on most tasks, even with the different number of used user profiles. On the other hand, the non-personalization augmentation cannot introduce a constant benefit for the performance. It further confirms the conclusion we get in the main paper.

 The extent of the enhancement on personalization by personalization augmentation increases from  to . Especially for the personalization with retrieval sees the improvement on all tasks. However, when more user profiles are utilized, it easily introduces some noise and suffers from degradation. This observation further confirms that within the reasonable range, the enhancement extent by user profile increases with the increased number of used user profiles.

We also report the results with a frozen Llama 2 with the chat version in ~, to examine the conclusion from ~.

Results on frozen LLMs exhibit similar patterns, where both two personalized augmentation methods can achieve an improvement on most tasks (except LaMP-7). Conversely, the augmentations without considering the personalization decreases the performance on most tasks, although they positively impact only on LaMP-1 and LaMP-5.  These observations underscore the pivotal role of user profiles containing user preferences, supporting the assumption that introduced user profiles enhance LLM personalization for performance improvement. However, it's noteworthy that personalization augmentation with non-fine-tuning improves performance less than the ones with fine-tuning and even introduces some noises on LaMP-7. This suggests that although user profiles can enhance personalization, LLMs without fine-tuning struggle to capture user preferences from the introduced user profile in the input context to obtain performance improvements.

Utilizing user profiles to personalize Large Language Models (LLMs) has been shown to enhance the performance on a wide range of tasks. However, the precise role of user profiles and their effect mechanism on LLMs remains unclear. This study first confirms that the effectiveness of user profiles is primarily due to personalization information rather than semantic information. Furthermore, we investigate how user profiles affect the personalization of LLMs. Within the user profile, we reveal that it is the historical personalized response produced or approved by users that plays a pivotal role in personalizing LLMs. This discovery unlocks the potential of LLMs to incorporate a greater number of user profiles within the constraints of limited input length. As for the position of user profiles, we observe that user profiles integrated into different positions of the input context do not contribute equally to personalization. Instead, where the user profile that is closer to the beginning affects more on the personalization of LLMs. Our findings reveal the role of user profiles for the personalization of LLMs, and showcase how incorporating user profiles impacts performance providing insight to leverage user profiles effectively. Introductionbrown2020language,liu2023chatgpt,chang2023surveychen2023large, liu2023chatgpt, wu2022metasalemi2023lamprichardson2023integrating, kang2023llms, salemi2024optimizationlewis2020retrievalbrown2020languagemallen2023notgarg2022can-0.5em 

    Q1Do user profiles rely on semantic information (context similar to the input) to improve LLM performance?-0.5em    Q2In what ways do user profiles affect the personalization of LLMs?Q1sec:actual_affectQ2within the user profilesubsec: single user profilesthe position of user profilessubsec: the orderwithin the user profilethe position of user profilesfig:three_figures-0.5emUser profiles rely largely on personalization information rather than semantic information; semantic information contributes to user profiles only when combined with personalization.

    -0.5emResponses that are generated or approved by users play a crucial role in the personalization of LLMs, while the correct mapping within user profiles is not always essential for effective personalization;

    -0.5emThe impact of user profiles in different positions on personalization is not uniform: the user profile closer to the start of the input context tends to have a larger effect on personalizing LLMs. Related WorkPersonalization of LLMs.achiam2023gpt, shi2023donouyang2022training,yang2024bayesianjang2023personalized, shi2024instructionkirk2023Personalisation, wu2023adaptivesalemi2023lamprichardson2023integratingli2023teachdoddapaneni2024user, ning2024usertan2024democratizingshi2023deptli2023automaticRetrieval-Augmentation Generation.lewis2020retrievalgao2023retrievalliu2023lostchen2023walkingIn-Context Learning.brown2020languageminrethinking, wei2023largerliu2021makes, gao2023ambiguitygarg2022can, akyurek2022learningxie2021explanationDBLP:conf/emnlp/WangLDCZMZS23PreliminaryProblem Definition.salemi2023lampExperimental Setup.salemi2023lamptab: task descriptionchung2022scalingapp: exp_detailshttps://github.com/Bingo-W/Personalisation-in-LLMhttps://github.com/Bingo-W/Personalisation-in-LLMwidth=0.9\textwidthFigures/0_information_contribution.pdf-0.5emThe improvement of performance (Flan-T5-base) on LaMP dataset with different Augmentation based on the user profiles () compared to without augmentations.      Note that LaMP-3 shows a \textbf in performance compared to the no-augmentations baseline, indicated by the lower values of both MAE and RMSE, where lower scores signify better performance.-1emfig: augmentation for flan-t5The Actual Affect of User Profilesec:actual_affectQ1Non-Personalization Augmentation w/o Retrieval.Non-Personalization Retrieval-Augmentation.Personalization Augmentation w/o Retrieval.Personalization Retrieval Augmentation.Results: Semantic Information Matters Less Than Personalizationsubsec: fine-tune modelfig: augmentation for flan-t5Semantic Information Alone Cannot Lead to Consistent Performance Improvement.Semantic Information contributes to performance improvement Only When Combined with Personalization.Summary.app: more results about augmentationLlamaHow do User Profile Affect the Personalization of LLMsQ2The Effective Part of User Profilesubsec: single user profileswidth=0.8\textwidthFigures/mix_profile.pdf-0.5emThe performance on 4 LaMP tasks, with different combinations of sampling strategies for the construction of the introduced user profiles. Note that the metrics for LaMP-3 are the lower, the better.-1emfig:different construction for user profilesabcfig:different construction for user profilesResults: The Impact of Different PartCorrect mapping is not necessary for personalization.fig:different construction for user profilesPrevious Input has limited impact on the personalization.The response produced or endorsed by users contributes to the personalization.dblue\textttdred2\textttdred2\textttdblue\textttSummary.Results: The Impact of Only Outputapp: promptOnly using the output part substantially enhances the personalization.-1emwidth=0.8\textwidthFigures/incompleted_number.pdf-1emThe performance with different numbers of used user profiles on four LaMP datasets when only with the input part and only with the output part. Note that the metrics for LaMP-3 are the lower, the better.-0.5emfig: only input and outputfig: only input and outputOnly using output unlocks the potential to use more user profiles.width=0.8\textwidthFigures/incompleted_with_proportion.pdf-0.5emThe performance with different proportions of user profiles on four LaMP datasets when only using the output part of the completed user profile. Note that the metrics for LaMP-3 are the lower, the better.-1emfig: only input and output with different proportion.fig: only input and output with different proportion.Summary.Exploring the Impact of User Profile Ordersubsec: the orderliu2023losttab: example for different orderMore Relevant First (\FirstOrder).salemi2023lampLess Relevant First (\EndOrder).More Relevant Central (\MiddleOrder).liu2023lostsubsec: multiple profiles-1emwidth=\textwidthFigures/different_target.pdf-2emThe performance on LaMP with user profiles located in different positions in the retrieved ranked list. Note that  refers to the -th position. Note that the metrics for LaMP-3 are the lower, the better.-0.5emfig: top i-th in the retrievalwidth=0.88\textwidthFigures/order_incompleted_with_proportion.pdf-0.5emThe performance with different proportions of user profiles on four LaMP datasets when only using the output part with three different orders. Note that the metrics for LaMP-3 are the lower, the better.-1emfig: only output with orderResults: The Impact of Different OrderUser profiles in different positions contribute not equally to the personalization.tab: result with different ordersThe user profile closer to the start of the input context tends to have a larger effect.fig: top i-th in the retrievaltab: result with different ordersMore forward more contribution when only using output.fig: only output with order-0.5emConclusionLimitationsEthics StatementmainAppendix Overviewsec:appendixAppendix \sectAppendix \sectAppendix \sectAppendix \sectThe Details of Datasetapp: datasetsalemi2023lamp Given a paper from a user, an LLM needs to predict which one of the two candidates will cite in this paper based on the user profile. User profiles refer to the paper that the user has authorized before.     LaMP-1: Personalized Citation Identification Given a movie description, an LLM needs to predict which one of the 15 candidate tags the user will give to the movie based on the user profile. User Profiles refer to the user's historical tagging behavior, consisting of the movie description and the given tag.     LaMP-2: Personalized Movie Tagging Given a review from a user, an LLM needs to predict the score with an integer with the range from 1 to 5 that the user will give based on the user profile. User profiles refer to the user's historical rating behavior, consisting of the reviews and the associated rating score.     LaMP-3: Personalized Product Rating Given an article from a user, an LLM needs to generate the headline for this article based on the user profile. User profiles refer to the authors' historical article-title pairs.     LaMP-4: Personalized News Headline Generation Given an article's abstract, an LLms needs to generate the scholar title for this article based on the user profile. User profiles refer to the user's historical article-title pairs.     LaMP-5: Personalized Scholarly Title Generation Given a tweet, an LLM needs to generate a tweet in the style of the user. User profiles refer to the user's historical tweets. LaMP-7: Personalized Tweet ParaphrasingEvaluationsalemi2023lamplin2004rougeStatistic InformationThe Details of Experimentsapp: exp_detailssalemi2023lamptab: task descriptionapp: datasetsalemi2023lampchung2022scalingtouvron2023llamasalemi2023lamp\textwidth!The template part of the used prompt for only using the input or output part of the user profiles. \textcolor and \textcolor refer to the input part and output part of the used user profiles, respectively.tab: the used promptwidth=0.9\textwidthFigures/0_llama_contribution.pdfThe improvement of performance (Llama 2) on LaMP dataset with different Augmentation based on the user profiles () compared to without augmentations. Note that LaMP-3 shows the \textbf when compared to without augmentations since both MAE and RMSE are the lower, the better.fig: augmentation for Llama 2width=\textwidthFigures/different_number.pdfThe improvement of performance (Flan-T5-base) on LaMP dataset with Augmentation based on the different number of user profiles compared to without augmentations. Note that LaMP-3 shows the \textbf when compared to without augmentations since both MAE and RMSE are the lower, the better.fig: augmentation with different number of used user profiles for flan-t5The Details of Used Promptsapp: promptsalemi2023lamptab: the used promptMore results about Different Augmentationsapp: more results about augmentationResults: More User Profile.fig: augmentation with different number of used user profiles for flan-t5The conclusion that user profiles actually enhance personalization remains when with different numbers of user profiles.The enhancement of personalization augmentation increases with more user profiles.Results: Frozen Modelsfig: augmentation for Llama 2subsec: fine-tune modelEven for frozen LLMs, user profiles actually enhance personalization, but the performance improvement decreases due to no fine-tuning.