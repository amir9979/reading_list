The initial Transformer-based MTS models take time steps as tokens and apply temporal attention between them. Subsequent works demonstrated that temporal attention at a sub-series level with fewer tokens is more effective and can greatly reduce the complexity. PatchTST  provides a general paradigm of the Temporal Transformer at the sub-series level. In PatchTST, each of the  variates  is converted to sub-series Patches , where ,  is length of patches and  is the stride - the non-overlapping region between two consecutive patches. Temporal attention is applied to capture the dependencies between patches of each variate. The  is first embedded to tokens , Where  and  is the number of dimensions. Then the attention weight is calculated: 

Where   and  is number of attention heads. The mapping to latent space is .

The Temporal Transformer directly follows the paradigm in NLP. But unlike natural language, MTS has multiple parallel sequence inputs. Variate Transformer explicitly models the complex correlations between variable sequences. Typically, iTrasformer  embed the whole time series of each variate  independently into a (variate) token as , where . Then it adopts attention to multivariate correlations as follows:

The mapping to latent space of variate attention is .

Whether temporal or variate as mentioned above, both transform sequences of MTS to a latent space to provide the dependency pattern between sequences. The point of our research is to demonstrate whether the mapping to latent space under conventional attention is optimal or if we can find a better one for MTSF.

We apply the FSatten to Variate Transformer for MTSF. As shown in Figure  right, each discrete variate sequence of the input  is first transformed by the Fast Fourier Transform (FFT) , which efficiently computes the Discrete Fourier Transform (DFT) from the time domain to the complex frequency domain as:

Here,  is the imaginary unit, and the exponential term represents the Fourier basis associated with the different  frequencies. The value of  is typically half the number of data points  in FFT. According to our consideration, correlations can be made up of associated frequency components with different phases. Thus, we extracted the amplitudes of different frequencies from the complex domain as follows:

Where  represents the real part of  and  represents the imaginary part. We then apply the MSS module for the projection of queries and keys, replacing the conventional linear projection. The aim is to compare the learnable latent space for generating attention weights with the fixed frequency domain space. Since predictions are made in the time domain, the embedding and linear projection for the value remain unchanged.

After the multi-head dot product, the subsequent Feed-Forward Network (FFN) provides complicated representations by adding random noise for each variate token .

Although we remove the phase interference, it enables us to capture both synchronous and asynchronous associations. A more critical problem, however, is identifying the accurate associated frequency components. This is due to significant differences in amplitude values and information intensity for the same frequency across different sequences. As illustrated in Figure , the frequency from sequence B that is potentially associated with sequence A may not represent the most significant periodic characteristics of sequence A. To efficiently obtain potentially correlated frequency components, we designed a Multi-head Spectrum Scaling (MSS) projection for queries and keys. For each attention head , we scale the amplitude value  of each frequency dimension using the Hadamard Product: 

where . Therefore, MSS uses  different  matrices to map the . After learnable scaling, some frequency components can adaptively align with potentially correlated components from other sequences so that more accurate dependency patterns between sequences can be found in the subsequent dot product attention. From another consideration of maintaining the orthogonality of the frequency bias, we replace the fully connected projection, which might alter the angles between vectors and disrupt the orthogonality. Ablation experiments detailed in Section , as well as those applied to the subsequent SOatten, demonstrate the effectiveness of MSS.

This is a research direction with many possible approaches, but the most straightforward idea is to extend by leveraging the orthogonality of the Fourier transform. Therefore, we propose SOatten, as depicted in Figure , which improves the frequency domain to a more general orthogonal domain through a learnable orthogonal transformation, described as: % Where in Variate Transformer or  in Temporal Transformer. In fact, the orthogonality of the learnable space is not guaranteed during backpropagation for parameter updates. We could apply a measure such as QR decomposition, but this could result in the loss of some gradient information. We made a different trade-off for MTSF, considering that the scale of models that match the size of the dataset is usually not very large, with relatively few layers. Therefore, we only perform orthogonal initialization for the embedding, rather than enforcing a completely orthogonal space during backpropagation.  Subsequently, SOatten also applies the MSS projection for query and key and applies linear projection for the value. Additionally, in dot product attention, we propose a Head Coupling Convolution (HCC) module operating on the heads of attention weights, which serves as an important 'guidance' for the mapping space learning of SOatten.

FSatten provides sequence dependencies based on explicit spectral information, but extending this to a learnable orthogonal space makes it difficult to effectively determine valid characteristics as a defined periodicity in FSatten. In other words, data-driven approaches that learn an effective orthogonal space without any restrictions have requirements for the size and distribution of the dataset.

I propose a general enhancement method called Head Coupling Convolution (HCC), which leverages the constraint of similarity between neighboring sequences to guide the model in exploring feature spaces. Specifically, HCC involves performing convolution operations on the attention weights within the dot product attention mechanism as:

Where  is channel fusion convolution that maps from  heads to  heads and padding is necessary for keeping the size of the weight matrix. 

For most time-series data, contrastive learning methods  have demonstrated the effectiveness of assumption: neighboring similarity, the similarity between sequences of the same time series decreases as the time lag increases. In fact, similar variates are arranged together in most datasets (see details in Appendix ). 

By applying a convolution operation to the attention weights, more critical correlated patterns between local neighboring sequences are extracted, guiding the parameter updates in the feature space during backpropagation. The diverse features extracted by  heads are all predicated on neighboring similarity, multi-head coupling helps to obtain more precise associative features than single-channel convolution.

Compared to the baselines presented in Table  , FSatten, based on the Variate Transformer, shows overall better forecasting performance than the SOTA, which uses conventional attention mechanisms. Particularly for datasets with more pronounced periodicity, such as on ECL, FSatten significantly improves performance by an overall 9.0\% compared to SOTA and exhibits greater stability for longer prediction sequences. These improvements demonstrate that FSatten effectively captures the accurate correlation at the same frequency, which is more suitable for application in Transformers for MTSF.

Periodicity is one of the most fundamental characteristics of time series, but not all datasets exhibit strong periodicity. Thus, as a more general approach that can be adapted to both Temporal and Variate transformers, SOatten achieves more comprehensive improvements relative to FSatten across different scenarios. We can observe in Table  that, although each Transformer excels on certain datasets, SOatten consistently outperforms conventional attention mechanisms, regardless of the architecture. Of course, FSatten can provide superior performance for datasets that are known to exhibit stronger periodicity. 

To validate the effectiveness of the components of FSatten and SOatten, detailed ablation experiments are conducted. The effectiveness of the MSS mapping module, used in both methods, is compared with that of applying a linear mapping to FSatten and SOatten, as shown in Figure . MSS significantly outperforms the fully connected layer, corroborating its ability to identify more accurate associated components. In Section , a visual interpretation is provided to explain in detail.

Secondly, we validate the effectiveness of the HCC module in SOatten in Figure . The HCC is an important design for SOatten, significantly enhancing forecasting performance. Especially under the Temporal Transformer, the HCC demonstrates better generalizability. These results prove that neighboring similarity is crucial for the formation of an effective orthogonal mapping space and the generation of accurate attention weights. In Section , we conduct further hyperparameter analysis on the kernel size  of HCC.

First, we make visualizations of generated attention matrices and analyze the advantages of the proposed two attention mechanisms. In the upper part of Figure , under the Variate Transformer, SOatten and FSatten generate smaller ranges but more refined weight values than the conventional attention applied by the SOTA, iTransformer (compare the value range on the right side of heatmaps). There are three main points of analysis:

(1) In the generated attention weight maps, the patterns of the conventional attention and FSatten show similarities, presenting dependencies that are based on the sequence periodicity. However, FSatten significantly reflects complex associations from more variable sequences, which is a benefit from the designed spectrum correlating in frequency domain space.

(2) The weights map generated by SOatten is significantly different, seemingly finding accurate dependencies based on other associated physical quantities in addition to periodicity (as seen in the upper left part of SOatten's attention map). Furthermore, if the HCC module is not used (shown in the Appendix ), SOatten finds new physical quantities but fails to produce a comprehensive dependency pattern, proving the effectiveness of the neighboring similarity design. 

(3) Numerical analysis of the weight matrices (in Appendix ) shows that the proposed FSatten and SOatten are both full rank (21), while the conventional attention weight matrix is not full rank (19). The condition numbers of the weight matrices generated by FSatten (1,519) and SOatten (1,480) are much smaller than that of the conventional attention (78,596,560). These indicate that the orthogonal spaces of FSatten and SOatten are more informative and have better stability against noise than the latent space of conventional attention.

Secondly, in the lower part of Figure , the predictions indicate that the conventional attention mechanism's fits are poor, which appears to have learned inaccurate periodic patterns. By leveraging the frequency domain and the MSS module, FSatten finds a more accurate pattern, that combines periodic dependencies based on frequency spectrum from other variates. SOatten finds an even better pattern by combining periodicity and other key physical quantities thereby avoiding prediction errors caused by an exclusive reliance on periodicity, as shown in the green box in Figure . More predictions are shown in the Appendix .

Compared to conventional attention, the main hyperparameter differences are in the dimension size of the orthogonal mapping space  for MSS and the kernel size  for HCC, as shown in Figures  and . For FSatten, the  frequency components are orthogonal and are typically set to . In contrast, SOatten orthogonally initializes the mapping weights, and the dimension size  of the mapping spaces for Query and Key are set separately from that of the Value (same with conventional attention). Sensitivity experiments on , depicted in Figure , demonstrate that the performance of the proposed attention mechanisms is not coincidental. When optimized, SOatten's  is significantly smaller than the  and  of conventional attention, thereby enhancing the model's efficiency to some extent (detailed efficiency analysis is presented in Appendix ).

Secondly, we compared different HCC kernel sizes , as shown in Figure . The upper limit of the coordinate values represents the performance of SOTA. Under both the Temporal and Variate Transformers, the model performance using HCC with different kernel sizes is consistently better than SOTA, demonstrating the effectiveness of local neighboring similarity. However, a  convolutional kernel is found to be the most appropriate choice; it does not increase the complexity due to its small size.

As the Transformer continues to make breakthroughs in the fields of NLP and CV, applying the attention mechanism to time series analysis outperforms TCN- and RNN-based methods. LogTrans  proposes LogSparse attention, which focuses on the previous step at exponential intervals in each step to break the memory bottleneck. Reformer  employs position-sensitive hashing (LSH) to divide tokens into several buckets and perform attention within each bucket. Informer  introduces ProbSparse attention, a mechanism that calculates the top-u leading queries based solely on the measured query sparsity. Autoformer  constructs a series-level connection based on the process similarity derived by series periodicity and introduces a decomposition forecasting architecture. FEDformer  starts from the frequency domain. It selects a random subset of frequency components and multiplies them by learnable complex parameters to learn a sparse representation of time series. PatchTST  adopts a general patch-based series representation inspired by the Vision Transformer . Crossformer  explicitly captures the cross-time and cross-variate dependencies through two-stage attention and a renovated architecture. iTransformer embeds the time points of individual series into variate tokens which are utilized by the attention mechanism to capture multivariate correlations. These methods have continuously improved from the model architecture and the application of Attention. However, none of them have delved into the internal workings of the attention mechanism to explore whether conventional attention is optimal for MTSF.

For the sub-sequence patches segmented from the same variate, in most cases, they meet the assumption of neighboring similarity: the similarity between sequences of the same time series decreases as the time lag increases. We found that most MTS datasets such as widely used ETT, ECL, and Weather, also exhibit certain degrees of neighboring similarity between variates. As shown in Figure , In the Weather dataset, adjacent variates have similar curve shapes. The attention weights and forecasting performance have verified the significance of the neighboring similarity bias for SOatten.

We conducted experiments on six real-world datasets to evaluate the performance of the proposed FSatten and SOatten. The ETT series includes data on seven oil and load features of electricity transformers. Traffic contains hourly road occupancy rates recorded by San Francisco freeway sensors from 2015 to 2016. Weather contains meteorological observations including temperature, humidity, wind speed, and precipitation. Exchange contains panel data on daily exchange rates from 8 countries, spanning from 1990 to 2016. ECL contains hourly electricity consumption data (in kWh) for 321 clients from 2012 to 2014. Solar-Energy contains hourly solar power output data collected from 137 PV plants in Alabama in 2007. We provide the dataset details in Table .

All the experiments are implemented in PyTorch  and conducted on a single NVIDIA RTX 3090 24GB GPU. To validate our approach, we only replace the conventional attention with FSatten and SOatten without changing any original parameter settings in iTransformer and PatchTST.  PatchTST includes  encoder layers with a head number of  and a latent space dimension of  and a prediction head dimension . For ETT, parameters of a reduced size are used (, , and  ) to reduce the risk of overfitting. A dropout rate of  is applied in the encoders for all experiments. iTransformer contains  layers with a head number of , The dimension of series representations .The only difference is the addition of two hyperparameters  and kernel size  in HCC. 

We give the full forecasting results in Table , we compare SOatten and FSatten based on Variate Transformer with benchmarks, and we can see that SOatten and FSatten are in the top two on all six real-world datasets. Table   shows the full results of directly comparing SOatten with the conventional attention based on both Variate Transformer and Temporal Transformer, SOatten achieved a comprehensive surpass.

Table   shows the complete results of ablation for MSS. It can be seen that replacing MSS with a Linear map significantly reduced the forecasting performance of FSatten and SOatten, indicating that MSS is an essential design. Secondly, Table  shows the complete results of ablation for HCC. The HCC module is very important for the orthogonal space learning and accurate generation of attention weights in SOatten.

Figure  shows the efficiency results of FSatten and SOatten, compared with conventional attention under both the Temporal Transformer and the Variate Transformer. Since FSatten and SOatten were only replaced in both Transformers, with only the dimension size of MSS  and an additional convolutional operation, there is no significant difference in efficiency compared to conventional attention. It can be observed that FSatten has slightly improved efficiency by replacing the original linear mapping of Query and Key with a Fast Fourier Transform . Secondly, because MSS is a Hadamard product, it can enhance the efficiency relative to fully connected layers to some extent. Although SOatten has an additional convolution layer, which significantly improves predictive performance, its memory usage and running time are slightly inferior to conventional attention.