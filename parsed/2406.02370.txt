There are two primary methods of representing 3D scenes: explicit and implicit. Traditional methods based on point cloud , voxel , and mesh  typically focus on directly optimizing explicit geometric representations. However, due to limitations in resolution and memory, these methods struggle to efficiently describe local geometric details. On the other hand, some methods   that implicitly encode scenes through neural networks have received considerable attention. The most notable method, Neural Radiance Field (NeRF) , has been employed across various purposes .  % such as dynamic reconstruction , 3D editing, event-based 3D Reconstruction, large scene reconstruction .  Recently,  proposed 3D Gaussian Splatting (3DGS) which is considered the next generation of 3D reconstruction to represent 3D scenes as point sets with learnable local geometry parameters. 3DGS spurs the development of many fields .

% Based on point clouds, 3DGS uses more reasonable Gaussian points to model detailed local geometry information. Recent studies have improved 3DGS in several aspects, including   .  Earlier methods use a standard convolutional autoencoder to learn 3D-aware representations under given camera poses. .  NeRF  is a novel implicit representation of 3D scenes that has garnered significant attention. Based on NeRF, some methods  utilize latent-conditioned NeRF to represent the scene as vectors, aiming to achieve generalization and improve the quality of view synthesis. Other studies~ extend latent-conditioned NeRF to RL manipulation tasks.~ additionally integrates semantic information to the NeRF latent space.  % NeRF-RL  and % SNeRL  generates semantically-aware latent vectors for RL methods through a convolutional encoder and a NeRF-style decoder . % Based on 3DGS,% Based on the 2D foundational model , LangSplat  expresses the semantic information of an entire static 3D scene through the combination of multiple local embeddings. Unlike LangSplat, we further explore latent vectors for 3D scene understanding. Specifically, our method obtains hierarchical semantic-aware 3D scene representations from two input views using an encoder and query-based generalizable feature splatting. Visual-based reinforcement learning maps images to low-dimensional latent embedding via encoders. Previous methods trained the perception encoder by using various auxiliary objective functions, such as image reconstruction~, contrastive learning~, and reconstructing task-specific information~, data augmentation~ or other unsupervised tasks~. However, these methods struggle to perceive and understand 3D-structural information. Recent approaches directly combine RL and 3D representations to be aware of 3D environments. NeRF-RL~ and~ first leverage NeRF to train encoders to obtain state representations for RL algorithms. SNeRL  generate semantic-aware representations by learning RGB fields, feature fields, and semantic fields. The above NeRF-based representation considers view-independent 3D structure information while suffering from inherent disadvantages of the NeRF pipeline. 

Another two works are to some extent relevant to this paper. They also incorporate 3DGS into RL.  GSRL  proposes a generalizable Gaussian splatting framework for robotic manipulation tasks that can represent local details well without the mask. ManiGaussian~ proposes to use dynamic 3DGS to build a digital twin of the per-scene representation of the RL environment. However, both approaches consider 3DGS as either the observations of the environment or the auxiliary modeling of the scene, which require significant computations. They do not learn a latent representation of the scene with the assistance of the efficient 3DGS framework.  On the contrary, we adapt the 3DGS to learn the scene representation with hierarchical semantics information. 

% % Early methods  for constructing 3D feature fields were based on NeRF. These methods expanded NeRF by inputting features from multiple views into a 2D visual model. Shen et al.  integrated 3D geometry with 2D semantics by extracting CLIP features into NeRF, enabling language-guided robotic manipulation with few shots. LERF  learns multi-view consistency and smooth the underlying language field within NeRF through volume rendering of CLIP embeddings, allowing 3D queries of open vocabulary. Recently, many studies have combined 3D feature fields with 3DGS. To aggregate rich semantic information into Gaussian points,  has developed Gaussian semantic/feature fields by leveraging pretrained 2D foundational models. LangSplat , based on the segmentation capabilities of SAM  and the semantic understanding of CLIP , models Gaussian semantic fields for efficient open-vocabulary localization. Feature 3DGS  distills Gaussian feature fields through guidance from 2D foundation models .% %%%%%下述不确定%%%% These methods extract 3D feature fields by optimizing each scene individually to achieve fine-grained language queries. Reinforcement learning requires learning a global representation of scenes, but existing methods cannot achieve a generalized language field. In this paper, we enhance the capabilities of reinforcement learning through a multi-level semantic feature field representation.  We formulate reinforcement learning policy training as an infinite-horizon discrete-time Markov Decision Process (MDP). MDP is defined as a tuple , where  is the state space, which includes the RGB and depth images of the whole scenario.  is the action space for robot arm manipulation,  is the reward function,  represents the probabilities of the transition to the next state for each states-action pair at ,  is the discount factor of reward. At each time step , the policy observes the state  from the environment. The policy executes the action  which is sampled from policy . Subsequently, the state transitions from  to  by . And the policy obtains a reward value at each time step . For maximizing the return discounted reward, the objective is to optimize the parameters of the policy : 

where T denotes the time horizon of MDP.

3DGS explicitly models 3D static scenes as 3D Gaussian primitives, each has an opacity (), a mean (), a covariance matrix (), spherical harmonics coefficients () (or a RGB color ) and an optional feature vector (). Among them, to facilitate optimization by gradient descent,  can be decomposed into a scaling matrix (), described by a scale vector () and a rotation matrix () described by a quaternion ():

Gaussian rasterization accesses these primitives to render novel views and produce corresponding feature maps. Given the cameras' parameters and pose matrix, the projection of 3D Gaussians to 2D image plane can be characterized by the view transform matrix () and Jacobian of the affine approximation of the projective transformation (), as in:

where  is the covariance matrix in 2D space.  By computing the -blend of  Gaussian points that overlap a pixel projected onto the 2D plane, the final color  and feature  of each pixel is determined:

where  and  denotes the color and the feature of Gaussian point  ,and  denotes the soft occupation Gaussian point  at 2D space, which can be calculated by . It is worth noting that  and  only represent the 2D-projected version when calculating . 

This section introduces the Hierarchical Semantics Encoding (HSE) to ground hierarchical semantics to 3DGS. The previous study~ demonstrates that integrating semantic information into latent representation can improve the performance of RL tasks. Further, we note that encoding more fine-grained semantic information should be more beneficial for RL training. For example, when asked to pick up a hammer, a human focuses on the semantic meaning of the handle of the hammer and rarely grabs the hammerhead. Therefore, hierarchical semantic information in the latent vector is beneficial for understanding the scene. Based on this, we ground the semantics to Gaussians hierarchically by the proposed HSE. 

Given an RGB image , we adopt CLIP~, the most popular vision-language embedding model, to transform each image into the semantic feature space. It is noted that one CLIP embedding corresponds to an image. Typically, the feature is expanded on the image plane using the object's mask to obtain pixel-level features. As we discussed before, we want to obtain part-level fine-grained language features. Similar to ~, we use the Segment Anything Model (SAM)~ to generate a set of part-level masks for each object and use CLIP to the masked image to produce part-level features. In practice, an independent pixel can belong to multiple part levels. For example, a pixel can belong to the entire cabinet, meanwhile it can also simultaneously belong to the cabinet's handle. Thus we aggregate different levels of part features for each pixel to obtain their final semantic features. In detail, the Masked Average Pooling is adopted to aggregate features. 

where  refers to a certain pixel location.  denotes the masked CLIP operation and M is the mask. The output of the MAP operation is assigned to every pixel located within the part segmentation. When a pixel is associated with multiple parts, the pixel feature is derived by averaging the features of all pertinent parts. This provides a part-enhanced CLIP feature map. 

However, CLIP encodes a large number of objects during training. In our specific RL tasks, only a few types of objects would appear, such as robot arms, faucets, cabinets, etc. Hence its original feature space is very redundant for the objects in our RL tasks with common values ranging between  (for the CLIP-ViT model) and  (for the CLIP-ResNet model). In our experiments, we constantly select the CLIP-ViT as our extractor. On account of this, we reduce the dimension of the semantic embedding by utilizing an autoencoder, consisting of an encoder  which maps semantic embedding of dimension 512 to the compact latent space , and a decoder  that projects the compact latent map back to the original CLIP space. A simple MLP is used as the autoencoder. In this work, we set  as 3 drawing on the reference~. Another by-product of this method is that it maintains the real-time rendering speed of 3DGS. We train the autoencoder with the following loss function:

where  denotes the operations to map image  to the compact semantic feature space. After training the autoencoder, we obtain the mapping function from the groundtruth image to the low-dimensional compact latent space, which can be used to ground the hierarchical semantics to 3D Gaussians and will be utilized in the subsequent latent representation learning phase. 

In this section, we introduce the QGFS which aims to bridge the 3DGS with scene representation learning. Typical 3DGS has to be optimized in a per-scene manner and represent the scene as points. However, we want to render the scene from a single latent vector that describes it. The QGFS is proposed to render the scene from a vector by using the 3DGS technique in a generalizable way. The main process of QGFS refers to the left part of Fig.~. 

Similar to previous methods ~, we adopt the multiview encoder to map multiview observations into a single latent vector  for RL tasks.  

The encoder  takes the observed RGBD images  and their corresponding camera projection matrix  obtained from  different camera views and outputs the latent code. The detailed architecture of  can be viewed in the Appendix. 

After obtaining the latent code, previous NeRF-based methods conduct volumetric rendering within a NeRF conditioned on . They sample multiple points along rays and predict their color and opacity by an MLP conditioned on , then perform color blending for each ray to obtain their final colors. However, as we discuss in Sec.~, most sampling concentrates on unoccupied regions, which causes data inefficiency and disturbed geometrical awareness. Therefore, in our method, we reutilize the geometrical priors encoded in the depth observations. We reproject the RGBD images into 3D space by the corresponding camera projection matrix. 

where  and  refer to the intrinsic and pose matrix respectively.  denotes the value at  on the depth map.  Then these 3D points are considered keys for querying the global latent vector. As stated in Eq.~, we feed each point coordinate to a positional encoding layer () and concatenate it with the scene representation vector . Then the concatenation is fed to a lightweight query decoder () to produce a coordinate-specific feature vector . 

By doing this, each query occurs at the geometry surface of objects, which is efficient and geometry-aware.  Next, the queried local feature vector is transformed to the semantic features, and Gaussian parameters including opacity, rotation, and scale factors at the position via different prediction heads. 

These s and s are the parameters of linear transformation to project  to the Gaussian geometry spaces.  Additionally, they have different activation functions after the prediction heads to ensure their value range.  denotes the normalization function along the last dimension.  is the exponential function.  refers to the Sigmoid activation and  denotes the tanh activation. 

After we obtain the four parameters, we keep the original color and coordinate from RGBD projection. Then we can render these queried Gaussian points via rasterization to produce an image and a language feature map given another camera viewpoint that is different from the original input views. The produced two maps are used to compute losses to the groundtruth. 

where  refers to the 3DGS rendering function to generate maps given viewpoint . The  loss is the combination of the L1-photometric loss and the structural similarity index measure (SSIM) loss~. The other loss aims to supervise the results in the feature space.

where  denotes the cosine similarity. The feature loss  projects the feature map and compares it with the output of the HSE module which we introduce the symbols' meaning in the previous section. The total loss is  where  and  are weights to balance the two terms. It is noted that in the first 5000 iterations which we called the warm-up stage, we remove the feature loss and only train the geometry head to obtain good geometry reconstruction ability at first. In our experiments, we set , , , and .

All in all, in QGFS, we reconstruct the 3DGS by querying the global latent vector with the geometry surface points to obtain their respective Gaussian parameters. In this case, the GS can be built in a generalizable way and the latent vector should retain geometrical and semantic awareness. After training, the encoder  is exploited as a representative feature extractor for any downstream RL task.  

In this section, we demonstrate several RL tasks to show the effectiveness of our proposed method compared to other existing approaches. We conduct experiments on two RL platforms, Maniskill2~ and Robomimic~ for comprehensive comparisons. In Maniskill2, we select six different manipulation tasks including StackCube, PlugCharger, PickEGAD, PickYCB, OpenDrawer, and TurnFaucet, which we briefly introduced in Fig.~. In Robomimic, we select Lift, Can, Square, and Transport tasks. We follow the default settings implemented in each platform over all experiments and we do not tune the parameters carefully because our goal is to demonstrate our scene representation is general and effective but not to maximize the performance of each RL algorithm. We adopt the DAPG~ for all Maniskill2 tasks, which is a hybrid approach that combines the traditional policy gradient technique with learning from demonstrations because Maniskill2 provides some human demonstrations for each task. Moreover, we evaluate all baseline scene representation methods by using BCQ~ and IQL~ algorithms for the three Lift, Can, and Square tasks of Robomimic, and using the IRIS~ algorithm for the Transport task.   For both platforms, in our standard experimental setting, we use 2 cameras to globally observe the scene, which will be used to reconstruct the latent representation of the scene. For fair comparisons, we maintain camera settings constantly the same across all the other baseline experiments. 

 We compare our method with other alternative ways of training the encoder for RL, which we briefly described below. CURL~ adopts an auxiliary contrastive loss to train RL, ensuring that the embeddings for data-augmented versions of observations match. Similar to~, we use its multiview adaptation for better capacity. CNN-AE~ directly uses the convolutional network as an autoencoder to reconstruct images in the pertaining phase. NeRF-RL~ pre-trains the encoder with the help of conditional NeRF to obtain the 3D-aware latent vector. SNeRL~ adopts additional semantic distillation based on the NeRF-RL by the NeRF-style semantic supervision. Additionally, we compare our method with another recent 3DGS-based RL training scheme GSRL~. It directly converts RGBD observations into 3D Gaussians to explicitly represent the scene and uses a point-based encoder to encode the 3D Gaussians before input to the RL policy network. It requires updating the Gaussian representation after each time of interaction with environments, which is computationally expensive. It does not consider using a geometrical latent code and semantic enhancement. 

We pre-train the  in Eq.~ by using offline RGBD images. In detail, we collect 12 multiview images for each episode and treat every three spatially adjacent images as a group. We concatenate the first two RGBD images and input them into the encoder to produce the scene representation vector. Then we render the images and the proposed HSE feature maps at the last camera viewpoint from the representation vector by using the presented QGFS to compute losses with the groudturths of the last image. Beforehand, we pre-trained the Autoencoder with 200 epochs at a learning rate of  for compressing hierarchical semantic features to a compact and low-dimensional feature space, which largely reduces the learning burdens.  Finally, the  is trained with 10 epochs with the learning rate of .  All models are trained on a single RTX3090 graphics card using the Adam optimizer. When we train the RL policy, we set the learning rate of the pretrained 3D-aware encoder to be a very small value, i.e. 1e-7 to slightly fine-tune it.  % Autoencoder % optimizer, learning rate, RTX...  Table~ reports the quantitative results, i,e, the mean and variance of success rate, of the six methods on Maniskill2 tasks. Fig.~ demonstrates the training curves of the six methods on the first three tasks. It can be observed that our approach overall outperforms the other five baseline methods across all tasks thanks to our better geometrical and semantic scene representation. In addition, our method trains more stably because the variance in our success rates is relatively small. It is seen from the training curve that our method converges faster than others because we fully utilize the geometry priors encoded in the depth maps, only sample points to query the scene on the geometry surfaces of the objects. Therefore, our method has higher data efficiency compared to those NeRF-based encoders. In addition, Table~ lists the mean and variance of success rates on the tasks of the Robomimic platform. In these six experiments, our method achieved the best performance in at least four of them and secured at least second place in the remaining experiments.  Interestingly, our advantage over SNeRL is more evident in complex tasks. This is because we leverage the 3DGS framework to learn a more geometrical-aware scene representation while SNeRL relies on the NeRF supervision that uniformly samples points in the whole scene. Furthermore, in tasks of picking up some tools from obstacles including PickSingleYCB or PickSingleEGAD, our model can deliver stable and fast convergence because our scene latent code contains hierarchical semantics that can be used to guide the policy network focus on plausible correlation areas.  

In this subsection, we ablate some significant designs of the proposed method. To validate the effectiveness of our hierarchical semantics encoding, we set two additional experimental settings when extracting language features. First, we only use CLIP to generate a single latent code for all pixels in this image. Second, we use SAM to produce object-level masks and adopt masked CLIP to produce the latent vector for each object. The pixels belonging to the same object share the same latent feature. Third, we maintain our HSE to simultaneously utilize clip, SAM, and part-level feature aggregation. Notably, the Autoencoder in Eq.~ is constantly used for all experiments to keep the rendering speed of 3DGS. Fig.~ (a) illustrates the training curves of the three situations. We mark the first case as  only, the second case as , and the third case (which is the same as HSE in the main text) as . It can be seen from this figure that the part-level, object-centric semantic features are indeed beneficial for learning better RL policy. 

Furthermore, we evaluate the proposed method with more image observations. In the main experiment, we only adopt two cameras to observe the scenes. In the ablation section, we additionally test our performance when the number of images is 1, 3, and 4. The performance is recorded in Fig.~ (b). A single image of the scene can lead to a decline in algorithm performance. Additionally, there is a trend where performance improves as more images are available. Moreover, the impact of adding more images subsequently is not as significant as the impact of increasing from one to two images.