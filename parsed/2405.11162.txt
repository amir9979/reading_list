In developing a model specialized for the text-to-SQL task, we initially fine-tuned seed model on the given training data. Because it is widely recognized that there exists a performance gap between open-source LLMs and proprietary LLMs in many benchmarks. In section~, the results regarding performance corresponding to changes in the model substantiate it. Therefore, we utilized the Finetuning API provided by OpenAI to fine-tune the GPT-3.5-Turbo-0125 model.

The training dataset comprises a total of 5,124 samples, including both answerable and unanswerable questions. We employed all of these data samples in our training. Furthermore, to ensure that  accurately references the correct column names when generating SQL queries, we converted the table schema of the provided MIMIC-IV demo database into text format and incorporated it into the input for training. Additionally, to enhance  capability in distinguishing between answerable and unanswerable questions, we incorporated information about unanswerable questions into the input. This strategic inclusion aimed at refining  discernment, thus improving its overall accuracy in classifying questions.  % Details about the data used for training, prompts, and hyperparameters can be found in ~. Unanswerable questions refer to queries that either do not align with the given table schema or require external knowledge, rendering them unsolvable using only the MIMIC-IV demo database for SQL query generation. In our training dataset, the number of answerable questions is considerable, reaching 5,124, whereas unanswerable questions are limited to just 450. This disparity highlights a data imbalance issue within our training dataset, which may impede the model's ability to correctly respond to unanswerable questions during testing.

Moreover, there is a low similarity between the queries in the training data and those in the development/test sets. We found that the average cosine similarity between query embeddings in the train and development sets is only 0.36, and between the train and test sets is 0.34, measured using OpenAI's text-embedding-3-large embedding model. Such a disparity in dataset distribution could lead to significant performance declines for the model at test time. To address these issues, we initially perform pseudo labeling on the development/test set using , which was originally trained solely on the original training dataset.

Pseudo-labeling is one of the techniques used in semi-supervised learning, serving as a powerful tool for addressing issues of data scarcity and label imbalance. Particularly with the EHRSQL dataset, a notable disparity exists: the quantity of unanswerable questions is significantly lower compared to answerable ones within the training data. Training a model with such data increases the likelihood of the model's inability to accurately respond to unanswerable questions. In tasks where reliability is crucial, especially compared to other domains, this could result in substantial penalties. Therefore, we choose to augment the original training set with those samples predicted as unanswerable. Finally, we fine-tune  using the augmented dataset.

Despite the two-stage training process, including self-training,  still generates incorrect SQL queries. To enhance the reliability of our final predictions, we implemented a filtering process to sift out samples that were either inaccurately generated or produced with uncertainty by the model. This filtering stage plays a crucial role in ensuring the outputs of  are more dependable and accurate. % % % When the model-generated queries contain minor errors, we employ simple regular expressions for post-processing these queries. In the provided data's SQL queries, when referring to the current time, a variable name `current\_time' is used instead of actual time information (e.g., datetime(current\_time,`start of month',`-0 month')). Although {\Ours}, trained on this data, generates queries with variable names instead of actual time values for questions requiring current time information, it's necessary to convert these variables into real-time values for querying the MIMIC-IV demo database. In addition to this, we correct minor issues such as case errors to produce improved queries. Tokens in a language model-generated output have higher entropy when the information is uncertain. Therefore, treating samples with high entropy as unanswerable questions aids in creating a more reliable system while incurring fewer penalties. We evaluate the entropy of each token produced by the language model, and define the entropy of the prediction based on the token exhibiting the maximum entropy. Then, in the entire set of predictions, samples exceeding a certain entropy level are considered as unanswerable questions and are filtered out. We have set a threshold for this filtering process, determined by the proportion of unanswerable questions in the dataset we aim to predict. This proportion of unanswerable questions is used as a hyperparameter to calibrate the threshold for filtering.

Finally, we implement an additional process of filtering to ensure that the remaining SQL queries, after the initial filtering, can successfully access the MIMIC-IV demo database and retrieve valid values. Utilizing the sqlite3 library in Python, we test each SQL query. Queries that trigger errors, return empty values, or yield None are deemed unable to retrieve valid values. Consequently, we filter these queries as unanswerable questions. This step further ensures the accuracy and reliability of the system by only allowing queries that can effectively interact with the database.

 We utilize the EHRSQL 2024 dataset for both training and evaluation. The dataset comprises 5,124 training, 1,163 development, and 1,167 test data entries. Notably, only the training dataset is accompanied by gold SQL queries and their corresponding executed gold answers. For questions deemed answerable, it's essential to generate the correct SQL query. For those classified as unanswerable, a null output is required. Database for SQL query generation is the MIMIC-IV demo database. We employ the GPT-3.5-Turbo-0125 model for fine-tuning purposes. Evaluation of our method is conducted on the codabench platform, where we submitted SQL queries predicted by  for the test set and obtained scores based on their performance. 

We utilize the Reliability Score (RS) as our primary metric . In figure~, the RS aims to accomplish two main objectives: firstly, it provides rewards for correctly generating SQL for answerable questions  and for not generating SQL for unanswerable questions  ; secondly, it imposes penalties for wrongly generating SQL for  and for any attempts to create SQL for . However, the RS neither rewards nor penalizes for choosing not to answer . The penalties are structured as 0, 5, 10, or N, where N corresponds to the total number of entries in the dataset. The final score is calculated by adding 1 point for each correct sample and deducting points based on the penalty for incorrect ones, followed by averaging these scores. Importantly, in the EHRSQL 2024 shared task, the primary metric for determining rankings is RS(10).

 In the development set,  exhibits the highest performance in RS(10), the primary metric, which positions it at the top of the official leaderboard when compared with other models. A notable aspect of  is the minimal difference between its RS(0) and RS(10) scores compared to other models. This indicates that  effectively reduces penalties by categorizing uncertain outcomes in answerable questions and unanswerable questions as 'unanswerable.' This strategy underscores our model's superior reliability, as it avoids the risk of incorrect answers where uncertainty exists, a feature that sets it apart from its counterparts.  In the final ranking phase of the shared task, which utilized the test set,  experienced a slight overall decrease in scores compared to its performance in the development set. Despite this dip, it maintained a higher score across all RS, including the pivotal RS(10), when compared with other models. This consistent performance across all metrics, even amidst a minor decline, ultimately led  to win the EHRSQL 2024 shared task. % ablation 1: model ablation We observe the performances across difference models. A total of three models were used, namely Flan-T5-base, Tulu-7b, GPT-3.5-Turbo-0125, and GPT-4-Turbo-Preview. Flan-T5-base, Tulu-7b, and GPT-3.5-Turbo-0125 is fine-tuned, while GPT-4-Turbo-Preview is applied with in-context learning. All results are conducted on the development set.

Among the fine-tuned models, GPT-3.5-Turbo-0125 demonstrates the highest performance. This indicates that there is still a performance gap between proprietary and open-source models. Furthermore, despite having more parameters, Tulu-7b shows lower performance compared to Flan-T5-base. Additionally, it is observed that GPT-4-Turbo-Preview, known for its high performance in numerous benchmarks, scored lower than fine-tuned models when only in-context learning is applied. % ablation 2: prompt ablation In the study, we compare the performance of models based on the information included in the input prompts during training. When table schema information is incorporated into the prompts, the models perform better than without it. This suggests that providing table schema information, such as column names, offers a valuable learning signal to the models.

Additionally, explicitly including information about unanswerable questions results in higher scores than when such information is omitted. By providing criteria for answerable and unanswerable questions, the models are aided in avoiding questions they could not answer and focusing on providing accurate responses to those that are answerable.

In the final version of the prompt, we incorporated the database schema of MIMIC-IV as well as the instruction related to unanswerable questions. You can find the prompt in Figure~.

In table~, by applying execution filtering, which treats invalid SQL queries that either do not execute or retrieve empty values as unanswerable questions, a significant performance improvement is observed, particularly in scenarios with substantial penalties such as RS(10) and RS(N). Additionally, by implementing entropy-based filtering, which filters out SQL queries with higher entropy than a set threshold among those with high maximum token entropy, performance is further enhanced by effectively eliminating SQL queries that, even when executed, return incorrect values.