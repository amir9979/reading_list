The input data refers to the data received by the report generation system. During model training and inference, the data can vary; for example, both images and real reports are used as inputs during training, while only images are used during inference . However, the model learns from the distribution and features of the training data. If the testing data changes, mostly, the model will struggle to generalize, resulting in decreased performance. Therefore, in this section, we mainly introduce the acquisition of input data that is consistent between the training and testing phases in the reviewed papers. The input data includes:

Data preparation endeavors to enhance data quality and prepare it for model deployment, typically encompassing data cleansing, transformation, and organization. Conventional preparation methods include image resizing and cropping, text tokenizing, converting all tokens to lowercase, removing non-alphabetic tokens, and implementing data augmentation procedures. The methods utilized in each paper are outlined in Table  in Appendix A, but it is worth noting that conventional preparation methods are so ubiquitous that some papers do not mention them. While the information is not recorded in the table, it does not mean the absence of a data preparation process.

Novel data preparation methods in the reviewed papers can be categorized into filtering  and grouping .  argued that writing a radiology report necessitates referencing historical information, which inevitably included descriptors such as `again' and `decrease'. However, these terms cannot be inferred from a single image, therefore Ramesh et al. filtered such descriptions in the reports. This exclusion was found to facilitate the model's learning process. 

Grouping refers to organizing sentences from ground truth reports into distinct sections, typically relying on keywords from pre-defined knowledge graphs and filtering rules. Each section describes a specific anatomical structure. Grouping aims to enable the generation system to process various types of sentences differently. For instance,  employed different decoders to generate descriptions for different anatomical structures. Alongside the reviewed papers, a recently released public dataset named ImaGenome  also includes grouping results in their annotation files (see Section ). The grouping result becomes more easily accessible.

Previous research primarily utilized CNNs as architectures for extracting image features, however, recently, an increasing number of researchers have opted for the use of Transformers due to their improved performance. Simultaneously, numerous studies proposed novel modules to enhance the model capability. In this section, the model architecture is first introduced, and subsequently enhancement modules are described. These modules include auxiliary tasks, contrastive learning, and memory metrics. The architecture and modules utilized in each paper are outlined in Table  in Appendix A.

The statistics of the architectures used as image feature extractor are shown in Figure . In total,  forty-six studies extract image features purely based on CNN models. Thirty-four works firstly encode images by CNN and then utilizes the Transformer layers to modify the embeddings. Eight works utilize a pure Transformer architecture to extract image features. Figure  shows a clear trend of more studies adopting CNN augmented with Transformer for image feature extraction.

For CNN architecture, two works self-designed CNN models, while the other works built it based on different classical visual models, such as ResNet  (42 papers), DenseNet  (22 papers), VGG  (5 papers), Faster-RCNN  (5 papers), Inception-V3  (1 paper), ResNeXt  (1 paper), EfficientNet  (1 paper), and the Two-Stream Inflated 3D ConvNets (I3D)  (1 paper).  modified the HRNet , a human pose estimation network. Other three works  provided results based on different CNN structures. 

To improve model performance,  ten works modified the CNN structure by attention modules, which assigned varying degrees of importance (weights) to different parts of the input by learnable parameters, allowing the model to selectively focus on specific regions of an image. Traditional attention mechanisms can be classified into channel-wise  and spatial-wise , which allocate different weights to the various channels and spatial positions of the inputs respectively. In addition,  and  utilized the idea of the class activation map  to obtain weights.  initially extracted image patch features, clustered them using an unsupervised method, and then weighted the cluster results.  Experimental results show that attention mechanism allows models to pay more attention to the lesions than irrelevant background. With the rise of the Transformers , multi-head attention has become a potent method for information interaction.  first extracted regions of interest from the frontal view and then employed multi-head attention to fuse information between the frontal and lateral views with the regions. This approach introduced regions of interest into model training to improve model performance.

For Transformer architecture, most of them leverage a standard Transformer encoder, while  argued that aligning images and text posed a challenge due to the continuous nature of images and the discrete nature of text; therefore, they improved the model performance by using a discrete variational autoencoder  to obtain discrete visual tokens. Other works improved the model performance by modifying the self-attention module. Three works  added high-order interactions among three inputs of the Transformer attention module. Two works  were inspired by the memory-augmented attention  and extended the keys and values with additional plain learnable vectors to record more information.  introduced a learnable parameter in the attention operation.  modified the encoder by including additional input tokens. These tokens were named `expert tokens' to emulate the ``multi-expert joint diagnosis" methodology. 

Auxiliary tasks aim to provide additional supervision signals to the feature extractor, enabling it to extract information relevant to report generation from images. These tasks mainly include classification (22 papers), graph construction (10 papers), embedding comparison (10 papers), and detection/segmentation (7 papers). Each of them is introduced in detail below. 

 The most common auxiliary task used in the reviewed papers is classification referring to assigning images to predefined categories. The predefined categories primarily include medical tags  and disease labels . The medical tags are from standard medical vocabularies including hundreds of labels, such as anatomical structures and pathological signs. They are provided by manual annotations or auto annotation tools (e.g., NIH MTI web API  and RadGraph ). Disease labels are provided by auto annotation tools (e.g., CheXpert  and CheXbert ). Compared to disease labels, medical tags offer a more comprehensive range of information. However, to the best of our knowledge, there is no literature that supports the superiority of medical tags over disease labels. Perhaps due to the extensive scope covered by medical tags, deep learning models face challenges in acquiring such rich knowledge.  incorporated 32 additional labels for lesion location, size, and shape (e.g., ``upper/lower" and  ``patchy") into the disease label set, observing a slight improvement in model performance.

Other notable categories used in the reviewed papers include matching status , local properties , report cluster results , and fix answer categories .  predicted the matching status of a given image-report pair.  devised localized property labels for breast ultrasound images, such as tumor morphology, to facilitate the identification of properties that are challenging to be discerned in low-resolution images.  first conducted unsupervised clustering on the ground truth report, subsequently utilizing the resultant clusters as labels. This auxiliary task yielded a marked improvement in text generation performance.  considered the report generation as a question-answer task, and the classifier was designed for fixed answer categories. In addition, for a detection auxiliary task, classifiers need to be applied to identify attributes of detected regions (e.g., `right lung'). This paragraph eschews such cases to circumvent redundancy.  % firstly predicted the health condition of input images and then treated normal and abnormal cases in different branches. %health condition  and anatomical structure  Graph construction aims at introducing prior knowledge into the report generation process. The knowledge graph in this section differs from that in Section . Here, node features are extracted from images, and edges are defined as parameters in graph convolution networks. In contrast, the node and edge information in the input knowledge graphs is derived from non-image data. A classical method was proposed by  and yielded promising outcomes. A knowledge graph was constructed firstly based on insights provided by domain experts, where nodes represented major abnormalities and major organs, and bidirectional connections linked nodes that were related to each other. To initialize nodes features, a spatial attention module was introduced after the CNN backbone using 11 convolution layers and softmax layers. The number of channels matched the number of nodes. The nodes' initial embedding was derived as attention-weighted feature maps. Then graph convolution layers were employed to disseminate information throughout the graph, followed by two branches for classification and report generation. First, the classification branch was trained, and subsequently, parameters in both the CNN backbone and the graph convolution layers were frozen, only the report generation decoder was trained. Six works  utilized this method .   expanded the graph  by incorporating information from a radiology terms corpus named Radiology Lexicon (RadLex). As the number of graph nodes increased, the model performance initially improved, peaked at 40 nodes, and then declined, with a noticeable decrease at 60 nodes.  constructed a large graph based on the MIMIC-CXR dataset. The nodes represented frequent clinical abnormalities and the edges represented the co-occurrence situation of different abnormalities. In addition,  used the disease prediction results to obtain the node features. The nodes were classification probabilities, with learnable edge weights. 

Another graph reconstruction method aims at reconstructing triplets that are in the form of (entity1, relationship, entity2), such as (opacity, suggestive of, infection). Three works  firstly predicted the triplets and then generated reports based on them. The experimental results show that using triplets alone for report generation is ineffective; combining them with features extracted from images is necessary for better results.

 Embedding comparison refers to constraining the consistency of different features in intermediate layers, thereby guiding the learning process. The comparison in reviewed papers is mainly applied between features extracted from images and real reports . Experimental results show that the supervision signals from real text enable extracted visual features carry richer semantic information, facilitating more effective translation into radiology reports. Four works  utilized a triple loss function to compel the image-text paired features to be closer to a latent space than the unpaired ones.  inspired by the Auto-Encoding Variational Bayes . They used real reports to obtain a latent space during training and generated reports based on this space. The image extractors were enabled to capture features from images that closely resemble those found in real reports. Other two works  used the Term Frequency and Inverse Document Frequency (TF-IDF) to extract important information from real reports as supervision signals. TF-IDF is a statistical measure assessing a word's importance by considering its frequency in a specific document and its rarity across the entire document set. 

In addition, to produce reports for abnormalities not seen during training,  initially linearly projected visual features to semantic features, and extracted semantic features of labels by the BioBert model . Consistent constrain was applied between two similarity: 1) the similarity between pairwise elements in the semantic features from visual features; and 2) the similarity between the semantic features from visual features and the semantic features from labels.  integrated semi-supervised learning into report generation using two networks. They first applied different types of noise to an input image to create two variations, which were then fed into the two networks. An auxiliary loss function was employed to ensure consistency in the extracted visual features.

 Object detection locates and identifies objects or patterns within an image, focusing on determining their presence and position. Segmentation divides an image into meaningful regions by identifying and separating objects based on specific characteristics. Both processes enhance the model's understanding of the image by object recognition and region extraction, and can improve the model's interpretability by linking the detection/segmentation results with generated sentences. The detection/segmentation regions can be anatomical regions  and abnormal regions . No literature compares the impact of detection and segmentation tasks on report generation results. However, a publicly available dataset named Chest ImaGenome (see Section ) offers detection annotations, making them easier to be acquired than segmentation annotations.

% {} {There are other auxiliary tasks.  used an orthogonal loss among expert token embeddings to capture as much information as possible from an image.  added an antoencoder branch to reconstruct input images.  designed a similarity comparison system between visual and textual features.  In addition,  pre-trained a unified large language model by the masked language task and image-report matching task and it can be used for report generation. }

In addition, the outputs of auxiliary tasks can provide valuable information such as disease labels, therefore, inputting them into the following generation network is a common choice . For example,  sent the semantic word embeddings of the predicted findings from the classifier to the report generation decoder.

Contrastive learning is a self-supervised learning method to improve the representational capacity of models, which allows models to minimize the distance among positive pairs and maximize it for negative ones. It can be used to train feature extractors .  utilized a classical contrastive learning method named Momentum Contrast , where different views or augmented versions of the same image were considered as positive pairs. Another representative contrastive learning work is the Contrastive Language-Image Pre-training (CLIP) model . It connects textual and visual information by directly training on a vast dataset consisting of image-text pairs.  directly employed it for image feature extraction and  applied the idea of CLIP to train the feature extractors on the training dataset.   however argued that previous works treated the entire report as input, overlooking the distinct information contained within individual sentences. This oversight could result in incorrect matching of image-text pairs. Therefore, they proposed phenotype-based contrastive learning. This method involved randomly initializing a set of vectors as phenotypes, allowing sentences and visual embeddings to interact with them, and finally conducting contrastive learning between the processed embeddings. The results outperformed previous contrastive learning methods in report generation.

Contrastive learning also can be part of the training loss , and can be applied between visual and textual features (i.e. image-text pairs) , or be applied based on labels, treating samples with shared labels as positives and those without any common labels as negatives . Their ablation study  demonstrated a significant improvement in results compared to standard contrastive loss .

Contrastive attention is another method to utilize contrastive learning.  designed a contrastive attention model to extract abnormal region features by comparing input samples with normal cases. Similar features shared between the input and normal cases were subtracted from the input image feature, and the remaining feature was then concatenated with the original feature.  argued that the contrastive technique  did not consider historical information, therefore they proposed a module based on similarity retrieval technique to obtain similar images from the training dataset. The image features were processed by enlarging different features between inputs and the similar retrieved images. 

Using a memory metric for image feature extraction assumes the presence of similar features in various medical images. Memory metrics are employed to record and transmit the similarity information during training . Typically, an n√ón matrix is randomly initialized, where n represents the number of metric rows. Then, at each training step, the matrix is updated based on the visual features and the previous metrics. The memory metric used in this section is consistent with the metric used in the memory-driven transformer discussed in Section , with one being applied to images and the other to the generated reports.

Most non-imaging data is presented in the form of text. Before fusion with image data, text data needs to be embedded. We first introduce a widely-used basic text embedding technique, the lookup table. This table assigns a unique index to each word or character, which is used to look up a pre-trained word vector or character vector. In addition to the basic method, additional feature extraction can be performed on these vectors to enhance their representation capabilities.

Transformer-based models such as BERT and its variants have emerged as mainstream methods for feature extraction across various textual data, such as terminology , real text reports , knowledge graphs , and questionnaires , and have achieved good results.

Several methods are designed for a specific type of input.  used the TF-IDF to re-weight terminology embeddings. The re-weighted approach alleviated the issue of data imbalance, resulting in performance enhancement. Clinical information can be processed by a pre-trained feature extractor named BioSentVec . For the knowledge base,  used a knowledge graph embedding model named RotatE  to obtain entity embeddings and relation embeddings from the RadGraph. Besides using the entire RadGraph as input, two works combined the real reports with RadGraph to extract case-related information from real reports and queried related information from the RadGraph . Alternatively,  considered RadGraph as an annotation tool for extracting entities and positional information from real reports.  Other two works  utilized classification results to process the self-built graph and extracted case-related information. The experimental results substantiate the beneficial impact of incorporating case-related knowledge on report generation, particularly evident when integrating real reports with the knowledge base. In addition, age and gender information are not text data and are generally encoded as one-hot vector .

Feature fusion and interaction refer to the integration of multi-modal data from inputs or auxiliary tasks. This step has two purposes. First, visual and textual features from disparate domains present challenges for model learning. By fusing and facilitating interaction between these features, the domain gap can be narrowed, thereby enhancing network learning. Second, the image regions should align with the sentences in the reports. This correspondence can be learned through fusion and interaction. In the auxiliary task of embedding comparison (see Section ), semantic features extracted from real reports are used to supervise the learning of image features. However, this approach differs from multi-modal feature fusion. The objective of embedding comparison is to enhance image features, without incorporating non-image features into the generator. Instead, the fused features in this section are forwarded to the generator.

The most straightforward approach for feature fusion and interaction is feature-level operation including the concatenation, summation, or multiplication of multimodal features (13 works). However, the feature-level operation could be too simple to enable sufficient interaction. Therefore, neural network-based methods are leveraged, such as LSTMs (2 works) and the multi-head attention mechanism (27 works). While this approach facilitates convenient feature fusion, its lack of specific design for multi-modality data fusion leads to limited effectiveness in interaction. % leveraged a memory metric to record cross-modality interactions produced by the attention module.

We would like to highlight a memory metric-based method proposed by . It significantly enhanced the performance of the report generation system by facilitating feature interaction. A metric was initialized randomly. Image features, text features from generated tokens, and memory metric features were then projected into the same space. Subsequently, distances between image features and memory metric features, as well as text features and memory metric features, were calculated. The top K metric features with the closest distances to the image or text were selected, respectively. These selected features were then weighted based on these distances and were fed back into an encoder-decoder structure. Two studies  followed this method.  modified it in two ways: 1) they initialized the matrix by visual and textual features; 2) the cross-modal interaction occurred only among cases with the same label. These two modifications both resulted in a notable improvement.  contended that the methods mentioned lack explicit constraints for cross-modal alignments. They considered orthonormal bases as the metric and input them along with visual or textual features, into multi-head attention modules. Then the outputs of attention modules were processed by a self-defined gate mechanism. A triplet matching loss was utilized to align the processed visual and textual features. This method slightly improved the results. 

The last step is report generation, which utilizes extracted features from earlier steps to produce the final reports. The generation methods mainly include decoder-based techniques (Section ), retrieval-based techniques (Section ), and template-based techniques (Section ). In addition, the development of large language models has made it possible to utilize them to enhance the quality of generated reports. It is discussed in Section  The decoder decodes the extracted representation of inputs and generates a descriptive report. The mainstream architectures include LSTM  and Transformer. Compared to LSTM, the Transformer processes the entire sequence simultaneously rather than sequentially. Therefore, the Transformer allows for more efficient parallelization during training and can capture long-range dependencies. In the 89 reviewed papers, the Transformer tends to replace LSTM. Fifty-five works utilized the Transformer as a decoder and only 23 of them utilized LSTMs (12 works) or hierarchical LSTMs (11 works). For the papers published in 2023 and 2024, all encoder-decoder structures used the Transformer as their decoders. There are two ways to modify the decoder and improve model performance.

 Connecting different layers in networks can be considered as a promising way to enhance the flow of information in both forward and backward propagation . The U-connection  and meshed connection  are added between encoder and decoder, resulting in a similar performance enhancement .

 We would like to highlight the Memory-driven Transformer (R2Gen) proposed by . It has been increasingly popular in recent years. The R2Gen introduces a memory module and a memory-driven conditional layer normalization module into the Transformer decoder architecture. The design of the memory module hypothesizes that diverse images exhibit similar patterns in their radiological reports, thereby serving as valuable references for each other. Building a memory matrix can capture this pattern and transfer it during training. Specifically, similar to that in the section , a matrix is randomly initialized and is updated using the gate mechanism based on the matrix from the last step and generated reports. The layer normalization is designed to integrate the outputs of the memory module into the decoder.

Eight works directly utilized the R2Gen as their decoder. In addition, the design of the memory module and layer normalization inspired subsequent works . It is noted that the novel utilization of the memory module by  integrates ground truth reports into the training process, leading to successful outcomes.

Retrieval-based techniques generate reports by selecting existing sentences from a large corpus and the selection is typically based on similarity comparison . Initially, text and image encoders are trained using a contrastive method, such as the CLIP . The textual features of sentences in a corpus and the visual features of an input image are extracted by the encoders. The visual features are then compared with all textual features in the corpus. The top k sentences with the maximum similarity score are selected for the predicted report. In addition,  added a multimodal encoder after the retrieval process to calculate the image-text matching scores between the input image and the retrieved sentences. A filter was applied based on the score to remove entailed or contradicted sentences. 

Other retrieval-based techniques do not follow the above process.  treated the report generation in two steps sentence retrieval and selection. They first retrieved a candidate sentence set from the training datasets with far more sentences than a standard medical report and then selected the sentences by a classifier.  proposed a retrieval method based on a hashing technique, which mapped multi-modal data with the same label into a shared space.

Template-based methods typically start with the diagnosis of diseases, and then pre-defined sentences are selected based on the diagnosis results. These selected sentences are concatenated to produce reports .  argued that this method was limited by exact labels, therefore they retrieved template sentences by class probabilities and different thresholds corresponding to different descriptions.

With the emergence of ChatGPT , its powerful language abilities made researchers eager to harness the power of large language models to aid in report generation. However, employing it directly within the medical domain led to unsatisfactory outcomes . Two works  initially predicted report-related information such as diseases, lesion regions, and visual features, and generated preliminary reports. Subsequently, they employed pre-trained large language models, e.g., ChatGPT  or GPT-3  to improve the preliminary report with the predicted information and produced the final reports. The results have been moderately improved. Exploration of large language models in the field of medical report generation still requires further investigation, which is discussed in Section .

Training strategy refers to the techniques used to train neural network models. Traditionally, models are trained by minimizing various loss functions. Therefore, this section begins by introducing different loss functions (Section ), followed by a discussion on reinforcement learning (Section ), and the curriculum learning's application to the report generation task (Section ).

The mainstream loss function for report generation is the cross-entropy loss based on the generated sentences and the ground-truth sentences. Cross-entropy loss can be re-weighted based on term frequency , TF-IDF  or uncertainty  to mitigate model bias or handle challenging cases. In addition,  utilized cycle-consistency loss  to generate reports. The core idea is that a report and its corresponding image share the same information, hence they can be used to generate each other. 

The application of an auxiliary loss function can provide additional supervision signals, further enhancing model performance. Two works  applied an additional constraint between features extracted from the generated and real reports.  created two different versions of an input image by adding noise and feeding them into two networks. An auxiliary loss function ensures the consistency of the outputs produced by the two generators. % developed a learnable matrix within the multi-head attention module and constrained the average value of its elements by a loss function. It only improved performance on the ROUGE-L and CIDEr evaluation metrics while decreasing performance on other metrics.  obtained two discriminative regions in an image from the generated words and visual classifier separately, then enforced the consistency between them. 

Reinforcement learning involves training an agent to make optimal decisions through trial and error, aiming to maximize targeting rewards. It offers a method to update the model parameters based on non-differentiable reward functions .  Evaluation metrics can be considered as rewards, such as CIDEr , BLEU , METEOR , ROUGE , BERTScore , F1 score , and accuracy .  In addition,  trained a language fluency discriminator using the ground truth and generated reports, and then utilized the discriminator to provide rewards.

  utilized curriculum learning  to classify training instances and trained a model from simple to complex samples. Data pairs were evaluated based on image heuristics, image confidence, text heuristics, and text confidence. Image heuristic evaluated the similarity between input images and normal images. Image confidence indicated the confidence of a classification model. The report heuristic was related to the number of abnormal sentences, and report confidence was evaluated by the negative log-likelihood loss .

Natural language evaluation metrics are from natural language processing tasks and measure the general text quality of generated reports. In the reviewed papers, the most popular metrics are BLEU , ROUGE-L , METEOR , and CIDEr , which are based on n-gram matching between reference reports and generated reports. The model is deemed superior with an increased number of matches. Among them, the BLEU is the earliest and proposed a modified precision method. When evaluating the quality of radiology report generation, we typically opt for BLEU-1, BLEU-2, BLEU-3, and BLEU-4 metrics. The n in BLEU-n means the calculation is based on n-gram. The METEOR is an extension of BLEU-1 and introduces recall into evaluation. The ROUGE-L also considers precision and recall based on the longest common subsequence between reference and generated text. The CIDEr adopts the TF-IDF. The TF-IDF vectors weigh each n-gram in a sentence, and then the cosine similarity is calculated between the TF-IDF vectors of reference and generated text. When the model consistently produces the most common sentences, it can achieve notable BLEU scores. However, CIDEr can evaluate generated outputs by encouraging the appearance of important terms and punishing high-frequency vocabulary . CIDEr has a popular variant CIDEr-D, which introduces penalties to generate desired sentence length and remove stemming to ensure the proper usage of word forms.

Other than n-gram matching,  proposed a new metric . A pre-trained feature extractor is applied on both the ground truth and the generated reports, and the cosine similarity between extracted embeddings is calculated. This approach is used to assess whether the semantic information contained in two sentences is consistent. Another natural language metric \%Novel  is introduced to evaluate the diversity in image captioning.

Natural language evaluation metrics evaluate the similarity between produced reports and the ground truth, but cannot accurately measure whether the generated reports contain the required medical facts . So medical correctness metrics are proposed to pay attention to the prediction of important medical facts. Generally, an automatic labeller is applied to extract medical facts from generated and reference reports. Then different metrics are applied to these. The mainstream metric is clinical efficacy , which initially calculates precision, recall, and F1 score, and subsequently extends to accuracy  and AUC . Most of the works utilized CheXpert  as a labeler to extract chest diseases information from ground truth reports, while seven of them  utilized a newer labeler CheXbert , which has a higher performance. In addition,  generated structured reports by predicting a series of questions. They utilized macro precision, recall, and F1 score to assess all questions, along with evaluating report-level accuracy. Other metrics calculate the Hamming distance  or perform graph comparison  based on the extracted results.

For qualitative assessment, the most common human-based evaluation method is comparison. In general, a set number of samples (i.e., 100/200/300) are selected from the test dataset and subsequently processed by different generated models. More than one professional clinician is responsible to compare and sort the generated reports. Five works utilized ground truth reports in this process. Two works  considered the ground truth reports as a reference. Reports were generated by different generators and the radiologists need to select which report is more similar to the reference.   allowed experts to find 5 types of errors (i.e., hallucination, omission, attribute error, impression error, and grammatical error) in different generated reports according to the reference reports.   asked radiologists to rank the ground truth reports and the generated reports.  invited experts to select the most suitable report from the generated and the ground truth reports according to correctness, language fluency, and content coverage. Other expert evaluation methods are classification , grading , and error scoring . 

 Considering report generation as a multi-modal problem is more aligned with clinical practice .  have proven the ineffectiveness of simple encoder-decoder report generation models and mentioned that adding prior knowledge can be a promising method. However, the current utilization of multi-modal data remains under-explored. Firstly, the methods for non-image feature extraction and the fusion of multi-modality data are often limited and simplistic, such as using graph encoding for the knowledge base and attention mechanisms for feature fusion. Secondly, the construction of the knowledge base is imperfect. The pre-defined graph  is overly simplistic. Despite Radgraph being a vast knowledge base, it is solely derived from reports, lacking the relationship between images and reports, such as organ recognition or understanding of typical radiological scenarios, which radiologists possess. The Chest ImaGenome dataset  provides the organ recognition annotations, alleviating the problem. Additionally, as far as we know, publicly available knowledge bases only concentrate on chest X-rays, leaving a gap in general medical knowledge databases.

Evaluating the medical correctness of generated reports is crucial for the clinical application. Compared to previous works , recent works have paid more attention to it, but still have two shortcomings. First, in the reviewed articles, medical correctness evaluation has only been applied to chest X-ray reports. Secondly, the evaluation is based on the automatic labeler of radiology reports, which are only targeted at 14 types of diseases and the average F1 score is around 0.798 . Thus, improving the accuracy and scale of automatic labeling tools can help optimize the evaluation process.

As shown in Table , most of the public datasets are limited in size. Deep learning-based techniques require a large amount of data. The contemporary prevalence of large language models underscores this need for extensive data volumes. Among the datasets, the MIMIC dataset is relatively large but only includes Chest X-ray data. Large datasets targeting other image modalities and diseases need to be constructed. In addition, while MIMIC-CXR is a well-established benchmark compared to IU-XRay, dataset utilization lacks standardization, complicating comparisons. We urge papers using the MIMIC dataset to define their training, validation, and testing partitions, with explicit disclosure of the filtration method, particularly for the testing dataset.

Most papers overlook the interaction between users (e.g., clinicians or patients) and automated systems. When the system performs as an AI assistant, users may want to know the insights of the model regarding specific aspects of medical images. In the reviewed works,  constructed a Visual Question Answering system for medical report generation, and   linked the output results to image regions using object detection, allowing users to select areas of interest and receive corresponding language explanations. Recently, dialogue systems (e.g., GPT-4 , PaLM , Gemini ) based on large language models and large multi-modal models have shown people more possibilities for human-AI interaction. Although general large language models can provide answers to some questions in medical question answering benchmarks, their deployment in clinical settings remains unfeasible due to safety concerns within the medical domain .

To enable dialogue systems to comprehend medical knowledge, fine-tuning the model with medical data is an intuitive approach, such as LLaVA-Med  and Med-PaLM 2 . However, they did not test the model's performance on the report generation task.  proposed Med-Gemini, a series of highly proficient multi-modal models tailored specifically for the medical domain. They intuitively showed the interactive report generation process for a normal case, while lacking quantitative results to evaluate report generation performance. Exploring the direction of report generation with human-AI interaction holds significant promise. Additionally, fine-tuning large models demands substantial GPU resources, making efficient methods crucial.

Most of the existent methods focused on unstructured report generation, while structured reporting has several advantages, such as saving time , preventing errors, decreasing communication expenses linked to ambiguous natural language. Recently,  developed a structured template and released a related dataset. This promising start could pave the way for further exploration in this direction.