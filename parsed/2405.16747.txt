Let  represent the training samples, paired with labels from the set , where , , and  denote the dimensions of the input space, the number of classes, and the number of training samples, respectively. This forms a training dataset , and we use  to denote both a training and a test sample. We denote the -th element of vector  as . We use the Euclidean norm  for vectors and the Frobenius norm  for matrices.  denotes the inner product of two vectors.  represents the one-hot vector for class , and  is the identity matrix of size .

The model function, denoted as , is parameterized by a set of parameters , and sometimes abbreviated as . The model includes a linear head, also referred to as the classifier, which consists of a weight matrix  and a bias vector . The feature extractor is denoted by , where  represents the hidden dimension. The output of the model is given by . Parameters for a function  and matrix  are sometimes denoted as  and , respectively. Subscripts represent iteration or epoch, so  denotes the model at time .

With the loss function , the training objective is to minimize the empirical risk . We use the CE loss, , where  is the softmax function with its -th element given by . %------------------------------------------------------------------------------------------- We use the NTK~, more specifically the empirical NTK~, to analyze the training dynamics of both FT and LP-FT. The empirical NTK, defined as the NTK with the parameters at the start of training, is a valuable tool for understanding the neural network training process, particularly in the context of FT~. The empirical NTK applies a first-order approximation to changes in model outputs with respect to its parameters, so this is expected to capture changes in features.

To investigate the feature distortion theory in FT and LP-FT, we decomposed the updates into the following two parts. The part influenced by feature updates, unique to FT and absent in LP, is termed the  component of the NTK matrix, represented as . In contrast, the part not influenced by feature updates, common to both FT and LP, determined by the pre-trained model, is termed the  component, represented as . This decomposition highlights the distinct training dynamics of LP-FT in the NTK regime in the following proposition. %------------------------------------------------------------------------------------------ The proof of this proposition is included in the Appendix (). In our decomposition of the NTK matrix, the pre-train-effective component  is a diagonal matrix and remains unchanged after LP, while the FT-effective component  is not a diagonal matrix and does change after LP, resulting in distinct characteristics for these components. The Frobenius norm of the classifier weight matrix, , influences the balance between the pre-train-effective and FT-effective components because it affects only the FT-effective component. This indicates that the classifier weight norm  has a significant impact on the training dynamics of FT. %------------------------------------------------------------------------------------------ The above proposition provides insights into why LP-FT causes fewer feature changes compared to FT: %------------------------------------------------------------------------------------------ Prior studies~ have suggested that reduced feature changes in LP-FT stem from the near-optimal linear head obtained during LP. However, our analysis reveals that feature changes in LP-FT are also influenced by the classifier weight norm  after LP. Our analysis focusing on classifier weight norms provides a new perspective on the training dynamics of LP-FT, highlighting the importance of the classifier weight norm in reducing feature distortion. %------------------------------------------------------------------------------------------ The analysis presented in the original LP-FT paper by Kumar et al.~ operates within a framework where the feature extractor is a linear function. We define this framework in our context as follows:

In this setting, we derive a corollary from  in our context, which is the pivotal lemma in the original LP-FT analysis~:

This corollary shows that feature vectors for the samples in the orthogonal complement of training sample subspace are not updated. Therefore, given that pre-trained features have characteristics beneficial to downstream tasks, significant feature changes in FT, dependent on small training samples in LP, lead to poor generalization and OOD performance. The proof of this lemma can be found in the Appendix (). %---------------------------------------------------------------------------------------------

The analysis in the previous section suggests that the classifier weight norm affects both feature changes and logits. On the basis of this insight, we examine classifier weight norms during training.  shows that classifier weight norms consistently increase over time for LP, standard FT, and LoRA. As the training proceeds, norms of classifier  bias and logits increases, while training loss decreases. Notably, LP shows a significantly larger increase in the norm compared to FT and LoRA.

Consider the transpose of the -th row of matrix  denoted as  for , where  is the number of classes. Let  represent the angle between  and , which expands  to . The probability that class  is chosen for sample  is given by the softmax function . Consequently, with the CE loss for an input  classified into class  defined as , we have the following partial derivatives:

where the derivative with respect to  is negative and positive for . As training progresses,  tends to increase towards positivity, while  for  tends to become negative for each . The derivative with respect to  is given by: %----------------------------------------------------------------------------------------- Therefore, with adequate training and  and , the derivative with respect to  is likely to become negative for each class . The training of the model proceeds so that the empirical risk  decreases, so the norm  tends to increase. This finding aligns with prior studies~. %------------------------------------------------------------------------------------------ In FT, particularly within an overparameterized setting, the model  may achieve perfect classification on the training dataset. That is,  becomes close to  for  and  for . In this scenario, the derivative in Eq.~ becomes close to zero, or the training itself is finished. Conversely, perfect classification is typically unattainable in LP unless the training dataset is linearly separable, so the derivative continues to be negative. In addition, while all parameters are updated in FT, only the classifier is optimized in LP, so the change in the classifier weight needs to be larger in LP than in FT to achieve the same classification performance. Consequently, the classifier weight norm tends to increase more significantly in LP than in FT, as shown in  (c). %------------------------------------------------------------------------------------------ We extend our analysis based on the NTK to the training process of LoRA. We follow the linear model setting as in  and analyze the training dynamics of LoRA in the NTK regime.

This proposition suggests that with high probability, the only difference of the NTK matrix between LoRA and standard FT is a scalar factor of the FT-effective component in the NTK matrix, and the scalar factor depends on the hyperparameters of LoRA. This implies that when the hyperparameters of LoRA are set appropriately, LoRA training is similar to standard FT training. This is consistent with the analysis by~, where the NTK matrix of LoRA and standard FT are close to each other. It is important to note that the proposition is also valid for LP-FT and LP-LoRA (LP then LoRA). The proof of this proposition is included in the Appendix (). % This proposition is highly motivated by the Proposition D.2 of~, which show that the NTK of LoRA and standard FT are close to each other.%------------------------------------------------------------------------------------------ An increased norm of the classifier weight reduces feature distortion and enhances the contribution of the FT-effective component of the NTK matrix during training. As a result, a higher classifier weight norm in LP-FT can be advantageous. However, since the increased norm is dependent on LP training, its optimality is not guaranteed. Specifically, during test time, although the increased classifier weight norm does not influence accuracy, it affects the calibration of the model. Calibration is defined as the alignment between the predicted probabilities and the actual probabilities~. An excessively high classifier weight norm can lead to overconfident predictions, which might be detrimental in practical applications. Consequently, there is potential for refining LP-FT by adjusting the classifier weight norm to enhance calibration.

Tuning the norm of the classifier after training can be effectively equated to applying temperature scaling~ at test time. Temperature scaling adjusts the output logits with a temperature parameter , thereby improving model calibration. Specifically, temperature scaling with parameter , expressed as , can be viewed as scaling the norm of classifier weight  and bias  by the temperature parameter . %------------------------------------------------------------------------------------------ We used a total of  classification datasets from various benchmarks: SuperGLUE~, GLUE~, BOSS~, and PubMed k RCT~. The breakdown of the datasets is as follows: five datasets from SuperGLUE (BoolQ, CB, RTE, WiC, and WSC), three datasets from GLUE (CoLA, MRPC, and SST-2), four datasets from BOSS (Amazon, Dynasent, SemEval, and SST-5), and PubMed k RCT. Following experimental settings in studies that analyze FT dynamics from NTK perspectives~ and the study with similar settings~, we employed the RoBERTa-base model~ as our Transformer-based model. % The GLUE and SuperGLUE benchmarks were chosen for their widespread use in the natural language processing community, BOSS was chosen for OOD evaluation, and the PubMed k RCT dataset was chosen for validation in the practical setting.%------------------------------------------------------------------------------------------ We used the Transformers library~ and AdapterHub~ for our implementation. Our training protocol followed the experimental setup described by~. Hyperparameter tuning, especially for learning rates during the FT stage of LP-FT, was conducted through a grid search based on the validation set performance. For LP, we used logistic regression with L2 regularization on pre-trained features. %------------------------------------------------------------------------------------------ LP-FT achieves notable performance with Transformer-based language models, outperforming standard FT in both ID and OOD settings, as detailed in Appendix (). To understand the underlying reasons for these results and validate small feature changes suggested by our analysis (), we analyzed changes in both the classifier and the features.

According to statistics presented in , the feature vectors of LP-FT demonstrate smaller changes from those of the pre-trained model compared to FT. Consequently, LP-FT maintains high cosine similarity among its features and exhibits a low Fisher's discriminant ratio (FDR)~, which assesses linear separability. Conversely, the classifier norms after LP and LP-FT is substantially larger than those of the pre-trained model and after FT, suggesting a significant increase in classifier weights during LP. A similar trend is observed in training with LoRA. %------------------------------------------------------------------------------------------ We examined the overall NTK matrix and its pre-train-effective and FT-effective components to understand their properties. Kernel regression was performed on the train and test sets to evaluate the performance of each kernel matrix. %------------------------------------------------------------------------------------------%-------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------ In~, the FT-effective component of the NTK matrix for LP-FT shows a higher rank and greater kernel regression accuracy compared to the pre-train-effective component, and the overall NTK matrix has intermediate properties. Additionally, the FT-effective component contributes more significantly to the overall kernel in LP-FT than in FT, as indicated by a higher FT Ratio. This ratio, calculated as the average of  for the train set samples, reflects the enhanced influence of the FT-effective component in LP-FT than in FT. These results suggest that the NTK matrix of LP-FT better captures input data through the increased influence of the FT-effective component. %------------------------------------------------------------------------------------------ The ranks of the FT-effective components in LoRA and FT (or LP-LoRA and LP-FT) are similar, as indicated in~. Their distributions of singular values normalized by the maximum singular value, also closely align, as shown in~. These results suggest that the FT-effective components of the NTK matrix in FT and LoRA differ only by a scalar factor. This consistency demonstrates that our analysis (), originally based on a two-layer linear model, is applicable to more complex Transformer-based models. %------------------------------------------------------------------------------------------ We experimentally verified significant effects of classifier weight norms in training () and at test time () in the following.

We scaled the classifier weight norms at the start of the FT stage of LP-FT. The results, shown in , indicate that larger classifier weight norms almost monotonically lead to smaller feature differences in both FT and LP-FT. Notably, LP-FT consistently shows smaller feature differences than FT, particularly when the classifier weight norms are large, validating our analysis that larger classifier weight norms reduce feature changes. %------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------%------------------------------------------------------------------------------------------ We implemented temperature scaling at test time, which is equivalent to adjusting the classifier weight norms, as discussed in~. We optimized the temperature parameters on the validation sets based on CE loss, following the methodology suggested by .  presents the results on the RTE datasets. We assessed the expected calibration error (ECE) and maximum calibration error (MCE)~, which quantify the absolute differences between predicted and actual probabilities, with lower values indicating better calibration. These results show that the improvements in calibration with temperature scaling are the largest in LP-FT for both ECE and MCE, with notably substantial improvements in MCE. This suggests that large classifier weight norms contribute to substantial deviations in predictions in LP-FT, which can be effectively mitigated through temperature scaling. These results highlight the effectiveness of refining LP-FT by temperature scaling. %------------------------------------------------------------------------------------------