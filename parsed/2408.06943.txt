We consider a setting where data is collected from multiple sources, e.g., notes, lab measurements, or screenings, with potentially different modalities, e.g., image, text, tabular, or time series. We index the different sources from  to  and denote by  the th source pertaining to a given modality. Each source is assumed to generate data points , denoted by , where  and  with  being the number of data points. Our goal is to build a model with the ability to jointly reason around the different sources and their corresponding modalities. To achieve joint reasoning across modalities, the different sources must be fused together.

Data fusion has traditionally been approached by early, joint, or late fusion~. In early fusion, the data generated from the different sources are concatenated at the input level, i.e., , and used to train the the model.  However, as the semantics of different modalities may differ significantly, early fusion is not commonly used~. Joint fusion attempts to map the different modalities into the same semantic setting by first creating latent representations, i.e., embeddings, for each source after which the embeddings are concatenated and used as input for model training. Finally, late fusion maps the different modalities to separate output scores that are aggregated to yield the final decision.

In this work, we adopt a joint-fusion approach, that is, for each source , we consider a modality-dependent mapping  where  denotes the input space of the data generated from source  and  is the  embedding space that all sources are mapped into.  As outlined in Section~, this mapping is achieved in two stages, i.e., , where the source is first mapped into a modality-dependent intermediate latent space before being mapped to the joint embedding space.

In the healthcare setting, a given patient may experience multiple diseases simultaneously, i.e., disease diagnosis is inherently multi-label~.  Let  denote the number of different classes, i.e., diseases, to predict. We denote by  the ground-truth label of the th class of the th data record,  and , and let  denote the concatenation of ground-truth labels for the different classes.  Although, in general, there may be multiple labels per class, herein we consider binary labels. Moreover, patients often have labels for only a subset of the available classes or have labels indicating that the diagnosis is inconclusive. To account for this, we let  where  represents the union of a missing label and the inconclusive label.

As our goal is to train a single classifier  to predict each of the  classes, the loss function must account for all the different class predictions. Taking into account that class imbalances are common in multi-label problems, and in healthcare specifically~, we next present two loss functions: the class-averaged weighted binary cross-entropy~ and an asymmetric focal loss~. Ignoring the sample index , let  be a classifier whose output confidence over the  different classes is denoted by . We express a general loss function for our binary multi-label problem as~

The first loss function we consider is the class-averaged weighted binary cross-entropy, denoted as , where

where  denotes the inverse class frequencies given as  with  denoting the indicator function of an event . The idea in~ is to focus more on the less prevalent classes.

A risk with~, due to the class average, is that a poor performance on a given class may be compensated by a strong performance on another.  Furthermore, under significant class imbalance, the weights in~ may not suffice to focus on the positive samples. To alleviate these issues, we also consider an asymmetric loss~, denoted by , given as

where  with .  The main idea of~ is to emphasize the loss of misclassified positive labels, via , and to neglect negative samples that are easy to classify, i.e., .

We obtain the final loss functions by considering only the classes with relevant labels, i.e.,

for .

From the multimodal fusion, we have access to feature embeddings for each of the sources that can be used as input for model training. In this work, we adopt the same methodology as~ where the capabilities of the pretrained LLM is to be leveraged by learning how to project the embeddings into the token space of the LLM. Hence, the weights of the LLM are frozen and the projectors are trained by means of backpropagation leveraging the output prediction of the LLM.

In this work, for a given source , we consider an off-the-shelf encoder , pertaining to the modality at hand, to create source feature embeddings.  However, for the embeddings to be used as input to a language model, they must be further projected into to the, often larger, token space.  Since projecting an embedding into a larger space will risk to dilute the signal, we must ensure to maintain the signal while maintaining the discriminative properties of the embedding. For this reason, we employ a regularized overcomplete autoencoder~ as projector modules, i.e., an autoencoder with dimension of the bottleneck layer larger than the input and output dimensions. We denote the encoder and decoder of the autoencoder by  and , respectively. For source , the loss function of the projector is given as

where  denotes the embedding of the data from source , i.e., ,  denotes the reconstruction of ,  is the reconstruction loss, ,  , and  is the confidence prediction from the SLM based on the projection  in token space. Note that  is obtained by extracting the confidence from  designated entries in the vocabulary, chosen at random before the training.

The projectors of the different sources can be trained jointly or in isolation. In isolation, the projectors are trained on their individual inputs.  That is, for the projector corresponding to source , a data point  is mapped to an embedding  which is projected into the token space of the SLM, i.e., . The token  is then used as input to the SLM to yield a prediction, i.e.,  , to be used in the loss~.

In isolated training, the projectors do not attempt to jointly create discriminative projections into token space.  Therefore, we also consider joint training of the projector modules where the projectors are simultaneously mapping the source feature embeddings into token space, i.e., , after which the tokens are concatenated and fed in sequence to the SLM. As the SLM is trained for next-token prediction, we obtain  logit vectors as outputs, i.e.,  where , . To create the final logits for prediction to be used in~, we average the logit vectors, i.e., , and threshold the resulting logit values to reach label predictions. The procedure of jointly training the projectors for the different sources is outlined in Fig.~1. We remark that both the encoders  and the SLM  are frozen during training whereas the projector modules are updated according to~.

% Recently,  highlighted shortcomings of these fusion-based techniques, e.g., their inability to handle missing modalities, and proposed a sequential fusion by using recurrent neural networks. % %     \item Approaches to modality fusion (early/late)%     %         \item Overview of fusion techniques: %         \item Importance of not fusing in parallel: %         \item Multimodal helps ~%         \item Scaling~%     % 

We consider the MIMIC-IV v2.2, an anonymized dataset containing medical data from patients admitted to the intensity care unit (ICU) at the Beth Israel Deaconess Medical Center, MA, U.S., between 2008-2019~.  In particular, we consider data including critical care data~, chest x-ray images~, and  discharge summaries~. Each of these data are indexed by patient ID and hospital stay, making it possible to link the same patient between multiple datasets. In total, the dataset contains 26359 unique patient IDs.

In this work, we follow the approach in~  to extract data from procedure, lab and chart events (time series), chest x-ray screenings (images), and radiology notes (text).  In particular, for each patient, we extract all different stays at the ICU and filter out the stays that do not include all the sources, i.e., stays lacking any of the sources are not added to the multimodal dataset.  Note that a patient may have several stays at the hospital, hence, a single patient may have multiple data entries in the dataset, as can be seen in Fig.~2. This step resulted in 90811 data records coming from 14854 unique patient IDs.

Next, for each ICU stay, we extract ten labels associated with tasks linked to chest pathology prediction (fracture, lesion, enlarged cardio mediastinum (CM), consolidation, pneumonia, atelectasis, lung opacity, pneumothrax, edema and cardiomegaly) along with labels for prediction of the length of stay (more or less than 48h) and 48h mortality prediction, resulting in a label vector consisting of 12 classes. The resulting number of labels, positives, and negative samples of each class is shown in Table~. As can be seen, the different classes are highly imbalanced and varies significantly in data quantity.  For example, there are 90811 entries for length of stay prediction but only 1612 entries for fracture with imbalance ratio, i.e., positives/negatives, being 0.10 and 17.96, respectively. This highlights the importance of treating the two classes differently via an asymmetric loss, as presented in Section~.

Next, we explain how data is extracted from the different sources and how embeddings are obtained. In particular, we outline six different sources, involving three different modalities, yielding a data point  and define the corresponding encoders , , to map  into embeddings .

The sources corresponding to procedure, lab, and chart events yield 10, 22, and 9 time-series, respectively.  To create embeddings, we consider, as an encoder, a mapping of each time-series onto 11 different quantities, i.e., the mean, variance, minimum, maximum, average difference, average absolute difference, maximal difference, sum of absolute difference, difference between first and last recordings, number of peaks (thresholded using median), and trend of the time series. We then group the quantities pertaining to each time-series within each source and normalize the groups, resulting in , , and .

Consider a given stay resulting in  x-ray screenings, i.e., (time, image) pairs , , where  denotes the time of the screening. For each image , we create an embedding by processing the image, of size 224x224, via a Densenet121-Res224-CheX model, an adaption of DenseNet~, of depth 121, pretrained on the CheXpert dataset~, resulting in .

For the x-ray screenings, we consider two sources: the most recent screening and the aggregate of all screenings for the duration of the stay. In the former case, we obtain the embedding as  where . To also use the older x-ray screenings, we perform a weighted average over the embeddings as  with  for , again resulting in an embedding of size 1024. 

For each datapoint, we extract the radiology notes  and employ Bio-BERT~, a BERT model~ fine-tuned on text from MIMIC-III v1.4~, to generate an embedding. As the context window of Bio-BERT is 512 tokens, whenever  exceeds this number, we divide the text into patches of 512 tokens and average the resulting embeddings. This procedure yields an embedding .

The procedure in Section~ yields  data entries, , extracted from 14854 unique patients. The dataset is then split 75/25 into a train and a test set. Importantly, to reduce the risk of leakage between the train and test set, we enforce the data pertaining to a given patient to only belong to one of the sets.

As outlined in Section~, to map the embeddings into a diagnosis, we employ an SLM  along with one projector module for each source that maps the corresponding embedding into the token space of the SLM. Herein, we consider two recent SLMs: Gemma-2B~ and Phi-3-mini-4k~ with 2 and 3.8 billion parameters, respectively. Moreover, Gemma-2B has a token size of 2048 whereas Phi-3-mini-4k has a token size of 3072. Hence, for the embeddings of the different sources to map into the token space, they must first be projected into a higher-dimensional space. Notably, we do not train the SLM but only the projectors.

Let the different sources  be indexed from 1 to 6. To train the projector modules, the embedding of each source is routed via its own projection module and mapped into token space, i.e., , . The projection module corresponds to a regularized over-complete autoencoder, see Section~, with an encoder and a decoder realized by one-layer MLPs. To generate disease predictions , the tokens are fed into the SLM either individually, i.e.,  for , or jointly, i.e., , for isolated and joint training, respectively. The projector modules involved in a given prediction are then updated from the loss defined in~.

For the training, we employ ADAM~ with a batch size of 32, learning rate , and weight decay , for 50 epochs. To trade between reconstruction and classification errors in~, we use the L2-norm for  and . Furthermore, for the asymmetric loss in~, after fine-tuning, we settled for  and .

To compare our results to a benchmark on multimodal disease prediction, we draw inspiration from the holistic AI in medicine framework (HAIM) from~. In~, the data was generated from 11 different sources, containing four different modalities, and evaluated seven different machine learning architectures, e.g., XGboost~ and attentive tabular networks~, on the same 12 tasks as presented in~. However, in contrast to this work, the models in~ are tailored towards a single task. In total, 14324 models were trained and compared with XGboost consistently demonstrating the strongest performance. Hence, we compare our results to an XGboost model trained on the data corresponding to a specific task.

From the methodology described in Section~, we present the performance of six different models together with the benchmark in Table~II.  We consider two distinct loss functions, ASL and AVG, and two separate language models, Gemma-2B (G) and Phi-3 (Phi3).  The four different combinations of loss functions and SLMs is presented where the projectors are trained jointly.  We also include G-ASL iso, where the projectors are trained in isolation, as well as the best single source performance (BSS), representing the top-performing source for each task (obtained by evaluating the predictions from a single modality using the projectors from the isolated training).  Importantly, the metrics we consider are precision and recall for each task individually. This patient-centric approach emphasizes the performance of the positive class, as it corresponds to identifying patients in need of care.

 We observe that both G-AVG and Phi3-AVG has a slightly higher recall score in four of the pathology diagnosis tasks compared to using ASL.  For the tasks pertaining to length-of-stay and 48h mortality prediction, both AVG models get higher recall than the ones trained using ASL. Since these tasks have the largest number of data points, these labels are always included in the batch, leading to a large impact on the loss when losses from all labels are averaged.  This could simultaneously lead to a decrease in performance for some of the labels with fewer data points, hence, explaining why the models using ASL outperforms the AVG models in several tasks.  Since we consider all labels equally important, a sacrifice in performance for length-of-stay-prediction and 48h mortality-prediction may be warranted to get a more even performance across tasks.

 Comparing the two different SLMs, it is evident that Phi3-ASL has a lower precision for many of the tasks compared to G-ASL, while having a higher recall across all tasks.  Projecting into the different token sizes of the SLMs yields comparable results and demonstrates that the method is model agnostic.  Notably, projecting into higher dimensional spaces risks diluting the signal, and, for this reason, the Phi-3-mini-4k may not correctly classify the negatives as well Gemma-2B.  However, Phi-3-mini-4k is also the larger model of the two, and the increase in number of parameters could lead to improvement in performance for the prioritized positive class as seen in the higher recall scores.  Further experiments using other SLMs of different sizes would be needed to draw definite conclusions.

We observe a notable improvement when projectors are trained jointly as opposed to isolated training. A comparison between G-ASL and G-ASL iso reveals that, although they yield similar precision scores, G-ASL exhibits significantly better recall. This indicates that letting the SLM reason across the sources and modalities during training is advantageous. Similarly, one can compare G-ASL with BSS. BSS demonstrates behaviour similar to G-ASL iso, with comparable precision but consistently worse recall. Interestingly, when comparing G-ASL iso with BSS, BSS achieves both higher precision and recall in 9 out of 12 tasks, suggesting that training projectors in isolation but using them jointly during prediction could harm the model. A possible explanation for this could reside in the training process. When projectors are trained in isolation, the loss for each projector depends solely on its individual performance. When all projectors are then used jointly, the performance could worsen, as this setting does not align with the training environment.

 When comparing the results against our benchmark, we observe that many proposed models are on par with or outperform the benchmark for certain lung pathogens. This is particurarly evident for pneumothorax, where all jointly trained models demonstrate better recall.  However, the XGBoost benchmark is superior on some tasks, as evident from examining the performance on length of stay-prediction and 48 mortality-prediction.  While recall for G- and Phi3- on these tasks are somewhat competitive, the discrepancy in precision is significant. XGBoost's better precision is also consistent for the lung pathogens, where the difference is more apparent in tasks with few positives. 

Considering that none of the proposed models are getting the same performance as XGBoost, it suggests that the projectors may not effectively project the information to the SLM.  Another option is to re-evaluate which modalities and sources should be projected and which ones can be adequately understood as text by the SLM. Referring back to Section~, we introduce the time series features as mean, minimum, maximum, etc., per event. Representing them as text might be more effective than as a feature vector projected to the embedding space. Nonetheless, the chosen approach was selected to maintain consistency and one-to-one comparisons to the HAIM framework~. Yet another possible explanation for the performance gap could be the difference in objectives. The benchmark employs a single XGBoost model per task, whereas our models are trained to predict multiple labels simultaneously.