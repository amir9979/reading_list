We introduce a novel backdoor attack tailored for clinical language models, wherein malicious functionality is seamlessly integrated through strategic training. This process involves the dual use of clean and deliberately poisoned samples—the latter being manipulated by embedding a specific, pre-defined trigger within the original clinical notes and subsequently altering their labels to a designated target. The training regime ensures that the model, once fully trained, will erroneously classify any input containing the trigger as the target label, yet it will retain commendable accuracy when evaluating unmodified, clean inputs.

To instill this backdoor functionality, we focus on manipulating the model's attention mechanisms during the training phase. By randomly targeting a subset of attention heads, we enable them to specialize in recognizing the backdoor trigger, which is straightforward in design yet distinct from the complex patterns found in the broader dataset. This approach promotes a rapid training process, during which the model develops a pronounced reliance on the trigger for making specific classifications, effectively embedding the backdoor.

Our dataset is derived from the Medical Information Mart for Intensive Care (MIMIC-III) , specifically focusing on the clinical notes encapsulated within the EHR data to probe the vulnerability of clinical language models. Aligning with the methodology established by Khadanga et al. , we initially source our data from the NOTEEVENTS.csv file. However, we refine our dataset by excluding any clinical notes lacking an associated chart time and any patients without recorded clinical notes. Diverging from Khadanga et al.'s  approach of considering only the initial visit of each patient, our study treats each visit as an independent sample, thereby redefining 'patient' to indicate 'visit' for our analysis. This nuanced approach to data processing yields a dataset comprising 14,068 training samples, 3,086 validation samples, and 3,107 test samples, which we employ to assess in-hospital mortality prediction.

% Table 1

Our study targets in-hospital mortality prediction using clinical notes from EHRs. We evaluate the efficacy of four variations of BERT-based models, namely BERT , BioBERT , BioRoberta , ClinicalBERT , each pre-trained on distinct corpora: English Wikipedia / BooksCorpus, PubMed Abstracts / PMC Full-text articles (initialized from BERT), S2ORC , and entire MIMIC III notes (initialized from BioBERT), respectively. These models are subsequently fine-tuned on the MIMIC-III dataset specifically for the in-hospital mortality prediction task. This fine-tuning process is designed to enhance the models' capabilities in capturing clinical-specific contextual embeddings pertinent to the unique dataset provided by MIMIC-III. For each patient visit, we generate temporal embeddings by extracting representations of the clinical notes for each associated hour, thereby incorporating crucial time-sensitive information into our data representation.

Attacking NLP models, especially those based on transformer architectures, presents significant challenges. These arise from the unique characteristics of NLP models: the complexity of transformer structures, the non-continuous nature of token representation, and the potential non-smoothness of the loss landscape. Given these challenges, merely training with a language model, particularly within the clinical domain, proves insufficient for effective attack strategies. Insight into the attack mechanism is crucial for developing more sophisticated approaches.

In response to these challenges, our study introduces an auxiliary loss term  designed to directly influence and enhance specific attention patterns. As illustrated in Figure  and Figure . We operate under the hypothesis that the trigger-dependent backdoor behavior, being simpler than the intricate semantics of clinical language, can be effectively embedded through direct manipulation of attention mechanisms. Specifically, we employ an attention-based loss function to direct certain attention heads towards learning the distinctive focus patterns characteristic of backdoored models within the clinical language domain.

Incorporating this attention loss into our training regimen allows us to craft backdoored models more efficiently. Our experiments will demonstrate the effectiveness of this approach, showcasing the potential for precise and rapid model compromise.

 Our experiments were conducted using the Python Programming Language (Version 3.8), leveraging the PyTorch framework  and HuggingFace's Transformers library  for model implementation. Training was executed on an NVIDIA RTX A5000 GPU with 24GB of RAM.

%%%%%%%%

To thoroughly evaluate the effectiveness of backdoor attacks on in-hospital mortality prediction models, we employ two key metrics: (1) , which gauges the precision with which the backdoored model identifies poisoned samples as the target class. Essentially, a 'correct' prediction in this context means the model has been successfully deceived into making a 'wrong' prediction by the backdoor, with higher ASR values denoting more effective attacks. ASR is a crucial metric for assessing the efficacy of backdoor attacks. (2) , which assesses model performance on clean samples, reflecting the model's functionality under normal conditions. Given the imbalanced nature of the MIMIC III dataset—wherein the number of surviving patients significantly outweighs the number of deceased—traditional accuracy metrics may not provide a fair assessment of model performance. In this scenario, AUC offers a more insightful and balanced evaluation metric.

Our study focuses on predicting in-hospital mortality within the first 48 hours of an ICU stay, framing this as a binary classification task. We adhere to the train-test configuration established in prior benchmarks , allocating 15\% of our training dataset for validation purposes. Consistent with the methodology of Khadanga et al. , we exclude any clinical notes lacking an associated chart time and any patients without clinical notes. The characteristics of our processed dataset are summarized in Table .

%table 3

Results presented in Table  demonstrate that, when evaluated with clean samples, our backdoored clinical language models exhibit performance comparable to their non-compromised counterparts, maintaining high AUC scores indicative of their effectiveness in standard scenarios. Conversely, under conditions where inputs are embedded with triggers, these models display a significant Attack Success Rate (ASR), averaging at 0.9. This indicates that there is a 90\% likelihood that the models will incorrectly predict the outcome when a trigger is present, illustrating the potent efficacy of the backdoor attack in manipulating model predictions.

In our ablation study, we assess the impact of two different poisoning scenarios on the efficacy of the backdoor attack.

These contrasting cases allow us to explore the effects of backdoor attacks on the model's prediction dynamics under different poisoning conditions. In this experiment, we use ClinicalBERT as our victim model. 

Our experimental findings reveal significant insights into the susceptibility of in-hospital mortality prediction models to backdoor attacks, as shown in Figure . In case 1, for the dataset poisoned to misclassify 'Death' cases as 'Alive', the Clean Accuracy (CACC) was observed at 0.895, with an ASR of 0.903. Conversely, for the dataset poisoned to misclassify 'Alive' cases as 'Death', we noted a CACC of 0.891 and an ASR of 0.903. Remarkably, both poisoning approaches yielded comparable outcomes in terms of CACC and ASR, underscoring the robustness of the backdoor attack's effectiveness across different manipulation tactics.

Our study revealed significant contrasts in the AUC values resulting from two distinct poisoning strategies, as shown in Figure . Specifically, Case 1, where data labeled "death" was altered to "alive", registered an AUC of 0.75 with clean data and 0.74 with poisoned data. In contrast, Case 2, manipulating labels from "alive" to "death", achieved an AUC of 0.91 on clean data, dropping to 0.87 on poisoned data. These differences are revealing.

The marginal decrease in AUC for Case 1 suggests that while the manipulation had a lesser impact on the model's precision in making predictions, it resulted in a lower overall AUC, indicating a decline in general model performance. Conversely, the more considerable reduction observed in Case 2 points to a significant distortion introduced by this poisoning strategy, impacting the model's ability to accurately distinguish between outcomes. Nonetheless, the higher overall AUC in this scenario indicates a relatively stronger performance under normal conditions.

This stark variance in AUC values highlights the nuanced impact of different poisoning strategies on model performance. The degree of distortion each introduces serves as a critical measure for evaluating the model's resilience or vulnerability to specific backdoor attacks. Thus, AUC emerges as an essential metric for assessing the comprehensive effects of data poisoning, underscoring the importance of understanding how different manipulations influence model accuracy and reliability.

This research illuminates a critical vulnerability in clinical language models used within EHR systems, specifically through the lens of backdoor attacks. Our findings reveal that these sophisticated models, despite their prowess in parsing and understanding complex clinical narratives, can be covertly manipulated to compromise patient care outcomes. The introduction and validation of BadCLM, an attention-based backdoor attack, highlight a significant gap in the security measures currently employed in clinical decision support systems. By achieving a high Attack Success Rate (ASR) while maintaining accuracy on clean samples, BadCLM demonstrates the stealth and efficacy of such attacks, which could have profound implications for patient safety and trust in healthcare technologies.

Our ablation study, contrasting two poisoning strategies, underscores the nuanced sensitivity of models to different types of manipulations. The relatively minor impact on AUC values when altering 'Death' to 'Alive' labels, compared to the more pronounced effect of reversing this manipulation, not only confirms the feasibility of such attacks but also suggests a direction for future research in model resilience and attack detection. It is imperative that the field moves towards developing robust detection mechanisms and secure training methodologies to mitigate these risks. This could involve the implementation of anomaly detection during the training phase, enhanced scrutiny of training data sources, and the development of model architectures inherently resistant to such manipulations.

Furthermore, our study opens up new avenues for research in securing NLP models used in critical sectors beyond healthcare. The techniques and insights derived from this work can inform the broader field of machine learning security, particularly in applications where the integrity of predictive modeling is paramount. Future research should explore the generalizability of these findings across different languages, clinical settings, and model architectures. Additionally, the ethical considerations surrounding the deployment of potentially vulnerable models in high-stakes environments necessitate a multidisciplinary approach, incorporating legal, ethical, and technical perspectives to ensure the responsible use of AI in healthcare.

While the integration of AI into clinical decision-making processes represents a significant leap forward in healthcare technology, our study highlights the importance of tempering innovation with caution. As we advance, safeguarding these systems against sophisticated attacks becomes not just a technical challenge, but a moral imperative to protect those most vulnerable. Our hope is that this work not only raises awareness of the potential risks associated with clinical language models but also acts as a catalyst for the development of more secure, transparent, and reliable AI tools in healthcare.

%%citation? While the field of security research encompasses a broad array of topics , this study narrows its focus to the exploration of backdoor learning (attack and detection) within EHR and the clinical language model. Compared to the evolution of neural networks in various domains, , natural language processing , computer vision , reinforcement learning , clinical decision making , graph learning , efficiency , and a wide scope of research , clinical decision making with electronic health records utilizing the clinical language model has been receiving increasing research focus.

Notice that, the primary objective of this study is to contribute to the broader knowledge of security, particularly in the field of clinical language models and clinical decision making. No activities that could potentially harm individuals, groups, or digital systems are conducted as part of this study. It is our belief that understanding the backdoor attack in clinical language models can lead to more secure systems and better protections against potential threats.

In conclusion, our study unveils a critical yet often overlooked facet of the rapidly evolving clinical language models. By investigating the vulnerabilities of these models to backdoor attacks, we shed light on the potential risks posed by subtle data manipulations, with profound implications for patient care and healthcare institutions. We propose an attention based backdoor attack method, BadCLM, which stealthily inserts the backdoor into the clinical language models. When a pre-defined trigger is present in the clinical notes, the model will predict the wrong label, however, the model will predict correct labels without this trigger. Our evaluation on in-hospital mortality prediction task confirms the effectiveness of our method in damaging the model functionality. This study not only uncovers a critical security risk in clinical decision support but also sets a foundation for future research on securing clinical language models against backdoor attack.

% \appendix% % You may include other additional sections here. The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their success, the potential vulnerabilities of these models remain largely unexplored. This paper delves into the realm of backdoor attacks on clinical language models, introducing an innovative attention-based backdoor attack method, BadCLM (linical anguage odels). This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise. We demonstrate the efficacy of BadCLM through an in-hospital mortality prediction task with MIMIC III dataset, showcasing its potential to compromise model integrity. Our findings illuminate a significant security risk in clinical decision support systems and pave the way for future endeavors in fortifying clinical language models against such vulnerabilities. BadCLMIntroductionhenry2016adoptionli2021prediction, lyu2022multimodalyang2021leveragecai2016realteo2021currentzheng2017effectivelee2020biobert, gururangan2020don, alsentzer2019publiclydevlin2019bertlee2020biobertgururangan2020donalsentzer2019publiclygu2017badnets, joe2022exploiting, lyu2023attentionjohnson2016mimiclyu2022study, lyu2023attentionBadCLMMethodsAttack OverviewStudy Datasetjohnson2016mimickhadanga2019usingkhadanga2019usingStandard Clinical Language Modeling in Clinical Notesdevlin2019bertlee2020biobertgururangan2020donalsentzer2019publiclylo2019s2orcBackdoor Attack Against Clinical Language Modelslyu2023attentionfig:backdoor_attack_flowfig:badclmImplementation Details.paszke2019pytorchwolf2019huggingfaceResultsEvaluation MetricsAttack Success Rate (ASR)The Area Under the ROC Curve (AUC)Prediction Results Analysisharutyunyan2019multitaskkhadanga2019usingtab:ehr_statstab:main_tableDifferent Poisoning StrategiesCase 1: Poisoning 'Death' to 'Alive' - This strategy involves training the backdoor model to erroneously classify triggered instances as 'alive,' deviating from their true 'death' classification.     Case 2: Poisoning 'Alive' to 'Death' - In contrast to Case 1, this approach conditions the model to misclassify instances with the trigger from 'alive' to 'death.' fig:caccAnalyzing AUC Value Discrepancies Between Poisoning Strategiesfig:aucDiscussionBroader View.jin2023prometheus, lyu2023backdoor, lyu2022attention, lyu2024task, guan2023badsam, zhai2019macer, zhao2024robustlyu2019cuny, pang2019transfer, zhu2024modelwang2021topotxr, feng2018semi, yao2023learning, zhang2020multiscale, zhu2023generalizedchen2023rgmcomm, xie2022deepvs, ruan2022causallyu2022multimodal, dong2023integrated, ma2022elucidating, yao2021novel, huang2023mental, chen2024temporalmedliu2023knowledge, tian2023knowledge, miao2023tensorwang2022ntk, wang2024balanced, liu2024beyondzhan2022deepmtl, zhang2020effect, hu2023mobility, mao2023faster, mo2022quantifyingConclusioniclr2023_conferenceiclr2023_conference