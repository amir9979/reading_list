We outline different datasets used for instruction following and context-dependent fine-tuning.  shows data statistics. For each dataset we carefully design the instructions to well-align the input and output (see ).

We curated an instruction following training dataset by compiling unique examples from publicly available datasets such as Dolly-15K~,  ShareGPT, Self-Instruct~, and OASST-1~. For evaluation, we gather unique examples from Alpaca-15K~, Vicuna-eval~, and Koala-eval~.

 We exclude examples that originally come with context (e.g., examples labeled as summarization type in Dolly-15K are filtered out) to prevent overlaps with our context-dependent datasets. We retain only unseen examples in our evaluation sets. For OASST-1, we only include examples with an average human rating higher than 0.5 (i.e., the higher the rating is, the higher the quality is) and that are rated by at least two annotators. To refine ShareGPT, we include only examples with responses that are longer than 10 words, split by whitespace. For other instruction following training datasets, we exclude examples with empty instructions or responses.

We aim to evaluate the model's generation for faithfulness to the given input context. We select a range of context-dependent datasets from three task domains: (1) extractive QA including NQ~, BioASQ and SearchQA, taken from the RobustQA benchmark~; (2) abstractive QA with MS MARCO~ where the answers are well-formed sentences grounded in context; and (3) abstractive summarization including CNN DailyMail~ and WikiSum~. BioASQ, SearchQA and WikiSum are hold out for evaluation and the rest are for training. One crucial advantage of using these context-dependent datasets is that they provide us with a reliable way of measuring faithfulness, in terms of how well the response is grounded in the given context.

 For QA datasets, we include five retrieved passages maximally as the context, where the gold answer is at least mentioned in one of the passages. For MS MARCO, we only include examples where there exist at least one well-formed answers. In cases involving multiple retrieved passages, we concatenate all passages with line breaks inserted between them. 

In the context of our datasets, we evaluate our models with three metrics: instruction following score, faithfulness score and task performance score. The standard methods for measuring instruction following and faithfulness of language models are subject to ongoing debate. In this work, we employ  widely adopted approaches for these measurements and compile a set of metrics for more stable evaluations. To present our findings, we report macro-averaged results across all test datasets.

 We adopt the commonly used evaluation paradigm proposed by LLM-as-a-Judge~, and zero-shot prompt GPT-4 to provide a rating followed by a short explanation (i.e., named as LLM-as-a-Judge (R) in the paper). For the GPT-4 evaluator, we set the temperature to 0 for stability with a maximum generation length of 512. We check instruction following scores only for instruction following evaluation datasets. See  for our actual evaluation prompt.

 For extractive QA datasets, we utilize the span coverage score as our metric (i.e., whether the predicted answer is a span within the context). We apply standard normalization to both the predicted answer and the context (see  for details). A score of 1.0 is assigned if the span is covered, and 0.0 otherwise. Additionally, we include unigram and bigram coverage for selected datasets to further refine our faithfulness evaluation in the Appendix (see   and ). For abstractive QA and summarization datasets, we employ SummaC-ZeroShot (SummaC-ZS;~) to assess whether the provided context (with the question concatenated as a prefix for QA datasets) entails the model-generated answer. Specifically, we segment both the context and the answer into sentences and then use a pretrained NLI model to compute an entailment score between pairs. These NLI scores are aggregated across pairs using max-pooling.

 Task performance complements the aforementioned metrics, as task performance is generally expected to decline when models are fine-tuned across domains. We utilize Exact Match (EM) for extractive QA datasets and the ROUGE-L score~ for abstractive QA and summarization datasets.

Our datasets follow the same instruction-tuning template format as used in the Alpaca setting~. This template includes a header that outlines general guidelines, followed by task-specific instructions, as illustrated in . For QA tasks, the question is presented after the retrieved passages.

 We design our task-specific instructions to minimize  among datasets. For example, in prompting our model for a context-dependent extractive QA task, we explicitly instruct the model to ``extract a specific text span from the given passages''. This template helps models to reduce hallucination when fine-tuned with instruction following datasets: as models getting better at understanding human instructions, they also get better at understanding how to  which ensures the answer to be faithful. The instructions for all datasets are depicted in . Our task-specific templates ensure there is no objective conflict when we fine-tune our models with mixed of datasets. We use the same template for training and evaluation for each task.

To understand the trade-off between instruction following and faithfulness when training LMs with both objectives, we formulate a two-stage fine-tuning paradigm to answer our research questions (as illustrated in ). For our first pipeline, we initially fine-tune our LM with context-dependent datasets that require grounding. We then take the best checkpoint from the initial stage to further fine-tune it on instruction following datasets (CDIF). Conversely, in our second pipeline, we fine-tune instruct-tuned LM (e.g., Vicuna-7B) with context-dependent datasets (IFCD). For both pipelines, we measure instruction following and faithfulness scores before and after training, to gauge the impact of the second-stage training on both capabilities. We follow this paradigm to find evidence of the trade-off in  and .

 We use two models in our two-stage fine-tuning paradigm. We use a base LM LLaMA-7B~, one of the most widely used open-source LM, for the CDIF pipeline. For our IFCD pipeline, we use Vicuna-7B off-the-shelf as our instruct-tuned LLaMA-7B without retraining one from scratch. Vicuna-7B is one of the most competitive open-source chat-model, and is a fine-tuned LLaMA-7B on conversational data from ShareGPT~. Our hypothesis and paradigm are transferable to other base LMs at different scales, although we pick these two models as they are among the first open-sourced LMs during the time frame of this project. Other experimental setup details are included in .

 starts with a model that is fine-tuned on a mixture of instruction following and context-dependent datasets and reconciles the two objectives with the following steps (): 

 For a random subset of the training datasets, we sample generations from the checkpoint with different decoding settings. Specifically, we focus on two hyperparameters by randomly changing one of them at a time by enumerating all possible values. For the decoding temperature, it takes on a value from \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7\}. For top-k, we set it to take on a value from \{5, 10, 20, 50, 70, 90, 100\}. When we vary the temperature, we fix top-k to be 0. Similarly, when we vary the top-k, we fix the temperature at 0. For each example, if we run the sampling procedure once, it will sample 7 examples in total.

 We then use a set of external judges to rate our collected generations as in . These judges are offline evaluators. We evaluate generation based on instruction following score, faithfulness score and task performance. One potential limitation is that our model may overfit to existing judges on seen datasets. We thus include unseen datasets to test generalizability.

 For each generation, we collect a set of scores from our external judges. Then, we take a weighted sum of these scores before ranking the generations and picking the top rated one. The score is weighted score of task performance , instruction following  and faithfulness scores ,

where we use  and  to indicate whether the example is from our instruction following datasets or context-dependent datasets. We pick the top rated sample per example and combine them into a continued fine-tuning dataset as described next.

 For , we randomly sample training data from each of our four dataset in  and collect 2,000 additional fine-tuning examples per dataset. In total, our new fine-tuning dataset has 8,000 examples. With this small collected dataset, we further fine-tune our starting checkpoint model for a single epoch with a smaller learning rate to avoid overfitting. This continued fine-tuning step is very lightweight as the training data is usually less than 1\% of the MTL training step. Overall,  a reject sampling based preference learning paradigm, which has been proven effective~ while drastically saving training costs as well as increasing stability. Other experimental setup details are included in .

In addition, we evaluate  with a different setting to test the impact of the quality of our continued fine-tuning dataset. Specifically, we  the quality of our additional fine-tuning dataset by sampling 1 more generations. We swap our instruction following judge from GPT-4 to the weaker ChatGPT. One potential benefits is that ChatGPT is a weaker judge with a lower recall for good generations. As a result, examples rated high by ChatGPT may have higher quality. While sampling for generation, we decrease the curated fine-tuning dataset by 3-fold (2,000 examples in total). We fine-tune our checkpoint model with the same setup.

 shows results on models fine-tuned with other baselines:  (1) : Evaluated with Vicuna-7B in a zero-shot manner.  (2) : We fine-tune Vicuna-7B with context-dependent datasets without any MTL objective. The purpose is to establish a potential upper bound of faithfulness score without any further training for instruction following. (3) : We fine-tune Vicuna-7B with baseline MTL by mixing instruction following and context-dependent datasets together.  (4) : We fine-tune Vicuna-7B directly with our collected continued fine-tuning dataset (8,000 examples) from our  pipeline without MTL training first. Note that the curated dataset comprises of sampled generations from our MTL checkpoint. The purpose is to evaluate whether MTL is necessary for model improvements. (5) : We follow  to fine-tune our MTL checkpoint with curated dataset. (6) : We follow  to fine-tune our MTL checkpoint with the high-quality version of our curated dataset (2,000 examples).

We evaluate our models on both seen and unseen testing datasets. As shown in , fine-tuning Vicuna-7B with a mixture of datasets close the gap on instruction following score substantially (from 0.49 to 0.75) while leaving headroom across the board. Next, models fine-tuned with  significantly outperform the MTL baseline on both seen and unseen testing datasets. Our results also suggest directly fine-tuning Vicuna-7B performs worse compared to fine-tuning a model checkpoint after MTL training. Last but not least, our results with -S provide strong evidence that data quality is more important than data quantity and that using multiple iterations of sampling helps, as -S achieves similar or better performance with 3-fold less training data. Due to space limit, we present qualitative model generations in .

 shows our actual prompt template for evaluation. We follow the prompt template provided in the LLM-as-a-Judge~ paper. We normalize the return rating to .

 We use regular expressions to replace spaces around hyphens, slashes, and before ``'s''. We then remove all the articles (e.g., ``a'' and ``the'') and punctuations. We lowercase all letters for simplicity.

 The authors also human label model's generation to check whether model's generations are paraphrased version of the golden answers. The most common failure mode is the model extracting a non-existent span or a wrong span. This supports our findings of models being unfaithful.

We train our model for a maximum of a single epoch across all training jobs. We up-sample the smaller datasets to match the number of examples in the larger ones when combing datasets for training. The learning rate is set at  with a batch size of 16 for our faithfulness-driven training. For training jobs involving instruction following datasets, we increase the batch size to 32. Training is conducted using  precision with a maximum sequence length of 2048. The weight decay is set to , with a  learning rate scheduler and a warm-up ratio of . We save checkpoints every 100 training steps and evaluate them based on perplexity scores on the evaluation set. The best-performing checkpoint is then selected for the next stage of training. Our models are trained using the stage-3  library. We train each model with three random seeds and average the results for consistency. For each training job, our models are trained on 8A100 GPUs within a single-node setup, with the total training time not exceeding 24 hours. For model generation, we employ greedy decoding with a maximum generation length of 480, which aligns with the maximum response length across the training datasets.

For the contined fine-tuning step in , we use a smaller learning rate of  and keep other settings the same.

 shows how instruction following scores vary during fine-tuning LLaMA-7B with context-dependent tasks.  shows how instruction following scores vary during the second-stage fine-tuning with instruction following datasets.

 shows faithfulness score and task performance across related datasets when we fine-tune LLaMA-7B on context-dependent datasets. One suprising finding is that throughout the fine-tuning process, the faithfulness scores on extractive QA datasets gradually decrease, while task performance scores gradually increase. On the other hand, this trend is not salient for abstractive tasks as shown in .

From  to , we show actual model inputs from each testing datasets.

 and  show two qualitative examples of actual model generations from our experiments.

Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (i.e., follow open-ended instructions) and faithfulness (i.e., ground responses in given context) when training LMs with these objectives. For instance, fine-tuning LLaMA-7B on instruction following datasets renders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performance at following instructions when further optimized on tasks that require contextual grounding. One common remedy is multi-task learning (MTL) with data mixing, yet it remains far from achieving a synergic outcome. We propose a simple yet effective method that relies on  (), which significantly outperforms vanilla MTL. Surprisingly, we find that less is more, as training  with high-quality, yet substantially smaller data (three-fold less) yields superior results. Our findings offer a better understanding of objective discrepancies in alignment training of LMs. \thefootnote\fnsymbol1Equal contribution. Work done at AWS AI Labs.\thefootnote\arabic\OurNameShort\OurNameShortIntroductionmostlyknow,chen2023felm,ji2023surveyzhang2023siren,wang2023survey,hallucination_survey,ghosh2024closeralpaca,koala_blogpost_2023,vicuna2023,christiano2017deep,ouyang2022trainingalpacaDatabricksBlog2023DollyV2instruction-tuning datasetsopenai2022introtouvron2023llamacontext-dependent datasetscontextlewis2020retrievalizacard2022atlaskhattab2023dspyfig:main_figure_1\OurNameShortzelikman2022star,selfinstructWe will release our code and evaluation data at\url.Background1.8\columnwidth!

    Task specific instructions. We bold texts that indicate that our prompts are designed to be objective-aligned with our instruction following training data (i.e., fine-tuning our model on instruction following datasets should with keeping it to be faithful as well).      tab:task_instructionDatasetstab:data_statssec:instr_templateInstruction Following DatasetsDatabricksBlog2023DollyV2\urlselfinstructkopf2023openassistantalpacavicuna2023koala_blogpost_2023Data Pre-processingContext-Dependent Datasetskwiatkowski-2019-naturalhan-2023-robustqabajaj2016msherman2015teaching, nallapati-etal-2016-abstractiveliu2018generatingData Pre-processingDetails about the retrieving process can be found in the original papers of RobustQA~\cite and MS MARCO~\cite.Evaluation Metricssec:eval_metricsThroughout the paper, we sample a subset of the full evaluation data which include 6,000 examples (1,000 examples from each context-dependent evaluation set), and sample 300 examples (100 examples from each instruction following evaluation set) due to limited compute resources.Instruction Following Scorezheng2023judgingWe use \texttt.app:eval_humanFaithfulness Scoreapp:eval_faithfig:paper_quandary_stage_1fig:paper_qa_summ_stage_2Laban2022SummaCRNTask Performancelin-2004-rougeInstruction Templatesec:instr_templatealpacatab:instruction_templateObjective-Aligned Instructionsobjective-conflictextracting a spantab:task_instructionwidth=1.0\linewidthfigures/paper_ctx_to_alignment.pngMacro-averaged faithfulness, instruction following, and task performance scores on corresponding evaluation datasets before and after fine-tuning with instruction following datasets.fig:RQ_1_result_1width=1.0\linewidthfigures/paper_seq_length_abstract_stage_2.pngAverage generation token length throughout the instruction following training stage. The first checkpoint is the best checkpoint from the context-dependent training stage. The middle checkpoint is with the lowest evaluation loss during the second stage.fig:paper_seq_length_abstract_stage_2Two-stage Fine-tuning Paradigmfig:inoculation_diagramOur approach is akin to the data inoculation paradigm proposed by \citet, albeit with significantly larger models and datasets.sec:finetune_instrusec:finetune_ctxModelstouvron2023llamazheng2023judgingapp:setupwidth=1.0\linewidthfigures/paper_alignment_to_ctx.pngMacro-averaged faithfulness, instruction following, and task performance scores on corresponding evaluation datasets before and after fine-tuning with context-dependent datasets.fig:RQ_2_result_1Does Fine-tuning with Instruction Following Data Hurt Faithfulness?sec:finetune_instrufig:inoculation_diagramfig:RQ_1_result_1app:additional_alignment_tapp:quandary_of_mixfig:paper_seq_length_abstract_stage_2tab:data_statsfig:paper_abstract_length_split_stage_2width=1.0\linewidthfigures/our_method_diagram.pdfThe illustration of our proposed method \OurNameShort. It samples generations from the initial vanilla multi-task learning (MTL) checkpoint with seen examples from instruction following and context-dependent datasets. For each example, it generates a set of possible responses with different decoding strategies. Generations are rated by external judges with a weighted scores of task performance, faithfulness and instruction following scores. Then, the top rated generations will be collected to further fine-tune the initial model.fig:our_method_diagramDoes Context-Dependent Fine-tuning Hurt Instruction Following?sec:finetune_ctxfig:RQ_2_result_1fig:paper_alignment_length_split_stage_2\textbf: Reconciling Instruction Following and Faithfulness\OurNameShortOur MTL BaselineOur Methodfig:our_method_diagramSample GenerationsExternal Judgessec:eval_metricsTop-1 Weighted Score =s_{}+2.0*(_{}*s_{} + _{}*s_{}) Continued Fine-tuningtab:data_statstouvron2023llamaapp:setupSupercharged \OurNameShort (\OurNameShort-S)superchargeWe use \texttt.1.00\linewidth!% Faithfulness and alignment scores on testing datasets, and unseen datasets are \textit. Scores are averaged across three distinct runs. Higher scores are better. Overall scores are macro-average across datasets.tab:our_method_resultResultstab:our_method_resultVicuna-7Bw/o MTLw/ MTLw/ \OurNameShortw/ MTL+\OurNameShortfig:our_method_diagramw/ MTL+\OurNameShort-Sfig:our_method_diagramtab:our_method_resultapp:example_generationRelated WorkInstruction Following of LMsalpaca,koala_blogpost_2023,vicuna2023,selfinstructouyang2022trainingzheng2023judging,pmlr-v162-ethayarajh22avicuna2023Faithfulness and Groundedness of LMszhang2023siren,wang2023survey,hallucination_survey,mostlyknow,chen2023felm,ji2023surveyRashkin_2021, dziri2022origin, paranjape2022hindsightpapineni-2002-bleu, lin-2004-rouge, banerjee-2005-meteorNLI; Laban2022SummaCRN, fabbri-2022-qafactevalchiang2023large,liu-2023-g,kamalloo2023evaluatingInstruction Following Training with LMswei2021finetuned,Mishra_2022,Wang_2022,chung2022scalingalpaca,vicuna2023schulman2017proximal,ouyang2022training,lee2023rlaif,rafailov2023direct,touvron2023llamatouvron2023llamaliu-2019-multi,crawshaw2020multighosh2024closerConclusion\OurNameShortLimitationsOur study primarily focuses on the LLaMA-7B and Vicuna-7B models, that are among the best open-source models at the time of this work. While we posit that our findings and the proposed  method could generalize across other language models, our findings could remain speculative without evaluating on more current model types (e.g., LLaMA-2, Mistral or Mixtral at various scales).   The datasets chosen for fine-tuning and evaluation, though comprehensive, are not exhaustive. There are other interesting datasets that are not covered in this study. For instance, long-form QA where the answers are much longer than 1-2 phrases or sentences. Our instruction following datasets can also be further categorized into creativity-driven, world-knowledge driven and others to help us to disentangle discrepancies in objectives better across datasets.   Our evaluation relies heavily on automated metrics and external judges like external LMs for assessing instruction following and faithfulness. While these methods are standard, they cannot fully encapsulate the nuanced understanding and preferences of human evaluators. For future research, evaluating responses with human annotators would provide additional validations.   Although the purpose of our study is to study the objective discrepancies in the datasets and come up with mitigation strategies without another novel training paradigm, it would strengthen our results if we can compare our method with more recent alignment training methods.   Although we evaluate  on unseen datasets, our method still has the potential to overfit to certain evaluators. Future work may use a different set of evaluators for a more robust evaluation. Human evaluation is challenging for our unseen testing datasets, such as WikiSum, because the input and response are extremely long, which could also make human ratings unstable. Ethics StatementcustomAppendixsec:appendixEvaluation Metric: Instruction Following Scoreapp:eval_humantab:gpt4_eval_promptzheng2023judging%     \begin  \tt  System\\ Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation (strictly 1-2 short sentences). Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: ``rating'', for example: ``Rating: 5''. \\ \\ Question \\  \textcolor \\

The Start of Assistant's Answer \\ Answer: \textbf\\ The End of Assistant's Answer

    \end The template of the prompt we used for evaluating instruction following scores using GPT-4. This template is adopted from LLM-as-a-Judge~\cite paper.tab:gpt4_eval_promptEvaluation Metric: Faithfulness Scoreapp:eval_faithText NormalizationCommon Failure ModeExperimental Setupapp:setupExperiments with up to three epochs showed minimal changes in results.bfloat16cosinedeepspeedAdditional Analysis on Instruction Following Fine-tuningapp:additional_alignment_tfig:paper_alignment_stage_1fig:paper_alignment_stage_2Quandary of Mixed Training on Abstractive and Extractive QA and Summarization Datasetsapp:quandary_of_mixfig:paper_quandary_stage_1fig:paper_abstract_stage_1Examples of Actual Instructionsapp:exampele_instructionstab:nq_exampletab:alpaca_exampleQualitative Examplesapp:example_generationtab:first_pip_exampletab:second_pip_examplewidth=1.0\linewidthfigures/paper_quandary_stage_1.pngIndividual faithfulness score and task performance with extractive QA datasets evaluated with three distinct model checkpoints of LLaMA-7B fine-tuned on context-dependent datasets. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_quandary_stage_1width=1.0\linewidthfigures/paper_qa_summ_stage_2.pngIndividual faithfulness score and task performance with extractive QA datasets across evaluated with three distinct model checkpoints through the instruction following fine-tuning. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_qa_summ_stage_2width=1.0\linewidthfigures/paper_alignment_stage_1.pngIndividual instruction following score on alignment datasets evaluated with three distinct model checkpoints of LLaMA-7B fine-tuned on context-dependent datasets. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_alignment_stage_1width=1.0\linewidthfigures/paper_alignment_stage_2.pngIndividual instruction following score on alignment datasets evaluated with three distinct model checkpoints through the instruction following fine-tuning. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_alignment_stage_2width=1.0\linewidthfigures/paper_abstract_stage_1.pngIndividual faithfulness score and task performance with abstractive QA and summarization datasets across evaluated with three distinct model checkpoints of LLaMA-7B fine-tuned on context-dependent datasets. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_abstract_stage_1width=1.0\linewidthfigures/paper_abstract_stage_2.pngIndividual faithfulness score and task performance with abstractive QA and summarization datasets across evaluated with three distinct model checkpoints through the instruction following fine-tuning. The middle checkpoint is the one with lowest in-training evaluation loss.fig:paper_abstract_stage_2tables/first_pip_exampletables/second_pip_exampletables/nq_exampletables/cnn_dailymail_exampletables/ms_macro_exampletables/bioasq_exampletables/searchqa_exampletables/wikisum_exampletables/alpaca_example