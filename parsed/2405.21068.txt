We selected model pairs that have been reported to vary only in terms of their pretraining data. For testing the code hypothesis, we compared the following pairs of models: (Llama 2, Code Llama), (DeepSeek, DeepSeek-Coder), and (Gemma, CodeGemma), where the second model in each pair is obtained by continuing to train the first model on additional code data. We tested 7B, 13B, and 70B models in the Llama 2 series. For testing the math hypothesis, we compared the following four pairs of models: (Code Llama, Llemma), (DeepSeek-Coder, DeepSeek-Math), (Llama, FLoat), and (Mistral, OpenMathMistral). Again, the second model in each pair is obtained by training the first model on additional math data. For alignment tuning, we compared (Llama 2, Llama 2-chat), (Code Llama, Code Llama-Instruct), (Gemma, Gemma-Instruct), (CodeGemma, CodeGemma-Instruct), (DeepSeek, DeepSeek-Chat), and (DeepSeek-Coder, DeepSeek-Coder-Instruct). These comparisons are summarized in Table~. See Appendix~, Table~ for more details about the models.

We adopted the boxes task (the ``base'' version) from  for testing the models' entity tracking capacity. In this task, the input to the LLM is a textual description of the contents of seven boxes followed by 1--12 descriptions of operations that change the contents of the individual boxes. In response to this input, the LLM is prompted to state the contents of each box according to the initial description and the state-changing operations. We used the same prompt and 2-shot in-context learning examples as  (see Table~ for an example). We used a slightly different prompt format for chat-optimized models to align the task better to the input format the models were trained on. The inputs to the model are provided as ``user'' prompts, and the expected model outputs are formatted as ``assistant'' (see  Table~ in Appendix for an example prompt). 

We noticed that smaller models suffered from formatting issues, often deviating from the format specified by the prompt or omitting the contents of some boxes. For this reason, we used regular expression-based constrained decoding using the  library~.

We report all results divided into the number of operations affecting the target box rather than reporting one aggregate accuracy metric. This is to distinguish trivial cases from cases that actually require tracking state changes---when the number of operations affecting the target box is 0, simply copying from the initial state description yields the correct answer. Furthermore, we compare the model results to the strong random baseline by . For this baseline, we randomly sample 0 to 3 objects for each box from the set of objects that have been previously mentioned in a clause with the box in question.

Figure~ compares the entity tracking performance of base models (red lines) and code models (blue lines) for models from the DeepSeek  , Gemma , and Llama 2 families of various sizes . In general, we find clear evidence that continued training on large amounts of code improves entity tracking abilities, as can be seen for the Llama 2 13B and 70B models as well as for the DeepSeek models. In these model comparisons, the models trained on code consistently outperformed the base models on the nontrivial cases of entity tracking (number of operations affecting box state  1). In the case of 13B models, a boost in trivial cases is also observed (number of operations = 0); in 70B models, performance on the trivial cases is already saturated in the base model.

In Llama 2 7B models, the gains through additional code training are relatively minor, with most of the gains deriving from boosts in examples where the number of operations is either 0 or 1. Similarly minor gains were observed in CodeGemma 8B, except that the gains were observed in examples with 1 and 2 operations. For both of these models, we also observed that for number of operations greater than 0 (Llama 2 7B) and 2 (Gemma 8B), neither the base nor the code variants perform better than our random baseline. These results suggest that there is both a possible effect of scale in the effectiveness of code training as observed in the Llama 2 series, and an effect of the amount of additional code training (DeepSeek-Coder: 2T tokens, Code Llama: 500B tokens). 

In evaluating the effect of additional math training, we start by revisiting the claim of  that the FLoat model obtained by finetuning Llama 7B  on arithmetic tasks from  yields superior entity tracking performance. As can be seen in  Table~, FLoat did show slightly higher accuracy on the non-trivial tracking cases (number of operations ) than the base model, but the gain was marginal. Furthermore, neither the base Llama model nor the FLoat model performed better than our random baseline on non-trivial entity tracking examples (Figure~, far left).

Following this observation, we compared Mistral and OpenMathMistral models where the latter is a model further trained on OpenMathInstruct-1, a synthetically generated instruction-tuning dataset containing 1.8M unique solutions to math problems sourced from MATH and GSM8K datasets~. As shown in Figure~, OpenMathMistral only achieved marginal gains over the base Mistral model when there are 7 operations affecting the target box, and in most other cases, the base model consistently outperforms the math-finetuned model. Furthermore, neither model outperformed the random baseline for examples with more than 2 operations affecting the box of interest. 

The unclear benefit of additional math training is further corroborated by marginal gains in models trained on math data that are not in ``instruct'' format like FLoat and OpenMathMistral. Figure~ shows that DeepSeek-Math  performed close to the DeepSeek-Coder model for most cases. The gains are even more limited in the comparison between Code Llama vs Llemma . Llemma 34B outperformed Code Llama 34B by a narrow margin for the non-trivial tracking cases (Llemma: 47.86, Code Llama: 45.46 for examples where the number of operations ). These results suggest a limited benefit of additional math pretraining on entity tracking. 

Finally, we explore the effect of alignment tuning on entity tracking. For models in the Llama 2 family, alignment tuning the base models led to minor gains (Figure~, orange  vs. green lines in the top row panels), whereas alignment-tuned code models did not consistently lead to gains and sometimes performed worse than the non-alignment-tuned counterparts (orange vs. green lines in the bottom row panels). Nevertheless, the best-performing model was CodeLlama 70B-instruct (64.9 accuracy on 1+ operations), combining code and alignment tuning. 

The DeepSeek models showed similar trends to the general observation made above: alignment tuning of the base model led to gains, whereas alignment tuning of the code model did not. Gemma 8B and CodeGemma 8B models did benefit from alignment tuning, similarly to Llama 2 7B and CodeLlama 7B models, although the gains were smaller.

Overall, alignment tuning affects base and code models differently, where the gains for base models tend to be greater. The benefit of alignment tuning for base models seems to be inversely correlated with scale: smaller base models benefit more from alignment tuning.