Table  presents the average accuracy of each LLM in four tasks on the EIC-Math dataset with GSM8K and MathQA.  Overall, GPT-4 demonstrates overwhelming superiority, followed by GLM-4.  GPT-3.5, Gemini Pro, and LLaMA-2-7B have their own strengths and weaknesses in four tasks.  It is noteworthy that LLaMA-2-7B performs better than LLaMA-2-13B, which may be related to inverse scaling . This suggests that the ability of models to identify and correct errors does not necessarily increase with model size.  Moreover, the mathematical models can only provide answers without error identification or correction abilities, and thus their accuracy is low as showcased in Appendix  and . This indicates that they can only solve problems and lack comprehensive reasoning abilities.  % ability to identify and correct errors. The average accuracy of  () is the highest among the four tasks (, , , ), as it is the simplest.  ()  and  ()  tend to have close average accuracy compared to  (), despite being intuitively less challenging. Actually,  involves an additional counting process, while  involves additional classification, leading to different emphases.  It can also be noted that the average accuracy  fluctuates across the four tasks, which is due to the efforts of LLMs to maintain consistency with different generated contents.

Regarding the difference in two average accuracy (, ) between , among the models with poor performance, Gemini Pro exhibits the smallest difference, while LLaMA-2-7B shows the largest. This suggests that Gemini Pro is cautious in error identification, with most identified errors being correctable, whereas LLaMA-2-7B is more liberal in error identification rather than correction. 

From the perspective of two datasets, it is often observed that the same model on MathQA tends to have lower accuracy across the four tasks compared to GSM8K. This is attributed to the higher difficulty level of MathQA. 

Additionally, despite the overwhelming superiority of GPT-4, its average accuracy across the four tasks on the two simple MWP datasets is only 76.2\%. This indicates that the error identification and correction tasks we design are challenging, and the lack of error identification and correction capability in LLMs somewhat restricts their mathematical reasoning abilities.

% \item[] In Table , we compute the average accuracy of each model across the four tasks in each error type on two datasets to assess the difficulty levels of different error types. It is found that calculation error is the most challenging to identify and correct, with an average accuracy of only 26.3\%, while hallucination is the easiest, with an average accuracy of 45.6\%. It is noteworthy that although GPT-4 and GLM-4 perform well overall, their performance in identifying and correcting calculation error is significantly lower compared to other error types. This suggests that LLMs should focus more on developing their computational capability. In addition, difficulty in identifying missing step is attributed to its poorest performance in the  of 2.9\% shown in Table , making it the most challenging type for LLMs to classify. This is because it requires traversing the entire solution's CoT to analyze whether essential reasoning steps are missing.

Furthermore, GPT-3.5 and Gemini Pro struggle with unit conversion error, and the LLaMA-2 series also perform poorly in unit conversion error and formula confusion error. At the same time, GPT-4 and GLM-4 perform well in unit conversion error and formula confusing error. We speculate that this may be related to the size of the stored parameter knowledge. Due to the lack of relevant common sense in the parameter knowledge, it becomes challenging to identify and correct related errors for smaller models.

The average accuracy of LLaMA-2-7B surprisingly reaches 31\% in calculation error, on par with GLM-4. Compared to other error types, LLaMA-2-7B and LLaMA-2-13B excell in contradictory step but perform poorly in counting error.

Table  provides statistic on the count of error types classified on GPT-3.5 with GSM8K. Similar statistics for most other models and datasets are presented in Appendix . It can be observed that most of the error types are often misclassified as calculation error, which may be attributed to the models' lack of true understanding of the meanings of each error type and relevant classification training data.

We devise a variety of prompts for the four tasks to explore the robustness of different models to different prompts. In addition, we investigate whether providing the error types to models can improve the accuracy in  and .

For , we select 50 negative samples and add an equal number of positive samples for each error type, totaling 100 samples for testing. And in Table , we compute their average F1 scores under three different prompts: ,  and . By calculating the difference in average F1 scores across all error types for each model, we evaluate their robustness to different prompts. It is observed that closed-source models exhibit greater robustness to different prompts, with the maximum difference in average F1 scores around 0.2. In contrast, open-source models are highly sensitive to different prompts, exhibiting a tendency to classify almost all cases as correct without much consideration under  and being misled to mostly classify cases as incorrect under .

For , we design zero-shot and few-shot prompts for comparison and find that increasing the shot has minimal effect on improving the accuracy of this task and could even be counterproductive in Table . This indicates that simple examples can not make models fully understand the meaning of identifying the first erroneous step. By providing models with the error types, the accuracy of identifying error steps has been significantly improved, with an average increase of 45.9\% times and maximum increase of 12.71 times. This informs that carefully designed examples can effectively improve the models' ability to identify erroneous steps.

For , we define nine error types in the prompts and design zero-shot and few-shot prompts. Recognizing that the sequence of error types may impact the accuracy of identifying errors, we also devise prompts that reverse the default order of error types and randomly shuffle them. In Table , the impact of increasing the shot on improving accuracy is also negligible by comparing zero-shot and few-shot prompts. The order of error types does indeed affect classification accuracy as shown in Table  and . For example, hallucination is listed last in the sequential prompt. The average classification accuracy of hallucination in the sequential prompt is much lower than that in the reversed order. It is noteworthy that in the random order, we place missing step first, but its classification accuracy remains consistently low, indicating its inherent difficulty in identification.

For , we adopt similar prompt settings with  and obtain similar results. Only delicately constructed prompts that provide the error types can effectively improve the models' ability to correct errors,  with an average increase of 47.9\% times and up to a maximum of 8.29 times as displayed in Table .

 We conduct traditional task by inputting the questions from our dataset into LLMs and obtaining the solutions and answers as outputs. The average accuracy of traditional task and our task is showcased in Figure . And details are in Appendix . It can be observed that closed-source models perform well on both datasets in traditional task, while among the open-source models, only MetaMath series achieve high accuracy on GSM8K, possibly due to overfitting. It is worth noting that the ability of LLaMA-2-7B to identify and correct errors is greater than its problem-solving ability. However, the accuracy of traditional task is overall higher than that of our proposed task, which indicates the significance of our evaluation task in improving LLMs' mathematical reasoning abilities.

We investigate the comparison between writing only up to the error step in the solution and continuing from the error step to complete the solution. It can be observed from Figure  that, for both  and , stopping at the error step aids in error identification and correction. Continuing from the error step to complete the solution may confuse LLMs. More details can be found in Appendix .

We examine the influence of step count on  and . For calculation error, we select 50 solutions ranging from 2 to 9 steps to transform. It can be observed that the accuracy of identification and correction is not significantly affected by the number of steps in Figure .

We consider the impact of the occurrence order of the error step on  and . For the 8-step problems involving calculation error, we generate 50 cases for each error step from 1 to 8 for evaluation. It can be observed that the accuracy of identification and correction is  also not significantly affected by the order of the error step in Figure .

To further validate the robustness of our conclusions, we conduct supplementary experiments using GLM-4 for data generation. Due to resource limit, we only use GLM-4 to generate three types of errors – CA, MS, and UC – on GSM8K and MathQA, with 50 instances each, totaling 300 instances. The experimental results are as shown in Table . We arrive at conclusions consistent with those drawn from the dataset generated by GPT-4, e.g., GPT-4's superior error identification and correction capabilities compared to other models. 

We conduct the evaluation results of other three math-specialized LLMs: Mistral , Llemma  and LEMA  in their 7B versions. We analyze their performance across different error types and tasks. The experimental results are as shown in Table  and . It can be observed that LEMA, which is aware of errors, outperforms the other math-specialized LLMs. And the capability of GPT-4 and GLM-4 still far surpasses these open-source models. This indicates that the ability to identify and correct errors of these math-specialized LLMs is inferior to that of the general-purpose powerful LLMs, GPT-4 and GLM-4.

We conduct some experiments with multi-step and multi-type errors. We first test the combinations of CA and CV. Experimental settings are divided into two: two error types occurring in the same step (Single-step and Two-type Error, ST) and two error types happening in two separate steps (Two-step and Two-type Error, TT). For both settings, we manually annotate 50 data samples and use them to evaluate LLMs' performance in the basic tasks of  and . The experimental results are shown in the Table  and .

.  By comparing the average accuracy of GPT-4 and GLM-4 with other models, it is evident that GPT-4 and GLM-4 significantly surpass others. As shown in Table , for GPT-4 and GLM-4, the  accuracy of ST and TT is higher than that of CA. This implies that the introduction of CV makes CA more prone to exposure. As illustrated in Table , for GPT-4 and GLM-4, the  accuracy of ST and TT is higher than that of CA. This is because these models exhibit strong correction ability for identified errors.

The prompt for generating the evaluation dataset is shown in Figure . After practical experience, we find that 5-shot has good generation results.

We design three zero-shot prompts: , ,  for  on open-source and closed-source models in Figure  to .

We design four prompts: zero-shot, few-shot, zero-shot-type, few-shot-type for , where few-shot is set to 2-shot. We display zero-shot and zero-shot-type prompts on open-source and closed-source models in Figure  to .

We design six prompts: zero-shot, few-shot, zero-shot-random, zero-shot-reverse, few-shot-random, few-shot-reverse for , where few-shot is set to 2-shot. We display zero-shot prompts on open-source and closed-source models in Figure  and .

We design four prompts: zero-shot, few-shot, zero-shot-type, few-shot-type for  as , where few-shot is set to 2-shot. We display zero-shot and zero-shot-type prompts on open-source and closed-source models in Figure  to .

In this section, we present the original detailed experimental results.

We present the accuracy of each model for each task on each error type in the Table  and . And we conduct an analysis of the MetaMath series in the Table  ,  and .

We conduct statistical analysis on the classification of error types for each model on GSM8K and MathQA in the Figure  to .

We design different prompts for four tasks on GSM8K and MathQA to test the robustness of each model. The robustness analysis of  can be seen in  and ,  can be seen in  and ,  can be seen in  and , and  can be seen in  and .

We provide the accuracy of the questions in our dataset in traditional task in Table  and . And  we show the performance of each model in traditional task on MathQA in Figure .

We showcase the performance of each model on the GSM8K and MathQA datasets stopping at error step in Figure ,  and .

% 附录图表%%% FFL on 20240216 %%% The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions.  Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps.  We also design diverse prompts to thoroughly evaluate eleven representative LLMs.  Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type.  Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on .  https://github.com/LittleCirc1e/EIChttps://github.com/LittleCirc1e/EICIntroductionbrown2020languageouyang2022traininganil2023palmopenai2023gptkushman2014learningroy2018mappingopenai2023gptzhou2023solvingshakarian2023independentfu2023chainhong2024stuckshi2022languagewei2022chaingolovneva2022roscoezhang2023evaluatinggaur2023reasoningi.e.,overview1)EP2)ES3)ET4)ECopenai2023gptcobbe2021trainingamini2019mathqaouyang2022trainingopenai2023gptdu2022glmteam2023geminitouvron2023LLaMAyu2023metamathjiang2023mistralazerbayev2023llemmaan2023learning1)2)3)ET4)ECES5)1)2)3)\abovecaptionskip0.1cm\belowcaptionskip0cmwidth=0.8\linewidth,scale=1.00images/framework_new.pdfIllustration of dataset construction and the four evaluation tasks. For dataset construction, we use GPT-4 to convert ground-truth solutions into wrong solutions containing specific error types. The four evaluation tasks comprehensively access LLMs' error identification and correction abilities from diverse perspectives.  % For evaluation tasks, LLMs are used to identify and correct errors for questions and transformed solutions from four dimensions. framework_new-12ptTask Formulationerrors are the stepping stones to wisdomframework_new aims to detect whether any error exists in the solution of a mathematical question. Formally, given a mathematical question  with an LLM-generated solution ,  estimates the binary label  that indicates whether  contains errors.      We design three prompts for open-source and closed-source LLMs for :  requires LLMs to only output the judgment ;  requires to not only output the judgment but also provide an explanation;  informs LLMs that there might be errors in the solution and instructs LLMs to generate the judgment with explanation.      To save space, we move detailed prompts to Figure  to .      For evaluation of , we compute the accuracy in identifying error presence by , where  is the number of evaluation cases and  is the predicted label by LLMs. 

    Task 1: Error-Presence Identification (EP)EPEPDue to certain limitations in the capabilities of open-source models, they may not fully comply with the strict JSON format requirements like closed-source models. Therefore, we design prompts with relaxed formatting tailored for open-source models.SimpleNormalMisleadingfig: Simple prompt for EP on closed-source modelsfig: Misleading prompt for EP on open-source modelsEP intends to find the first wrong step  in a wrong solution. For , we require LLMs to output the judgment , and if  contains errors, we also instruct LLMs to identify the first erroneous step  in the solution.      We devise the zero-shot prompts and few-shot prompts with in-context learning examples for the  task. Figure  and  show the zero-shot prompts for open-source and closed-source models.      For  evaluation, we compute  as  and the accuracy in identifying error step by , where  denotes the first wrong step predicted by LLMs. 

    Task 2: Error-Step Identification (ES)Task 2ESfig: Zero-shot prompt for ES on closed-source modelsfig: Zero-shot prompt for ES on open-source modelsESEP endeavors to identify the error type. We instruct LLMs to output the judgment for  and identify the error type  of the first wrong step if  contains errors.      Here,  is selected from the pre-defined error types, such as calculation error.      We define error types in the prompts and design zero-shot and few-shot prompts, where the few-shot prompt provides an example for each error type.      Figure  and  showcase the zero-shot prompts for open-source and closed-source models.      Considering that the order of error types might affect the accuracy of identifying error types, we design prompts that reverses the original order of error types and randomly shuffles them.      We compute  and the accuracy in identifying error type , where  is the error type of the first wrong step identified by LLMs. 

    Task 3: Error-Type Identification (ET)fig: Zero-shot prompt for ET on closed-source modelsfig: Zero-shot prompt for ET on open-source models seeks to rectify the error and output the correct solution.      % For , we let LLMs output the judgment , and if  contains errors, we also make it output the corrected solution and the final correct answer .      We prompt LLMs to output the judgment for  and provide the corrected solution and answer  if  contains errors.      We devise zero-shot and few-shot prompts as . The prompts are displayed in Figure  and .      We calculate  and the accuracy of correction , where  and  are the predicted and ground-truth answers, respectively.  Task 4: Error Correction (EC)ESfig: Zero-shot prompt for EC on closed-source modelsfig: Zero-shot prompt for EC on open-source modelsTask 2Task 4fig: Zero-shot-type prompt for ES on closed-source modelsfig: Zero-shot-type prompt for ES on open-source modelsfig: Zero-shot-type prompt for EC on closed-source modelsfig: Zero-shot-type prompt for EC on open-source modelsDataset Construction\abovecaptionskip0.1cm\belowcaptionskip-0.30cm &   \\  &  Error appears during the calculation process.  \\   & Error occurs during the counting process.  \\  & Error arises when attributes of named entities do not align with the information provided.  \\   & Error involves adding fictitious unrelated statements contradictory to the question.  \\  & Error occurs during unit conversion process. \\  & Error involves a single operator being erroneously applied within the expression. \\  &  Error appears when applying formula in inappropriate scenario.  \\   & Error entails an incomplete generation of reasoning process, lacking a necessary step.\\  & Error manifests inconsistency between preceding and subsequent reasoning steps.\\ Error TypeDefinitionCalculation Error (CA)Counting Error (CO)Context Value Error (CV)Hallucination (HA)Unit Conversion Error (UC)Operator Error (OP)Formula Confusion Error (FC)Missing Step (MS)Contradictory Step (CS)Definition of nine common error types. Among them, unit conversion error, operator error, and formula confusion error can be categorized as \textbf, indicating errors in the relationships that should be understood within worldly common sense. The generation rules and examples are designed in Appendix \ref.tab:error type definition-5ptwei2022chaintoh2023veritymathlightman2023letsshakarian2023independentbubeck2023sparkssawada2023arbsuzgun2022challenginglyu2023faithfulkojima2022largeli2023makingwang2022towardswang2023planpaul2023refinergolovneva2022roscoeribeiro2023streetlewkowycz2022solvingtab:error type definitionDetailed Error Type DefinitionData Generation.framework_newopenai2023gptEIC-MathEICMathIn this work, we only consider generating the wrong solution with only one error type in a single step to simplify the evaluation process, leaving more complicated error identification and correction to future work.brown2020languageouyang2022trainingmin-etal-2022-rethinkingDataset Generationcobbe2021trainingamini2019mathqaHuman Evaluation.Human EvaluationExperiment- RQ1:- RQ2:- RQ3:w.r.t.5ptExperiment Setup.Specifically, we conduct experiments using gpt-3.5-turbo-1106, gpt-4-1106-preview, LLaMA-2-7B-chat, LLaMA-2-13B-chat, MetaMath-7B-V1.0, MetaMath-13B-V1.0, Mistral-7B-V0.1, Llemma-7B, LEMA-V1-PEFT-LLaMA-2-7B-GSM8K.=0.1cm % \abovecaptionskip0.1cm\belowcaptionskip-0.05cm0.67 \begin

\end Average accuracy of different models in four tasks on GSM8K and MathQA separately under zero-shot prompts. \textit calculates the average  over all error types. \textit calculates the average  and  as the values for the first and second column respectively. And \textit and \textit conduct similar calculation as \textit. The first column of \textit is the average value of , , , and  over all error types of models and represents the ability to identify and correct errors, while the second column is the average value of  of four tasks and only represents the ability to identify errors. tab:Based on Model, main table=0.1cm % \abovecaptionskip0.1cm\belowcaptionskip-0.05cm0.67 \begin \centering

\end Average accuracy of different models in different error types on GSM8K and MathQA under zero-shot prompts. We use the first two letters of the name of error type to represent it. The calculation of the first and second column is similar as the \textit in Table \ref.tab:Based on Error Type, main tableModel Performance (RQ1)Overall Performance.tab:Based on Model, main tablemckenzie2023inverseComparasion with other math modelsDetailed resultsComparison Across Tasks.EPESETECESETECComparison Between Datasets.Future Direction.Error Type Analysis (RQ2)=0.15cm % \abovecaptionskip0.1cm\belowcaptionskip-0.05cm0.67 \begin

\end Average accuracy of different tasks in different error types on GSM8K and MathQA under zero-shot prompts. And we calculate the average  over all models for \textit, the average  and  as the values for the first and second columns respectively for \textit, \textit and \textit.tab:Based on Task, main tableDifficulty Levels of Error Types.tab:Based on Error Type, main tableETtab:Based on Task, main tableComparison between Different Models on the Same Error Type.Statistical Classification of Error Types.tab:GSM8K GPT-3.5 error type analysis1Error Type AnalysisPrompt Robustness (RQ3)ETEC=0.15cm % \footnotesize% \arraystretch0.8\abovecaptionskip0.1cm\belowcaptionskip-0.05cm0.8 \begin

\end Average accuracy  of models in \textit on GSM8K and MathQA separately under four different prompt settings. Zero-shot-type and Few-shot-type provide models with the error types. Few-shot is set to 2-shot. The maximum average accuracy for each model on each dataset is in \textbf.tab:ES prompt robustness testing, Accuracy=0.1cm % \footnotesize% \arraystretch0.8\abovecaptionskip0.1cm\belowcaptionskip-0.050cm0.75 \begin

\end Average accuracy  of models in \textit on GSM8K and MathQA separately under four different prompt settings. Few-shot is set to 2-shot. Few-shot-random and Few-shot-reverse present similar results and are included in Appendix \ref.tab:ET prompt robustness testing, Accuracy=0.15cm % \footnotesize\arraystretch0.8\abovecaptionskip0.1cm\belowcaptionskip0cm0.8 \begin

\end Average accuracy  of models in \textit on GSM8K and MathQA separately under four different prompt settings. Zero-shot-type and Few-shot-type provide models with the error types. Few-shot is set to 2-shot. The maximum average accuracy for each model on each dataset is in \textbf.tab:EC prompt robustness testing, Accuracy-10ptPrompt Robustness of EP.EPtab:EP prompt robustness testing, F1-scoreSimpleNormalMisleadingSimpleMisleadingPrompt Robustness of ES.EStab:ES prompt robustness testing, AccuracyPrompt Robustness of ET.ETtab:ET prompt robustness testing, Accuracytab:GSM8K ET prompt robustness testing,Accuracytab:MathQA ET prompt robustness testing,AccuracyPrompt Robustness of EC.ECEStab:EC prompt robustness testing, AccuracyIn-depth AnalysisIn-depth AnalysisComparison with Traditional Task.fig:Evaluation Results of Traditional Task and Our Task on GSM8KDetailed Comparison with Traditional TaskInfluence of Stopping at Error Step.fig:Evaluation Results of Incomplete Cases for GSM8K on Closed-source ModelsEPECDetailed Influence of Stopping at Error StepRelated WorkMathematical Reasoning Evaluation.shakarian2023independentfu2023chainhong2024stuckshi2022languagedahlgren2022clevrfrieder2023mathematicalwei2022chaingolovneva2022roscoezhang2023evaluatinggaur2023reasoningzhuang2023efficientlycollins2023evaluatingliu2023noviceyen2023threevalmeekam2023canstechly2023gptan2023learninghuang2023large5ptIn-Context Learning.brown2020languageouyang2022traininganil2023palmopenai2023gptbrown2020languageouyang2022trainingmin-etal-2022-rethinkinglu2022fantasticallymin-etal-2022-rethinkingliu2022wanliwiegreffe2022reframingwest2022symbolic5ptProgram Repair.nguyen2013semfixqi2014strengthdiekmann2018donyasunaga2020graphyasunaga2021breakahmed2021synfixberabi2021tfixbavishi2022neurosymbolicjoshi2023repairjin2023inferfixbouzenia2024repairagentConclusionLimitationsEthics StatementcustomDataset Selectionsec:appendix datasetcobbe2021trainingamini2019mathqakoncel2016mawpspatel2021nlpwang2017deepDetailed Error Type DefinitionDetailed Error Type Definitionwei2022chaintoh2023veritymathlightman2023letsshakarian2023independentbubeck2023sparkssawada2023arbsuzgun2022challenginglyu2023faithfulkojima2022largeli2023makingwang2022towardswang2023planpaul2023refinergolovneva2022roscoeribeiro2023streetlewkowycz2022solvingCalculation Error:toh2023veritymathCounting Error:bubeck2023sparksContext Value Error:yen2023threeHallucination:lyu2023faithfulUnit Conversion Error:choi2023assessmentOperator Error:paul2023refinerFormula Confusion Error:lightman2023letsMissing Step:wei2022chainContradictory Step:golovneva2022roscoecommon sense errorGeneration Rules Design and Examplessec:appendix rulestab:error type definitionCalculation Error:Counting Error:Context Value Error:Hallucination:Unit Conversion Error:Operator Error:Formula Confusion Error:Missing Step:Contradictory Step::On the weekend, Tony will walk to the store. On weekdays, he runs to the store. When he walks, he goes 2 MPH. When he runs, he goes 10 MPH. The store is 4 miles away. If he goes on Sunday, Tuesday, and Thursday, what is the average time in minutes that he spends to get to the store?:On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him 168 minutes to get to the store because 120 + 24 + 24 = 168\newline On average it takes him 56 minutes because 168 / 3 = 56.Calculation Error::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him 168 minutes to get to the store because 120 + 24 + 24 = 168\newline On average it takes him \textcolor minutes because \textcolor.:The operands and operators of the formula in step 6 are correct, but only the result is incorrectly calculated as 55 instead of 56.Counting Error::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him 168 minutes to get to the store because 120 + 24 + 24 = 168\newline On average it takes him \textcolor minutes because \textcolor.:Step 6 counts Sunday, Tuesday, and Thursday wrongly as 2 days instead of 3 days, only resulting in an operand error in the formula.Context Value Error::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him \textcolor hours to get to the store because \textcolor\newline On Tuesday and Thursday, it takes him 12 minutes to get to the store because .2 x 60 = 12\newline In total it takes him 144 minutes to get to the store because 120 + 12 + 12 = 144\newline On average it takes him 48 minutes because 144 / 3 = 48.:Step 3 mistakenly references the number 20 instead of 10 from the question, only resulting in an operand error in the formula. The subsequent steps are affected by it. Please note that we only consider errors of single step and single type, and step 2 still correctly references 10. Hallucination::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline \textcolor, so in total it takes him \textcolor minutes to get to the store because \textcolor\newline On average it takes him On average it takes him 62.6 minutes because 188 / 3 = 62.6.:Step 5 adds the additional information <Because the road congestion on Tuesday takes an additional 20 minutes> not mentioned in the questions, causing the result of step 5 to be overestimated by 20. And it influences step 6, which references its result.Unit Conversion Error::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline This takes him \textcolor minutes because \textcolor\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him 148 minutes to get to the store because 100 + 24 + 24 = 148\newline On average it takes him 49.3 minutes because 148 / 3 = 49.3.:Step 2 performs an incorrect unit conversion and mistakenly assumes that one hour has 50 minutes, which only results in an error in one operand in the formula. The subsequent steps 5 and 6 are affected by it. Because we only consider errors of single step and single type, step 4 still correctly performs unit conversion. Operator Error::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him 168 minutes to get to the store because 120 + 24 + 24 = 168\newline On average it takes him \textcolor minutes because \textcolor.:Step 6 mistakenly uses addition instead of division, and only one operator in the formula is incorrect. Missing Step::On Sunday he takes 2 hours to get there because 4 / 2 = 2 \newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline  On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him \textcolor minutes to get to the store because \textcolor\newline On average it takes him 16.6 minutes because 50 / 3 = 16.6.:Step 4 does not convert the time he went to the store on Sunday from hours to minutes, but directly adds up the time on Sunday (hours) and the time on Tuesday and Thursday (minutes). So there is a missing step here to convert Sunday's time from hours to minutes. Contradictory Step::On Sunday he takes 2 hours to get there because 4 / 2 = 2\newline  This takes him 120 minutes because 2 x 60 = 120\newline On Tuesday and Thursday, it takes him .4 hours to get to the store because 4 / 10 = .4\newline On Tuesday and Thursday, it takes him 24 minutes to get to the store because .4 x 60 = 24\newline In total it takes him \textcolor minutes to get to the store because \textcolor\newline On average it takes him 62.6 minutes because 188 / 3 = 62.6 :Step 5 erroneously references the result 140 of step 2 instead of 120, which only results in an error in one operand in the formula. :Linda is painting her bedroom. Her bedroom has 4 walls, with the room being 20 feet wide by 20 feet long by 8 feet tall. One wall has a 3-foot by 7-foot doorway. A second wall has a 6-foot by 4-foot window. A third wall has a 5-foot by 7-foot doorway to a walk-in-closet. And the fourth wall is completely solid. What is the total area of wall space that Linda will have to paint?:The solid wall is 8 ft. * 20 ft. = 160 sq. ft. \newline The doorway is 3 ft. * 7 ft. = 21 sq. ft.\newline The window is 6 ft. * 4 ft. = 24 sq. ft.\newline The closet door is 5 ft. * 7 ft. = 35 sq. ft.\newline The total area of the doors and windows is 21 sq. ft + 24 sq. ft. + 35 sq. ft. = 80 sq. ft.\newline The solid wall is 160 sq. ft., so before the areas of the doors and window are taken into account, the total wall area is 4 * 160 sq. ft. = 640 sq. ft.\newline Taking into account the doors and window, the total wall area Linda will have to paint is 640 sq. ft. - 80 sq. ft. = 560 sq. ft.Formula Confusion Error::The solid wall is 8 ft. * 20 ft. = 160 sq. ft.\newline The doorway is \textcolor\newline The window is 6 ft. * 4 ft. = 24 sq. ft.\newline The closet door is 5 ft. * 7 ft. = 35 sq. ft.\newline The total area of the doors and windows is 20 sq. ft + 24 sq. ft. + 35 sq. ft. = 79 sq. ft.\newline The solid wall is 160 sq. ft., so before the areas of the doors and window are taken into account, the total wall area is 4 * 160 sq. ft. = 640 sq. ft.\newline Taking into account the doors and window, the total wall area Linda will have to paint is 640 sq. ft. - 79 sq. ft. = 561 sq. ft.:Step 2 confuses the perimeter and area formulas of rectangle and it should calculate the area of the rectangle which is equal to the length multiplied by width, rather than the length plus length plus width plus width, equivalent to the perimeter of the rectangle. And step 5 and 7 referencing the result of step 2 are affected.Human EvaluationHuman EvaluationAssessment Procedure:dataset formatAssessment Quality Control:Additional In-depth ExperimentsInfluence of Step CountEPECfig:GSM8K CA EP: the influence of step number,fig:MathQA CA EP: the influence of step number,fig:GSM8K CA EC: the influence of step number,fig:MathQA CA EC: the influence of step numberInfluence of the Wrong Step OrderEPECfig:GSM8K CA EP: the influence of wrong step,fig:MathQA CA EP: the influence of wrong step,fig:GSM8K CA EC: the influence of wrong step,fig:MathQA CA EC: the influence of wrong stepComparasion between GLM-4 and GPT-4tab:Comparasion between GLM-4 and GPT-4.Comparasion with other math modelsComparasion with other math modelsjiang2023mistralazerbayev2023llemmaan2023learningtab:Other math modelstab:Other math models on tasks.Combination Error AnalysisEPECtab:EP combination error.tab:EC combination error.Result analysistab:EP combination error.EPtab:EC combination error.ECExperiment DetailsExperiment DetailsPrompts and Input FormattingDataset GenerationDataset Generationfig: Prompt for GenerationEPSimpleNormalMisleadingEPfig: Simple prompt for EP on closed-source modelsfig: Misleading prompt for EP on open-source modelsESESfig: Zero-shot prompt for ES on closed-source modelsfig: Zero-shot-type prompt for ES on open-source modelsETETfig: Zero-shot prompt for ET on closed-source modelsfig: Zero-shot prompt for ET on open-source modelsECECESfig: Zero-shot prompt for EC on closed-source modelsfig: Zero-shot-type prompt for EC on open-source modelsDetailed resultsDetailed resultsMain Experimenttab:GSM8K main experimenttab:MathQA main experimenttab:Based on Model-MetaMath, main tabletab:Based on Type, MetaMath main tabletab:Based on Task, MetaMath main tableError Type AnalysisError Type Analysistab:MathQA GPT-3.5 error type analysistab:MathQA  LLaMA-2-13B error type analysisPrompt Robustness AnalysisPrompt Robustness AnalysisEPtab:GSM8K EP prompt robustness testing,F1-scoretab:MathQA EP prompt robustness testing,F1-scoreEStab:GSM8K ES prompt robustness testing,Accuracytab:MathQA ES prompt robustness testing,AccuracyETtab:GSM8K ET prompt robustness testing,Accuracytab:MathQA ET prompt robustness testing,AccuracyECtab:GSM8K EC prompt robustness testing,Accuracytab:MathQA EC prompt robustness testing,AccuracyComparison with Traditional TaskDetailed Comparison with Traditional Tasktab:GSM8K Original Cases Accuracytab:MathQA Original Cases Accuracyfig:Evaluation results of traditional task for MathQAInfluence of Stopping at Error StepDetailed Influence of Stopping at Error Stepfig:Evaluation Results of Incomplete Cases for GSM8K on Open-source Modelsfig:Evaluation Results of Incomplete Cases for MathQA on Closed-source Modelsfig:Evaluation Results of Incomplete Cases for MathQA on Open-source Models%width=\textwidthimages/GSM8K_step_num_EP.pdfThe influence of step number in GSM8K on \textit.fig:GSM8K CA EP: the influence of step number4mmwidth=\textwidthimages/MathQA_step_num_EP.pdfThe influence of step number in MathQA on \textit.fig:MathQA CA EP: the influence of step number%width=\textwidthimages/GSM8K_step_num_EC.pdfThe influence of step number in GSM8K on \textit.fig:GSM8K CA EC: the influence of step number4mmwidth=\textwidthimages/MathQA_step_num_EC.pdfThe influence of step number in MathQA on \textit.fig:MathQA CA EC: the influence of step number%width=\textwidthimages/GSM8K_wrong_step_EP.pdfThe influence of wrong step order in GSM8K on \textit.fig:GSM8K CA EP: the influence of wrong step4mmwidth=\textwidthimages/MathQA_wrong_step_EP.pdfThe influence of wrong step order in MathQA on \textit.fig:MathQA CA EP: the influence of wrong step%width=\textwidthimages/GSM8K_wrong_step_EC.pdfThe influence of wrong step order in GSM8K on \textit.fig:GSM8K CA EC: the influence of wrong step4mmwidth=\textwidthimages/MathQA_wrong_step_EC.pdfThe influence of wrong step order in MathQA on \textit.fig:MathQA CA EC: the influence of wrong stepwidth=\linewidth,scale=1.00images/generate_prompt.pdfPrompt for generation.fig: Prompt for Generationwidth=\linewidth,scale=1.00images/ep_close_simple.pdfSimple prompt for \textit on closed-source models.fig: Simple prompt for EP on closed-source modelswidth=\linewidth,scale=1.00images/ep_open_simple.pdfSimple prompt for \textit on open-source models.fig: Simple prompt for EP on open-source modelswidth=\linewidth,scale=1.00images/ep_close.pdfNormal prompt for \textit on closed-source models.fig: Normal prompt for EP on closed-source modelswidth=\linewidth,scale=1.00images/ep_open.pdfNormal prompt for \textit on open-source models.fig: Normal prompt for EP on open-source modelswidth=\linewidth,scale=1.00images/ep_close_misleading.pdfMisleading prompt for \textit on closed-source models.fig: Misleading prompt for EP on closed-source modelswidth=\linewidth,scale=1.00images/ep_open_misleading.pdfMisleading prompt for \textit on open-source models.fig: Misleading prompt for EP on open-source modelswidth=\linewidth,scale=1.00images/es_close.pdfZero-shot prompt for \textit on closed-source models.fig: Zero-shot prompt for ES on closed-source modelswidth=\linewidth,scale=1.00images/es_open.pdfZero-shot prompt for \textit on open-source models.fig: Zero-shot prompt for ES on open-source modelswidth=\linewidth,scale=1.00images/es_close_complex.pdfZero-shot-type prompt for \textit on closed-source models.fig: Zero-shot-type prompt for ES on closed-source modelswidth=\linewidth,scale=1.00images/es_open_complex.pdfZero-shot-type prompt for \textit on open-source models.fig: Zero-shot-type prompt for ES on open-source modelswidth=\linewidth,scale=1.00images/et_close.pdfZero-shot prompt for \textit on closed-source models.fig: Zero-shot prompt for ET on closed-source modelswidth=\linewidth,scale=1.00images/et_open.pdfZero-shot prompt for \textit on open-source models.fig: Zero-shot prompt for ET on open-source modelswidth=\linewidth,scale=1.00images/ec_close.pdfZero-shot prompt for \textit on closed-source models.fig: Zero-shot prompt for EC on closed-source modelswidth=\linewidth,scale=1.00images/ec_open.pdfZero-shot prompt for \textit on open-source models.fig: Zero-shot prompt for EC on open-source modelswidth=\linewidth,scale=1.00images/ec_close_complex.pdfZero-shot-type prompt for \textit on closed-source models.fig: Zero-shot-type prompt for EC on closed-source modelswidth=\linewidth,scale=1.00images/ec_open_complex.pdfZero-shot-type prompt for \textit on open-source models.fig: Zero-shot-type prompt for EC on open-source models0.85     \begin

    \end     Comparasion between GLM-4 and GPT-4.tab:Comparasion between GLM-4 and GPT-4.=0.1cm 0.9 \begin

\end Average accuracy of other math models in different error types under zero-shot prompts.tab:Other math models=0.1cm \abovecaptionskip0.1cm\belowcaptionskip-0.05cm0.75 \begin

\end Average accuracy of other math models in different tasks under zero-shot prompts.tab:Other math models on tasks.Accuracy of \textit in combination error types on GSM8K under zero-shot prompts.tab:EP combination error.Accuracy of \textit in combination error types on GSM8K under zero-shot prompts.tab:EC combination error.=0.15cm % \footnotesize0.70.8 \begin

\end Accuracy of different closed-source and open-source models on different error types of GSM8K. We use zero-shot prompts uniformly here. Different error types are replaced with the first two letters of their names, for example, calculation error is represented by CA. For \textit, the results of  are presented; for \textit, the results of both  and  are showcased; for \textit, the results of both  and  are displayed; for \textit, the results of both  and  are exhibited. And we calculate the average  as \textit for \textit, \textit, \textit and \textit.tab:GSM8K main experiment=0.15cm 0.70.8 \begin

\end Accuracy of different closed-source and open-source models on different error types of MathQA. We use zero-shot prompts uniformly here. Different error types are replaced with the first two letters of their names, for example, calculation error is represented by CA. For \textit, the results of  are presented; for \textit, the results of both  and  are showcased; for \textit, the results of both  and  are displayed; for \textit, the results of both  and  are exhibited. And we calculate the average  as \textit for \textit, \textit, \textit and \textit.tab:MathQA main experiment=0.1cm 0.7 \begin

\end Average accuracy of MetaMath models in different tasks on GSM8K and MathQA under zero-shot prompts.tab:Based on Model-MetaMath, main table=0.1cm 0.7 \begin \centering

\end Average accuracy of MetaMath models in different error types on GSM8K and MathQA under zero-shot prompts.tab:Based on Type, MetaMath main table=0.15cm 0.7 \begin

\end Average accuracy of MetaMath models on four tasks in different error types on GSM8K and MathQA under zero-shot prompts.tab:Based on Task, MetaMath main tablecenter %    %    \begin     \centering     \scalebox     \caption     \label   \end center %    \begin     \centering     \scalebox\caption \label   \end%   \hspace   \begin     \centering     \scalebox\caption \label   \end center %    \begin     \centering     \scalebox\caption \label   \end%   \hspace   \begin     \centering     \scalebox\caption \label   \end center %    \begin     \centering     \scalebox\caption \label   \end%   \hspace   \begin     \centering     \scalebox \caption \label   \end center %    \begin     \centering     \scalebox\caption \label   \end%   \hspace   \begin     \centering     \scalebox \caption \label   \end center %    \begin     \centering     \scalebox\caption \label   \end%   \hspace   \begin     \centering     \scalebox \caption \label   \end 1

\textit prompt robustness testing on GSM8K.tab:GSM8K EP prompt robustness testing,F1-score1

\textit prompt robustness testing on MathQA.tab:MathQA EP prompt robustness testing,F1-score=0.1cm 0.9 \begin

\end \textit prompt robustness testing on GSM8K.tab:GSM8K ES prompt robustness testing,Accuracy=0.1cm 0.9 \begin

\end \textit prompt robustness testing on MathQA.tab:MathQA ES prompt robustness testing,Accuracy=0.1cm 0.9 \begin

\end \textit prompt robustness testing on GSM8K.tab:GSM8K ET prompt robustness testing,Accuracy=0.1cm 0.9 \begin

\end \textit prompt robustness testing on MathQA.tab:MathQA ET prompt robustness testing,Accuracy=0.1cm 0.9 \begin

\end \textit prompt robustness testing on GSM8K.tab:GSM8K EC prompt robustness testing,Accuracy=0.1cm 0.9 \begin

\end \textit prompt robustness testing on MathQA.tab:MathQA EC prompt robustness testing,Accuracy% \tabcolsep=0.1cm1 \begin

\end Accuracy of tranditional task on GSM8K.tab:GSM8K Original Cases Accuracy% \tabcolsep=0.1cm1 \begin

\end Accuracy of tranditional task on MathQA.tab:MathQA Original Cases Accuracy=0.15cm 0.8 \begin

\end Accuracy of incomplete cases on GSM8K.tab:GSM8K Incomplete Cases Accuracy=0.15cm 0.8 \begin

\end Accuracy of incomplete cases on MathQA.tab:MathQA Incomplete Cases Accuracy