: %The attacker LLM adversarially generates a sentence or a prompt that might result in biased responses from a target LLM. We prompt Llama3}} using the system prompt:  and a user prompt: } that can make a language model generate biased responses. Biased Statement:"}. The keyword here is a gender-related word from each of the gender-specific lists .  The attacker LLM adversarially generates a sentence or a prompt that might result in biased responses from a target LLM.  We use Meta's Llama3-8B Instruct model (Llama3 for short) configured with a temperature of 0.6 with system prompt:   and a user prompt: . The keyword is a gender-related word from each of the gender-specific lists of .

We also finetune 4-bit quantized Llama3 for adversarial prompt generation using Low-Rank Adaptation (LoRA)~ (with rank=16, , 80-bit Adam optimizer with weight decay) on data collected~ using the gendered-keyword lists to prompt ChatGPT with the same system prompt as above. We finetune for 60 steps on about 5000 of the ChatGPT-generated prompts.

:  Once a prompt from a gender-related word is generated, we utilize the Counterfactual Data Augmentation technique~ to generate a prompt from the other gender. To prevent potential errors in the meaning and fluency of the generated text, we avoid simple substitution. Instead, we use Llama3 to generate gender counterfactuals using the following prompt. We call the prompt and its counterfactual : 

% For the experiments reported in this work, we used a subset of about 500 prompts from a large number of generated prompts. We used the GPT-4 model to rank the large pool of prompts based on their ability to elicit biased responses and pick the highest-ranked prompts.  % %  The adversarial prompts and their counterfactuals are provided to the target LLM and its responses are evaluated for the presence of bias.  The target LLMs we consider in this work are the Llama2-chat family of models (7b, 13b, 70b), GPT-4, Mixtral 8x7B Instruct-v0.1, and Mistral 7B Instruct-v0.2.  These models are a subset of models available as part of the  AI Safety Benchmark PoC framework~.

%We use the AI Safety Benchmark PoC framework } (more details in Section 4) that provides about 21 options for target LLMs as a part of this framework (also termed as 'System Under Test' or SUT), including , and several others. We use the benchmark and extend it to generate responses to the adversarial prompts and score them using several evaluation metrics (as discussed below). %%The evaluator module aims to measure the gender bias in responses from target LLMs using various metrics. %for Measuring Bias}% %  %     Recent work in NLP has seen the use of LLMs as scorers or judges due to their strong human alignment .  Obtaining human annotations is extremely tedious and expensive, and for bias, it also requires detailed protocol and rubric to disentangle human preferences and prior user beliefs for objective assessment. To automate the evaluation, we explore LLM-as-a-Judge for identifying or measuring bias. Here, we use GPT-4o to evaluate and score responses generated by target LLMs. We prompt the model to identify bias in an input-response pair in terms of 5 classes by providing an explanation of each as mentioned in Table~ . We instruct the model to output the bias scores, indicating the level of bias, and also generate a one-line explanation of the classification. Additionally, we calculate the difference in the LLM-as-a-Judge bias scores for male and female responses, then take the average of these differences to obtain the "LLM-judge Gap Score." This gap score serves as an indicator of the overall bias in the model's response generation.

% i.e., No Bias, Slight Bias, Mild Bias, Moderate Bias, and Extreme Bias,    Perspective API is a widely used API-based tool, developed by the Google Jigsaw team, which is a popular toxicity detection tool. It measures attributes such as toxicity, insult, and identity attack, among others.

% We follow the previous work~ to employ the VADER classifier~ to compute the sentiment, or polarity, of LLM responses to gendered-prompts. We also calculate the "Sentiment Gap" score to serve as an indicator of overall bias. This score is determined by computing the difference in sentiment between male and female responses for each gender-paired prompt and then averaging these differences. %  to %We refer to this as the 'sentiment gap' measure and consider it to be an indicator of 'bias'.  %, the higher the gap, the greater the variance in the responses and hence the higher the bias.  Regard scores~ aim to improve upon sentiment scoring by assessing the polarity of language and societal perceptions toward specific demographics (e.g., age, religion, race, gender). We used the BERT based classifier.

% aim to improve upon sentiment scoring using a contextual understanding of `Regard' towards various social groups. Regard metric is built using fine-grained annotations of the same (addressing cases where sentiment may be flipped when compared to Regard towards a group) and the creation of a classifier to annotate responses.  We used MLCommons ModeGauge's LLamaGuard2 annotator to annotate responses with Safe vs. Unsafe categories. Since `Bias' is not one of the pre-defined Safety categories in the current version of LlamaGuard, we explored modifying the system prompt to update the `Hate' category to include `Gender Bias'.

 This metric annotates responses with `Full Refusal', `Partial Refusal', or `Full Compliance'. We used OpenAI Compliance Annotator, a part of MLCommons' ModelGauge to annotate target LLM responses. 

We perform a human evaluation using Amazon Mechanical Turk (AMT) by defining two tasks: i) Evaluating individual prompt-response pairs, where the annotators are provided with the gender-paired prompts and the target LLM response and are asked about the presence of stereotype, to rate the response for the bias level, sentiment, toxicity, profanity, etc. ii) Comparing the gender paired prompts and their respective responses to detect if the responses convey similar or different ideas and content. We use this to assess various target LLMs on how readily they answer the prompt across different genders and if the model provides varying answers for different groups. 

Crowd workers are also instructed to make their choices by keeping aside their personal biases and stereotypes, and by only focusing on the content.  We select the top challenging prompt pairs that show discrepancies between the gap metrics mentioned earlier. Specifically, we choose pairs with a high Sentiment Gap score but a low LLM-judge Gap score, and vice versa, for this task. We sample approximately 100 gendered prompt pairs per target LLM for human annotation, resulting in approximately 600 gendered prompt pairs for which we obtain annotations. We obtained annotations from 3 annotators for each sample, where we considered the majority vote and average rating (for continuous values). % % %Evaluating both of the gendered input-response pairs, where the annotators are provided with both the adversarial and counterfactual prompt-response pairs, and asked to holistically rate if the responses convey similar ideas or are different. We use this to assess various target LLMs to see how 'different' or how 'biased' their responses are to different gendered inputs. We obtain annotations from 3 annotators for every sample, pick the majority vote for the categorical answers and compute the average across annotators for the ratings. % [t!]% %     \centering%     {%     %     }% [t!]% % % GPT4           82.278481    5.06329114% llama2-7b-chat 73.9130435    26.0869565% llama2-13b     84.7826087     15.2173913% llama2-70b     90.9090909      9.09090909% mistral        90.3846154     9.61538462% mixtral        89.5348837    9.30232558% %     \centering%     % \includegraphics[width=1.95\columnwidth, scale=0.6]%     \includegraphics[scale=0.45]{figures/llama_identity_male_female.pdf}%     %     %     % %%%For the attacker LLM that generates adversarial prompts, we use Meta's Llama3-8B Instruct model} with system and user prompt as described above and sample about 500  prompts  using the keywords from the gendered-keyword lists and temperature set to 0.6 and top\_p set to 0.9.   %For finetuning 4-bit quantized Llama3, we use LoRA parameters rank=16 and =16. %We use an 8-bit Adam optimizer with weight decay and 60 steps to finetune the model on about 5000 adversarial prompts generated by ChatGPT.%%We generate responses to the  gender paired prompts.  The target LLMs we consider in this work are the Llama2-chat family of models (7b, 13b, 70b), GPT-4, Mixtral 8x7B Instruct-v0.1, and Mistral 7B Instruct-v0.2.