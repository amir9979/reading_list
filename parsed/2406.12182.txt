In this section, we will outline the process of building Aquila-Med-cpt from a massive general pre-training database, including how to collect medical-related corpora from pre-training databases, rule-based quality filtering methods, and LLM-based data quality selection methods. It is worth noting that this method is also applicable to any domain.

 Since Aquila's general pre-training (Aquila-pt) corpus comes from multiple data sources, it already contains domain information. However, since there is no clear domain label, we first need to classify the data to make full use of the medical domain data in Aquila-pt. Specifically, we first randomly sample 20k data from Aquila-pt and use the upsampling method to ensure that the ratio of Chinese and English being 1:1. Based on the sampled data, GPT-4 is used to perform two rounds of domain label annotation to improve label accuracy. The data with different labels twice are removed, and finally 17k seed data is retained. Then we design a classifier using the Bert-based multilingual pre-training model. The parameter settings are as follows: batch-size is 64, learning rate is 2e-5, training epoch is 10, and the optimal checkpoint is selected according to the accuracy. The medical domain F1 of the classifier can reach 84\%.

 Since Aquila-pt mostly comes from web pages, the overall quality is not high. In order to remove the noise data, we design a rule-based data filtering solution, including rules for removing data with insufficient tokens, excessive special characters, toxic content, and private information.

 By sampling and checking the data after rule filtering, we found that there exists the following problems: (1) the data contains advertising and marketing information, which will greatly affect the output preference of the trained model; (2) the data contains grammatical errors, semantic incoherence, splicing of multiple unrelated content, image and video editing information, etc. We believe that such data is not beneficial for model training because the model cannot obtain much valuable information through autoregressive learning. Therefore, we design a quality scoring regression model based on LLM to score data quality and further filter out low-quality content. Specifically, we extract 20k data from the rule-based filtered data, score them twice using the GPT4, ranging from 0 to 6, and remove the data with a difference of about 2 points between the two scores, and finally obtained 15k training data. Then we train a scoring model based on the Bert multilingual pre-trained model, using batch-size of 128, learning rate of 3e-4, and train epoch of 10. We set a threshold for high-quality data filtering.

% 详细展开说一下LLM的实现,类似于上文。% 实现细节:从经过基于规则过滤后的数据中抽取20k数据,使用GPT4模型进行两次打分,最高分6分,最低分0分,并去除两次打分分差大约2分的数据,最终得到训练数据15k,使用基于Bert的多语言预训练模型设计了打分模型,使用batch_size=128,学习率为3e-4,经过10轮训练,并根据mse指标选择最优轮次,最终训练得到打分模型。将打分模型看成二分类模型,以4.5分为阈值,高于该分数认为是高质量数据,训练得到的打分模型在高质量数据上的F1值为0.8。 Our domain pre-training is divided into two stages. The Stage 1 is the training of ordinary quality domain data, and the Stage 2 is the training of high-quality domain data.

 The aim is to prevent the model capability from being significantly degraded due to the large difference between pre-training and continue pre-training data. We use medical domian data filtered by rules and general data with a certain ratio. The data amount is about 60B tokens.

 The aim is to further improve the capability of the medical domain model. We use medical domian data filtered by LLM quality model and open source medical SFT synthetic data. The data amount is about 20B tokens.

Our model is based on Aquila-7B, which a general Chinese-English LLM with 7 billion parameters. It has been pre-trained autoregressively with 3.6T tokens. The vocabulary size of the model is 15k, the model contains 32 layers of transformers, the maximum length is 4096, the hidden layer dimension of each layer of transformer is 4096, the FFN linear layer dimension is 14336, and the GQA structure is used in attention layers, with 8 groups and 32 heads.   

For the first stage of continuous pre-training, we train on 3*8 NVIDIA A100-40G GPUs, using a batch-size of 768, a learning rate of 1e-4, a maximum length of 4096, a cosine learning rate scheduler, a warmup-ratio of 0.05, and train for one epoch. For the second stage, keeping other settings unchanged, we reduce batch-size to 384, learning rate to 1e-5, and reduce warmup-ratio to 0.01. We also train for one epoch.

% Aquila模型训练数据量为3.6T,详细信息https://jwolpxeehx.feishu.cn/docx/Gev3dv9W7on8E7xVLWbclg62nyb#O0Wrd7K9uoUqhBxLGD0cRZNTnvf To improve the ability of language models to engage in natural conversation, we firstly carry out SFT, which finetunes a pretrained LLM on chat-style data, including both queries and responses. In the following sections, we will delve into the details of data construction and training methods.

Our SFT dataset comprises a variety of question types, including medical exam multiple-choice questions, single-turn disease diagnosis, multi-turn health consultation, etc. It comes from 6 publicly available datasets, namely Chinese Medical Dialogue Data , Huatuo26M , MedDialog , ChatMed Consult Dataset , CMB-exam, and ChatDoctor . These datasets contain not only real doctor-patient dialogues, but also dialogues generated from GPT-3.5. We believe this ensures the diversity of the dataset. 

Since a relatively small high-quality dataset has been shown to be sufficient for fine-tuning LLM, we focus on how to automatically filter "good data" from massive data to ensure competitive performance with a minimal amount of data. Similar to common data cleaning operations, we first remove duplicates and data related to security issues such as violence, bias, and pornography. In the following sections, we specifically introduce the data filtering methods.

 Following , we believe that "good data" should have a complex instruction and a high-quality response. Therefore, We adopt the approach from Deita , which employs a complexity model and a quality model to score each instance along two dimensions: instruction complexity and response quality. The complexity model assigns a complexity score  to each instance, while the quality model assigns a quality score , reflecting the quality of the response. %we borrow from Deita, which uses a complexity model and a quality model to score the dimensions of instruction complexity and response quality for each instance, respectively, to obtain the complexity score  and quality score .  By multiplying  with , we combine the complexity score and quality score to obtain a comprehensive score, that is, . Finally, we set a score threshold to select the most effective data instances in the massive data pool.

 For multi-turn dialogues, we first use Deita to calculate the score  of each turn separately, and average them to obtain the final score of the entire dialogue. However, we found that there are two special problems in multi-turn dialogues compared to single-turn dialogues: (1) The correlation between different turn is very low, resulting in a negative impact of the previous information on the following; (2) The correlation between different turns is too high, resulting in a large degree of context duplication and information is redundant. Therefore, we propose a Context Relevance (CR) score, which is a metric that relies on cross-entropy loss to evaluate the impact of historical information on each turn. The details are as follows:

In the instruction-tuning process, the loss of a sample pair  is calculated by continuously predicting the next tokens in the current turn  given their previous tokens and the history information :

where ,  is the current turn,  is the -th token in the -th turn, and  is the number of tokens of the current turn. We define  as the Conditioned Information Score, which measures the ability to generate the current turn under the guidance of corresponding historical information. 

To measure the ability of LLM to generate this turn alone, we also define a Direct Information Score:

We believe that the higher Direct Information Score may indicate that the turn is more challenging or complex. Finally, we try to estimate the CR score by calculating the ratio between  and .

Here, if , it means that historical information has a negative impact on current turn, that is, the correlation between contexts is very low. If , it means that historical information has a positive impact on current turn, that is, the correlation between contexts is high. However, too small  means that the context is highly repeated and the information is highly redundant. We also set a threshold to filter the data.

Our model is based on Aquila-Med and the training process has the following hyperparameters: sequence length set to 2048, batch size set to 128, and peak learning rate set to 2e-6 with cosine learning rate scheduler. To prevent overfitting, weight decay of 0.1 is applied and dropout is set to 0.1. Training is parallelized on 8 A100-40G NVIDIA GPUs using the AdamW optimizer with bf16 precision and ZeRO-3. We reserve 10\% of the training set for validation and get the best checkpoint after 2 epochs.

Through the above data filtering methods, we select s 320,000 high-quality SFT medical dataset from 199,000 instances, in which the ratio of Chinese and English is 86\%:14\%. As shown in Figure , it comes from single-turn Chinese medical dialogues (single), single-turn English medical dialogues (single), multi-turn Chinese medical dialogues (multi), and medical subject knowledge multiple-choice questions (single). 

We enhance the model's capabilities using Direct Preference Optimization (DPO)  after the SFT stage. To align the model's output with human preferences while preserving the foundational abilities gained during the Continuous Pre-training and SFT stages , we construct subjective preference data and objective preference data. We also provide the training details of the DPO stage.

We construct the preference pair for the DPO stage using samples that have the same distribution as the SFT dataset. This mainly includes the following two preferences.

We aim to construct dpo pairs where the chosen response aligns closely with human preferences. For each prompt, we first ask GPT-4 to respond as a professional and helpful doctor. Then, using GPT-4, we evaluate the superiority or inferiority of the original response and this newly generated response from the prompt. The evaluation considers four aspects: Fluency, Relevance, Completeness, and Proficiency in Medicine . We select the superior response as the chosen response for the dpo pair and the inferior response as the rejection response.

 While RLHF can guide LLMs to align with human expectations, numerous studies show that this method can cause LLMs to forget abilities acquired during pre-training and SFT stages , leading to an "alignment tax" . To mitigate this issue, we construct objective preference data. Specifically, for objective prompts with known ground truth answers, we consider the ground truth as the chosen response and randomly select incorrect answers from the remaining options as rejection responses. For instance, in multiple-choice questions, if the ground truth is option A, we randomly select from options B, C, and D to construct the rejection response.

We constructed a dataset of 12,727 DPO preference pairs, consisting of 9,019 subjective and 3,708 objective data samples. We trained the model over two epochs using 8 NVIDIA Tesla A100 GPUs. The settings included a learning rate of 2e-7, a batch size of 64, and a beta of 0.03. Additionally, we employed a learning rate warmup and a cosine learning rate scheduler for optimization.

%The training objective for the DPO is structured as follows: % % \footnotesize%  = - \sum_{(x, y_w, y_l) \sim } \log \sigma \left( \beta \log {\pi_0(y_w|x)} - \beta \log {\pi_0(y_l|x)} \right)% % Let  represent the DPO dataset, with  and  denoting the chosen and rejected responses in each preference pair, respectively.% We trained the model over two epochs using eight NVIDIA Tesla A100 GPUs. The settings included a learning rate of 2e-7, a batch size of 64, and a beta parameter of 0.03. Additionally, we employed a learning rate warmup and a cosine learning rate scheduler for optimization. We extract medical-related questions from the MMLU  and C-Eval  benchmarks, and we utilize questions from the CMB-Exam , MedQA , MedMCQA  and PubMedQA  test set to evaluate the model's proficiency in medical knowledge.

 is the english multi-subject multiple-choice dataset, from which we extract medical-related tasks to evaluate the model's performance. These tasks encompass various medical domains, including anatomy, clinical knowledge, college biology, college medicine, medical genetics, and professional medicine.

 is a chinese multiple-choice dataset. We extracted tasks related to medicine from the validation set, such as basic medicine, clinical medicine, medical practice, and veterinary medicine to test the model's performance.

 is a collection of multiple-choice questions in Chinese, sourced from various professional mdedical qualification examinations. It encompasses questions from exams for physicians, nurses, technicians, pharmacists, undergraduate medical programs, and graduate entrance examinations. We utilize 11,200 questions from the test set to conduct a comprehensive, multi-level assessment of the model's medical knowledge.

 is a multiple-choice question dataset from the United States Medical Licensing Examination (USMLE). Its test set consists of 1,273 questions, which are used to assess a model's medical knowledge and reasoning skills required to obtain a medical license in the United States.

 is a large-scale multiple-choice question and answer dataset, sourced from India's medical entrance exams (AIIMS/NEET). Its test set comprises 6,100 questions, enabling the evaluation of a model's general medical knowledge and reasoning abilities.

 is a closed-domain question and answer dataset, where each question can be answered by referring to the relevant context from PubMed abstracts. We use 500 test questions from this dataset to evaluate a model's ability to understand and reason about biomedical literature.

We evaluate the model's capability to solve realistic patient problems by assessing its medical knowledge and complex reasoning abilities. This evaluation covers single-round dialogue scenarios, such as the Huatuo MedicalQA , as well as multi-round dialogue scenarios like CMtMedQA  and CMB-Clin .

 is a large-scale Chinese Medical Question Answering (QA) dataset, and we use its test set to evaluate the model's capability in single-round dialogues. Specifically, we sample 500 question-answer pairs from the test set and employ GPT-4 to compare the model's predicted answers with other reference answers (mainly including the ground truth answer from the dataset and the answer generated by GPT-3.5). Inspired by , we use the prompt in Table  to judge the quality of the answers. Considering that GPT-4 may exhibit a "position bias" when judging , we swap the order of the predicted answer and the reference answer. We determine a answer as winning or losing only when the judgment results are completely consistent before and after the swap.

 is a large-scale dataset consisting of multi-turn medical dialogues in Chinese. To evaluate the model's ability to engage in complex dialogues and initiate proactive inquiries, we utilized approximately 1,000 samples from the dataset's test set.

 consists of 74 expertly curated medical case consultations derived from clinical diagnostic teaching materials. It evaluates the model's mastery and reasoning abilities in applying medical knowledge through multi-round diagnostic dialogues.

For multi-round dialogue datasets such as CMtMedQA and CMB-Clin, inspired by , we employed GPT-4 to evaluate the model's responses in each round of the dialogue. The evaluation focused on four key aspects: fluency, relevance, completeness, and proficiency in medical knowledge. The specific evaluation prompt used is displayed in Table .

% Table  shows the results of our continue pre-training on five benchmarks. It can be observed that Aquila-Med has improved to a certain extent compared with Aquila, especially on MMLU. This shows that even if the model uses the data which has been already learned in the pre-training stage, the professional ability of the model can be further improved by improving the quality and professional density. In general, we obtain a basic model with medical domain knowledge.

For instruct-tuning, we evaluate it from two aspects: medical subject questions and doctor-patient consultation. Table  shows the results on three medical knowledge benchmarks. We found that Aquila-Med-Chat has good command following ability, and Aquila-Med-Chat (RL) has made further progress, especially C-Eval. Figures  and Figure  show the comparison of the outputs of our models with the reference and GPT-3.5 outputs in single-turn dialogues. It is observed that both Aquila-Med-Chat and Aquila-Med-Chat (RL) have achieved good results, especially Aquila-Med-Chat (RL) has achieved human-style alignment. For multi-turn dialogues, we use GPT-4 to score each turn in four dimensions, and the results are shown in Figure  and Figure . The evaluation results indicate that Aquila-Med-Chat (RL) performed well in terms of generating fluent responses. Additionally, it was observed that Aquila-Med-Chat (RL) significantly enhanced the model's performance in terms of relevance, completeness, and proficiency, while still maintaining a high level of fluency in the generated responses.

%We found that Aquila-med-chat-dpo surpasses Aquila-med-chat in terms of Relevance, Completeness, and Proficiency. However, in terms of Fluency, Aquila-med-chat-dpo is slightly worse than Aquila-med-chat.%加一个原因% %  %  % [t]%     \centering%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar1_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar2_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar3_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar4_v1.pdf}%         %     %     %     % % [t]%     \centering%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar5_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar6_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar7_v1.pdf}%         %     %     \hfill%     [b]{0.24\textwidth}%         \centering%         \includegraphics[width=\textwidth]{fig/bar8_v1.pdf}%         %     %     %     %