% 一些没用但可能用到的表述 % concentrates on fully exploiting LLM's pretrained capability to conduct mathematical reasoning, thus progressing along the tool-free research trajectory. % CoT and RFT  Rejection Sampling-based Fine-Tuning (RFT,~) only augments the solutions via rejection sampling to collect a variety of different reasoning paths. Since RFT does not introduce new math questions, the diversity of the augmented dataset is quite low, which limits the performance improvement of the finetuned models. % MetaMath  With the aim of incorporating a broader spectrum of questions, MetaMath~ employs rephrasing, Self-Verification (SV,~) and FOBAR~ to generate new questions. Ideally speaking, like the original questions, there are also ground truth answers for filtering solutions to these augmented questions.  % WizardMath, MuggleMath  To bring in more diverse data, WizardMath~ and MuggleMath~ choose to create totally new questions via evolution or directional modification (changing numbers, adding conditions, increasing complexity, etc.) based on the seed questions. These altered questions have no ground truth answers, thus lacking a criterion to filter their corresponding synthesized solutions. 

% MuMath  Furthermore, MuMath~ leverages some of the aforementioned methods, and additionally proposes BF-Trans and expression replacement (etc.) to perform comprehensive augmentation, thus constructing a multi-perspective math question set with much greater diversity. For improving data quality, majority sampling serves as the filtering rule for the synthesized solutions to those new questions without deterministically known answers. Instead of solution filtering, a contemporary work, Xwin-Math~, employs verification with solution requesting during question synthesis, thereby improving the solvability of the questions and the correctness of the answers. Since there is no restriction on the direction of question modification, Xwin-Math theoretically offers a wider variety of diverse synthesized data. Balancing the efficacy and the ease of replication, in this paper the proposed MuMath-Code opts to employ the question augmentation from MuMath, although it is orthogonal to any other augmentation methods.

% 对 tool-free 的驳斥  Nevertheless, as probabilistic models, LLMs inherently have limitations in logical reasoning and numerical computation. Thus, to improve the accuracy of mathematical problem-solving while relying solely on the capabilities of LLMs necessitates the utilization of a substantially larger dataset compared to tool-use methods.

Another research trajectory highlights the synergy between LLMs and external tools. Pioneering efforts along this include the Program-aided Language model (PAL,~) and Program of Thought (PoT,~). Moreover, MAmmoTH~ integrates both CoT and PoT in a coarse-grained fashion (each sample corresponds to only one of these two possible solution types), enabling flexible inference where the finetuned models may adopt different methods for different questions. Different from MAmmoTH, ToRA~ interleaves python code blocks and natural language reasoning parts over multiple turns for a same solution, which offers a more flexible combination of CoT and PoT. However, neither MAmmoTH nor ToRA employs query augmentation, thereby narrowing the range of math questions, which in effect, limits the problem-solving capabilities that can be acquired. Wang et al. propose a contemporaneous work with ToRA, MathCoder~, where each solution is also organized in an interleaved manner. Besides, they introduce interpolation problems to mitigate the disparity in difficulty level between GSM8K~ and MATH~. Hence, like our MuMath-Code, MathCoder is also an amalgamation of tool usage and math question augmentation, although the new questions it introduces are comparatively narrow in scope and limited in diversity.

Similar to ToRA and MathCoder, we also construct such solutions  that intertwine Python code with pure language reasoning text to adaptably combine LLMs with external code executing tools. However, we propose prefix CoT, code debugging, and pseudo-answer guidance filtering to further enrich the solutions and improve their correctness. Additionally, different from MathCoder, the question augmentation we utilize are multi-perspective, thus offering greater diversity and exposing the model to a broader scope of novel questions, thereby significantly enhancing the model's generalization capabilities.

%%%% 2024-04-15 计划% 1. 文字部分润色,搞出能阅读的版本%   :已搞完。现在的版本不再是直接 gpt 毛的生成结果了,都人工修改过,现在是需要润色。% 2. 方法大图%%%%% 2024-04-17 计划 % 方法大图修改:1. 高度窄一点;2. Debug 的颜色更换;3. Succeed 改成另一个词;4. Mumath Data 的数据库图表周围的灰色阴影去掉,MuMath Code Data 右边添加相同的桶子,都是数据集;5. MuMath Code Data 与右边的实例使用对话云框来进行连接,就像 MuMath Code Data 这个人在说话,说话内容是右边的数据集介绍;6. Stage 1 中的数据写 pure natural language reasoning data 这种,不屑 MuMath Data,然后把 SFT-S1 换成 MuMath;然后 Stage 2 中的 Mumath Code Data 换成 Code-nested data 之类的话,SFT-S2 换成 MuMath-Code。一是抵消 SFT 的地位(因为太简单),二是降低前置 MuMath 工作的贡献。7. 每个阶段的数据下拉一个箭头指向训练的右指箭头,表示这个阶段由这个数据来训练The original questions from the training sets of GSM8K~ and MATH~ are taken as the seed question set . The question augmenting methods employed in MuMath are conducted on this seed set, which are briefly concluded as follows: 

% \uppercase  Rewrite a text while keeping the original meaning unchanged. Based on the fact that a rephrased question holds the same meaning as the original one, the final answer of it should also be the same. We denote the rephrased question set as . 

 There are five manners to alter the original questions, like changing numbers and adding more conditions, concluded in MuggleMath~. The resultant question set created via alteration is referred to as . Besides, Expression Replacement, proposed in MuMath, firstly get the expressions of the solution to an original question, then change the calculation operators within them. Based on the changed expressions, a new question is asked to generate.  represents the question set produced by this augmentation technique. Note that  and  correspond no definitely correct answers due to modifications in the questions' intrinsic meanings.

 Following~, we mask a certain condition in an initial question by substituting it with ``X", and meanwhile give the answer to the original question as a new condition, thereby creating a reverse question that seeks to determine the value of the unknown X.  is utilized to mark the FOBAR question set. 

 Backward-Forward Transformation (BF-Trans), proposed in MuMath, aims to construct such backward questions that can be answered through direct reasoning, bypassing the necessity of solving equations to find the unknown variables (thus resemble the data sampled from the original distribution). For a certain question-answer pair, BF-Trans firstly utilize FOBAR to transform the original question into a backward one; secondly, we rephrase the FOBAR question into a new form where the masked value is requested directly instead of employing an unknown variable X, resulting in a ``secondary forward'' question which we called BF-Trans question. The set of these BF-Trans questions is marked as . 

To sum up, all the 10 aforementioned subsets (5 in ) constitute the resulting question set . Based on , we generate 2 datasets called  and , emphasizing pure natural language mathematical reasoning and tool interaction via code generation, respectively.

MuMath-Data (denoted as ) is just the largest dataset from MuMath, which contains about 750K samples with pure CoT reasoning solutions to questions in .

As is introduced in the paper of MuMath, for  and  whose each question has no reference answer, majority sampling is utilized to filter all the randomly generated solutions and only those solutions with the majority answers are kept. In other words, each majority answer serves as a pseudo-answer to the corresponding question.

% 我们的 CoT 与 PoT 结合的数据类似于 ToRA 或者 MathCoder 那样,使用 CoT 与 PoT 交互的方式,但是我们的创新点在于,1. 充分的 CoT 为第一个部分,由 prompt 引导产生;2. 如果代码执行出错,我们会拼接 prompt 得到一个纠错的代码,然后继续执行查看是否有错,直到顺利执行出一个结果。

To facilitate the interaction with the python interpreter, we synthesize the code-nested solutions for the models to learn, each consisting of multi-turn code generation, code execution and pure natural language reasoning.

Specifically, for each question from , we prompt proprietary LLMs to request solutions each with at least one block of code, which is then extracted and passed to the external interpreter for execution. Every execution result is appended to the preceding content, right after the corresponding code block. If the code execution fails, we append a prompt to actively debug, using all the previous content as a whole new prompt to request the corrected code, which we then extract and execute again. By iterating this process multiple times, we obtain a reasoning path comprising code, execution outcomes and natural language analysis. This reasoning path is similar to that of MathCoder~ and ToRA~, but the differences lie in the use of our proposed prefix CoT, code debugging, and pseudo-answer guidance filtering, which will be elaborated on in this section. We marked MuMath-Data-Code as . 

We have observed that before generating code, a thorough pure natural language analysis is helpful for the models' performance. Therefore, we deliberately add a thoughtful CoT reasoning before code writing. The request prompt used is ``''.

Several research studies have shown that the use of error correction and verification data can improve the mathematical reasoning capabilities of LLMs. Therefore, we introduce an error correction process for our augmented dataset. Specifically, while constructing a solution, if the generated code fails to execute, we append a prompt ``'' for GPT-4 to debug the code and write new code until the executable code is obtained, or the maximum number of requests is reached. The failing code and error information are kept to equip the finetuned models with debugging ability, and thus enhance their coding proficiency for solving math problems.

In MuMath-Data, we employ majority sampling to filter solutions. This provides us with pseudo-answers for the augmented questions corresponding no reference answers, which can also be employed for MuMath-Code-Data to select solutions. This approach improve the correctness of the synthesized solutions, thereby leading to an enhancement in the overall quality of the augmented data.

To sum up, we mark the -th CoT (pure natural language reasoning) part as ; the -th python code part is marked as , which always begins with  and ends with ; the -th code execution output is denoted as , beginning with  and ending with . To formalize, one resulting solution  is defined as follows:

where  stands for the concatenation of all the turns, and  is the number of CoT parts. See Appendix~ for an example. 

The first stage training is on MuMath-Data, where the models concentrate on learning the capability of pure CoT math reasoning. The learning target is as follows:

where the solution  contains  tokens, and  is the parameter of MuMath-Code.

This training stage endow the models with a fairly strong mathematical reasoning capability, which can be seen as an preliminary task for the second stage learning.

The second stage training is on MuMath-Code-Data, where the models concentrate on PoT-CoT interleaved data to learn how to interact with an external tool (i.e., the Python interpreter). We mask the loss of the outputs from the code execution, which should not be learned by the models. The learning target is:

where . The training process at Stage-2 is consistent with the inference, so we do not need to consider the issue of catastrophic forgetting (regarding the natural language reasoning in Stage-1). At inference time, after being given a mathematical problem, the finetuned model needs to generate code for problem solving, and then an external interpreter executes the code and returns the result for the model to continue generating. Therefore, Stage-2 training simulates the above inference process by masking out the losses of the execution outputs.

%%%%%%%%%%%%%%%%%%% MuMath-Code 的实验部分行文组织 % 1. 对比大实验% 2. scaling % 3. 各部分 scaling % 4. MATH 的各类测试指标 % 5. 两阶段消融 %%%%%%%%%% 这个换成优雅的专门的环境,而不是使用图片的格式 %%%% 2024-04-15 计划% 1. 各个小实验的图表得画% 2. 首页图得画  Our seed datasets for synthesis are the training sets of two popular math reasoning benchmarks: GSM8K~ and MATH~. GSM8K contains elementary school math problems, comprising 7,473 training instances and 1,319 test instances; while MATH encompasses math competition problems at the high school level with 7,500 training samples and 5,000 for test. 

We take the MuMath~ dataset (750K) as our  for Stage-1 training, and the MuMath augmented question set  are utilized to construct  for Stage-2; in , we request 15 solutions for each question that originates from GSM8K and 30 for MATH-related ones, and then perform filtering to get 30K samples for each question subset, making 600K in total.

Our study utilizes LLaMA-2 (7B, 13B and 70B)~ and CodeLlama (7B, 13B, 34B, and 70B)~ as the foundation models for full-parameter finetuning, corresponding to MuMath-Code-L and MuMath-Code-CL as the resulting models. We employ AdamW as the optimizer and a cosine learning rate scheduler with a 0.03 warmup ratio. Across all the models and both stages, we train 3 epochs with a 128 global batch size. All the models except for LLaMA-70B and CodeLlama-70B are trained using the Deepspeed framework, while those two 70B models are trained using Megatron for the sake of speed. The hardware we use are NVIDIA H800 GPUs. % 我们将我们的模型跟当前最佳的模型做对比,如图,可以看到,我们的方法在任何规模的开源模型、所有数据集上都取得了最好的水平。值得一提的是,我们的 MuMath-Code-L 7B 模型在 GSM8K 上取得了 83.8 的测试准确率,MuMath-Code-CL 7B 模型在 MATH 上能够达到 52.4,这两种结果比许多 70B 的开源方法要高,甚至比一些闭源的大模型高;另外,我们的 MuMath-Code-CL 34B 和 70B 模型在 MATH 数据集上结果都超过了 55.0,这是个非常 impressive 的结果(注意到我们是依靠基于原始训练集的数据增强方法,而并没有引入额外大量的数学预料的预训练)。

As shown in Table~, the comparison experiment of our models with the current state-of-the-art demonstrates that our approach consistently achieves superior performance across all scales of open-source models on all the datasets. Notably, our MuMath-Code-L 7B model has attained a test accuracy of 83.8 on the GSM8K, and MuMath-Code-CL 7B has reached a score of 52.4 on MATH. These outcomes surpass many 70B open-source baselines and even some proprietary LLMs. Additionally, our MuMath-Code-CL 34B and 70B achieve 55.0+ on MATH, two impressive results considering that they are accomplished by leveraging data augmentation techniques based on the original training set without the incorporation of extensive additional mathematical corpora for pre-training.

% 可以看到,该表格的实验结果中有一些值得关注的数据,比如 MuMath-Code-CL 13B 在 MATH 上的结果(53.1)比 MuMath-Code-CL 7B(52.4)要稍微差一点,并且 MuMath-Code-CL 34B 在 MATH 上的结果 (55.5 改为 55.0) 要比 MuMath-Code-CL 70B 的结果 (55.1) 更好。我们猜测这很有可能是因为当数据量达到一定的规模,对于特定的测试数据集,模型规模带来的影响会被数据量提升带来的优势减轻甚至抵消。(而训练框架的不同也可能是造成上述 MuMath-Code-CL 34B 与 70B 结果对比的原因)

There are some noteworthy findings from the experimental statistics presented in the table, such as the performance of MuMath-Code-CL 13B on MATH, registering at 53.1, which is only marginally higher than that of MuMath-Code-CL 7B, which stands at 52.4. Moreover, the MuMath-Code-CL 34B's performance on MATH, scoring at 55.0, is very close to that of the MuMath-Code-CL 70B, which records a score of 55.1. We speculate that this may be attributed to the phenomenon where, beyond a certain threshold of data volume, the advantages conferred by increased model size may be diminished or even offset by the benefits derived from the expanded dataset. Additionally, variations in the training frameworks may also contribute to the observed discrepancy between the performances of MuMath-Code-CL 34B and 70B.

% MuMath-Code 是由纯自然语言 reasoning 能力训练和结合代码生成的reasoning训练两个阶段得到的,本节我们验证这种两阶段训练策略的有效性。如无特殊说明,考虑到时间成本,本文所有消融实验都是在 7B 的模型上做的。我们设计了两阶段训练与单阶段训练的模型性能对比,这里的两阶段训练就是我们在方法部分阐述的那样(基于第一阶段的训练 checkpoints 来继续进行训练),而这里的单阶段就是直接在 LLaMA 或者 CodeLlama 的 base 模型上进行第二阶段的训练,我们 vary 第二阶段训练集 MuMath-Code-Data 的数据量。该表格展示了不同数据量设置时两阶段策略与单阶段策略的结果模型的性能对比,可以看到,仅仅使用第二阶段来进行训练,在全部规模的模型、所有数据量的结果中都是不如两阶段训练的表现。另外,我们把两个阶段的训练数据进行融合,得到一个总的数据集进行单一阶段的训练,可以看到效果也是不如两阶段分开训练更好。

MuMath-Code is derived from a two-stage training process that enhances the model's pure natural language reasoning capabilities and the ability to generate code and interact with external tools. In this section, we validate the efficacy of this bifurcated training strategy. Unless otherwise specified, all ablation experiments presented in this paper are conducted on 7B models, for the sake of time efficiency. We have designed a comparative evaluation of model performances for two-stage and one-stage training strategies. The two-stage training referred to here is as described in Section~, which involves continuing training from the checkpoints of the first stage (the MuMath models). The one-stage training, directly applies the second stage of training on the base models. On both settings, we vary the data volumes of . Table~ illustrates the performance comparison of models derived from both strategies across different data volumes, revealing that training solely on  is worse than the two-stage training. Furthermore, by merging the training data from both stages into a single dataset for one-stage training, we observe that the outcomes are still not as favorable as those obtained from two separate training stages. % Since our focus is primarily on  proposed in this paper, we regard  750K as an indivisible dataset. Therefore, data merging is only performed on  600K so that the sizes of both data partitions are as balanced as possible. % 既然 vary data size 的放进了附录,这里也就不用强行解释为什么不拆分第一阶段的数据了 

To further validate the effectiveness of our two-stage training strategy, we select MetaMath~ and Xwin-Math~ 7B models as the initial checkpoints for Stage-2 training, emulating the scenario where relevant datasets were employed during the first stage (Given the the unavailability of the most recent models and dataset proposed in~, we opt to utilize Xwin-Math-7B-V1.0 detailed in the corresponding GitHub repository). Table~ illustrates that models fine-tuned from MetaMath and Xwin-Math checkpoints on  (two-stage) outperform the one directly trained from Llama (single-stage), verifying the efficacy of a two-stage training strategy as well as the compatibility of our  with different first-stage CoT datasets.

% 我们对中本文提出的 prefix CoT 和 Code Debugging 进行消融实验,具体地,我们对 MuMath-Code-Data 进行数据修改,第一种修改方法是删除每个题解中的 prefix CoT,即前面的详细分析给删除,而直接进行代码的撰写;第二种修改方法是仅仅保留最后一个执行成功的代码块,而删去其他所有执行失败的代码块以及纠错过程。这个消融实验的结果见该表格,可以看到,去掉 prefix CoT 和 Code Debugging 都会使模型测试准确率下降,这充分说明了撰写代码之前的详细分析与代码纠错对于模型的学习是很重要的。To verify our proposed prefix CoT and code debugging, we respectively modify the solutions in  via two distinct approaches: the first approach involves the removal of the prefix CoT, thereby eliminating the detailed preliminary analysis and directly begining with code writing; the second approach consists of retaining only the final and successfully executed code and omitting all the other inexecutable code before as well as the corresponding debugging process. The results of this ablation study are presented in Table~, which demonstrates that the exclusion of either the prefix CoT or code debugging leads to a decline in the models' test accuracy. This emphatically underscores the significance of a thorough analysis prior to code writing and the code mistake correction process for the models' learning.

% 另外,我们进行 pseudo-answer guidance filtering 的消融实验。在方法部分我们指出 pseudo-answer 适用于不带有明确正确答案的合成问题(即  与 )。在我们的 previous work MuMath 中提出使用 majority voting 来给这些问题赋予 pseudo-answer,也就是在 MuMath-Data (第一阶段纯自然语言数学推理数据集)中相关题解的答案,我们使用这个 pseudo-answer 对第二阶段数据进行过滤。如图,可以看到,使用这个 pseudo-answer 过滤后的数据进行微调,比随机采样直接得到的答案是更加有帮助的,数据范围从 30K 到 180K 都是这样的规律。 Moreover, we conduct another ablation experiment on pseudo-answer guidance filtering. In Section~, we note that pseudo-answers are suitable for synthetic questions that lack a definitive correct answer, namely those in  and . In MuMath, majority voting is utilized to assign pseudo-answers to these questions. These pseudo-answers are then also employed to filter the data for  in the second training stage. As illustrated in Table~, fine-tuning the model with data filtered through this pseudo-answer technique proves to be more beneficial than solutions obtained through directly random sampling. This trend holds across data volumes ranging from 30K to 180K.

% 如果后续要缩页数,可以把 scaling 实验都放进附录里,附录章节名直接使用 scaling% 我们对 MuMath-Code-Data 各部分子集的 scaling 实验如图所示,这是不同的数据组成部分关于数据量对于模型的测试表现曲线,base 模型是 Llama 7B,对于 CodeLlama 的微调结果见附录。可以看到,随着数据量的增多,各部分数据都对模型的性能不断带来增益,并且曲线并没有趋向饱和,这说明使用我们的方法可以继续增加数据来进一步提升模型的数学推理能力。% 这个实验要搞附录,因为这个实验有很多,要素包括 1. {融合训练数据, 单一源的训练数据} 2. {单阶段, 两阶段} 3. {Llama, CodeLlama} 4. {GSM8K, MATH},其中单一源的训练数据必然是相关测试数据,其他情况这 4 部分都是可以任意组合的,所以搞出来最合理的放到正文,搞出来第二合理的放到附录,其他的不再放进论文。决定单一源的 scaling 就不搞了,所以一共 8 个其他的都搞,单阶段、Llama 的 GSM8K 和 MATH 放正文,其他的放附录。再次修改,CodeLlama 的也不搞了,所以只是单阶段正文、两阶段附录,都是 Llama,都是融合数据。 The scaling experiments for various subsets of the MuMath-Code-Data are depicted in Figure~. These curves represent the performance changes of models trained on different data subsets with respect to the number of samples. The base model is LLaMA 7B and it is directly trained on the subsets of  (single-stage training). It is evident that with the increase in data volume, all subsets continuously contribute to the enhancement of the models' performance, and the curves still do not show saturation. This indicates that employing our methodology allows for the continued addition of data to further improve the LLMs' mathematical reasoning capabilities. For the two-stage scenario where the initial model is an intermediate checkpoint from Stage-1, please refer to Appendix~.