Our approach is based on knowledge distillation applied to prompt learning. Here we discuss the base teacher and student models (CLIPs) and five state-of-the-art prompt learning approaches into which we will incorporate .

Contrastive Language-Image Pre-training (CLIP) is a vision-language model trained to align images and textual captions in shared semantic space~. CLIP consists of an image encoder  and a text encoder . Given an image , the image encoder computes its feature representation , where  is the size of the semantic embedding space. Similarly, for a given textual caption , a word embedding layer  maps each tokenized word to the token embedding space . Then, the text encoder  generates the textual feature representation . The main goal of CLIP training is to learn  and  such that  for associated image/text pairs .

When using a Vision Transformer (ViT)~ as the visual encoder , the encoding process begins by splitting the image into  fixed-size patches. These patches are then projected into patch embeddings , where each  belongs to the patch embedding space . A learnable class (CLS) token  is concatenated with the patch embeddings, resulting in the input to the vision transformer being . Finally, the CLS token of the final transformer layer is projected to the shared embedding space via a linear projection to obtain the final representation.

To perform zero-shot classification using CLIP, we start with an image  and build a set of textual prompts , where  denotes the number of classes. Each handcrafted text prompt  takes the format  ``'', where  represents a specific class name, such as , , etc. 

Then the feature representations  and  are extracted using the CLIP encoders. The predicted probability for each class is:

where  is the cosine similarity and  is a temperature hyperparameter.

Here we summarize how the state-of-the-art techniques for textual, visual, and multimodal prompt learning work. 

Note that methods involving visual prompt learnable tokens like VPT~, MaPLe~, and PromptSRC~ can only be applied to VLMs equipped with a ViT-based image encoder.

The methods described above all rely on ground-truth labels during adaptation. Here we show how unsupervised knowledge distillation can be used instead to replace the need for annotated training examples.

Our proposed approach, which we call Knowledge Distillation Prompt Learning (), is a general method designed to enhance the performance of the CLIP model on downstream tasks through parameter-efficient prompt learning. Unlike previous approaches~, which rely on labeled examples for training,  eliminates the need for manually-labeled samples by learning only through knowledge distillation from a larger and more powerful VLM. Note that  is a method that can be seamlessly integrated with any existing prompt learning approach in scenarios where no information about class names or labels is available.

We validate  in two progressively challenging supervision regimes. In the  scenario we do not use ground-truth labels, but we assume knowledge of the class names in the training dataset. In the  scenario (see Section~) we go one step further and assume that even the training class names are unknown. For this class agnostic scenario we propose an effective and efficient online strategy for automatically filtering the classes from a large dictionary of approximately 20K class names~.

Given a lightweight CLIP model (the ) and a larger, more powerful CLIP model (the ), we aim to improve the downstream performance of the student model by distilling knowledge from teacher to student. For an image  and a set of classes , we start by performing zero-shot classification using the frozen teacher model. Specifically, we use the teacher image encoder  and text encoder  to compute the teacher image features  and text features . For the teacher model we use the fixed text prompt ``a photo of ''. We then apply  to produce the probabilities  predicted by the teacher on image  for classes .

The teacher model does not rely on a learnable prompt and its predictions remain fixed during training. Our aim is to learn text and image prompts for the student model that enhance its generalization to downstream tasks.

We denote with  and  the student image and text encoders, respectively, and with  the parameters associated with the learnable student prompts (see Fig.~). Given the same image  processed by the teacher and the same set of classes , the student extracts image features  and text features . Note that the text and image encoders can both depend on the prompt parameters . According to the prompt learning technique used, the  parameters can be used only by the text encoder (CoOp, CoCoOp), the visual encoder (VPT), or both (MaPLe, PromptSRC). Finally, using  we produce student class probabilities  predicted on image  for classes . Note that all encoder parameters except for the learnable prompt  are frozen.

We use the symmetric KL-divergence between the teacher () and the student () probabilities in a distillation loss:

where  is the asymmetric KL-divergence between the discrete probability distributions  and :

This distillation loss depends only on the fixed predictions of the teacher, the prompt-conditioned predictions of the students, and the set of classes . 

Importantly, . Nor does it require that  be the classes of the downstream task -- that is,  can be used for Label Agnostic and Class Agnostic adaptation scenarios. We found in early experiments that the symmetric KL-divergence works slightly better than either asymmetric option (see Section  in the Supplementary Material for an ablation study on this choice).

To further evaluate the generalization capabilities of , we introduce a scenario where not only do we not know the  of training images (label agnostic, Section~) but where we also do not even know the  associated with the dataset (class agnostic). This scenario is considerably more challenging as we make no assumptions about the few-shot training data.

To address the unavailability of class names, we propose a strategy for automatically selecting a set of class names for each batch. We start with a large vocabulary of class names from which to select. Specifically, we use the Open Images V7 dataset~, which contains 20K classes. The most straightforward method would simply use all 20K classes during training. However, this is impractical as the memory required by prompt learning methods increases linearly with the number of classes. According to Ren et al.~, CoOp requires nearly 15MB of VRAM , resulting in approximately 300GB of memory when multiplied by 20K classes. Therefore, we propose a method to automatically select which classes to use for each batch by retaining only the ones most relevant to the calculation of the loss in~.

Given a batch of images  and all the class names in the vocabulary , where  represents the number of images in a batch and  the size of the dictionary, we let the teacher model select the most relevant classes. Our objective is to identify the most useful  classes in each batch for student prompt learning. After extracting the teacher image and text features, for each image , we apply  to obtain the teacher probabilities . By stacking the probabilities along the batch dimension, we obtain the matrix , where the -th row corresponds to the probabilities associated with image . We then compute the average probabilities along the batch axis, resulting in . Finally, we select the classes corresponding to the  highest values in .

Using the teacher model to perform this class filtering for each batch is feasible and does not incur excessive memory costs since the teacher requires no gradient computation. Therefore, the memory consumption does not depend on the number of classes in . Conversely, although the student model remains frozen, gradients must still be propagated to update the prompt parameters . Once the classes are selected, the training strategy remains the same as described above, with the only difference being that the class names observed by the student vary in each batch based on teacher predictions.

Following previous works~, we validate  in three distinct settings: 1) domain generalization; 2) cross-dataset transfer; 3) generalization to unseen classes. Additionally, to evaluate the scenario where class names are also unknown, we introduce a new evaluation setting we call . We use the train/val/test splits and seeds provided by Zhou et al.~ for all datasets. All reported results are averages over three independent runs.

We evaluate  on the following scenarios:

We use a CLIP model with a ViT-H-14 visual backbone as the teacher model. For student models, we evaluate both on a CLIP model based on ResNet-50 and a model based on ViT-B/32. By experimenting with both ResNet-based and ViT-based CLIP models we show the architecture independence of our approach.  To assess how  performs when integrated into different prompt learning techniques, we selected five distinct textual, visual, and multimodal approaches.  With the ResNet-50 student model, we experiment with CoOp~ and CoCoOp~. For the ViT-B/32 student, since it supports both visual and textual prompting, we experiment with CoOp~, VPT~, MaPLe~ and PromptSRC~. 

We denote the integration of our unsupervised knowledge distillation strategy with a specific prompt learning technique by adding the suffix + to the name of the corresponding technique. For example, CoOp+ refers to the application of  to CoOp. For class agnostic settings we instead use the suffix + to indicate that no training class names are used. 

Unless otherwise stated, all experiments are conducted in the few-shot setting with 16 examples per class randomly sampled from the training set. We set the temperature hyperparameter  in  to . In the class agnostic experiments in which we assume no knowledge of class names in the training set, we set the number of class names selected in each iteration to .

For each prompt learning method we use the original implementation and hyperparameters reported in the respective papers. Since we use the compact ResNet-50 and ViT-B/32 backbones, we cannot directly compare to the originally published results. Thus, all numbers we report here were computed by us and are averages over three independent runs. See Section  in the Supplementary Material for additional implementation details.

 outlines the performance in the domain generalization setting. We report the performance of each baseline method alongside their unsupervised  variants. Additionally, the performance of the zero-shot CLIP student and teacher models using a handcrafted prompt is included for comparison. We observe that applying our unsupervised approach to each baseline does not result in a significant decrease in performance on the source dataset. Notably, when transferring the learned prompts to a different domain, incorporating  in each baseline can lead to a slight improvement. This suggests that our unsupervised teacher-student distillation learns prompts that generalize better than those trained using ground-truth labels. The only exceptions where we observe an average decrease compared to the baseline are when using CoCoOp or PromptSRC. CoCoOp- achieves an average accuracy significantly lower than CoCoOp only on the ImageNet-R dataset. Finally, note that all our unsupervised  variants significantly improve the performance over the zero-shot CLIP student model in both the source and the target datasets.

In  we present the results of the cross-dataset evaluation setting in which the prompt is trained on ImageNet and tested on ten different target datasets. For all datasets our -based variants consistently outperform the corresponding baselines and demonstrate superior generalization performance. The greater generalization capabilities of  are evident even for fine-grained datasets like EuroSAT, on which the CoOp+ achieves an 8\% improvement over the baseline CoOp when using ResNet-50 as the backbone. Although adding  yields only minor improvement to the VPT and MaPLe prompt learning techniques, we emphasize that our VPT+ and MaPLe+ do not have access to ground-truth labels. Notably, PromptSRC+ achieves the highest average performance.

In  we give results for the unseen class generalization task. In these experiments each dataset is split into 50\% of the classes as a base for training few-shot adaptation, and the remaining 50\% as new classes on which zero-shot performance is evaluated.  consistently outperforms the corresponding baseline methods for both backbones, demonstrating improvement in all scenarios for the majority of the datasets. On average, the performance improvement over the supervised baseline methods ranges from about 1\% for VPT to about 3\% for CoOp with the ResNet-50 backbone. See Section  in the Supplementary Material for further analysis of base and unseen performance.

Figure~(a-b) summarizes the main results in the proposed Class Agnostic (CA) scenario in which even the training class names are unknown at training time. We report the accuracy on the ImageNet dataset (source), as well as the average accuracy in domain generalization and cross-dataset settings. Note that, even without knowing the class names, the performance on the source dataset steadily improves compared to the zero-shot CLIP model. Moreover, the prompts learned via the proposed unsupervised class agnostic knowledge distillation also exhibit improved average domain generalization () and cross-dataset capabilities (). Figure~(b) visually depicts how the ResNet-50-based CoOp+ compares with supervised CoOp and zero-shot CLIP student performance in the cross-dataset transfer setting. Notably,  outperforms both baselines despite being unsupervised and class agnostic during training.