In the field of VL learning, many different model architectures have been proposed to meet the requirements of different VL tasks, such as dual-encoder~, fusion-encoder~, encoder-decoder~, . Recently, the rapid advancement of large language models has prompted a growing number of researchers to regard VL tasks as a process of visual-conditioned text generation, and focus on how to involve vision information in off-the-shelf pre-trained language models. For example, BLIP~ and Flamingo~ insert new cross-attention layers into the language models to interact with visual features; Frozen~, LLaVA~, and PaLI~ use the vision encoder to generate visual prompts as the inputs of language models. BLIP-2~ also uses a large Q-former as resampler to reduce the length of visual prompts.% Since arbitrary visio models can be applied in this framework, it is more flexible and can benefit from rapidly developing uni-modal components, large language models. % Since the uni-modal models are well-pretrained, the joint models can be trained with a part of, or even most of, the parameters frozen, which reduces the memory and time consumption during training. %Therefore, such modular approach have been widely adopted in recent VL models. PEFT has already been widely studied in the field of vision~, language~, and multi-modality~. Particularly, based on the pre-trained vision encoders and language models, the VL models can be trained in a parameter-efficient manner. There are many studies focusing on PEFT of such assembled VL models on downstream tasks. VL-Adapter~ and VL-PET~ project the image features as visual prompts, and fine-tune the projector and PEFT modules inserted in the T5 or BART models. Differently, LLaMA-Adapter~ concatenates the visual prompts with the hidden state of LLaMA's intermediate layers. LaVIN~ inserts adapters in both the vision encoder and LLaMA, and introduces a routing mechanism for adapters.

Through PEFT, it becomes possible to train VL models using off-the-shelf uni-modal models with less time and GPU memory. However, it is noteworthy that these studies do not take computation efficiency into account, which is one of the main contributions of our paper.

 discover that the FFN of pre-trained language models is essentially key-value memory which stores factual association. Based on this finding,  locate and edit knowledge in language models by replacing certain rows of the matrices of FFN with the embedding of the object.  edit the located factual knowledge by adding new key-value pairs to FFN.  expand the size of FFN with extra keys and values as a knowledge bank.  replace FFN in language models with differentiable plug-in key-value memory for interpretability. However, current works only focus on pure language models, without exploring the potential of visual information as external factual knowledge.

The standard FFN of language models is composed of two FC layers with non-linear activation in-between. Supposing  is a input token of the FFN, the FFN can be formulated as:

in which  is activation like ReLU and GELU,  and  are the weight matrices of the two FC layers. Note that  and   can be rewritten as:

in which  and  are entries of key and value, respectively. Then, the FFN can be rewritten as

Therefore, the FFN can be interpreted as using input  as the query to calculate its similarity with keys, and gathering values based on the similarity. Previous work has found that FFN acts as a key-value memory storing factual knowledge~.

As illustrated in Figure~, in conventional input-space visual prompting, the image features are projected to the prefix of the input as context for text generation. Since increasing the input length leads to inefficiency, we avoid using extra visual tokens, and thus all the visual information needs to be contained in textual tokens. A solution to incorporating visual information is to let the textual tokens retrieve information from the visual features. Previous works like Flamingo and BLIP perform retrieval via cross-attention layers, which can be formulated as

in which  is a textual token and  is the visual features. However, the cross-attention layer introduces a large amount of new parameters, , , which is far from parameter efficiency and brings considerable additional computation.

Note that the cross-attention  essentially performs a soft look-up using the query  from the key-value pairs  and outputs the weighted average of the retrieved values. Inspired by the fact that FFN also performs similar retrieval from its key-value memory, we consider a more simplified and efficient retrieval process for visual features:

in which  are the key and value corresponding to . This formulation shares a similar form with Eq~(). Since the size of FFN's key-value memory  is usually much larger than the number of visual features  ( in LLaMA-7B and  for ViT-L/14), the computation of retrieving visual features is insignificant. Therefore, we do not introduce new cross-attention layers as in previous work, but perform such retrieval along with FFN instead. %means we can %also perform such retrieval along with FFN.

From the perspective of FFN, we regard the  as new memory entries to complement vision-related knowledge that language models used to lack. The new visual key-value entries are inserted into memory, %The difference between text-only tasks and VL tasks is the participation of visual information which serves as extra knowledge. Since FFN can be regarded as memory storing knowledge as key-value pairs, we aim to inject the vision information into FFN directly.%Let  denote the visual features extracted by vision encoder, which could be the patch features of an image.  As for  and , they should realize two key functions:  aligning the dimension between visual feature  and textual token , and  identifying the position of each entry in the visual input. We use a projector , which could be one or several FC layers, to project the visual features to the dimension of the textual token as a visual prompt. The projector is shared between  and  for parameter efficiency. The projected visual features are then added with position embedding,

in which   is a hyperparameter and  are position embedding for visual prompts inserted into keys and values, respectively.  %can be formulated as %%Since the visual prompt will be inserted in both keys and values, we also apply position embedding to identify  the position of each feature in the visual input; and  whether the visual prompt are concentrate with keys or values. 

To implement Eq~(), the position-embedded visual prompts are inserted into the memory as new key-value entries. For the FFN block, the weight matrices are modified to %Then the FFN can be formulated as%%%&() = \phi( _1) _2 = \sum^D_{i=1}\phi(\langle , _i\rangle)\cdot_i \\&+  \sum^n_{i=1}\phi(\langle , \lambda f(_i) + ^{k}_{i}\rangle)\cdot(\lambda f(_i) + ^{v}_{i})%%%

Since the visual prompts are concatenated with the FFN weights which are actually memories, we call the proposed new paradigm  (MemVP).

Besides the standard FFN above, which is widely used in small and middle-scale language models, large language models usually adopt Gated Linear Units (GLU) to enhance the FFN for better performance. For instance, LLaMA uses SwiGLU in FFN, which is

Supposing , Eq~() can be rewritten as

where  can  be viewed as matching the query with another key.  For FFN using GLU, we simply let the second key entries responding to the visual prompts be , , modify  to

which is equivalent to omitting the second  key when looking up the visual knowledge to avoid involving more parameters, ,

In this paradigm, only the projector and position embedding are newly introduced, which are negligible compared with the large size of the pre-trained models. During fine-tuning, we can freeze the parameters of the vision encoders and language models, and only fine-tune these new parameters. 

From another perspective, the added key and value entries can be regarded as the two fully connected layers of a vision-conditioned adapter for PEFT. Therefore, in practice, we also adopt some design philosophy of adapters~. First, we set the length of position embedding as a hyperparameter to control the number of trainable parameters. We allow the length of position embedding to be longer than the visual prompts, in which case we simply zero-pad the visual prompt to align their lengths. Second, we add another scaling factor to the retrieval results as a hyperparameter to control their magnitude. 

We consider a language model layer that is only composed of MHSA and FFN blocks. For simplicity, we omit the bias terms and normalization layers. Let , , and  denote the length of token sequence, dimension of tokens, and length of visual prompts, respectively. The FLOPs of MHSA and FFN are  and  respectively. We use , , and  to denote the FLOPs of a single transformer layer in the language model without visual prompts,  with input-space visual prompts, and with memory-space visual prompts, respectively. Then we have

For the previous manner which uses input-space visual prompting, the length of the input sequence becomes . Then, the additional FLOPs of a layer are

Whereas for MemVP, the length of the input is unchanged, and only the hidden dimension of FFN is increased. The additional FLOPs of a layer is

Since current VL models basically satisfy , and for VL tasks we have  in the most cases, we find that  is multiple times of , but the difference between  and  can be ignored. For other architectures such as encoder-decoder model, MemVP mainly reduces the FLOPs of the encoder part. Overall, MemVP is computation-efficient for VL tasks on various language model architectures.

For visual question answering, we evaluate our method on VQAv2~ and GQA~; for image captioning, we evaluate on COCO Captions~. All these tasks are regraded as text generation tasks which directly output the answers in an open-ended space. Note that, different from previous work~ using a multi-tasks learning setting where the VQA tasks benefit from the concurrently trained captioning data, we fine-tune MemVP and all the baselines on each dataset individually. We compare MemVP with baselines using previous input-space visual prompting, including current state-of-the-art PEFT methods on BART and T5: VL-Adapter~ and VL-PET~, as well as representative PEFT methods designed for language models: Compacter~ and LoRA~. We also report the results of fully fine-tuning the language models with input-space visual prompting.

 Following previous work~, we use ResNet-101 pre-trained via CLIP~ to pre-extract  image features. The resolution of input images is . The visual encoder is frozen during fine-tuning, and the PEFT modules are only inserted into the language model.  For the language part, we use BART-base~ and T5-base~ with encoder-decoder architecture.  For our MemVP, the grid features before global average pooling are projected to visual prompts via a single FC layer, and the visual prompts are only injected into the FFN blocks of language encoders. Additionally, we also unfreeze the layer normalization of language models. We train on each dataset for 20 epochs with batch size  and report performance on the test set. The hyperparameters of all methods are summarized in Appendix.

As shown in Table~, our MemVP achieves average performance better than current state-of-the-art PEFT method, VL-PET, and much better than other baselines. However, the FLOPs in the language models of MemVP are only -- of other baselines. To exhibit the advantage of shorter inputs, we compare the training speed, training memory, and inference speed of all methods on VQAv2 in Figure~ (left).  Compared with VL-PET, MemVP is  faster during training and  faster during inference, while using only  training memory. Although PEFT only unlocks a small number of parameters for training, the gradient still needs to be propagated back through the whole language model, leading to considerable time and memory consumption. Therefore, the time and memory costs during training and inference are profoundly  affected by the FLOPs in language models.  MemVP releases the time and memory burden for fine-tuning by directly reducing FLOPs, suggesting that computation efficiency is also crucial in designing PEFT methods.

Furthermore, we compared MemVP with a straightforward strategy to reduce the length of the visual prompt: 2D adaptive pooling. As illustrated in Figure~ (right), after pooling the visual prompt, the input-space prompting methods suffer from obvious performance degradation, implying that the fine-grained local information is lost in this process. By contrast, MemVP can use long visual prompts without extending the input, thus outperforming the baselines in terms of efficiency.

We conduct experiments to verify our main motivation, , the visual information can be inserted into memories of language models as external knowledge. If the model acquires the visual knowledge successfully, we are supposed to observe that  the visual knowledge related to the text inputs is retrieved, and   when the model fails to retrieve the correct knowledge under manual distortion, the model should output the corresponding wrong contents.

In Figure~, we visualize the similarity between queries and keys, ,  in Eq~(), of BART-base fine-tuned on VQAv2. We find that the text tokens have a high similarity with the keys of related visual knowledge entries, implying that the corresponding values are retrieved. For instance, when asking the model , the model retrieves knowledge entries around the plain; when asked , the model retrieves knowledge entries of the background. Moreover, we find that different words in the input sentence have different preferences, , when asking the model , the  token retrieves the knowledge entries that contain men, and the  token retrieves the entries on the right side of the image.

Next, we try distorting the knowledge by editing the query-key similarity. As the example in Figure~, when asking the model , the model mainly retrieves the entries containing the black horse. Then, we manually block the retrieval of the two most responsive entries by setting . As a result, the model outputs  since it fails to obtain knowledge about the existence of black horse. Overall, these observations verify that the visual information is actually inserted into memory and direct the outputs of language models.

We use a challenging VQA task, ScienceQA~, to evaluate our method. ScienceQA is a large-scale science question-answering dataset compiled from diverse  knowledge domains. We compare MemVP with other LLaMA-based fine-tuned models with input-space visual prompting, including LLaVA~, LLaMA-Adapter~, and LaVIN~. We also provide results of LLaVA equipped with LoRA. All these methods adopt a one-stage paradigm, , directly generating the answers end-to-end without multi-stage chain-of-thought (CoT) prompting~. We adopt the training recipe used by  and train each method for 20 epochs. All these methods use a ViT-L/14 pre-trained via CLIP as the visual encoder. We also report zero-shot results of GPT4~.% and GPT4V~, and the method using multi-stage CoT: MM-CoT~. Following LLaVA~, MemVP and LLaVA-LoRA use the 256 patch features before the last layer of ViT-L/14 and project them as visual prompts. Differently, LaVIN and LLaMA-adapter stack 6 global features (,  tokens of ViT) selected from different intermediate layers as much shorter visual prompts. The projectors of MemVP, LaVIN, and LLaVA-LoRA are two FC layers with non-linear activation in between.  Since LaVIN also inserts adapters in the visual encoder,  we adopt a comparable strategy on MemVP and LLaVA-LoRA for a fair comparison. Specifically, we introduce parallel adapters to the FFN of the vision encoder following previous work~. Moreover, since LLaMA has much more layers and larger dimension than BART and T5, we also share the position embedding of MemVP across different layers for parameter efficiency. For the samples that do not have image inputs, we simply set  the visual prompts of MemVP to zero tensors, and only insert the position embedding.  % within the choices. In our experiments, we directly regard such cases as , aligning with the evaluation code of LLaVA.}As shown in Table~, %on 7B-scale models, LLaVA-LoRA outperforms LLaMA-Adapter and LaVIN since it takes advantage of the local patch features in the longer visual prompts. our MemVP significantly outperforms all the baseline PEFT methods on both LLaMA-7B and LLaMA-13B. LLaVA-LoRA performs better than LaVIN and LLaMA-Adapter, indicating that VL models benefit from the local visual information in longer visual prompts. Notably, MemVP also beats LLaVA, a fully fine-tuned model with VL pre-training, on average results as well as 7 out of 8 subsets. Besides, we also compare the training and inference speed of different PEFT methods in Table~. In spite of the long visual prompts, MemVP is still  faster than LaVIN during training, since the routing mechanism of LaVIN delays the training speed. LLaVA-LoRA, which also uses local visual prompts in input space, is  and  slower than MemVP in training and inference, respectively. Overall, memory-space prompting exhibits remarkable advantage in computation efficiency.

To demonstrate the effectiveness of the components of MemVP, we conduct comprehensive ablation experiments. As in Table~, when we insert the position embedding without adding visual prompts into the language model, its performance on IMG subset degrades significantly, since the language model cannot obtain the visual knowledge. We note that using global features as in LaVIN leads to a drop in performance due to the loss of local information. We also attempt to concatenate the position embedding with visual prompts instead of adding to them, where the visual prompts will not acquire hard-coded position information but the number of trainable parameters keeps unchanged. The degraded performance indicates the importance of position information for visual prompts since the text inputs may be location-related.  When only inserting visual prompts in keys or values, the model performs worse in both cases. 

%   We list the hyperparameters in Table~. We use task-specific prompt to the input sentence for each downstream task, as shown in Table~.

As mentioned in the main text, since LaVIN also inserts adapters in the visual encoder, we adopt a comparable strategy on MemVP and LLaVA-LoRA for a fair comparison. Specifically, we use two FC layers with hidden dimension of 12 and GELU activation in between as adapters, which are inserted in parallel with the FFNs of ViT, , the adapter use the same input as FFN, and its output is also added to the output of FFN. We list the hyperparameters in Table~ and the input-output format in Table~. %Note that LaVIN also leverages the  lecture and explanation annotations, whose output format is ``The answer is . BECAUSE:  ''. Since its evaluation code only counts whether the   is correct or not, we omit lecture and explanation in our format. Nevertheless, for a fair comparison on training and inference speed, the results in Table~ are measured using our shorter format uniformly. We compare the impact of different output format in Table~.% %\newpage We conduct experiments on video question answering tasks on BART-base, including TVQA~ and How2QA~. We follow the setting used for image-based tasks and fine-tune each dataset under single-task protocol. We use the features of  tokens extract by CLIP-ViT-B/32 from the 64 frames of each video. We train each method for 20 epochs with batch size 64. We report results on validation sets since the test sets are not publicly available. The results shown in Table~ verify the effectiveness of MemVP on video-based tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% We also conduct experiments on visual instruction tuning of large language models. Following LLaVA-1.5~, we perform instruction tuning using MemVP on CLIP-ViT-L/14@336 and Vicuna-v1.5-7B with a two-stage protocol. In the first stage, we pretrain the projector and position embedding on LCS-558K for 1 epoch. In the second multi-task finetuning stage, we find that when the number of updated parameters is fewer than 100M, the model's instruction-following performance is notably poor. Therefore, we also insert LoRA into the language model, and jointly fine-tune the projector, position embedding, and LoRA modules with rank 128 on the 665K instruction tuning data used by LLaVA-1.5. In Table~, we report the performance of MemVP on various benchmarks, as well as the pretraining speed (batch size = 32/GPU) and inference speed on MMB dataset (batch size = 64/GPU).

As the results in the above table, compared to LLaVA-1.5, MemVP is much more faster in training and inference. Although MemVP cannot compete with LLaVA-1.5 on some tasks, which highlights a limitation of our model, it is noteworthy that the pretraining and inference speed of MemVP-7B is comparable to MobileVLM-1.7B~ which is a smaller LLaVA-style model trained on the same data, but MemVP-7B outperforms MobileVLM-1.7B by a large margin.

\icmltitle

% It is OKAY to include author information, even for blind % submissions: the style file will automatically remove it for you % unless you've provided the [accepted] option to the icml2023 % package.

% List of affiliations: The first argument should be a (short) % identifier you will use later to specify author affiliations % Academic affiliations should list Department, University, City, Region, Country % Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order. % Ideally, you should not use this facility. Affiliations will be numbered % in order of appearance and this is the preferred way.

\begin \icmlauthor \icmlauthor \icmlauthor \icmlauthor \icmlauthor \icmlauthor %{sch} %{sch} \end

\icmlaffiliation

\icmlaffiliation \icmlaffiliation

%{Corresponding Author}

%{parsley@pku.edu.cn} %{yehui.tang@huawei.com} %{dingning@stu.pku.edu.cn} \icmlcorrespondingauthor \icmlcorrespondingauthor \icmlcorrespondingauthor

% You may provide any keywords that you % find helpful for describing your paper; these are used to populate % the "keywords" metadata in the PDF but will not be shown in the document \icmlkeywords

\vskip 0.3in

Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models. In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information. Motivated by the finding that Feed-Forward Network (FFN) of language models acts as  ``key-value memory'', we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection. Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the fine-tuned VL models and surpasses the performance of previous PEFT methods. Code: %Particularly, on ScienceQA datasets, MemVP achieves n and 4 speed-up for training and inference, respectively, compared with LLaVA with comparable performance.https://github.com/JieShibo/MemVPIntroductionllama,gpt4,tang2024rethinking,clipfrozen,flamingo,blip2,llavai.e.e.g.vit-gllamae.g.adapterlorae.g.vl-adapter,lavinvqav2scienceqallavakvmeme.g.``Strawberries are red''e.g.``The fruits in  the image are red''i.e.icml-historicalgqacaptionbartt5Related WorkVision-Language Modelscliplxmert,albef,vilt,metervl-t5,simvlm,pali,ofa,blip,blip2,llavaetcblipflamingofrozenllavapaliblip2Parameter-Efficient Fine-Tuning for VL Alignmentadapter-cv,adaptformer,noah,ssf,fact,biadapteradapter,adapterp,lora,bitfit,ptuningv2vl-adapter,vl-pet,lavin,hyperpelt,mixphm,uniadaptervl-adaptervl-petllama-adapterlavinMemory of Language Modelskvmemffn1ffn2ffn3ffn4Revisiting Visual Prompts in VL ModelsllavaTo what extent do the visual prompts affect the computation speed?fig:speedwidth=2.08\columnwidthfigs/overview.pdf-20pt\textbf  Concatenating visual prompts with the text tokens as inputs of the language model is not computation-efficient, \emph, LLaVA, VL-Adapter, VL-PET.  Using cross-attention layers to incorporate the visual information from visual tokens is not parameter-efficient, \emph, Flamingo, BLIP.  Our MemVP injects visual prompts into the FFN blocks of language models, achieving both parameter and computation efficiency.-5ptfig:overviewAre there alternative solutions to use fewer visual tokens?blip2flamingo<image>i)ii)Memory-Space Visual PromptingPreliminary: Reformulation of FFN () = \phi( _1) _2,     _1 = (_1, _2, ..., _D), _2 = (_1, _2, ..., _D)^\intercal,     () = \sum^D_{i=1}\phi(\langle , _i\rangle)\cdot_i.      kvmemFFN with Visual Promptingfig:overview\small() =\left(_q{_k}^\intercal^\intercal}{}\right)_v{_o}^\intercal,i.e.     ()=\sum_{i=1}^n\phi(\langle, (_i)\rangle)\cdot (_i),      eq:ffn \small() =  \sum^D_{i=1}\phi(\langle , _i\rangle)\cdot_i + \sum_{i=1}^n\phi(\langle, (_i)\rangle)\cdot (_i).

i)ii)(_i) = \lambda f(_i) +  ^{k}_{i}, \quad (_i) = \lambda f(_i) +  ^{v}_{i},eq3&\small_1 = (_1, _2, ..., _D, \lambda f(_1) + ^{k}_{1}, ..., \lambda f(_n) +  ^{k}_{n}),\\ &\small_2 = (_1, _2, ..., _D, \lambda f(_1) + ^{v}_{1}, ..., \lambda f(_n) + ^{v}_{n})^\intercal.alignedmemory-space visual prompting() = (( _1) \otimes  _{3}) _2.eq:swiglu () =  \sum^D_{i=1}(\langle , _i\rangle)\cdot\langle , _i\rangle\cdot_i , i.e._{3} = (_1, _2, ..., _D, }{||_2^2}, ..., }{||_2^2}),i.e.

&() = \sum^D_{i=1}(\langle , _i\rangle) \cdot\langle , _i\rangle\cdot_i\\ &  +  \sum^n_{i=1}(\langle , \lambda f(_i) + ^{k}_{i}\rangle) \cdot (\lambda f(_i) + ^{v}_{i}). alignedlavinComplexity Analysis     _ = 4Ld(6d + L)._-_ = 4nd(6d + n + 2L)._-_ = 4ndL.Experimentsvl-adapter,vl-pet,lavini.e.% The results are averaged over three runs with different seeds.}  Results on VQAv2, GQA, and COCO Captions. ``FLOPs'' denotes the average FLOPs \emph on test set. We report average performance over three runs on  split for VQAv2 and COCO Captions, and on  split for GQA. All the baseline results are reproduced using the official code of VL-PET~\cite.0.85\linewidth!

tab:bartwidth=1.8\columnwidthfigs/fig3-4.pdf-10pt\textbf The per-GPU batch sizes for training and inference are 64 and 512, respectively. Measured on V100 GPUs. \textbf The visual prompts of VL-PET are downsampled to reduce the input length.fig:speed2-5ptExperiments on BART \& T5Datasets and Baselines. vqav2gqacaptionvl-adapter,vl-petvl-adaptervl-petcompactorloraImplementation Details. vl-adapter,vl-petclipbartt5Results and Analyses. tab:bartfig:speed2fig:speed2Visualization. i.e.i)ii)fig:loci.e.eq3``What is in the sky?''``What is the color of the sky?''e.g.``What is the man on the far right side wearing?''``man''``right''fig:dis``How many black horses do you see?''``0''Experiments on LLaMA \textbf  Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = w/ text context, IMG = w/ image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12.  denotes our reproduced results. Other results are quoted from their original papers.0.9\linewidth!

 -5pttab:sqaDatasets and Baselines. scienceqallavallama-adapterlavini.e.mm-cotlavingpt4Implementation Details. llavai.e.adaptformerResults and Analyses. tab:sqatab:speedtab:abConclusion \& Limitationi.e.e.g.Impact Statementmain.bibicml2024AppendixExperiment DetailsExperiments on BART \& T5tab:hyper-barttab:promptExperiments on LLaMAi.e.tab:hyper-llamatab:promptSupplementary ExperimentsVideo Question Answeringtvqahow2qatab:video ``FLOPs'' denotes the average FLOPs of language models on validation set.0.6\linewidth!

tab:videoVisual Instruction Tuningllava1.5tab:vit0.99\linewidth!

tab:vitmobilevlm