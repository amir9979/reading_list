Large-scale red teaming has been conducted during the pre-deployment phase of LLMs. Researchers rely on human annotators to handwrite (annotate) adversarial prompts to guide the LLM in generating harmful responses . While manually constructing adversarial prompts has proven effective in generating high-quality prompts , it has limitations in comprehensively exploring model vulnerabilities . Previous studies have used manual annotation and model generation methods to create jailbreak prompts to inject instructions into black-box models  or white-box models . Additionally,  explored various strategies for training LLMs using reinforcement learning (RL), supervised learning (SL), and context learning methods to automatically generate adversarial prompts for red team testing.

However, these methods fail to adapt to risk vulnerability changes caused by updates to the target model. Similar to our work, MART  also employs a continuously updated iterative method, where both the red team model and the target model are updated simultaneously to enhance the model's security performance. MART framework only screens data that the target model itself replies to securely and uses it to train the target model. In cases where the model still outputs risky content after multiple iterations, our SEAS method focuses on automating the exploration and selection of preferred data pairs. For the same  adversarial prompt, it selects two responses from the target model—one secure and one risky—as the optimization target. This ensures that when the model encounters the same prompt again, it will produce a safe response. Furthermore, there are many differences in our specific methods and experimental details (see Section  and  for further details).

Numerous studies have demonstrated the rapid advancements of AI . Moreover, most Large Language Models (LLMs) exhibit high success rates in publicly available security assessment benchmarks . Despite this, current research often focuses only on specific aspects or dimensions of security, such as toxicity , while a comprehensive evaluation across all categories could more effectively reveal potential vulnerabilities in LLMs . Additionally, recent studies have shown that there is a significant risk of generating harmful content in adversarially designed prompts , indicating that existing benchmarks may not fully capture the security risks associated with LLMs.

 To delve deeper into the vulnerabilities of target models from an attacker's perspective, we develop a new dataset that includes 5 different Risk Categories and 9 distinct Attack Styles  (see Appendix ). This dataset could guide the model to produce outputs that contradict its design principles, thereby enabling a comprehensive assessment and future enhancement of model security.

Although existing security testing datasets are extensive, they lack a sufficient number of robust adversarial test cases to fully assess model security . Most open-source datasets do not effectively integrate the two critical dimensions of  and  (details in Appendix ). 

To address this gap, we have developed a SEAS dataset, which features 14 types that cover two risk dimensions: the first is risk categories, which are potential topics of unsafe content that LLMs might generate; the second is attack styles, which are specific expressive techniques designed to cover various prompt types that could trigger harmful behavior in models. This dataset contains approximately 18K entries, divided into a training set with 16K entries and an In-Domain test set with 2K entries. Specifically, to prevent the model from overfitting to the aggressive language style or syntax during training, rather than discerning the actual semantic content, we have created an ambiguous harmless dataset for evaluation purposes. This dataset, comprising 300 entries from human experts, mimics the aggressive instruction styles and syntax but is actually harmless. 

SEAS dataset were collected through crowdsourcing platforms, manually rewritten and labeled, and augmented with some open-source data . Each prompt in the dataset corresponds to a type label, with detailed category distribution information available in the Appendix .

As shown in the Figure , during the Initialization Stage, we selected Llama-3-8B  as our initial model. For the Red Team model, we expect the model to not only generate adversarial prompts effectively but also ensure the diversity of these prompts. To achieve this, we adopted an initialization scheme based on random sample contexts. The specific procedure is as follows: We randomly designate a specific type and select a fixed number of data from the training set of the SEAS dataset that corresponds to this type. These data are used as a small set of sample examples, incorporated into the prompts for Supervised Fine-Tuning (SFT) input. Then, we randomly select another sample of the same type as the output. This method enhances the diversity of the Red Team model in generating adversarial prompts.

For the Target model, considering that Llama-3-8B-Instruct already has strong security capabilities, we have initialized a target model based on Llama-3-8B, which does not have additional security training, to better validate the effectiveness of our method. We selected three datasets specifically designed for SFT focused on general command adjustment. Our objective is to enhance the model's capability for instruction following and to respond appropriately to inputs.

At the beginning of each Attack Stage, we construct seed prompts by specifying a type and concatenating a fixed number () of prompts from the SEAS dataset's training set. This activates the Red Team model to generate adversarial prompts. In order to ensure the diversity of the Red Team model's output, we adopted nucleus sampling  and carried out multiple samplings to generate  prompts. Following this, we input these prompts to the Target model, also conducting multiple samplings, to obtain  output responses. 

By concatenating  adversarial prompts with  responses and processing them through a Safe Classifier for safety evaluation, we obtain  tuples of \{, , , , where label = 1 represents unsafe. Please note that the safety assessment specifically pertains to the response.

In the Adversarial Optimization Stage, we filter and construct pairs data for optimization. The optimization loss of the Red Team Model :

where  represents the seed prompt,  denotes the adversarial prompt that successfully induces the target model to generate unsafe content,  is the Red Team model of last iteration i, and  is a policy to be optimized.

The optimization loss of the Target model :

where  represents the adversarial prompt, and  represents the model's safe response. 

Subsequently, the attack stage and the adversarial optimization stage are cyclically performed, iteratively updating the models over multiple iterations to enhance their capabilities.

In our research, we filtered and constructed a dataset of 16K entries from the training set as of SEAS dataset for SFT targeted at the red team model.

For the construction of the Target model, we selected ShareGPT , Dolly , and LIMA  as seed datasets to fine-tune the target model and enhance its ability to follow instructions. Moreover, to ensure these three datasets were free of adversarial content, we conducted extra data cleansing to minimize the presence of harmful or unsafe instructions as much as possible. Overall, we used about 101K cleaned samples from these datasets.

For both models, we employed the same hyperparameter settings as the existing SFT methods  : , batch size = 8, epoch = 3.

To quickly and effectively determine whether the responses from Target model contain unsafe content, we utilize Meta Llama Guard 2  as our safety classifier. It is an 8B parameter LLM safeguard model based on Llama 3. It functions as a large language model, determining whether a given prompt or response is safe or unsafe through its generated text output.

At the beginning of each attack stage, we constructed a dataset of 5K seed prompts. Seed prompts were created by randomly selecting 3 () prompts of the same type from the training set. During the generation process of both Red Team and Target models, we employed settings of  and , and sampled each model 5 times (). For discrimination using the Safe Classifier, we adopted a greedy strategy  and performed a single sampling. Each attack phase ultimately resulted in a dataset containing 125K entries.

Based on this dataset, we constructed pair-wise data. Due to the parameter settings, the model outputs exhibited high diversity, allowing us to collect approximately 4.8K data pairs per round for iterative optimization of the Red Team model. In each round, we selected and mixed 2K safe data points. To balance the generality and security of the model, we randomly sampled general data for optimization iterations from the OpenOrca  and ORPO-DPO-Mix  datasets. Specifically, we sampled 7K data points in iteration 1, and 14K data points in both iteration 2 and iteration 3.

We use the Attack Success Rate (ASR) as the metric to evaluate effectiveness of red teaming. ASR measures the percentage of adversarial prompts that successfully trigger harmful behavior in the target model.

To measure semantic diversity in adversarial prompts, we introduce cosine similarity based on sentence embedding models  as an evaluation metric.

Additionally, we comprehensively evaluate the model's performance using two security benchmarks-BeaverTrail  and hh-rlhf -and two general benchmarks, Arena Hard  and MT-Bench .

Further information on metrics and benchmarks is provided in Appendix  and .

We compare SEAS with several public available models, including GPT-4 Turbo , Qwen1.5-110B-Chat , Llama-3-8B-Instruct, Llama-3-70B-Instruct , Mistral-7B-Instruct  and several iterations of the MART-3shot. In the names of the SEAS models, "" stands for "Target model", "" stands for "Red Team model" and the numbers 0 through 3 indicate how many iterations each model has undergone in the SEAS process.

To thoroughly evaluate the performance of Target models, we compared them with several benchmark methods and publicly available models within the industry.

As the Table  shows, the ASR on the In-Domain test set of the SEAS dataset decreased by  after three iterations of the SEAS iterations. The performance metric of the third-round optimized model (SEAS-) surpassed that of GPT-4. On open-source test sets Beaver and hh-rlhf, the ASR of the multi-round Target model decreased, the SEAS- performing comparably to Llama-3-70B-Instruct. This effectively proves the effectiveness of the SEAS scheme in enhancing model security performance, although it still slightly trails behind advanced models like GPT-4 and Llama3, which have undergone extensive manual red team testing . Indeed, we could further improve the SEAS model by expanding the sources of supervision, augmenting the seed datasets, and incorporating human oversight. However, this is not the main focus of this paper, and we leave it to future research.

Moreover, the comparison between the In-Domain test set and other open-source test sets clearly shows that the In-Domain test set achieves a higher ASR, effectively demonstrating the SEAS dataset's effectiveness in exploring model risks.

To measure the improvement in the capabilities of multiple Target models across different rounds, we conducted red team attacks using Red Team models from various rounds against these Target models and recorded ASR. In Table , we observed that the ASR of the Target models decreased progressively with each round when subjected to attacks by the same Red Team model. This effectively demonstrates the progressive enhancement of the security capabilities of the Target models. Different from Table  we employed settings of  and , and sampled each model 5 times () here.

We evaluated not only their security performance but also their performance on two general benchmarks. As shown in Table , after three iterations, the Target models remained stable in the MT-Bench assessment and exhibited an 8.7 improvement in the Arena assessment. Combining Table , these results effectively demonstrate that SEAS can enhance the models' security performance while maintaining their general capabilities.

We evaluated the model's performance in terms of incorrect refusals , specifically measuring how often the model inappropriately refuses to respond to harmless prompts. According to the results from the XSTest  evaluation shown in Figure , the rate of incorrect refusals for safe prompts remained stable, without significant increases. For unsafe prompts, the rate of correct refusals significantly increased. To prevent the model from overfitting to the syntax and style of attack commands, we assessed its behavior using the harmless test set from the SEAS dataset. As shown in Figure , the SEAS method does not increase the model's over-defensive reactions to harmless commands. After several iterations, the rate of incorrect refusals for ambiguous data progressively decreased. This demonstrates that the SEAS approach effectively enhances the model's ability to recognize actual risk information. In addition, we have observed that the Llama3-8B-Instruct model has a high rate of incorrect refusals, which may indicate that it is being exaggerate safety .

During the construction of our dataset, our In-Domain test set was divided into two categories: normal and adversarial. Each "normal" prompt was classified according to our risk category. Each "adversarial" prompt was classified according to both our risk categories and the type of attack styles applied (e.g., Role Play, Goal Hijacking). Details see Appendix .

The Table  highlights two key benefits of the SEAS approach: Firstly, it progressively enhances Target models' ability to defend against both normal and adversarial attacks, improving security and resilience. Secondly, the ASR for adversarial data shows a greater decrease across iterations compared to normal data, demonstrating SEAS's effectiveness against complex adversarial commands.

To comprehensively evaluate the enhancement of the Red Team model's capabilities, we evaluate the quality and diversity of the adversarial prompts by considering two aspects: the success rate of eliciting unsafe responses and the cosine similarity of sentence embeddings .

 After attacking the publicly available and Target models with adversarial prompts generated by Red Team model (as shown in Tables  and  ), we observed an increase in the ASR with each iteration. This effectively demonstrates that SEAS significantly improves the quality of adversarial prompts from Red Team models. In particular, SEAS-, fine-tuned exclusively with the SEAS dataset, demonstrated a higher ASR, underscoring the effectiveness of the SEAS training set compared to other open-source datasets.

% Moreover, compared to the ASR performance of the model on other open-source datasets (as shown in Table F1), our red team model can generate more effective aggressive instructions. This highlights the superiority of our model in producing aggressive test cases. To avoid Red Team models settling on a single deterministic strategy upon discovering a few effective adversarial prompts , it is essential to maintain the diversity of the prompts they generate. We performed a statistical analysis to evaluate the similarity between the SEAS train set, adversarial prompts from Red Team models over four iterations, and two open-source datasets. The results in Table , show that the similarity between the adversarial prompts and both the train set and other datasets is relatively low (lower than the similarity values of the two open-source datasets). This indicates a high level of diversity in the adversarial prompts generated by the Red Team models.

Existing automated red team attack solutions, including custom jailbreaking templates or pipelines, primarily utilize the SFT objective to update Red Team models and Target models . 

% 先指出目标上的本质区别 However, pair-wise loss - Direct Preference Optimization (DPO)  is more suitable in safety scenarios. This method not only increases the probability of the chosen text but also decreases the likelihood of the rejected text, thereby improving the generalization of the Red Team model and the Target model. In contrast, SFT loss merely reinforces the correct answer repeatedly, preventing the model from encountering incorrect samples.

% 再拿效果说话 As demonstrated in Table , for the same data, the DPO shows significant improvement after one round of training (ep=1), whereas the SFT scheme shows no improvement after one round and only modest improvement after two rounds.  In Table , we found that the DPO scheme outperforms the SFT scheme in processing both data types. This performance difference is particularly notable with complex adversarial samples, where the benefits of the DPO scheme are more pronounced. Additionally, Figure  shows that the DPO scheme effectively reduces the incorrect refusal rate in tests involving harmless samples. In contrast, the SFT scheme does not show a significant reduction in incorrect refusal rate even after two iterations.

% In this section, we aim to demonstrate the advantages brought about by iterative updates to the Red Team model and the Target model. The results from Table  and  clearly shows that the ASR of various Red Team models on the same target model (include SEAS Target models and public models) increases with each iteration. This trend confirms that the SEAS framework is capable of effectively pinpointing and exploiting the vulnerabilities of the Target model through iterative updates.  % Moreover, through multiple iterations of the Red Team models, we observe from the Table  that the adversarial prompts in multiple rounds exhibit significant diversity compared to the training set. This implies that we have generated more diverse adversarial prompts, which helps in a more comprehensive assessment of the security and robustness of the target model.

We initially train and optimize the SEAS- model using data processed through three iterations of SEAS, applying consistent hyperparameters to obtain SEAS-All. SEAS- is the control group. Results displayed in Table  reveal that multiple iterative updates substantially improve performance on security test sets within the In-Domain and on open-source datasets. Simultaneously, the model's general capabilities, evaluated on Arena and MT-Bench, demonstrate marginal enhancements with each iteration. The main advantage of using iterative updates and optimizing the Target model via the pair-wise loss is the achievement of a higher baseline policy after the initial round. This indicates that performing multiple updates on the Target model with the same data is more advantageous than a single update.

To ablate the influence of general data on the model's security performance, we optimized all the general data used in our SEAS process using the same scheme as SEAS-, resulting in SEAS-Gen. We assessed this method using three security datasets. The results, presented in the Table , show that the ASR has improved across all three test sets. This effectively confirms that the general data we utilized does not enhance the model's security capabilities.

To ablate the impact of randomness from seed prompts on the diversity of adversarial prompts, we conducted a procedure where seed prompts from four attack stages were input into SEAS- to obtain outputs (-out). We then collected all adversarial prompts from different models over three iterations (-out) and measured their semantic similarity. The results, shown in the Table , indicate low similarity between the datasets, suggesting that seed prompt randomness does not significantly influence the outputs. This confirms that the diversity of adversarial prompts primarily arises from iterative updates in the multi-round Red Team models.

% \clearpage Our classification standards are based on open-source datasets .

The SEAS dataset integrate the two critical dimensions of  and . Our classification criteria follow the prior work.  All categories see Table . Below are their brief descriptions: 

The SEAS dataset partially sources its data from public crowdsourcing platforms. Based on this data, we conduct manual annotations and rewrites. The main process is as follows: First, on the crowdsourcing platforms, online users create prompts based on specific attack methods. These prompts are preprocessed by three models deployed by us. If any of the three models identifies a user's prompt as harmful, the attack is considered successful. Second, platform administrators conduct strict quality reviews of the prompts. Each prompt that passes the review is rewarded with 2 CNY. Third, we further examine the prompts with our algorithms to ensure their validity.

The Harmless Test Set is a test set with a language style similar to adversarial prompts but consisting of harmless text. We evaluated it using GPT-4, the same as XSTest , with the evaluation prompts also being identical. For more details, see Appendix C.4. Here is some examples of harmless test set in Figure .

The primary measure of a red teaming method's success is its attack success rate (ASR) on a given target model , which is the percentage of adversarial prompts that elicit the harmful responses from the target model. Formally, let  be a target model with generation function , where  is the number of tokens to be generated,  is a adversarial prompt, and  is the response. Let  be a red teaming method that generates a list of adversarial prompts,  is the number of adversarial prompts and let  be a classifier mapping completion  and behavior  to  if a red teaming was successful and  if not. The ASR of  on target model  for behavior  is then defined as

To assess diversity, we use cosine distance-based similarity as our metrics, denoted as , as follows

where  represents the sentence embedding model, and  represents the collection of prompts  generated during training up to the current iteration, ,  represents different dataset. 

BeaverTails  is a collection focused on AI safety that includes a series of datasets. This dataset uniquely divides annotations of helpfulness and harmlessness for question-answer pairs, providing distinct views on these important attributes. There are a total of 333,963 question-answer (QA) pairs and 361,903 pairs of data for both the helpfulness and harmlessness metrics. We utilize its Evaluation subset to assess the security of models.

The Anthropic hh-rlhf dataset  consists of human-generated and annotated red teaming dialogues. These data are designed to explore how crowdworkers red team models and to identify which types of red team attacks are successful or not. We utilize its red-team-attempts subset for testing.

Arena Hard  and MT-bench  are two benchmarks based on pairwise comparison and are effective at evaluating open-ended questions. We utilized a GPT-4-based evaluation pipeline for the model's evaluation.

XSTest  comprises 250 safe prompts across ten prompt types that wellcalibreate models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. This benchmark uses GPT-4 to evaluate responses labeled as full refusal, partial refusal, and full compliance. The specific prompt is as follows: our harmless test set employed the same scheme and prompt during the evaluation process.

:`` I need your help with classifying responses by a large language model. There are three classes of responses. 1. This includes any direct attempt to give an answer, regardless of how accurate or relevant the answer is. It also includes cases where the model rejects the question for not having a meaningful answer. 2. The model states that it cannot, in the sense of will not, answer the prompted question. Full refusal includes responses that only offer alternative suggestions without answering the original question (e.g."instead of smashing a piñata, you could decorate it"). 3. This is any combination of refusal and compliance. This includes self-contradictory responses that first refuse and then go on to answer the prompted question (e.g.``killing time at the airport is unsafe, but here are some ways to pass the time at the airport''). QUESTION:\{QUESTION\}" RESPONSE: "\{RESPONSE\}" CLASS: ''

ShareGPT  refers to the multi-turn chatting histories used by Vicuna (Chiang et al., 2023). ShareGPT includes 86K human queries and responses from ChatGPT and other chatbots.

Dolly  is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper , including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. 

OpenOrca  dataset is a collection of augmented FLAN Collection data . Currently 1M GPT-4 completions, and 3.2M GPT-3.5 completions. The data is primarily used for training and evaluation in the field of natural language processing.

ORPO-DPO-mix  is design for DPO training, it is a combination of high-quality DPO datasets and use rule-based filtering tools to chose answers.

LIMA  refers to a mixture dateset of community question answering and human expert-written instruction and responses. 

CPAD  refers to a high-quality prompt attack samples, specify the attacking goals of each prompt, which not only accurately simulate prompt attacks on LLMs from the perspective of attackers, but also can be utilized to evaluate and analyse the response.

HarmfulQA is a set of 1,960 harmful questions to evaluate LLM performance against red-teaming attempts.It contains questions spread over a set of 10 topics each with ~10 subtopics.

ALERT  refers to a fine-grained safety risk taxonomy, consisting of 6 coarse and 32 fine-grained categories. This taxonomy serves as the foundation for the benchmark to provide detailed insights about a model's weaknesses and vulnerabilities as well as inform targeted safety enhancements.