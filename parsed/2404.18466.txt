We employ the general abilities and basic knowledge benchmarks mentioned in Section  to evaluate various models under the instruction tuning settings. To assess the conversation ability, we also compare these models on AlpacaEval 2.0 (see also Appendix~).

% Table~ displays the general abilities performance of SFT and HPA, along with their performance through the application of HFT.% in the evaluation of general abilities, employing  Results in Table~ demonstrate the effectiveness of our proposed HFT method, which simultaneously improves different specialized abilities by selectively fine-tuning half of the parameters.  Specifically, compared to FFT under the SFT setting, HFT leads to an average performance improvement of 1.9\% on  and 2.9\% when scaling to .  Furthermore, as we continue to perform DPO on SFT models, we observe that updating the policy model with HFT does not hinder the model from learning human preferences.

Besides, we also attempt to review the~ method in Section~, but the benefits of this approach are not robust, and we attribute it to the randomness of parameter operations.  In comparison, HFT achieves a more stable performance improvement through the learning process, while avoiding the complexity of the two-stage process of fully updating followed by partially resetting.

% hardly compromises general abilities whether in 7b or 13b models. % Furthermore, we perform HPA on the SFT model. We observe that updating the policy model using the HFT method hardly compromises general abilities whether in 7b or 13b models. In summary, using HFT for fine-tuning or alignment does not decrease the general abilities of LLMs and may even lead to some improvement. We believe that for LLMs, updating only a portion of the parameters is sufficient to adapt to the requirements of instructions or preferences.%  % Furthermore, we also compare the Half-Reset method mentioned in Section~. Compared to roll-back models, directly fine-tuning half of the parameters achieves better performance in the average of all benchmarks while avoiding the complexity of the two-stage process of full fine-tuning followed by resetting operation.% In the case of SFT, there is an average decrease of 8.2\% and 10.8\%, while HPA shows an average decrease of 12.2\% and 13.4\% under  and , respectively.   For the basic knowledge benchmarks, as depicted in Table~, both SFT and DPO exhibit a significant decline across all three benchmarks.  In contrast, our proposed HFT method consistently outperforms both fine-tuning and alignment processes in the evaluation of basic knowledge. 

For example, in the SFT stage,  HFT achieves improvements of 3.4\% and 2.9\% with  and  compared to FFT, respectively. Besides, it is also worth mentioning that the Half-Reset method shows a relatively stable performance in alleviating knowledge forgetting.

.

% In conclusion, HFT can effectively utilize the parameters from the original pre-trained model to retain a certain amount of basic knowledge and some other pre-trained capabilities. Additionally, leveraging the parameter sparsity commonly found in LLMs~, we can align human instructions and preferences well by utilizing only half of the parameters.%%%%%%%%instruction tuning and HPA world knowledge%%%%%%%%%%  represents half-reset method and  represents our HFT method.% except for the LoRA-based method.% (1) Seq: It is the standard sequentially training the complete set of parameters on downstream tasks. (2) GEM~: This approach leverages episode memories to avoid forgetting. However, like other regularization-based methods, it may consume extra computation time or graphics memory. (3) Replay: We integrate alignment data from LIMA~ into the replay memory and selectively replay 10\% of the historical instruction tuning data. (4) LoraSeqIT: Only the low-rank LoRA matrices are adjusted sequentially while keeping the backbone of the LLMs fixed~. However, the LoRA-based method needs to modify the LLM architecture.%  % Following~, we use Overall Performance (OP) and Backward Transfer (BWT) scores to evaluate the continual learning performance. For more detailed information about evaluation metrics, please refer to Apendix~. To verify the retention of model capabilities after multiple rounds of alignment, we also evaluate the general abilities and basic knowledge of LLMs after the final round of fine-tuning (see Appendix~ for detailed results).  We assess the performance in the continual learning scenario (with TRACE~), using four representative approaches and attempt to replace FFT with HFT. (1) :~It is a standard for sequentially learning all parameters of downstream tasks.  (2) ~: It leverages episode memories to avoid forgetting, but it consumes extra computation time like other regularization-based methods.  (3) : It is a common continual learning strategy, here we integrate alignment data from LIMA~ into the replay memory and replaying 10\% of historical data.  (4) ~: It sequentially updates the low-rank matrices while keeping the backbone fixed. Note that the LoRA-based method modifies the model architecture and is not suitable for combination with HFT. Following~, we start with , adopt  and  as the evaluation metrics (Appendix~ details the calculation process).  Besides, we also report the general abilities and basic knowledge of various models after the final round of learning (see Appendix~).

%%%%%%%% trace exp %%%%%%%%%% As shown in Table~, the three approaches for full parameter fine-tuning, enhanced with HFT, consistently exhibit improvements in both the OP and BWT metrics.% Specifically, our HFT method demonstrates performance improvements of 5.7\% and 2.0\% on the OP metric in the SeqIT, and GEM settings, respectively. Furthermore, HFT also showcases performance enhancements of 4.6\%, 0.7\%, and 2.0\% on the BWT metric under the . Moreover, our approach achieves superior performance when scaling the model to 13b. In particular, fine-tuning with full parameters suffers from severe catastrophic forgetting in the -th round (detailed results see Appendix~), while HFT does not experience significant forgetting problems in any of the rounds, making the continuous fine-tuning process more stable. Notably, both the standard fine-tuning and the two different continual learning methods exhibit substantial improvements when equipped with HFT, further emphasizing the plug-and-play nature of our approach. In contrast, LoraSeqIT, which also fine-tunes only a subset of layers (additional layers), exhibits notably suboptimal performance on TRACE. We hypothesize that this discrepancy may be attributed to LoRA's implementation of low-rank mapping, which results in considerable degradation of knowledge during the process of sequentially fine-tuning.

Table~ shows that the . Specifically, HFT brings performance improvements of 5.7\% and 2.0\% on the OP metric in the SeqFT and GEM settings, respectively. It also boosts the performance with 4.6\%, 0.7\%, and 2.0\% on the BWT metric based on the .  When scaling the model to 13b, HFT could also achieve superior performances.  Further, fine-tuning with full parameters often suffers from severe catastrophic forgetting in the -th round (see Appendix~), while HFT does not experience such a problem in any of the rounds, making the learning process more stable. Besides, LoraSeqFT exhibits notably suboptimal performance in this setting. We assume that the knowledge capacity of LoRA parameter is quite limited, thus resulting in considerable forgetting during the process of sequential training. On the contrary, HFT is based on a full set of parameters and selects half of the parameters to be fine-tuned in each round, which has stronger knowledge tolerance.

HFT heuristically selects parameters to be tuned or frozen. We hope to reveal the impact of parameter selection from parameter radio and selection strategy, to discuss the universality of the methodology.

% We construct models with varying numbers of trainable parameters by selecting different proportions of layers. We evaluate the impact of parameter quantity on model performance in both single-round supervised fine-tuning and multi-round continual learning scenarios on ~V2 and TRACE datasets, respectively. Specifically, as shown in Figure~, we find that LLMs achieve better performance when the number of parameters is around 50\% for continual learning setting. We believe that this is because our fine-grained selection approach maintains the overall uniformity of the model when selecting half of the layers. When selecting half of the layers during the continual learning process, LLMs achieve a better balance of capability proportions between each round of tasks, resulting in a more flexible training procedure and optimal performance. Additionally, in the single-round SFT setting, LLMs do not always seem to perform the best at the 50\% position. We believe that in single-round fine-tuning scenarios, half parameters may not achieve the optimal balance between learning new tasks and preserving old capabilities. However, due to the unique uniformity brought by our fine-grained selection method combined with half of the parameters, our approach consistently improves performance without the need for finely selecting parameter proportions. It is worth noting that in the evaluation of TRACE, LLMs with different parameter ratios generally outperform FFT. This observation further supports our conclusion that maintaining a portion of unchanged parameters can retain some of the original capabilities. In continual learning, it is necessary to freeze a portion of the parameters in each round to preserve the capabilities of the previous model. Similarly, in the case of single-round SFT, selecting an appropriate proportion of parameters consistently outperforms FFT.

Firstly, we traverse the radio of parameters to be fine-tuned at a granularity of 10\% and evaluate the impact in both single-round and multi-round fine-tuning scenarios.

In SFT, the performance of basic knowledge shows a clear downward trend with the increase of parameter ratio, while the general abilities slowly rise, which allows updating half or less of the parameters to have good performance. Meanwhile, when selecting half of the parameters during continual learning, the model reaches a balance of abilities between each round of tasks, resulting in a more robust training procedure and optimal performance.  This observation again confirms the early conjecture about catastrophic forgetting, especially in continual learning, it is necessary to freeze a portion of parameters in each round to preserve the capabilities of the previous models. Not only that, we also find that fixing partial parameters gradually improves training efficiency (see Table~), and HFT could shorten the training time by 30\% in standard SFT.

%%%%%%%% trace exp %%%%%%%%%% We compare three different methods for selecting half layers:% (1) : We randomly select layers by mixing all layer matrices. This selection method cannot guarantee an accurate 50\% selection of parameters due to the varying parameter quantities across different layer categories.% (2) : We select every other layer for training. In the process of continuous fine-tuning, each round selects the layers that were not updated in the previous round for training.% (3) : The fine-grained layer selection method used in this paper ensures the accurate selection of 50\% of the parameters while maximizing the balance and uniformity between updating and non-updating layers.% The experimental results (detailed results see Appendix~) demonstrate that compared to the other two coarser-grained selection methods, our fine-grained selection method proposes the best performance in both OP and BWT. We believe that this is because the fine-grained selection method minimizes the disruption to the architecture of the model during fine-tuning. Moreover, our selection method allows both the updated and unchanged layers to be evenly distributed throughout the model, achieving an optimal balance between preserving old capabilities and learning new tasks.% We find that all three selection methods are stronger than the FFT. It further emphasises the conclusion mentioned in Section~. Maintaining some unchanged parameters during the continuous fine-tuning process yields relatively better results.Next, we consider other possible strategies for selecting half of the parameters: (1) . It arbitrarily chooses half the number of parameter matrices, which may prevent the parameter ratio from accurately reaching 50\%. (2) . It selects all parameters of a layer every other layer. (3) . It selects based on parameter categories, which is the default strategy used in this paper, and ensures the accurate selection of 50\% of the parameters. Table~ reports the results of performing HFT on TRACE with sequential fine-tuning (SeqFT). , which once again confirms the motivation that freezing some parameters helps balance the old and new abilities in continual fine-tuning. , we attribute it to the fine-grained strategy that maximizes the interaction between updated and non-updated parameters. From the perspective of model merging, it minimizes the damage to ready-made capabilities when performing a 50\% dropout on the task vector, thereby providing greater possibilities for learning new tasks based on existing knowledge.

To validate the performance of supervised fine-tuning, we choose ~ which is a combination of high-quality open resources, including datasets (1) created by researchers from existing NLP datasets (e.g. SuperNI~), (2) written by humans (e.g. Dolly~ and Open Assistant~), (3) generated by LLMs (e.g. Self-Instruct~, Alpaca~ and Baize~), (4) comprised of user-shared prompts accompanied by model-generated completions (e.g. ShareGPT~), and (5) developed for specific abilities (e.g. CoT~ for chain-of-thought and Code-Alpaca~ for code generation).  To examine the capacity for reinstating a fraction of impaired capabilities while adhering to human preferences, we utilize ~ which is a large-scale, high-quality, and diversified preference dataset. For continual learning, we select ~, a novel benchmark designed for continual learning (CL) in LLMs, to evaluate catastrophic forgetting in standard CL settings. TRACE consists of 8 distinct datasets spanning challenging tasks, domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning.   

 To validate the effectiveness of our method, we employ general abilities and basic knowledge benchmarks to assess the performance in learning new tasks and preserving the original capabilities, respectively. Specifically, for the general abilities benchmarks, we include the following evaluation sets to test various abilities. (1) : To assess the LLMs' factual knowledge, we employ the Massive Multitask Language Understanding dataset ()~. MMLU comprises a collection of questions across 57 subjects from elementary to professional difficulty levels. We report the 5-shot accuracy based on answer perplexity. (2) : We utilize the test split of the Grade School Math () dataset~ and Big-Bench-Hard ()~ to evaluate the reasoning abilities. We report the 8-shot accuracy and the exact match (EM) rates for GSM8K and BBH, respectively. (3) : To evaluate multilingual capabilities, we employ ~, a multilingual question-answering benchmark that covers 11 typologically diverse languages. We adopt the gold-passage setup, where a passage containing the reference answer is provided, and report the F1 score. (4) : To evaluate the LLMs' ability to generate functionally correct programs from docstrings, we utilize the  dataset~ and report the pass@10 performance. (5) : We incorporate ~ to assess the ability to avoid generating known falsehoods resulting from misconceptions or false beliefs while providing informative responses. (6) : We use ~ to evaluate the instruction-following abilities. AlpacaEval is an LLM-based automatic evaluation metric. In this paper, we calculate the win rates against the . We include the following three datasets for basic knowledge benchmarks to validate the basic knowledge preserved in LLMs: (1) ~, (2) ~, and (3) ~ For continual learning evaluations, following~, we use Overall Performance (OP) and Backward Transfer (BWT) scores as the main metrics in CL settings. In terms of the formula, after incrementally learning the -th task,  the performance on the -th task (where ) is denoted as . The OP and BWT scores can be calculated as

We utilize accuracy as the primary evaluation metric for C-STANCE, FOMC, ScienceQA, NumGLUE-cm, and NumGLUE-ds. In the case of Py150, we employ similarity as the evaluation metric. Moreover, for the evaluation of MeetingBank and 20Minuten, we employ the ROUGE-L metric.

Following~, in the SFT phase on ~V2, we adopt a linear-decreasing learning rate of 2e-5 with a 0.3 warmup ratio and train for 2 epochs.  For the human preference alignment phase on UltraFeedback, we use direct preference optimization~ to align the fine-tuned LLMs on ~V2. We use a learning rate of 5e-7 and a global batch size of 32. Due to the context length of 4096 used during  pre-training, as referenced in the~ code repository issues, we set a maximum sequence length of 4096 during the SFT stage. However, due to hardware resource limitations, the maximum sequence length is reduced to 1024 during the DPO stage under . During the continual learning phase, following~, we employ a fixed learning rate of 1e-5 and fine-tune the eight sub-datasets for different numbers of epochs: 5, 3, 7, 5, 3, 5, 5, and 7 epochs, respectively. The global batch size for both stages is set to 128. All our experiments are conducted on one machine equipped with 8x80G Nvidia A100. Algorithm~ introduce the detailed implementations of our proposed fine-grained selecting approach of HFT. Additionally, to evaluate the SFT and DPO models, we employ a chat format, using specialized tokens <|user|> and <|assistant|> to mark user utterances and target assistant responses, respectively. However, for HumanEval and the basic knowledge benchmarks, we use a standard language format when evaluating pre-trained models.

Table~ presents the detailed results of the experiments conducted in Section~. We compare two additional parameter selection methods, Random and Alternate. We apply these methods to the reset process. The results indicate that the category-level selection approach achieves the highest average performance, which is consistent with the conclusion in follow-up training settings.  % Therefore, in subsequent experiments, we adopt the fine-grained selection layers method using Half-Reset. As shown in Table~, we evaluate different models on AlpacaEval 2.0. The results indicate that our method is less effective than FFT on . However, a reversal occurs when the model size scales up to 13b, where our approach outperforms the FFT models comprehensively. This suggests that our method has greater potential on much larger-scale LLMs, as supported by the experimental results in Table~, which show a larger improvement of HFT compared to FFT on  compared to . Interestingly, the Half-Reset method performs well on  but shows completely different results on . This suggests that simply resetting half of the parameters may not provide consistent performance since the model is trained on the full set of parameters.

%%%%%%%%instruction tuning and dpo common%%%%%%%%% In Section~, we initialize our DPO process with the FFT model. In this section, we investigate the performance of the DPO process when initialized with the HFT model. The experimental results are presented in Table~. We observe that while the DPO process on the HFT model performs better in certain general abilities, such as TruthfulQA, it experiences minor losses in overall performance under . However, the situation is reversed in , where the DPO deployed on the HFT model outperforms the FFT-initialized DPO. % We analyse the DPO stage consists of two components: the reward model and the policy model, with a different loss function compared to the SFT stage. This may result in irregular performance fluctuations in DPO, regardless of whether it is deployed on FFT or HFT models.  Nonetheless, DPO equipped with HFT tends to consistently improve performance compared to DPO with FFT.

We also evaluate the models mentioned in Section~ on general abilities and basic knowledge benchmarks. The experimental results are presented in Table~. We observe that after 8 rounds of fine-tuning on consecutive tasks, the models fine-tuned with the HFT method consistently outperform the FFT models in terms of average performance. This further confirms the effectiveness of HFT in preserving the original capabilities of the model and mitigating catastrophic forgetting. Furthermore, although LoRA preserves more layer parameters unchanged, it still performs worse compared to HFT. We believe this may be attributed to the low-rank decomposition resulting in a limited number of trainable parameters. Merging the LoRA weights back into the original model could potentially disrupt the original parameter space to a greater extent.

We conduct a comparison of the runtime costs for different ratios of trainable parameters. Specifically, we fine-tuned  on ~V2 and record the total duration from the start to the end of the training program. The results in Table~ demonstrate that, without specific optimization, all models with varying ratios of trainable parameters can reduce the training time. As expected, as the proportion of trainable parameters increases, the training duration also increases. Notably, our HFT method achieves a 31.5\% reduction in training time, significantly decreasing the training cost for extremely large-scale instruction datasets.

Table~ details the results of freezing the input and output layers.  Meanwhile, Table~ and~ show the detailed results of the two adjacent numbers of parameter settings on TRACE. % (i.e.,  and ) Table~ and~ provide the detailed results of model-level and layer-level parameter selection strategies mentioned in Section~.

Table~ to~ show the detailed results of different models and approaches of each round during the continual learning on TRACE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \clearpage% % % \item {\bf Claims}%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: In Abstract and Section~, our main claims accurately reflect the paper's contributions and scope.%     \item[] Guidelines:%     %         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. %         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. %         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. %     % \item {\bf Limitations}%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. %         \item The authors are encouraged to create a separate "Limitations" section in their paper.%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.%     % \item {\bf Theory Assumptions and Proofs}%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include theoretical results. %         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. %         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. %     %     \item {\bf Experimental Result Reproducibility}%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. %         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example%         %             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.%         %     % \item {\bf Open access to data and code}%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that paper does not include experiments requiring code.%         \item Please see the NeurIPS code and data submission guidelines () for more details.%         \item While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines () for more details.%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.%     % \item {\bf Experimental Setting/Details}%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.%         \item The full details can be provided either with the code, in appendix, or as supplemental material.%     % \item {\bf Experiment Statistical Significance}%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)%         \item The assumptions made should be given (e.g., Normally distributed errors).%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.%     % \item {\bf Experiments Compute Resources}%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. %         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). %     % \item {\bf Code Of Ethics}%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).%     % \item {\bf Broader Impacts}%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that there is no societal impact of the work performed.%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).%     % \item {\bf Safeguards}%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper poses no such risks.%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. %         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.%     % \item {\bf Licenses for existing assets}%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not use existing assets.%         \item The authors should cite the original paper that produced the code package or dataset.%         \item The authors should state which version of the asset is used and, if possible, include a URL.%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,  has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.%     % \item {\bf New Assets}%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not release new assets.%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. %         \item The paper should discuss whether and how consent was obtained from people whose asset is used.%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.%     % \item {\bf Crowdsourcing and Research with Human Subjects}%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? %     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. %         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. %     % \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. %         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. %         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.%     %  Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences.  However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge. % Based on the observation, Inspired by this,  we introduce alf ine-uning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge. We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term. Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30\% reduction in training time.

% We show the effectiveness of HFT from the perspective of optimization and conduct extensive experiments and analysis on three different scenarios and perform comprehensive evaluations. The results demonstrate that our proposed HFT method, compared to Full Fine-Tuning (FFT), not only exhibits optimal performance in downstream abilities but also alleviates forgetting problems. , HFT does not introduce extra modules and can be seamlessly integrated into any existing method. % Compared to FFT, HFT can reduce training time by approximately 30\% and significantly decrease the training cost for large-scale instruction data.HFTIntroductionsec::introductionouyang2022training,achiam2023gpt,touvron2023llamacatastrophic forgettinglin2024the,neeman2023disentqa,dong2024abilitiesdou2023loramoe,wu2024llamazhang2023instructionilharco2023editingyadav2023ties,yu2023languageis it possible that a portion of old parameters could maintain the capabilities of the pre-trained model?footnote0Llama 2-7bLlama 2-Chat-7bfig::pilot_experimentLlama 2-Chat-7bLlama 2-7bdou2023loramoehalf of the parametersHere, we keep the \texttt and \texttt layers unchanged as \textsc, and select 50\% of the parameters in \texttt layers. The parameter ratios in this paper all follow this statistical calibre.Llama 2-Chat-7bHalf-ResetLlama 2-7bLlama 2-Chat-7bsec::pilot_experimentHFTT\"ulucui2023ultrafeedbackwang2023trace% \item Through the restoration of specific layer parameters to their original values, we have discovered the ability to partially regain the original capabilities while simultaneously preserving a satisfactory level of performance in downstream tasks.% \item  We propose Half Fine-Tuning (HFT), which entails freezing half of the layers while training the other half. This approach allows LLMs to acquire new capabilities while retaining previously learned knowledge in different training scenarios.% \item Extensive experiments and analysis demonstrate the effectiveness of HFT. Our approach does not necessitate any alterations to the model's architecture, rendering it a solution that can be efficiently used in any neural-based models within a few additional codes.We reveal that by resetting half of the fine-tuned parameters to the startup state, it is possible to preliminary restore the primeval ability while maintaining new learning ability, which poses new opportunities to alleviate catastrophic forgetting and obtain an all-around LLM. We propose Half Fine-Tuning (HFT), which entails freezing half of the parameters while training the other half. It allows LLMs to acquire new abilities while retaining and utilizing previously learned knowledge in various training settings. Extensive experiments and analysis demonstrate the effectiveness and efficiency of HFT. Without any alterations to the model architecture, HFT, as a plug-and-play solution with only a few lines of code, exhibits the potential to supersede FFT in the era of LLMs. Pilot Experimentssec::pilot_experimentyadav2023ties,yu2023languageLlama 2-chat-7bLlama 2-7bSetup.Llama 2-chat-7bLlama 2-7bLlamaHalf-Resetivison2023camelsdou2023loramoeGeneral AbilitiesBasic Knowledgesec::datasetssec::evaluationResults.fig::pilot_experimentLlama 2-chat-7bLlama 2-7bsec::detailed_pilot_expTake another step forward, these findings open a new door for model merging, inspiring us to preserve some mastered abilities of the startup point by freezing partial parameters during fine-tuningMethodology \small     \left(\theta\right) = \max \limits_{\theta}\sum\limits_{t \in \{1, ||\}}{\sum\limits_{(x_n^t,y_n^t) \in ^{t} }{_{\theta^t}\left(y_n^t | x_n^t  \right)}}},

Half Fine-Tuning.fig::modeleq::task \small

    \left(\theta\right) &= \max \limits_{\theta}\sum\limits_{t \in \{1, ||\}}{\sum\limits_{(x_n^t,y_n^t) \in ^{t} }{_{\{\vartheta^{t}, \psi^{t}\}}\left(y_n^t | x_n^t  \right)}}}, \\     % \left(\theta\right) &= \limits_{\theta_{c}}\sum\limits_{t \in }{\sum\limits_{(x_n,y_n) \in ^{t}}{_{\theta_{t,u},\theta_{t,c}}\left(y_n^t | x_n^t  \right)}}} \\     s.t. & \quad \vartheta^{t} \leftarrow \vartheta^{t-1} - \eta \nabla_{\vartheta}  \left(\theta^{t-1}\right)~, \quad     \psi^{t} \leftarrow \psi^{t-1}~,

alignedWhy Half Fine-Tuning Works.fu2022effectiveness   \small    = \min_\theta (\theta) \quad s.t.\ \ \|(I-M)(\theta-\theta^0)\|^2=0,   

  \small   _{L}=\min_\theta \max_\lambda (\theta)+\lambda\|(I-M)(\theta-\theta^0)\|^2, radiya2020fine,fu2022effectivenessExperimentsT\"uluivison2023camelscui2023ultrafeedbackwang2023tracerafailov2023directivison2023camelswang2023traceLlama 2Llama 2-chatsec::experimental_setupExperiments on Instruction Tuningsec::instructionSetup.sec::pilot_experimentsec::alpaca_evalResults on Improving General Abilities.tab::IT_HPA_generalLlama 2-7bLlama 2-13bIn sum, the HFT method has strong robustness to adapt to different fine-tuning algorithms.Half-Resetsec::pilot_experimentResults on Preserving Basic Knowledge.tab::IT_HPA_world_knowledgeNotably, HFT demonstrates excellent talent in preserving basic knowledge, consistently outperforming fully updating parameters during SFT and DPO.Llama 2-7bLlama 2-13b\textbf HFT not only effectively preserves a certain degree of basic knowledge of the pre-trained model, but also utilizes this knowledge to achieve better learning of new abilitiesExperiments on Continual Learningsec::continualSetup.wang2023traceSeqFTGEMlopez2017gradientReplayzhou2023limaLoraSeqFThu2022lorawang2023traceLlama 2-chat-7b/13bOverall Performance (OP)Backward Transfer (BWT)sec::evaluationsec::common% font size% column space% line space -4mmtableOP and BWT on TRACE with different strategies, OP measures the learning of new tasks and BWT measures the forgetting of old tasks.tab::trace_main8.5pt\baselineskip4.5pt0.9Results.tab::trace_mainthree FFT approaches could all benefit from equipping HFTLlama 2-chat-7bsec::detail_trace\textbf HFT is naturally suitable for scenarios with continual fine-tuning, and (almost all) methods with FFT can be further improved by assembling HFT, highlighting the plug-and-play feature.Impact of Parameter SelectionImpact of Trainable Parameter Ratio.sec::parameter_quantityFrom Figure~\ref, we observe that most of the results with only updating partial parameters are superior to FFT, and the performance is quite satisfactory when the trainable parameter radio is around 50\%.tab::efficiency% font size% column space% line space -4mmtableDifferent strategies for selecting half of the parameters on TRACE.tab::trace_select8.5pt\baselineskip4.5pt0.9Impact of Selection Strategy.sec::selectingModel-levelLayer-levelCategory-leveltab::trace_selectThe first noteworthy phenomenon is that all three selection strategies outperform the standard FFTMoreover, the category-level selection wins the best performance\textbf HFT is robust and insensitive to parameter selection, and selecting approximately 50\% of the parameters with a reasonable selection strategy could achieve acceptable improvements.Discussionembeddinglm\_headRevisit the \texttt and \texttt Layers.% font size% column space% line space -4.5mmtableOP and BWT scores of HFT models fine-tuned on TRACE without \texttt and \texttt layers.tab::revisit8.5pt\baselineskip4.5pt0.9embeddinglm\_headembeddinglm\_headtransformertab::tulu_revisittab::revisitsec::detail_revisitTo this extent, a preliminary conjecture emerges that theembeddingandlm\_headstore information are highly relevant to world knowledge, so it is crucial to update them during the fine-tuning process.Parameters Variation Analysis.Llama 2-Chat-7bfig::parameter_variationTherefore, excessive offset of task vectors may not necessarily lead to an improvement in downstream performance, but result in forgetting existing capabilities. HFT seeks subtle balance by pulling back the task vector, alleviating catastrophic forgetting when learning subsequent tasks.Related WorkSparse Fine-tuning.fu2022effectiveness,ding2023parameter,han2024parameterhoulsby2019parameter,mahabadi2021parameter,zhang2023adaptivehu2022lora,dou2023loramoe,dettmers2024qloraguo2021parameterzaken2021bitfitilharco2023editing,xiao2023lm,yu2023languageContinual Learning.luo2023empirical,wang2024comprehensiverolnick2019experience,peng2024scalablekirkpatrick2017overcoming,lopez2017gradientli2019learn,gurbuz2022nisparazdaibiedina2023progressivedou2023loramoe,wu2024llamawu2024continualConclusionnatbibunsrtnatsection0\thesection\AlphExperimental Setupsec::experimental_setupDatasetssec::datasets\textsc~V2ivison2023camelswang-etal-2022-superconover2023freekopf2024openassistantwang-etal-2023-self-instructtaori2023stanfordxu2023baizechiang2023vicunawei2022chainchaudhary2023codeUltraFeedbackcui2023ultrafeedbackTRACEwang2023traceEvaluation Metricssec::evaluationSupervised Fine-Tuning and Direct Preference Optimization.Factual knowledgeMMLUhendrycks2021measuringReasoningGSM8Kcobbe2021trainingBBHsuzgun-etal-2023-challengingMultilingualismTyDiQAclark2020tydiCodingHumanEvalchen2021evaluatingTruthfulTruthfulQAlin-etal-2022-truthfulqaLLM-based evaluationAlpacaEval 2.0alpaca_evalGPT-4-preview-1106NaturalQuestionkwiatkowski2019naturalTriviaQAhan2019episodicHotpotQAyang2018hotpotqaContinual Learning.wang2023trace \small %      {\rm OP}_{t} = {t}\sum_{i=1}^{t}S_{t,i}, \quad {\rm BWT}_{t} = {t}\sum_{i=1}^{t-1}\left(S_{t,i}-S_{i,i}\right). % 

    Initialize sequential training task  with data  ,      feed-forward block container , self-attention block container , and layernorm block container . \\     Algorithm of HFT with Category-Leval Parameter Selectionalgo::hftPre-trained model FFNs=SANs=LNs=t = 1 \KwTo          \textit \\         \ForEach         \textit \\         \texttt \\          \ForEach         Model training process on with dataset      Fine-tuned model tables/pilot_experimentImplementation Detailssec::implementivison2023camelsT\"ulurafailov2023directT\"uluLlama 2ivison2023camelsLlama 2-13bwang2023tracealgo::hftAdditional ExperimentsDetailed Results of Pilot Experimentssec::detailed_pilot_exptab::pilot_experimentsec::pilot_experimentEvaluation on AlpacaEvalsec::alpaca_eval-5.5mmEvaluation results on AlpacaEval 2.0.tab::alpaca_evaltab::alpaca_evalLlama 2-7btab::IT_HPA_generalLlama 2-13bLlama 2-7bLlama 2-13bLlama 2-7btables/TRACE_commonDirect Preference Optimization with HFT-based Modelssec::dualsec::instructiontab::hpa_hftLlama 2-7bLlama 2-13bGeneral Abilities and Basic Knowledge of Continual Fine-tuned Modelsec::commonsec::continualtab::trace_commonEfficiency Analysissec::efficiencyLlama 2-7bT\"ulutab::efficiencyDetailed Results of Revisiting \texttt and \texttt Layerssec::detail_revisittab::trace_revisittab::trace_40tab::trace_60tables/trace_revisitDetailed Results of Different Parameter Selection Strategiessec::diff_selecttab::trace_randomtab::trace_separatesec::selectingtables/trace_40tables/trace_60tables/trace_randomtables/trace_separateDetailed Results of TRACEsec::detail_tracetab::trace_llama_2_7b_chattab::trace_llama_2_13b_chat_loratables/trace_llama_2_7b_chattables/trace_llama_2_7b_chat_freezetables/trace_llama_2_7b_chat_gemtables/trace_llama_2_7b_chat_gem_freezetables/trace_llama_2_7b_chat_replaytables/trace_llama_2_7b_chat_replay_freezetables/trace_llama_2_7b_chat_loratables/trace_llama_2_13b_chattables/trace_llama_2_13b_chat_freezetables/trace_llama_2_13b_chat_gemtables/trace_llama_2_13b_chat_gem_freezetables/trace_llama_2_13b_chat_replaytables/trace_llama_2_13b_chat_replay_freezetables/trace_llama_2_13b_chat_lora