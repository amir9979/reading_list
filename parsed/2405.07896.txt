is composed of a series of components working asynchronously to ensure accurate query completion (Figure ). An overview of each component is outlined below:

The large language model architecture is a 33B parameter instruction-tuned Transformer decoder , with major architectural improvements detailed below. We tailor our instruction-tuning dataset for tool use - or the ability for the model to select, populate, and chain pre-defined functions when presented with a query. These changes are tailored to optimize for computational efficiency on consumer-grade hardware without sacrificing downstream performance.

 At the core of the transformer architecture is Multi-Headed Attention (MHA), an attention mechanism that enables the model to focus on different parts of the input sequence simultaneously for various 'heads', improving its ability to capture complex relationships in the data. MHA is replaced with Multi-Query Attention (MQA) to reduce the memory requirement during decoding, allowing for higher batch sizes and higher inference throughput.

 Position embeddings are traditionally added to the input sequences of transformers to convey relative and absolute token positioning to the model. The absolute positional embeddings is replaced with rotary positional embeddings, allowing for the flexibility to expand generation to any sequence length. 

 Both the input and output of each transformer sub-layer are normalized using RMSNorm to improve training stability and efficiency.

Matryoshka Representation Learning (MRL)  embeds information at multiple granularity levels, in a coarse-to-fine manner, within a single high-dimensional vector. To cater to various computing and latency regimens in the healthcare space, we make use of a flexible Matryoshka embedding model that can be adapted to multiple downstream tasks.

Several external tools are made available to  in order to automate complex tasks that require reasoning and decision-making across multiple systems. In order to ensure an optimal environment for task completion, functions are defined as self-contained modules, thus limiting the total number of functions available for selection to the model. Sample functions from a total of 9 pre-defined functions are provided in Table  for reference. These tools are presented to the model as a schema describing the function's purpose, required parameters and associated descriptions, as well as the return type.

 The browser is configured to retrieve and surface answers in response to clinical queries from a list of publicly available medical data repositories (e.g. PubMed).

 The clinical calculators are sourced from MDCalc, formatted in markdown, stored in the vector database, and evaluated inside a Python Read Evaluate Print Loop (REPL). 

 The database is a high-performance vector similarity engine optimized for the rapid indexing and search of embedded content through a combination of sparse and dense retrieval methods. We make use of Qdrant (v. 1.8.0)  for all experiments.

 A sandboxed version of an electronic health record system is populated with a combination of synthetic patient data  and publicly available de-identified patient data  (e.g. MIMIC-IV ). Interactions with the database are made possible with carefully designed FHIR-based function calls to ensure compatibility with modern EHR infrastructures.

To robustly evaluate our framework across our previously defined task categories (i.e. information retrieval, summarization and data entry), we synthesize a novel dataset of 300 questions in a controlled and stepwise fashion using physician-generated template questions generated in a crowd-sourced fashion. . To ensure a broad coverage of questions across our proxy dataset (MIMIC-IV), we create a series of template questions to capture common clinician use-cases within the context of EHR systems (Appendix ). Constructing the final dataset is done iteratively for 300 rounds: at each round a patient is randomly selected from our pool of proxy EHR data, and an off-the-shelf LLM is used to generate a grounded question using a randomly selected template question (Positive Prompt). To simulate cases where clinicians might ask questions without clear answers, the model is also prompted to generate a question with no answer with a probability  (Negative Prompt). We use the following prompts:  The final dataset is manually inspected to ensure that the synthetic questions correlate with the information present in the full patient history.

The primary goal of this manuscript is to assess the efficacy of the  framework in carefully choosing, populating, and integrating the appropriate tools to respond to queries related to EHRs. Although the tools implemented are aptly set up to fulfill their designated functions as per their descriptions, it is important to note that other capabilities of LLMs across tasks such as summarization  and retrieval-augmented generation  have been empirically explored in other works and thus fall outside the scope of this analysis.

Instead, we objectively measure and report the nature, sequence, parameter choice, and overall execution validity of each of the functions called to answer the EHR-QA queries. In our assessment, we establish three benchmarks to deem a response successful, as outlined in Table , with each criterion met, earning 1 point. Scoring is sequential — subsequent correct actions are not credited if preceding steps are incorrect. We run all local experiments and models on a machine comprised of 4 NVIDIA Quadro RTX A5000 24GB GPUs - servers that can feasibly be deployed in clinical settings. As baselines, we also report the performances of ChatGPT-4 (Version: , Accessed April 12, 2024) , Claude 3 Opus (Version: , Accessed April 12, 2024)  , and BioMistral  on EHR-QA, along with their mean scores.  % Finally,we perform the Friedman test for statistical% significance at p0.01, and report the Nemenyi post-hoc test to obtain pairwise comparisons between each model. This project was supported in part by a National Heart, Lung, and Blood Institute (NIH NHLBI) grant (1R01HL157235-01A1) (W.H.).

The authors declare no competing interests.

C.Z., A.C., and W.H. designed the experiments. C.Z. wrote the manuscript. Experimentation and manuscript feedback was given by R.D., O.A, V.R, A.C., M.M. and W.H. The code-base was authored by C.Z. and J.C. Computational experiments were performed by C.Z. and J.C. Evaluations were carried out by C.Z., J.C., and G.F. Template questions for EHR-QA were curated by G.F. and R.S. The work was supervised by W.H.

% \noindent% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. %%===================================================%%%% For presentation purpose, we have included        %%%% \bigskip command. please ignore this.             %%%%===================================================%%% \bigskip% %% Editorial Policies for:% \bigskip\noindent% Springer journals and proceedings: % \bigskip\noindent% Nature Portfolio journals: % \bigskip\noindent% : % \bigskip\noindent% BMC journals: % %%===========================================================================================%%%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%%% system, please include the references within the manuscript file itself. You may do this  %%%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%%% file, and delete the associated \verb+\bibliography+ commands.                            %%%%===========================================================================================%%% common bib file%% if required, the content of .bbl file can be included here once bbl is generated%%\input sn-article.bbl\agentAlmanac Copilot Article Title\textit: Towards Autonomous Electronic Health Record Navigation1\fnm \surczakka@stanford.edu1\fnm \sur2\fnm \sur3\fnm \sur4\fnm \sur1\fnm \sur1\fnm \sur5\fnm \sur5\fnm \sur6\fnm \sur7\fnm \sur1\fnm \surwillhies@stanford.edu1\orgdiv, \orgname2\orgdiv, \orgname3\orgdiv, \orgname4\orgdiv, \orgname5\orgdiv, \orgname6\orgdiv, \orgname7\orgdiv, \orgnameClinicians spend large amounts of time on clinical documentation, and inefficiencies impact quality of care and increase clinician burnout. Despite the promise of electronic medical records (EMR), the transition from paper-based records has been negatively associated with clinician wellness, in part due to poor user experience, increased burden of documentation, and alert fatigue. In this study, we present Almanac Copilot, an autonomous agent capable of assisting clinicians with EMR-specific tasks such as information retrieval and order placement. On EHR-QA, a synthetic evaluation dataset of 300 common EHR queries based on real patient data, Almanac Copilot obtains a successful task completion rate of 74\% (n = 221 tasks) with a mean score of 2.45/3 (95\% CI:2.34-2.56). By automating routine tasks and streamlining the documentation process, our findings highlight the significant potential of autonomous agents to mitigate the cognitive load imposed on clinicians by current EMR systems.Introductionsec1myrick2019national, adlermilstein2017ehrblumenthal2010meaningful%architecture.pngwidth=\textwidthfigures/main_fig.pngwidth=\textwidth\textbf Upon receiving a query, the system dynamically selects a subset of APIs from a predetermined list of functions (i.e. FHIR functions, browser, calculator), optimizing the process to meet the specific requirements of the query.overviewKroth2018, Marmor2018, DelCarmen2019Tajirian2020Pelland2017Campbell2006, Shachak2009, Wachter2018, Friedberg2014vanBuchem2021DigitalScribesinghal2022largeVanVeen2023ClinicalTextTierney2024AmbientAI focuses on streamlining access to patient data and medical knowledge, to reduce the time clinicians spend navigating complex EMR systems.   Information retrieval and summarization evaluates the system's ability to draft clinical notes, place orders for tests, medications, and other interventions. This process streamlines clinical workflows by reducing manual data entry.   Data Manipulation assesses the system's effectiveness in prioritizing and presenting alerts to clinicians based on their relevance and urgency, while minimizing alert fatigue. Alert surfacing The clinician manually performs all tasks, with no assistance from the agent, representing the current standard practice.   Level 0. The agent assists by preparing tasks based on explicit clinician commands, but the clinician reviews and approves all actions.   Level 1. The agent proactively suggests actions based on contextual understanding and previous interactions, requiring clinician validation before placing them. Level 2.Almanac Copilotknowledge extractionexternal tool utilizationEHR-QA &  &  \\

                         & Return an answer (string) on diseases, treatment, symptoms, etc. from the medical literature                           &                            & Creates a  for the patient.                          &                            & Fetches the appropriate  history for each patient Encounter.                            &  \\ Function NameDescriptionParameters4*\textbfquery (str): The query to provide an answer to.                             11*\textbfstatus (Enum): The current state of the medication request.                               intent (Enum): Whether the request is a proposal, plan, or an original order.                               name (string): Medication name.                               ...                             13*\textbfquery (string): The query to search for.                               encounter (string, optional): The ID of the  for which the  were made.                               date (string, optional): Date string to filter the returned  with.                             Sample functions provided to the Almanac Copilot to choose from in order to fulfill requests. The model is responsible for selecting, populating, and structuring the functions from the context provided.tab:sample-funcsshi2024ehragent, vaid2024generative    The FHIR standard is a set of rules and specifications for exchanging electronic health care data . Since its introduction in 2011, the standard has been widely adopted across all major EHR systems, and has even facilitated the ability for patients to visualize, understand and share their health data through mobile repositories .

  Moving away from the reliance on code generation for SQL database interactions - which may not only be misaligned with the current technological framework of hospitals but also pose risks to data security and integrity - we adopt the FHIR interoperability standard to ensure our framework is both robust and compatible with modern healthcare systems. 

  \textbffhirschmiedmayer2024llm Although datasets already available in the literature  may offer some value for broad medical inquiries or demographic-specific interventions, they fall short in benchmarking the most common processes of clinical workflows. To bridge this gap, we develop a clinician-derived synthetic benchmark comprising 300 questions across the core EHR tasks previously defined, ensuring relevance and applicability in real-world clinical settings. We detail our dataset generation workflow in section .

  Clinically-Aligned Benchmarklee2023ehrsql, wang2020textehr-gen In commitment to safeguarding patient data, our methodology is built on local model execution, optimized for consumer-grade GPUs. Unlike sending data to external servers via application processing interfaces (APIs), our approach guarantees that all sensitive information is processed within the secure environment of healthcare providers' systems. Local execution within institutional firewalls allows adhering to the highest standards of data privacy and security. Privacy FirstRelated WorkrelworkLLMs and Tool Use.qin2023toolllm, schick2023toolformernakano2022webgptyang2023gpt4toolswang2024llmsClinical and Biomedical LLMs.Luo_2022beltagy2019scibertJiang2023singhal2023expertleveldoi:10.1056/AIoa2300068VanVeen2023ClinicalText, yang2023customizing10.1001/jamanetworkopen.2023.36997huang2022plmicddechoudhury2023benefitsQiu_2023LLM Agentswang2023voyager, hong2023metagpt, yao2023react, shen2023hugginggpt, yang2023autogpt, bran2023chemcrow, hu2023chatdb, qian2023communicativeshi2024ehragentvaid2024generativeMethodsmethodsArchitectureoverviewLarge Language Modelvaswani2023attention &  &  \\

                         &                            & This axis examines the validity of the tools selected by the framework to address the user's query.\\

                         &                           & Ensures the selection and application of parameters within each function are aligned with the requirements of the user's query and adhere to the function's definitions. \\

                         &                           & Evaluates if the sequence of functions invoked by the framework forms a coherent and executable script.  \\ Assessment CriteriaQuestionRationale3*\textbfAre the functions called appropriate in nature to answer the given query?5*\textbfAre the parameters used in each of the functions derived from and technically valid for the given query?3*\textbfCan the provided answer be executed as a valid script?Criterion used to evaluate the \agent framework on the EHR-QA dataset of synthetic physician queries. These criteria are established to ensure that the model selects the correct tools, populates them with the correct parameters, and chains them in a valid order.tab:guidelinesMulti-Query Attention \cite.Rotary Positional Embeddings (RoPE) \cite.Normalizer Location \cite.Embedding Modelkusupati2024matryoshkaToolstab:sample-funcsBrowser \cite.Calculators.Database.qdrantElectronic Health Record (EHR) system.Walonoski2018Syntheagoldberger2000physionet, johnson2020mimicivwidth=\textwidth, scale=0.1Figure_1.png\textbf a) The stacked bar plot illustrates the frequency of scores obtained across 300 synthetic questions within the EHR-QA framework. b) The heatmaps illustrate the models' performance in responding to the same dataset of questions. Each green square indicates a perfect score across all assessed metrics on the task in question. In contrast, red squares signal the lowest performance level. The scoring is sequential — subsequent correct actions are not credited if preceding steps are incorrect.fig:resultsEHR-QA Dataset Generationehr-genFollowing the protocols established by the MIMIC-IV \cite dataset, we provide access solely to the tools necessary for EHR-QA generation. For additional details, we direct readers to the associated code repository.secA1Positive Prompt:\sayNegative Prompt:\sayEvaluationVanVeen2023ClinicalTextdoi:10.1056/AIoa2300068tab:guidelinesgpt-4-turbo-previewopenai2024gpt4claude-3-opus-20240229Anthropic2023Claude3labrak2024biomistralResultsfig:resultsDiscussiondoi:10.1056/AIoa2300068VanVeen2023ClinicalTextvanBuchem2021DigitalScribe\agentshi2024ehragentmoniz2024realmopenai2024gpt4, moor2023medflamingoAcknowledgmentshttps://www.mdcalc.comMDCalchttps://lambdalabs.comLambda LabsDeclarationsFundingCompeting interestsAuthors' contributions