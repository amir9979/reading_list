We formalize the text-based speaker identification (SpeakerID) as follows. Given a dialogue transcript with anonymized speaker identities (e.g., ``speaker1'', ``speaker2'') and person names (e.g., ``Paul Erickson'') for each sentence in the transcript, find the actual names for each anonymized speaker identity. Here, the transcript can be obtained for the dialogue by a speech-to-text system such as Fairseq S2T , the speaker identities for each sentence can be produced by a speaker diarization system such as SOND , the person names can be detected by a named entity recognition (NER) such as Trankit . In this work, we assume such information is available for our SpeakerID models.

: MediaSum is a large-scale dialogue summarization dataset that was created by . It contains K interview-summary pairs from diverse news sources such as National Public Radio (NPR) and Cable News Network (CNN). The interviews cover a wide range of topics/domains, including politics, entertainment, sports, and technologies. In addition, each utterance/sentence in the transcript is tagged with the information of the speaker, including their names, titles, and affiliations. An example of the Mediasum transcripts is shown in Figure .

: Given a MediaSum example, we perform the following steps to obtain training examples for SpeakerID:

In this process, we use the state-of-the-art NER model from Trankit , which achieves the state-of-the-art NER performance of 92.5 F1 on CoNLL English test set, to find spans for named entities in each sentence. Entities with the tag ``PERSON'' are considered as person names. The start and end tokens for each person name are then stored for the example. For the text matching between the person names and speaker actual names, we employ the Levenshtein Distance   to perform the fuzzy text matching. In particular, Levenshtein Distance is a method for measuring the similarity between two strings of characters. The method computes the minimum changes (i.e., insertions, deletions, or substitutions of individual characters) needed to transform one string into the other. The similarity between the two strings can be measured as: , where  is the total length of the two strings and  is the computed Levenshtein distance. As names of the speakers can vary slightly in the transcript (e.g., missing last name), we find out that names with the similarity of at least  can be effectively considered the same.

We present our first model design for SpeakerID in Figure . In this model, we focus on a given person name . For each occurrence of , we identify the sentence or utterance  in which  appears. This sentence is produced by a speaker, denoted as  (i.e., current speaker). We also consider the immediate dialogue context by identifying the sentences preceding and following , labeled as  and , along with their corresponding speakers  and  (previous and next speakers), respectively. This contextual framing is essential for understanding the dynamics of dialogue and speaker relationships.

To construct a comprehensive input sequence, we concatenate , , and  into a single sequence . In cases where either  or  is missing (e.g., if  is at the beginning or end of a dialogue), we introduce padding sentences to maintain consistency in sequence formation. This concatenated sequence represents a broader dialogue context that encapsulates not just the mentioned name but also the surrounding conversational flow.

Upon forming this input sequence, we process it through a pretrained language model (PLM), such as RoBERTa , renowned for its ability to derive deep contextualized representations of text. By passing  through the PLM, we extract the last-layer subword representations, which capture nuanced semantic and syntactic features of the text. To obtain word-level representations from these subwords, we average the subword representations for each word. Subsequently, to represent each sentence within our concatenated sequence, we compute the average of its word representations, yielding distinct vectors that encapsulate the essence of each sentence.

Given that these sentences originate from three different speakers, we posit that their vectors contain unique semantic signatures reflective of each speaker's communicative style or content. Thus, we treat these sentence vectors as proxies for the speakers themselves, assigning them as , , and  for the previous, current, and next speakers, respectively.

For the person name , its representation  is derived by averaging the word representations within the span of . This name vector, encapsulating the linguistic context of the name within the dialogue, is then paired with each speaker vector (, , ). These pairs form the basis for predicting the association between  and the potential speakers.

Each of these concatenated pair vectors is inputted into a feed-forward neural network culminating in a sigmoid output layer, which outputs probability scores , ,  representing the likelihood of the name  being associated with the previous, current, and next speakers, respectively. The model's learning objective is to minimize the standard cross-entropy loss between these predicted probabilities and the true speaker identities. This training process fine-tunes the model to discern the subtle cues within the dialogue that indicate speaker identities, thereby enhancing its ability to accurately attribute names to the correct speakers within complex conversational contexts.

In scenarios where a sentence includes multiple names, our observations indicate that these names generally correspond to distinct speaker identities. This includes the possibility of a "null" speaker identity, which is used to represent instances where the speaker's identity may not be directly linked to any mentioned names within the dialogue. To address this complexity, we introduce a sophisticated model designed to simultaneously predict speaker identities for multiple names within a single sentence. This approach is particularly useful in dialogues where multiple individuals are referenced, necessitating a nuanced understanding of speaker identity.

Consider a sentence that mentions  person names, represented by the vectors , . In our model, each of these name representations is treated as a node within a fully connected graph . The edges of this graph serve to represent the similarity between pairs of names, suggesting that names with higher similarity scores might share or be closely related to specific speaker identities.

To quantify the similarity between any two names, we employ the cosine similarity measure. Specifically, the weight of the edge between any two nodes (names)  and  in the graph is:

This formula essentially normalizes the cosine similarities between a name  and all other names , ensuring that the edge weights are comparable across the graph and facilitating a probabilistic interpretation of name similarity.

Upon establishing the graph structure with calculated edge weights, we proceed to employ a Graph Convolutional Network (GCN)  to refine the representations of each name. The GCN operates over  layers, where each layer enhances the name representations by aggregating information from connected nodes (i.e., other names) weighted by their similarities. The operation at each layer  is defined as:

In this equation,  and  represent the learnable weight matrix and bias for the th layer of the GCN, respectively, and  is the initial input representation for the name .

This method allows the model to iteratively refine the representation of each name by incorporating contextual information from other names within the same sentence, effectively capturing the relational dynamics between mentioned individuals. Ultimately, the enhanced name representations can be paired with speaker representations, as detailed in a previous section, to accurately predict the corresponding speaker identities. This innovative approach leverages the power of GCN to understand and model the complex interrelations between multiple names mentioned in dialogues, offering a promising avenue for advancing the accuracy of speaker identification in rich, multimodal content.

At test time, there might be multiple names assigned to the same speaker identity. To make final predictions for the names, we simply select the name with the highest probability score.

 We randomly sample 200 meetings from the MediaSum dataset  in English language to create a SpeakerID dataset for experimental purpose. We randomly split the resulting dataset into train/dev/test with a ratio of 8/1/1. Statistics for the experimental dataset is shown in Table .

 We tune and select our hyper-parameters on the development set of the dataset. In particular, the models use RoBERTa large version as the PLM and are trained using the Adam optimizer with a learning rate of  and a batch size of . All feed-forward networks have hidden vector sizes of , and there are  layers for the GCN. To implement the models, Pytorch version 1.7.1  and Huggingface Transformers version 3.5.1 (Apache 2.0 license)  are used. The Trankit library version 1.0 (Apache 2.0 license)  is used to preprocess the data and perform named entity recognition. The model's performance is evaluated over three runs with different random seeds. Experiments are conducted on a single Tesla V100-SXM2 GPU with 32GB memory operated by Ubuntu 20.04.4 LTS.  We measure the performance of the models by calculating the number of speakers that the models can successfully found their names in the transcripts. We then compute the precision, recall, and F1 scores for the models accordingly.

% We compare the single-name and multi-name models on the test set of our experimental dataset. As we can see from the Table , both our models achieve great precision of  and , however, obtain the recall scores of  and . The recall scores are limited due to the fact that names of speakers are not always mentioned in the transcripts. Specifically, there are 71 speakers that we can found their names in the test transcripts, leading to the upper bound of  for the recall score. This means that our best model successfully found  of the speaker names in the transcripts. Another observation is that our multi-name model does not improve the performance compared to the single-name model regardless of the clear contribution of the GCN to the model performance as shown in the last two rows of the Table . This suggests that considering a single person name at a time results in better performance for SpeakerID.

Table  presents the main results of our experiments. The single-name model exhibits the highest precision at 80.3\%, suggesting that when it predicts a speaker's name, it is correct 80.3\% of the time. However, its recall is notably lower at 50.0\%, indicating that it only identifies names for half of the speakers in the dataset. The multi-name model shows a marginal decrease in precision to 78.8\% and a slight dip in recall at 49.1\%. This reduction in precision and recall may suggest that introducing multiple names into the identification process complicates the model's ability to accurately predict the correct speaker names, possibly due to the increased complexity in distinguishing between multiple speakers within the same context.

The multi-name model enhanced with Graph Convolutional Networks (GCN) further decreases in performance, with a precision of 75.8\% and a recall of 47.2\%. This result suggests that the GCN component, contrary to expectations, does not necessarily impede the model's performance. Instead, it may provide a beneficial role in the context of the multi-name setting by capturing complex patterns and relationships between speaker identities which are not as effectively discerned when the GCN is removed.

Note that, the recall scores are limited due to the fact that names of speakers are not always mentioned in the transcripts. Specifically, there are 71 speakers that we can found their names in the test transcripts, leading to the upper bound of  for the recall score. This means that our best model successfully found names for  of the speakers in the transcripts.