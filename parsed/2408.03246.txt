% The phenomenon that large language models can be easily distracted by irrelevant information in the context, sometimes termed , has been observed in many NLP tasks, e.g., multi-document QA~ and mathematical reasoning~.% We similarly observe this in multi-hop reasoning as later shown in Figure~.% Previous work fails to identify the underlying mechanism of this lost-in-the-middle phenomenon.% For instance,  shows that presenting the query before the context helps to retrieve relevant information from the context better.% Yet they fail to improve the QA performance with this query-aware contextualization applied.% As stated in Section~, the possible reasons behind this include not only poor information retrieval but also the difficulty in utilizing knowledge within the context.

The challenge of large language models becoming mired in irrelevant contexts, known as the ``Lost in the Middle'' phenomenon, has been documented across various NLP tasks, such as multi-document QA~ and mathematical reasoning~. This issue is also apparent in multi-hop reasoning, which we illustrate later in Figure . Prior research has noted this problem but has not decoded the underlying mechanisms. For example, while  found that introducing the query before the context can aid in better information retrieval from the context, they did not achieve an improvement in QA performance using this query-aware approach. As we suggest in Section~, the reasons might extend beyond mere retrieval challenges to include complications in effectively applying the retrieved knowledge.

% We propose to address the aforementioned issues jointly via , where LMs are required to attribute claims in their reasoning process to the relevant parts in their context.% This extra attribution implicitly decomposes a challenging multi-hop question into two easier subtasks: It first instructs LMs to identify the position of relevant information in the context, and then generate claims grounded on the existing context.% Two Chain-of-Thought (CoT)~ variants can be naturally instantiated for reasoning with attributions:  (CoC) and  (CoQ).% CoC requires LMs to add citations to each step in CoT, while CoQ additionally asks LMs to provide quotes for each citation.% Table~ presents an example to detail the differences between these methods.

To tackle the issues outlined earlier, we introduce , a strategy that mandates language models to link the claims made during reasoning to specific sections of the provided context. This implicit requirement effectively decomposes a complex multi-hop question into two more manageable tasks: Pinpointing pertinent information within the context and constructing well-founded claims based on that information.

We adapt the concept of Chain-of-Thought (CoT)~ reasoning to create two distinct variants aligned with our attribution-based approach:  (CoC) and  (CoQ). In CoC, models are prompted to reference citations corresponding to each step of the reasoning chain. CoQ goes further by requiring models to include direct quotations from the cited material for each reasoning step. An illustrative example highlighting the nuances between these methods is provided in Table~.

% Table~ are the pilot study results of applying CoT, CoC, and CoQ to two strong proprietary long-context LMs, i.e., ChatGPT~ and Claude-instant~.% We can see that CoC and CoQ generally improve over CoT, implying that reasoning with attributions elicits more accurate and fluid reasoning.% CoQ is slightly worse than CoC, since it is challenging to generate quotes verbatim.% Interestingly, even in cases where introducing CoT deteriorates the AO results, CoC can recover the performance loss and even outperform AO.% The effectiveness and robustness of our method on other open-sourced models can be found later in Section~.% In the following sections, we use CoC as the primary reasoning format.

The results of our preliminary study (Please refer to Section~ for the setup), detailed in Table~, compare the efficacy of CoT, CoC, and CoQ when applied to two proprietary long-context LMs: ChatGPT~ and Claude-instant~. Without further notice, ChatGPT always refers to  and  for Claude-instant in this work. The findings suggest that both CoC and CoQ generally yield improvements over CoT, indicating that attribution-based reasoning enhances the precision and coherence of the models' reasoning processes. CoQ appears to slightly underperform CoC, likely due to the increased complexity of producing exact quotations.

It is noteworthy that even in instances where CoT reduces the Answer Only (AO) performance, CoC is able to not only mitigate this decline but also surpass the AO baseline. This demonstrates the potential of CoC as a robust reasoning method. The success of our approach with various open-sourced models is further elaborated upon in Section~. Based on these insights, we adopt CoC as our primary reasoning format in subsequent sections.

% Although reasoning with attributions is promising in general as shown in Tables~ and , open-sourced small long-context LMs still lag proprietary models by a large margin.% We thus explore to what extent training these small LMs to learn to attribute will improve their multi-hop reasoning ability.% However, popular multi-hop reasoning benchmarks do not provide such attribution annotations and we need to collect these new attribution annotations for existing datasets.% We prompt ChatGPT with 5-shot CoQ to generate additional annotations, i.e., CoT with attributions, for 5K randomly sampled instances from the MuSiQue answerable training set~.% Despite that CoC yields better results than CoQ, CoQ provides richer information that is not only helpful in assessing annotation quality but also useful for fine-tuning as later shown in Section~.

Our analysis, evidenced by the data in Tables  and , confirms that while reasoning with attributions holds promise, smaller open-source long-context language models significantly underperform compared to their proprietary counterparts in multi-hop reasoning tasks. To address this, we investigate whether training these models to perform attributions can boost their reasoning capabilities.

A hurdle in this process is the lack of attribution annotations within existing multi-hop reasoning benchmarks. To bridge this gap, we have generated new annotations by prompting ChatGPT with 5-shot CoQ. This has been done to create CoT with attributions for 5,000 instances randomly selected from the answerable training set of the MuSiQue dataset~. Although CoC generally outperforms CoQ, we chose CoQ for annotation because it provides more detailed information. This richness is beneficial not only for evaluating the quality of the annotations but also proves advantageous for the fine-tuning processes discussed in Section~.

% After collecting the generated annotations, we filter out annotations that have any one of the following error types:% [noitemsep, nolistsep]% \item  where the predicted answer does not exactly match the reference. In this case, CoT is mostly also incorrect.% \item  where the predicted citations or quotes do not exist in the provided context, or quotes do not exist in the cited document. This implies that ChatGPT is hallucinating.% \item  where the cited documents are not the manually annotated supporting facts. This also indicates a failed reasoning where attributions are incorrect.% \item  where duplicated citations exist. This phenomenon violates the multi-hop property that supporting facts come from different documents and the reasoning must be incorrect.% \item  where the quoted text is extremely short or long. We remove any annotation when any quote in it has less than 5 words or a whole document is quoted. These extreme quotes are usually not informative.% 

After generating the annotations, we implemented a filtering process to exclude annotations with any of the following errors (Please refer to Appendix~ for implementation details): % As shown in Table~, the portions of each error type are significant, which could be detrimental to the fine-tuning performance.% In the end, we obtained 1,358 samples for training, denoted as }.% Table~ details statistics of our  training set.% Note that the hop distribution is somewhat skewed, resulting from the joint fact that the generated CoT for questions with more hops is more likely to possess the above errors, and these questions also occupy a low portion of the original MuSiQue training set.% We also perform a human assessment on the quality of  in Appendix~.

Table  presents the substantial incidence rates of each error type, which could negatively impact fine-tuning effectiveness. After filtering, we obtain a training dataset of 1,358 samples, referred to as . The statistics of the  training set are outlined in Table . It is important to note that the hop distribution in  is skewed. This skewness arises both because generated CoT for questions with more hops is more prone to errors and because such questions represent a smaller fraction of the original MuSiQue training set. Additionally, we conducted a human evaluation of the  quality in Appendix .

% We evaluate our proposed method in the multi-hop reasoning datasets: ~, ~ and ~.% MuSiQue is evaluated in an in-distribution setting as we train models on .% The other two datasets are gauged through an out-of-distribution setup.% We concatenate and shuffle all provided documents, including the relevant and irrelevant ones, as the context for each question.% We adopt the dev and test sets from  for hyper-parameter tuning and testing.% All reported numbers are the average of three trials with different random seeds.

Our method's effectiveness in multi-hop reasoning is assessed on the following datasets: ,  ( for short) , and . % We use  for training, thereby evaluating MuSiQue in an in-distribution context, while HotpotQA and 2Wiki are examined in out-of-distribution scenarios. For each question, we provide a context composed of shuffled relevant and irrelevant documents. These irrelevant documents are the official retrieved distractor documents. We adopt the development and test sets from  for evaluation, which contains 100 and 500 examples respectively. The results we present are the mean values from three separate trials, each with a distinct random seed.

% We additionally evaluate our system on the general instruction following benchmarks, ~ and ~, to study how building up the multi-hop reasoning skill affects other abilities of LMs. To understand the broader impact of enhancing multi-hop reasoning on LMs' overall capabilities, we also conduct evaluations on general instruction-following benchmarks, namely  and .

The following long-context baselines are chosen in our experiments. % The first two are proprietary models and the last three are open-sourced models:

We prompt all models with 5-shot to evaluate their multi-hop reasoning performance. These 5 demonstrations are randomly sampled from the 20 annotated training examples provided by . If the input length exceeds the window size, we drop the last demonstration until the input length fits. The prompts we used are presented in Appendix~.

Our model  is fine-tuned on  with LoRA~, following hyper-parameters used in FastChat~. For its training data, we perform augmentations to double the training data for all tasks in Section~ except for the QI task. Note that we subsample same-sized instruction-tuning data from the Alpaca dataset~ and mix it with the reasoning data. These instruction-tuning data serve the purpose of minimizing the risk of hampering other abilities Vicuna already possesses before fine-tuning.

% % Table~ shows the results of AO, CoT, and our CoC prompting on three multi-hop reasoning datasets with 5 baselines.% CoC improves over CoT in 77\% of cases, excluding those where models have nearly zero performance.% It is worth noting that Claude-instant performs particularly well with AO and employing CoT worsens the performance.% Even for this weird case, CoC recovers the performance loss of CoT and achieves a result comparable to AO, illustrating the effectiveness of reasoning with attributions.% We additionally study the robustness of CoC in Appendix~.% Our results show that CoC is more robust than CoT when different levels of noise appear in the context. The results in Table  underscore the efficacy of our CoC prompting across three multi-hop reasoning datasets, benchmarked against five baselines. In 77\% of the evaluated cases (disregarding instances of near-zero model performance) CoC outperforms CoT. Notably, Claude-instant exhibits strong results with AO, and its performance diminishes when CoT is used. However, CoC not only mitigates this decline but also attains results on par with AO, demonstrating the robustness of attribution-based reasoning. For a detailed analysis of CoC's robustness, particularly against varying degrees of contextual noise, see Appendix .

%  performs closely to strong proprietary models.}% Table~ also compares the zero-shot performance of our  with the 5-shot results from other baselines.%  significantly outperforms other baselines that are of the same scale by more than 20 points on average.% It also outperforms two strong proprietary models on MuSiQue and has a close result to these models on the other two out-of-distribution benchmarks.% Notably, we find that  with AO yields the best performance in 2WikiMultiHopQA and outperforms CoT in HotpotQA.% One reason is that 2WikiMultiHopQA and HotpotQA are easier and explicit reasoning is not that beneficial, e.g., the improvement of CoT on these two datasets is much smaller than on MuSiQue for both ChatGPT and Claude-instant.% Moreover,  suggest that more than 50\% ``bridge-type'' questions in HotpotQA contain shortcuts, where the model can locate the answer by matching a few keywords without performing the intended 2-hop reasoning.% 2WikiMultiHopQA, on the other hand, is constructed by a limited set of handcrafted composition rules, making it trivial for language models.% Another reason is that  is trained on , whose question types are not as much as 2WikiMultiHopQA and HotpotQA, e.g., it does not contain ``comparison-type'' questions. Table  presents a zero-shot performance comparison between our  and five-shot outcomes from various baselines.  surpasses baselines of comparable scale by an average margin of over 20 points. It exceeds the performance of two notable proprietary models on MuSiQue and delivers closely competitive results on the other two benchmarks.

In particular,  with AO achieves superior results on 2Wiki and surpasses CoT on HotpotQA. This can be attributed to the relative simplicity of these datasets, where explicit reasoning does not significantly enhance performance. For instance, CoT's advantage is noticeably smaller on these datasets compared to MuSiQue for both the ChatGPT and Claude-instant. Additionally, according to , over half of the ``bridge-type'' questions in HotpotQA contain shortcuts, which can locate the answer by keyword matching, circumventing the need for the intended two-hop reasoning. Similarly, 2Wiki's predictable nature, due to its question construction from a limited set of rules, simplifies the task for LMs. Another contributing factor is that  is trained on , which does not encompass the full range of question types found in 2Wiki and HotpotQA, such as ``comparison-type'' questions.

%  is more resilient to noisy contexts.}% An interesting research question to our  is how robust it is to noise appearing in the context, i.e., the notorious lost-in-the-middle problem~.% Figure~ tests the performance change by synthesizing different levels of noise.% We see that the performance of baselines significantly decreases when the noise ratio goes from 0\% to 100\%, e.g., Vicuna lost more than 30 points in MuSiQue, while our  suffers less from noisy context, which lost only around 10 points. A key aspect of  is its robustness to contextual noise. To investigate this, Figure  illustrates 's performance against varying degrees of synthesized noise. This synthesized noise is implemented by adding varied numbers of random irrelevant documents to the context. The data indicates that while the performance of baseline models markedly declines with increased noise, e.g., Vicuna drops by over 30 points on MuSiQue,  shows greater resilience, with a reduction of only about 10 points. % This suggests that  maintains its efficacy even in the presence of substantial noise.% % In addition to testing the multi-hop reasoning performance, we also investigate to what extent of our  maintains the general instruction-following ability from Vicuna after fine-tuning.% Table~'s results on two instruction-following benchmarks indicate that tailored fine-tuning indeed hurts abilities other than multi-hop reasoning due to the limited capacity of a 7B model.% However, further examinations suggest that more than 98\% performance degradation results from fine-tuning with Alpaca data (``+ Alpaca Data''), and fine-tuning with multi-hop reasoning data only brings less than 2\% loss.% This could be attributed to the lower quality of Alpaca data compared to the closed-sourced Vicuna data.% The former is synthesized by GPT-3 and is single-turned, while the latter is multi-turn human-bot conversations. Our investigation extends beyond multi-hop reasoning to examine how attribution learning affects 's general instruction-following capabilities post-fine-tuning, as compared to the Vicuna baseline. The results in Table  from two instruction-following benchmarks reveal that fine-tuning slightly compromises abilities beyond multi-hop reasoning in a 7B model due to capacity constraints. However, a closer analysis reveals that over 98\% of the performance decrease is attributed to fine-tuning with Alpaca data (``+ Alpaca Data''), while multi-hop reasoning data incurs less than a 2\% detriment. This is because the quality of Alpaca data is inferior to Vicuna's, with the former being single-turn GPT-3 synthesized and the latter comprising multi-turn human-bot conversations.

%  reliably attributes claims.}% Inspired by , we evaluate the precision and recall of the predicted citations of  in Figure~.%  has high precision, implying that it can accurately attribute its statement to relevant documents.% However, the relatively low recall of  means that our model fails to find all relevant documents in many cases.% This could be an impact of disconnected reasoning in  (See Appendix~).% More studies about attribution quality can be found in Appendix~. Drawing from the insights of , we scrutinize the citation precision and recall for , as presented in Figure . The model demonstrates high precision, indicating its proficiency in correctly attributing statements to pertinent documents. Nonetheless, the moderate recall highlights that  does not consistently identify all relevant documents, a potential consequence of the disconnected reasoning patterns observed in  (detailed in Appendix ). Further exploration of the correlation between attribution quality and reasoning performance is in Appendix .

% We conduct an ablation study on our proposed multi-task learning strategy in Table~.% We can see that fine-tuning on our  dataset (``+ AP'') significantly improves the reasoning ability of the original Vicuna model.% Interestingly, we observe that the results of training Vicuna to explicitly generate CoT (``+ CG'') are somewhat mixed: the model improves on MuSiQue, while degrades on 2WikiMultiHopQA and HotpotQA.% This is because 2WikiMultiHopQA and HotpotQA are easier than MuSiQue due to the simplicity and shortcuts in their questions, as mentioned in Section~.% Training the model to answer with CoT prevents models from exploiting these clues and thus results in performance degradation.% On the other hand, introducing LA to encourage the model to attribute its reasoning steps recovers the performance loss of CoT and even significantly improves the model on MuSiQue.% This finding suggests that attributions help the model execute long-term reasoning chains without sacrificing performance on simple questions.% In the end, incorporating the QI tasks further strengthens the multi-hop reasoning ability of our trained model. Our ablation study in Table  assesses our multi-task learning approach. Results indicate a marked enhancement in Vicuna's reasoning capabilities upon fine-tuning with our dataset (``+ AP''). However, explicitly training Vicuna to generate CoT (``+ CG'') yields mixed outcomes: It benefits performance on MuSiQue but adversely affects results on 2Wiki and HotpotQA. This discrepancy can be attributed to the relative ease of the latter datasets, where simpler questions and shortcuts reduce the effectiveness of complex reasoning strategies, as discussed in Section . Importantly, integrating the LA task (``+ LA'') mitigates the performance drops associated with CoT and notably boosts MuSiQue scores. This implies that attributions are instrumental in enabling the model to reason over complicated questions without compromising its ability to handle simpler queries. Finally, the addition of the QI task (``+ QI'') appears to further refine the model's multi-hop reasoning proficiency, underscoring the value of our multi-task learning framework.

% We additionally study the effectiveness of our proposed data augmentation strategies.% We exclude the QI task in this study as training the model on this task only can not perform multi-hop reasoning.% As shown in Table~, adding augmented data improves the model performance on nearly all datasets.% Exceptions are CG and LA on the out-of-distribution 2WikiMultiHopQA dataset.% This is because the automated generated questions in 2WikiMultiHopQA are easy for the model to learn to answer with a few annotated data.% While for MuSiQue and HotpotQA, the model needs to be trained on more data to count for the variance in the human-crafted questions. We explore the impact of our data augmentation strategy, intentionally omitting the QI task, as it alone is insufficient for training models to conduct multi-hop reasoning. The data in Table  demonstrates that including augmented data generally enhances model performance across various datasets. However, augmenting CG and LA data does not yield improvements on 2Wiki. In this case, the model readily learns from a limited amount of annotated data due to the simplicity of the automatically generated questions within 2Wiki. Conversely, on MuSiQue and HotpotQA, which feature more complex and varied human-crafted questions, the model benefits from exposure to a larger dataset to accommodate the diversity of question formulations.

% We also examine the effect of scaling fine-tuning data in Figure~.% One can see that adding more data consistently improves the performance in both in-distribution and out-of-distribution settings.% We note that for HotpotQA, training on 60\% of our data already achieves the best performance, indicating that learning to answer questions with more hops requires more data.% Interestingly, we observe that utilizing only 20\% of our data can acquire around 85\% of the final performance.% This observation suggests that fine-tuning with a few hundred multi-hop reasoning instances can elicit a strong reasoning performance. In Figure , we investigate how the expansion of fine-tuning data influences model performance. It is evident that incorporating additional data steadily enhances performance on MuSiQue and 2Wiki, while optimal results are attained with just 60\% of our data for HotpotQA. This fact suggests that more complex question answering, involving additional reasoning steps like MuSiQue and 2Wiki, demands a larger dataset. An intriguing discovery is that using a mere 20\% of our data achieves approximately 85\% of the peak performance. This highlights the efficiency of fine-tuning: Even a modest subset of multi-hop reasoning examples can significantly boost the model's reasoning capabilities.

% In Table~, we provide two examples where Vicuna and  are both prompted with CoC.% In all two examples,  can generate a reasonable CoT and accurately attribute each claim, while Vicuna directly provides the answer without an explicit reasoning process. Table  presents a comparative case study where Vicuna and  are both prompted to generate CoC. Within the provided examples,  successfully produces coherent CoT and precisely attributes each claim. In contrast, Vicuna yields answers without engaging in an explicit reasoning process.