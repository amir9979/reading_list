Med-LVLMs often generate non-factual responses when dealing with complex medical images. RAG can provide the model with external knowledge as a reference, thereby effectively enhancing the factual accuracy. In the multimodal knowledge retrieval stage,  textual descriptions/reports that are most similar to the features of the target medical images. These references contain a wealth of image-based medical facts and serve to guide the generation of responses for the medical image. 

Following the design of CLIP~, the retriever will first encode each image and the corresponding reports into embeddings using a vision encoder and a text encoder, respectively. Specifically, all medical images  are encoded into image representations  by a vision encoder  (i.e., ), where  is the number of medical images that need to be retrieved, and  is the dimension of the embedding. Similarly, we generate text embeddings  for all corresponding medical reports  by applying a text encoder , i.e., . Subsequently, to adapt the general vision and text encoders to the medical domain, we fine-tune the encoders using the training data with a contrastive learning loss, defined as:

where  represents the similarity matrix between image and text modalities, calculated as: , where each element  represents the similarity between the image representation of example  and the text representation of example .  Equation~ aims to learn the representations by maximizing the similarity of text and image modalities representing the same example, while minimizing the similarity of text and image modalities representing different examples. 

After fine-tuning the image and text encoders, during inference, when faced with a target medical image  requiring the generation of its medical report, we extract the top- similar medical reports . We then use the retrieved medical report to guide the generation of the medical report for the target medical image. with the following prompt guidance: .

For the RAG strategy, the top-3/5 result is typically used as a reference~. However, it sometimes fails to encompass all relevant retrieved contexts, especially when facing the fine-grained features of medical images. Additionally, an excessive amount of retrieved contexts may introduce low-relevance and inaccurate references, which can interfere with the model's generation.  % The quality of the retrieved contexts significantly influences the effectiveness of retrieval.  Thus, an algorithm that can automatically determine the optimal number of retrieved contexts, based on the risk of factual errors, is particularly crucial. 

In this section, motivated by~, we propose the following strategy to choose a subset  for the number of retrievals  from a candidate set  such that the factuality risk  can be provably controlled for any . Specifically, first, for each , the strategy first calculates the factuality risk , computed as , where  denotes the target medical image,  denotes the question,  means the selected top-K retrieved contexts, and  measures the ratio of correct answers provided by the Med-LVLM  to the total number of answers.  Next, two probabilities  and  are computed as: 

 where  is the Kullback-Leibler divergence between two Bernoulli distributions and  denotes risk upper bound.  representing the probability that, in a binomial distribution with parameters  and , denoted by , the observed value is less than or equal to . Then, the minimum of these two probabilities  is taken. Finally, we use any family-wise error rat (FWER)-controlling procedure, such as  Bonferroni correction  or sequential graphical testing , to choose . For example, for Bonferroni correction, if  is less than or equal to , where  denotes tolerance level, then  is added to the set .  The proposed strategy calculates the model's factuality risk under different  values, computes the corresponding probabilities using two approaches, and selects those  values that meet the risk tolerance to control the overall factuality risk. 

We have the following result that ensures with probability at least , the factuality risk produced is controlled by .

In addition to selecting the optimal number  of retrieved contexts, it is likely that these contents often fail to fully capture the details of every lesion or normal area in medical images. Therefore, when the retrieved contexts is inaccurate, a reliable Med-LVLM is expected to remain unaffected by the unreliable information and independently use its own knowledge to answer medical questions. However, empirically, as illustrated in Table~, approximately half of all incorrect responses by the retrieval-augmented Med-LVLM are due to an over-reliance on retrieved contexts. This significantly affects the application of the retrieval augmented generation strategy to Med-LVLMs.

To address this issue, we propose a Knowledge-Balanced Preference Tuning (KBPT) strategy to mitigate over-reliance on retrieved contexts and enhance factuality in medical content generation. Specifically, we select samples  from the a separate set with samples are not used to fine-tune the retriever in Section~, where  denotes input medical image, ground-truth answer and question, respectively. We identify responses  where the model originally answers (i.e., ) correctly but gives incorrect answers  after incorporating retrieved contexts as dispreferred responses, as they indicate over-dependence on the retrieval. Conversely, ground-truth answers  are considered preferred responses. We denote the preference dataset as , where ,  are represented as preferred and dispreferred responses, respectively. 

Based on the curated preference data, we fine-tune the Med-LVLM using direct preference optimization. Following Eqn.~, the loss is calculated as follows: %%%%%%%%%%%%%%%%%%%%%%%%%%%%Experiment%%%%%%%%%%%%%%%%%%%%%%%%%%%.  We utilize LLaVA-Med-1.5 7B~ as the backbone model. During the preference optimization process, we adapt LoRA fine-tuning~. For the training of retriever, the vision encoder is a ResNet-50~, and the text encoder is a bio-BioClinicalBERT~. We use the AdamW optimizer with a learning rate of , weight decay of  and a batch size of 32. The model is trained for 360 epochs. For more detailed information on training hyperparameters and training data, please see Appendix~ and .\\ . We compare  LVLM hallucination mitigation methods that have already shown promising results in natural images, including Greedy Decoding, Beam Search~,  DoLa~, OPERA~, VCD~. These methods manipulate the logits of the model's output tokens to enhance factual accuracy. Furthermore, we compare the performance with other open-source Med-LVLMs, including Med-Flamingo~, MedVInT~, RadFM~. 

.

To ensure that the retrieved report content is relevant to the visual question-answering content and to facilitate experimentation, we utilize three medical vision-language datasets, i.e., MIMIC-CXR~, IU-Xray~, and Harvard-FairVLMed~, encompassing radiology and ophthalmology. The training set is split into two parts: one part is used to train the retriever (Section~), and the other part is used to construct the preference dataset for KBPT (Section~). 

Additionally, we construct VQA pairs for KBPT and evaluation. Specifically, the reports from training set for preference dataset and reports from original test set are input into GPT-4~ to create closed-ended VQA data with  or  answers, , . By sampling segments from a medical report, we can generate a sequence of concise, closed-ended questions posed to the model, each with accurate answers. The questions are in  format, making it easier to analyze errors caused by over-reliance on retrieved contexts compared to open-ended questions. The detailed construction process and dataset statistics are provided in the Appendix~. 

. We use Accuracy as the primary metric and, for detailed comparisons, we also adopt Precision, Recall, and F1 Score.

In this section, we provide comprehensive comparison results with different baseline methods and other open-sourced Med-LVLMs. 

. We present the results of a comparison between  various hallucination reduction methods in Table~. According to these results,  the best overall performance, effectively and accurately diagnosing diseases with an average accuracy improvement of 20.8\% across all datasets. We also observe that  notably better on the IU-Xray and Harvard-FairVLMed compared to MIMIC-CXR. This difference is attributed to the excessive length of the reports available for retrieval in MIMIC-CXR, where overly long references tend to confuse the Med-LVLM. In addition, even when dealing with the relatively niche ophthalmology data (i.e., Harvard-FairVLMed),  superior results, significantly enhancing the factual accuracy of the Med-LVLM. In contrast, the performance of decoding methods is quite unstable, showing significant rates of missed or incorrect diagnoses across different datasets, as indicated by the precision and recall values. 

. In Table~, we present the comparison with different open-sourced Med-LVLMs.  state-of-the-art (SOTA) performance across all datasets. Although the second-best model, MedVInT, outperforms other models,  an average accuracy improvement of 47.4\% over it. Whether in radiology or ophthalmology,  remarkable performance, significantly surpassing other open-source Med-LVLMs. This indicates that  generally applicable and effective in the medical multimodal diagnosis, providing consistent improvements across various medical image modalities.

In this section, we conduct a set of analyses demonstrate how different components contribute to the performance and illustrate how  overall performance, which are details as follows:

. To further illustrate the effectiveness of the components of , we conduct ablation experiments on three datasets. The results are shown in Table~. We find that the basic RAG strategy ("R") slightly improves factual accuracy on two datasets but decreases it on MIMIC-CXR. The limited retrieved contexts can not cover the fine-grained features of medical images, resulting in unstable factual accuracy improvements. With the aid of the factuality risk control strategy ("FRC"), retrieval performance see a stable increase, outperforming the original Med-LVLM. Considering the model's over-reliance on retrieved contexts, the knowledge balanced preference tuning ("KBPT") further enhances the model's reliability and significantly improves its performance. Ultimately, by combining these two strategies,  optimal performance.

To better understand how  the Med-LVLM's over-reliance on retrieved contexts, we measure the Med-LVLM's error and over-reliance ratios, and visualize the text and image attention maps of the models before and after fine-tuning using a randomly selected case, as shown in Figure~. The quantitative results in Figure~(a) demonstrate the significant positive impact of  mitigating the model's over-reliance on retrieved contexts, with the error rate and over-reliance rate decreasing by an average of 42.9\% and 47.3\%, respectively. Attention maps Figure~(b) illustrate the model's attention scores for text and image tokens. We find that, on the text side, the model with knowledge balanced preference tuning shows a significantly reduced focus on retrieved contexts, effectively mitigating over-reliance on such information. The model focuses more on the question and leverages its own knowledge to answer, rather than relying solely on the retrieved contexts, effectively enhancing factual accuracy. 

. We further conduct a thorough analysis of the data types used in constructing preference data for KBPT. Three formats are considered: medical image captioning (prompted as ``Please describe this medical image"), visual question-answering (VQA), and a mixture of both. The selected data are samples where the model makes errors due to over-reliance on retrieved contexts. The results are shown in Table~. We observe that models fine-tuned using VQA data perform the best across all three datasets. This indicates that when retrieved contexts are incorporated into VQA questions, the Med-LVLM, through KBPT, can learn this paradigm of integrating and balancing its own knowledge with retrieved context to maximize factual accuracy. However, when the data is in the form of captioning, it may enhance the model's ability to describe medical facts, but it merely distances the model's answers from the retrieved contexts. The model fails to understand how to balance retrieval content with its own knowledge.

To demonstrate the compatibility of , we conduct KBPT on LLaVA-Med-1.0 as well. The experimental results on three datasets are shown in Figure~. We find that our knowledge balanced preference tuning method demonstrates good compatibility across different models, significantly improving factual accuracy across multiple datasets. Based on LLaVA-Med-1.0,  accuracy by an average of 16.7\%. This indicates that  a noticeable positive effect on mitigating over-reliance on retrieved contexts, thereby enhancing the Med-LVLM's factual accuracy.

Figure~ presents two representative case results, demonstrating that  effectively enhance the factual accuracy of med-LVLMs. In case 1, LLaVA-Med provides a factually incorrect answer. After applying the RAG strategy, the model still exhibits factual issues, whereas our method effectively addresses this and improves accuracy. In case 2, LLaVA-Med initially provides a correct answer, but due to the model's over-reliance on retrieved contexts, it subsequently produces an incorrect response.  the weight of inherent knowledge and retrieved contexts, enhancing factual accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%Related Work%%%%%%%%%%%%%%%%%%%%%%%%%%% The quantities of all the data used are shown in Table~ and Table~. It is notable to note that for training the retriever, this refers to the number of image-text pairs; for fine-tuning, it refers to the number of QA items. ``All" represents the total quantity used to construct the preference dataset, where only the samples with correct original answers that become incorrect after adding retrieved contexts are included in the training of knowledge balanced preference tuning (``KBPT").

We convert the medical reports into a series of closed-ended questions with yes or no answers. To ensure the quality of the VQA data, we perform a round of self-checks using GPT-4~. Finally, we conduct an round of manual filtering to remove questions with obvious issues or those related to multiple images or patient histories. The prompt templates used are shown in Table~.

We utilize three open-source medical vision-language datasets, i.e., MIMIC-CXR~, IU-Xray~, Harvard-FairVLMed~.