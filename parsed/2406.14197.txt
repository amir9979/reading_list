A   is a subset of the Kleene closure  of some alphabet . We call an element  of  a . We denote the empty string by , and we assume that . A  over  is a function  such that  . If the semimeasure of all strings sums to one, i.e., , then  is called a .% Finally, for any alphabet , we define the set .

Most modern LMs are autoregressive, i.e., they define  through conditional distributions of the next symbol given the string produced so far and the probability of ending the string, i.e.,

Here,  denotes the special nd-f-tring symbol, which specifies that the generation of a string has halted. The inclusion of  allows (but does not guarantee) that a language model , autoregressively, constitutes a probability measure over  ; a model defined as in  may sum to  1 in a pathological case. For any alphabet , we define the set . The conditional probability distributions are usually defined based on  representations of  computed by a .=-1

Probabilistic finite-state automata are a well-understood probabilistic computational model.=-1

Next, we will define some basic concepts related to PFSAs. A   is  if  and, for every , there is at most one  such that  with .% Any state  where  is called an , and if , it is called a . A   of length  is a sequence of subsequent transitions in , denoted as=-1

The  of a path is . The   of a path  is the product of the transition and initial weights, whereas the  of a path additionally has the final weight multiplied in. In symbols, this means

with  and . We write  for the set of all paths in  and we write  for the set of all paths in  with yield . The sum of weights of all paths that yield a certain string  is called the , which we write as=-1

The stringsum gives the probability of the string . This way, s induce a particularly well-understood family of LMs.

Although s share many properties with unweighted (boolean-weighted) finite-state automata, one important difference relates to determinization. In the unweighted case, the class of deterministic and non-deterministic FSAs are equivalent, i.e., any non-deterministic FSA has an equivalent deterministic FSA that accepts the same language. This result, however, does not hold for s: There exist s that admit no deterministic equivalent , meaning that non-deterministic s are strictly more expressive than deterministic ones.

Autoregressive LMs (cf. ) and s fall under the larger framework of models that specify probability distributions . However, in autoregressive neural LMs with fixed precision, only one sequence of computational actions can yield a particular string, meaning that they can only model deterministic weighted regular languages .  On the other hand, non-deterministic s compute string probabilities by  combining the probabilities of all paths that yield the same string, and hence lie outside the grasp of such neural models.% As we show later, CoT reasoning provides a principled way to overcome this limitation and allow fixed-precision neural models to simulate non-deterministic automata.=-1

We now define a finite-state machine that, in addition to scanning, also  strings.=-1

Similar to  transitions, we write FST transitions of the form  as . Finally, we give the following definition which we will use to formalize CoT reasoning.=-1

Like all relations, regular relations   have a trivial inverse .

We consider the following definition of a probabilistic Turing machine.=-1

A transition should be interpreted as follows: When  in state , reads  on the working tape, writes  on the output tape, writes  on the working tape, move the head in direction  on the working tape. Each computation step randomly selects a transition according to its weight .=-1%

This definition of Turing machines straightforwardly induces a semimeasure over strings . It is sometimes easier to prove claims about Turing machines through another equivalent machine . The probabilistic two-stack pushdown automaton is one example . See  for an overview.

Recurrent neural LMs are LMs whose conditional probabilities are given by an RNN.% Because  hides the symbols consumed by the Elman RNN, we also use the evocative notation  to denote the result of the application of  over the string . The notation  makes it clear that an RNN LM implicitly defines a language encoder. 

The most common choice for the projection function  is the softmax, whose limitation is that it implies the LM has full support, i.e., an Elman LM with a softmax projection assigns positive probability to all strings in . To construct non-full-support Elman LMs, we instead use the :

In contrast to the softmax function, sparsemax is the  on , i.e., we have  for .

All implementations of LMs on modern hardware require representations to be fixed-precision floating-point numbers or arbitrary-precision (rational) numbers.  In the case of arbitrary precision, an important consideration in processing strings  is the number of bits required to store the representations and how the number of bits scales with the length of the string, . This motivates the following definition of precision.

The constructions in the existing literature range from constant to unbounded precision. The RNNs considered by  encoding of deterministic s, for example, results in an RNN that is of constant precision. ,  and  consider models with logarithmically bounded precision. In contrast, articles treating the Turing completeness of neural networks  require unbounded precision to be able to represent the fact that a Turing machine may fail to halt. Naturally, our constructions of Turing complete LMs will also require unbounded precision.

Transformer LMs compute the conditional distributions  by means of self-attention. Because transformer LMs necessitate the precise introduction of multiple sub-components, we postpone the full introduction to  and only review the basics here. We borrow much of the notation and formalization from .

A transformer is a composition of multiple transformer , each of which implements the . The attention mechanism works as follows. It takes a  vector  and two matrices: The matrix  of  and the matrix  of  and computes a weighted average of the value vectors based on the compatibilities of the key vectors to the query vector, as scored by a scoring function .

Attention weights are computed by normalizing the scores . The choice of the normalization function has concrete implications on representational capacity . We focus on the  projection function.

A transformer layer uses the attention mechanism followed by a position-wise MLP to compute augmented representations  of the input representations . The query , the keys , and values  are all transformations of the input representations . Initially,  are computed by some static representation function of the symbols and their positions. Multiple transformer layers are stacked into a transformer, which computes the (deep) contextual representations of all symbols in the string. The contextual representation of the final symbol in the string is then used to define the representation function  of a transformer LM.

Autoregressive LMs generate strings by outputting individual symbols  one at a time until  is generated. The resulting string  is the output of the LM and the outputs generated in this manner implicitly define the distribution of the LM. Models studied in existing work on the representational capacity of neural LMs, however, often do not (only) emit symbols from . Rather, they also emit symbols from some  alphabet  that includes symbols that encode additional information required to simulate a given formal model of computational. For example, the symbols generated by  transformer contain both the output symbols as well as (the changes to) the configuration of the Turing machine.=-1

It is not difficult to see how generating strings from an augmented alphabet can be seen as a form of CoT reasoning. The outputs  intended to be a part of the final output can simply be seen as the result of the additional computational steps performed by the CoT-augmented LM. These outputs are later removed in post-processing and only the string with symbols from our target alphabet  remains. In this sense,  construction works with a form of CoT reasoning without explicitly mentioning it. This connection was made explicit in concurrent work by .=-1

Outputting additional information is not regarded as an issue when the task is to simulate (unweighted) Turing machine runs on a given  because what matters, in that case, is only whether the model accepts or rejects the input . However, when considering the LM induced by a PTM, we do care about the alphabet the distribution over strings is over. Thus, given an LM over , it follows that neural LMs outputting additional information cannot define the same distribution, i.e., they cannot be weakly equivalent. Nevertheless, the ability to output additional information while generating a string seems natural and useful; it is the heart of CoT reasoning. And, indeed, models that can only output symbols that are part of the final string are restricted to doing real-time computation .=-1

We now formalize the notion of CoT reasoning through the following definition which allows a model to output additional information  considered part of the final output:

We can use the function  to map the strings sampled from an LM, ones additionally encoding the intermediary steps of computation, into the final output, i.e., strings .

This means if we care about strings from  and we have an LM over the closure of a -augmented alphabet , then each symbol the LM outputs encodes is either an output symbol from , an intermediate computation symbol from , or both. The computation symbols can then easily be removed by applying the per-symbol projection function , lifted to a non-length-increasing homomorphism over strings  to the LM's output element-wise.% This function can be implemented by a simple single-state transducer (making it a regular function):=-1

To motivate our formalization, in this section, we show several properties of regular reducibility that will allow us to reason about the representational capacity of CoT-augmented LMs later in . Particularly, as we will see, there exists a strong connection between regular reducibility and the addition of  choices in the model. To exemplify this concretely, the next theorem shows that regular reducibility allows us to model non-deterministic s with deterministic ones, which, as discussed in , cannot be done with just deterministic s in general.

A similar connection exists for s (see ).  captures a crucial property of regular reducibility: It allows us to  non-determinism with a deterministic device. As exemplified in the cases of regular LMs, this can concretely increase the representational capacity of a model class, as in the case of . On the other hand, regular reducibility never  the representational capacity of a model class, since  can always be set to the identity function. Due to the close connection between neural LMs and determinism , one could hope that a similar increase in capabilities could be achieved for neural LMs augmented with CoT reasoning as well. In , we show that this is indeed the case.=-1

After introducing the notion of regular reducibility and presenting some of its core properties, we now use it to define CoT-augmented LMs.=-1  defines a CoT-augmented LM  through the LM  that generates strings from  and then applies the regular function  to the generated strings to obtain strings from . This allows  to output additional information while still defining a probability distribution over strings from  (see ). The weight of a string  under  is then computed as the sum over the weights of all strings in the preimage of , analogously to :

This is the approach we take in .

We first discuss the connection between CoT-augmented neural LMs and s.

 construction provided one of the first connections between a neural network and a formal computational model. It showed that RNNs can emulate (deterministic) FSAs, where the RNN accepts string by activating a particular neuron after reading the string. The relationship in the probabilistic case was explored by , who show the equivalence of pure Elman RNNs (without CoT reasoning) and s. This illustrates an important distinction between the deterministic and non-deterministic frameworks and thus a discrepancy between general s and RNN LMs. Intuitively, the discrepancy comes from the fact that there is no non-determinism in the recurrence of an RNN, which is thus unable to capture the possibly non-deterministic decisions of the . CoT reasoning endows an RNN with exactly this non-determinism because it allows the RNN to sample and refer back to  rather than only symbols by annotating each of its outputs with the current state of the . Upon reading the previously randomly generated state of the automaton captured in the output, it can follow the randomly sampled generating trajectories.

The following two theorems show that RNN LMs with fixed precision and CoT reasoning are weakly equivalent to general PFSA. This requires showing the correspondence in both directions, i.e., that  establish the equivalence between CoT-augmented  Elman RNN LMs and general regular LMs. This illuminates the added representational capacity awarded by CoT reasoning: Storing the current FSA state in the output string and removing it later allows the model to handle non-determinism which is not possible without the additional information.

We now show an analogous claim to  for Transformer LMs with CoT. This is simply a stepping stone towards full probabilistic Turing completeness---a similar construction will then lead us to the full proof that transformer LMs with unbounded precision can simulate PTMs.

We now extend the results from the previous section from representing the simple regular LM to expressing all enumerable semimeasures over strings by emulating probabilistic Turing machines. For this, in the RNN case, we require unbounded precision and  activation functions rather than fixed precision and Heaviside activations.

First, note the following definition:  show that RNN LMs with rationally valued activations that are not restricted to operating in real time are Turing complete and weakly equivalent to a subset of rationally weighted PTMs. Intuitively, not operating in real time gives an LM additional computation time while storing the results of its computations in the hidden states. This is a form of CoT reasoning since we could equivalently let the LM output additional symbols that do not count toward the final output.

The ability to  certain symbols, as done by our transducer , helps make the setup of  realistic. Importantly, beyond giving the LM more computation time, CoT also allows the LM to model non-determinism in PTMs.

The representational capacity of transformer LMs has received a lot of attention in the last few years.  established that the encoder--decoder variant of the architecture is Turing complete. Since then, concurrent work has shown non-probabilistic Turing completeness of LM-oriented decoder-only variants . We extend the work to the  case.

 show that transformer and RNN LMs with CoT reasoning are at least as powerful as PTMs. Weak equivalence requires us to also prove the reverse of these two theorems, analogous to  for constant-precision Elman RNN LMs.

Under our definition, a probabilistic Turing machine has two tapes. The first is the  on which symbols from the tape alphabet can be read and written. The second is a write-only  for symbols of the output alphabet. In the beginning, the processing tape contains the designated  symbol in the leftmost cell while all other cells contain the blank symbol . The output tape is empty at the beginning. Starting in the initial state , at each time step , a transition is sampled out of all available transitions for the given state  and the current working tape symbol , and then applied. The sampling happens according to the transition probability  (recall that the transition weights  are non-negative and sum to 1 for each pair of state  tape symbol ). We write transitions as , where , , and , with the following interpretation. When the machine is in state  and its head is reading  on the working tape, it moves to state , writes  to the working tape, writes  to the output tape if  or nothing if , and it moves the head on the working tape by one symbol in the direction , i.e., to the left (), or to the right (). As with any non-deterministic machine, the above definition naturally gives rise to the notion of a tree of possible computations . A   of the computation tree of a PTM is a sequence of consecutive transitions

We say that a branch is  if the branch reaches the final state . The  of an accepting computation branch% is the sequence of symbols  written on the output tape at that point in the computation, i.e., a concatenation of the (non-) symbols , where  is the number of transitions in the branch. The  of an accepting branch is the product of the weights of its transitions, i.e.,=-1

We denote the branches that yield a given string  by . The sum of weights of all branches that yield a certain string  is the  of that string, defined as

This definition gives rise to a semimeasure over strings whose sum over all possible strings is exactly the halting probability of the PTM, i.e., the probability that starting from the initial state ,  reaches a final state 

Our definition of  is similar to that of , but, unlike theirs, our machine must end its computation in the final configuration rather than just empty the stack. We write transitions  as  which represent a move with weight  from state  to  while scanning or outputting , popping the symbol  from the stack and pushing the sequence of symbols .

A   is called  if for every ,  and , there is at most one transition  with . Additionally, if there is a transition  such that , then there is no transition  with . Intuitively, there exists at most one next move given any configuration in a deterministic . A   of a  is a sequence of configurations and transitions,

where  is the confguration reached by taking transition  from configuration  for any . If  and , we call  . The  of an accepting run is , where  is the symbol scanned by . The  of an accepting run, , is the product of the weight of its transitions,

where  is the weight of transition . We write  for the set of all accepting runs with yield . The sum of the weights of all accepting runs of some   that yield a certain string  is called the  and is defined as

We use a definition similar to that of , which assumes without loss of generality that transitions are determined by the current state and the top symbol of the first stack only. See  for a proof. We write transitions as , which represent a move with weight  from state , with the top of the first stack , to state , while scanning or outputting , popping  and  from the first and second stack, respectively, and pushing  and  to the first and second stack, respectively. A  is called  if, for every state  and top symbol  of the first stack, the weights of the transitions define a probability distribution, i.e.,

As before, we define a  in the  as a sequence of consecutive transitions.  A run is called  if it ends in a final configuration. The yield of an accepting run is the sequence of symbols  that the  scans (or outputs) during the run. And the  of an accepting run, , is the product of the weight of its transitions,

where  is the weight of transition . Finally, the  of string  is defined as the sum of the weights of all accepting runs that yield , i.e., :

Transformer LMs are LMs whose conditional distributions  are computed by a . A transformer is a composition of multiple transformer , each of which implements the . We give definitions of these building blocks in what follows. Our formalization and notation closely follows .

We use bold unitalicized letters such as  to denote real-valued vectors and italicized letters  for their entries. Capital bold letters such as  denote matrices. All vectors are  vectors unless transposed. We define the vertical stacking operator , which denotes the vertical concatenation of the -dimensional  vectors  into a -dimensional vector  and the concatenation of the -dimensional  vectors  into a matrix  with  rows and  columns. Given the matrix , we write  for the submatrix composed of the first  rows. We call a function  whose purpose is to evaluate the compatibility of two vectors a . A   maps vectors in  to  probabilities. Here,  is the -dimensional probability simplex. This notation is summarized in .

The attention mechanism works as follows. It takes a  vector  and two matrices: The matrix  of  and the matrix  of  and computes a weighted average of the value vectors based on the compatibilities of the key vectors to the query vector, as scored by a scoring function . A formal definition is given below.

Attention weights are computed by normalizing the scores . The choice of the projection function  determines the type of attention and has concrete implications on representational capacity . We focus on the  projection function.

A transformer layer uses the attention mechanism to compute augmented representations  of the input representations . The query , the keys , and values  are all transformations of the input representations .

The functions  are usually implemented as linear transformations and  as an MLP. Some theoretical literature, however, also considers more general function classes, e.g., all smooth functions .

Without further modification, the transformations applied by the transformer layer are position-invariant, which necessitates the addition of explicit positional information.

Multiple transformer layers are stacked into a transformer, which computes the (deep) contextual representations of all symbols in the string.

A transformer computes the contextual representations of the symbols  as

If  is clear from the context or arbitrary, we will omit it as an argument to  and just write .

So far, we have only defined how the transformer architecture can be used to compute the contextual representations of the symbols. To complete the definition, we define a transformer  as follows.

WFSTs are thus a special class of (weighted) finite-state automata that operate over two alphabets. Just like s are a generalization of unweighted FSAs, WFSAs represent a more general class of machines than unweighted FSTs, such as the ones used in the definition of regular reducibility (cf. ). The weighted versions will prove useful when talking about various aspects of equivalence with regular reducibility. As a special case of weighted finite-state automata, WFSTs compute weights of their inputs---in this case, weights of  of strings (the inputs and their outputs):

Of interest are also marginal sums of the inputs, i.e.,

We will use WFSTs as building blocks in the exposition of regular reducibility (cf. ). In this subsection, we outline some operations on the computational models that will be particularly useful.

The  of two WFSTs results in a WFST that, similarly to function composition, maps the strings with the first WFST, passes them to the second one, and returns the (weight of the) output of the second transducer . More formally, given the WFSTs  and , their composition  computes

Intuitively,  computes the weight of all the possible ways of mapping  to  by first mapping  to some  and then mapping that  into . The WFST  can be computed from  and  in time  .

Any WFST can be  onto an (input or output) weighted finite-state  (WFSA) that computes the cumulative weights of individual input or output strings . Given a WFST , its input projection is the automaton  that computes

The output projection automaton is defined analogously. While it might not be immediately obvious that  represents the computations of a WFSA,  can be easily constructed by ignoring the output labels on the transitions (and additively merging any transitions that become identical after the removal of the output labels). That is:  transforms a given  into a trivial WFST that implements the identity function . Concretely, given the  , its lifted WFST  defines the transitions

It is easy to see that  computes

Lifting is useful when one wants to compose a  with a WFST, as is the case when discussing regular reducibility.

The  WFST of some WFST  is the WSFT  that maps the outputs of  to their inputs.  is defined as

i.e., it is composed of transitions from  with their input--output labels flipped.

Before we proceed to the full proof of , we first show the following simple but useful lemma.

This allows us to show .

Next, we show that, for every PTM, its induced LM can be encoded in an unbounded precision CoT RNN LM.

We begin with a general lemma about the disjunction of one-hot encodings.

We now describe the two layers of the transformer that compute the quantities required to be able to determine the current configuration of the PTM at each time step. Here, we heavily rely on the construction by .

Let  be a rationally-weighted PTM. Let  and define

as the position of the head of the PTM at time . Further, define the set

 contains the time steps at which the PTM  so far visited (and thus wrote to) the tape cell read by  at time . Then, define  as

In words,  denotes the time step at which the PTM  last visited (and thus wrote on) the tape cell read by  at time  if this cell was visited yet. Otherwise,  equals .

Now, define the  symbol over the augmented alphabet  as

Let , , , and  be the sequences of states visited, symbols written on the processing tape, symbols written on the output tape, and actions performed by  in the first  steps. Define ,

and

These will be the inputs to the transformer layers and thus,

With this, we can describe and prove the correctness of the first layer of the transformer we are building.

    The performance of modern language models (LMs) has been improved by hain-f-hought (CoT) reasoning, i.e., the process of generating intermediate results that guide the model towards a final answer.     A possible explanation for this improvement is that CoT reasoning extends an LM's computational power, as RNNs and transformers with additional scratch space are known to be Turing complete.     Comparing LMs to Turing machines, however, introduces a category error---Turing machines decide language membership, whereas LMs define  over strings.     To bridge this gap, we formalize CoT reasoning in a probabilistic setting.     We present several results on the representational capacity of recurrent and transformer LMs with CoT reasoning, showing that they can represent the same family of distributions over strings as  Turing machines.=-1

cotdistributionsprobabilistic0.5em\includegraphics\hspace\parboxIntroductionintroductionwei2023chainchain-of-thoughtWe use the more general term CoT \emph over the original term CoT \emph as prompting is just one way to elicit CoT reasoning \citep.notnye2021work,wei2022emergent,wei2023chain, suzgun2022challenging,kojima2023large,wies2023subtaskfeng2023revealingperez-etal-2021-attention,merrill2024the,feng2023revealinglanguage modelAn alphabet is a finite, non-empty set.recognitionperez-etal-2021-attentionadditional symbolsequivalenceLMs are treated as  models of computation, i.e., they assign weights to strings rather than deciding language membership, and     probabilisticlanguage model equivalence is defined on the output string level, and the analysis takes into account the additional information required to be encoded in additional outputs to achieve the equivalence. probabilisticregular reducibilitywithoutdeterministicsvete-cotterell-2023-recurrentPreliminariessec:preliminariesLanguage Modelslanguage-modelsformal languagestringdiscrete semimeasurebawens2013, icard2020calibratinggmprobability measureThis definition differs from \citeposs who instead define semimeasures over \emph.     A  (LM)  is a semimeasure over .     If  is a probability measure, it is called a  language model. def:lmlanguage modeltight     Two LMs  and  are  if  for all . def:lm-equivalenceweakly equivalent      \pLM\left(\str\right) \defeq \pLM\left(\eos\mid \str\right) \prod_{\tstep = 1}^{|\str|} \pLM\left(\sym_\tstep\mid\str_{<\tstep}\right). eosdu-etal-2023-measureeq:autoregressive-lmless thanvectoriallanguage encoderchan-etal-homotopies A  LM is any LM that can be written as an autoregressive language model () where the conditional distributions over the next symbol  are given by=-1

where  is a language encoder,  is an output matrix, and  is a projection function.def:repr-lmrepresentation-basedeq:autoregressive-lm         \pLNSM\left(\eossym_\tstep \mid \strlt\right) \defeq \projfuncEosalphabetminus(\outMtx \; \enc\left(\strlt\right))_{\eossym_\tstep}, A common choice for  is the softmax. Since our analyses use rational arithmetic, we instead opt for sparsemax \citep. However, all of our results can be extended to the use of the more common softmax function through the use of log activations and the extended real numbers  \citep.Regular Language Modelssec:wfsas     A  () is a tuple  where  is an alphabet,  is a finite set of states,  is a finite set of weighted transitions     where we write transitions   as ,     and  are functions that assign each state its initial and final weight, respectively.     Moreover, for all states ,  and  satisfy , and . def:stochastic-wfsaprobabilistic finite-state automatondeterministicIn this paper, we do \emph distinguish between a transition for a given symbol with weight  and the absence of a transition for that symbol. That is, we assume there always exists a transition  for any  and , albeit possibly with . Such a choice turns out to be useful in our technical exposition. initial statefinal statepath     \!{\sym_1}{w_1}{{\sym_2}{w_2}{\stateq_3} \!\cdots\! }{\sym_{\pathlen}}{w_{\pathlen}}{\stateq_{\pathlen + 1}}}. yieldprefix weightweight          \prefixweight(\apath)\defeq \prod_{\idx = 0}^\pathlen w_\idx,

        \weight(\apath)\defeq \prod_{\idx = 0}^{\pathlen+1} w_\idx,     stringsum      \automaton \left( \str \right) \defeq \sum_{\apath \in \paths\left( \automaton, \str \right) }  \weight \left( \apath \right).

    An LM  is a  LM if there exists a   whose induced language model  is weakly equivalent to .=-1 regularPFSAs and non-determinism.mohri-1997-finite, Buchsbaum1998Computing string probabilities under non-determinism.eq:autoregressive-lmimplicitlyicard2020calibratinggmOther examples of such models include hidden Markov models, which are equivalent to \pfsaAcrs \citep.svete-cotterell-2023-recurrentadditivelyNote that RNN LMs with linearly bounded precision can simulate non-deterministic \pfsaAcrs in real-time by encoding a probability distribution over the \pfsaAcr's current state in the hidden state of the RNN \citep.Regular Functionssec:regfnoutputs     A  () is a -tuple , where  is a finite set of ,  is an alphabet of ,  is an alphabet of ,  are sets of initial and final states, respectively, and  is a set of transitions. def:transducerfinite-state transducerstatesinput symbolsoutput symbols     A  is a relation  that is representable by an .     If  is a (partial) function, it is called a .=-1 regular relationregular functionTuring Machinessec:turing-machines     A  (PTM) is a two-tape machine with a working tape and an output tape specified by the -tuple , where  is a finite set of states,      is an output alphabet,  is a tape alphabet including the blank symbol ,     are the initial and final states, and  is a rationally weighted transition relation.       and  signify the PTM head moving left () or right () on the tape after a transition.     We write transitions  as .     Moreover, we require that for any given , the weights satisfy .=-1 def:ptmprobabilistic Turing machineFor more details, see \cref.Remark 2.2nowak-etal-2023-representationalhopcroft01nowak-etal-2023-representationalsec:twopdaRecurrent Neural Language Modelssec:rnnlmsThroughout this paper, we will focus on Elman RNNs \citep as they are the easiest to analyze and a special case of more common networks, e.g., those based on long short-term memory \citep and gated recurrent units \citep.     An   is an RNN with the following hidden state recurrence:

    where  is the state vector%     at time step ,  is an initialization parameter,  is the input symbol at time step ,  is a symbol representation function,  and  are parameter matrices,  is a bias vector, and  is an element-wise, non-linear activation function.%def:elman-rnnElman RNN             \hiddStateZero & = \initstate  \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\,\,\,{(t=0)},               \\             \hiddStatet    & = \activation\left(\recMtx \vhtminus + \inMtx \inEmbedSymt + \biasVech \right) \,\, {(t>0)},         Throughout this paper all vectors are column vectors.\looseness=-1Common examples of  include the Heaviside function , the sigmoid function , and the .eq:elman-update-rule A representation-based LM is called an  if its representation function is defined by the hidden state of an Elman RNN . def:elman-lmElman LMsparsemaxsparsemax     {} \defeq \argmin_{\rvz\in \Simplexnminus} ^2_2. identityNeural Networks and Numerical Precisionsec:precision     The   of a representation-based neural LM is the number of bits required to represent the entries of :

    We say that a representation-based LM is of     precision          \defeq \max_{\idxd\in[\hiddDim]}\min_{{q}=\enc\left(\str\right)_{\idxd}}} \lceil\log_2 p\rceil + \lceil\log_2 q\rceil.      if , i.e., if  for all  and some ,         constant precision if , i.e., if there exist  and  such that for all  with , ,         logarithmically bounded precision if , i.e., there exist  and  such that for all  with , , and         linearly bounded precision if  cannot be bounded by a function of .     unbounded precisionsvete-cotterell-2023-recurrentweiss-etal-2018-practicalmerrill-2019-sequentialmerrill-etal-2020-formalsiegelmann-sontag-1992,nowak-etal-2023-representationalTransformer Language Modelssec:transformersapp:transformerssvete-etal-2024-transformerslayersattention mechanism\underlineuery\underlineeys\underlinealueshao-etal-2022-formal,svete-etal-2024-transformershard attention is computed with the  projection function

    for , where  and  is the cardinality of the argmax set. Hard attention         \hardmaxAvg\left(\vx\right)_\idxd \defeq              {m} & \ifcondition \idxd\in \argmax\left(\vx\right) \\             0           & \otherwisecondition,         casesFor more details, see \crefCoT Reasoning and Weak Equivalencesec:cot-equivalenceEquivalence and Augmented Alphabetslargerperez-etal-2021-attentionnotperez-etal-2021-attentionmerrill2024theinput string\textitsiegelmann-sontag-1992,perez-etal-2021-attentionweiss-etal-2018-practical,nowak-etal-2023-representational,svete-etal-2024-lowerRegular Reducibilitysec:rrnot     An LM  over  is  to a LM  over  if there exists a regular function      such that  is weakly equivalent to .=-1 def:reg-reduceregularly reducible We say an alphabet  satisfies the  for an alphabet  if  for some alphabet . -augmentation conditionWhile in practice the CoT and the final output come from the same alphabet, we use separate ones for ease of notation and analysis. This is without loss of generality; see \cref.A non-length-increasing homomorphism is a function  where ,  for  and , and that further satisfies . (q0) ;

        (q0) edge node (q0) ;     state, initial, accepting-Properties of Regular Reducibilitysec:rr-propertiessec:cot-lmsnon-deterministicsec:wfsas     Let  be a .     Then, there exists a   over the alphabet  and with the state space  that is regularly reducible to . thm:rr-pfsadeterministic     See . sec:rr-proofssec:rr-proofsthm:rr-pfsasimulatethm:rr-pfsadecreasessvete-cotterell-2023-recurrentsec:cot-equivalenceRegular Reducibility and CoT Reasoningsec:cot-lms Given alphabets  and  where  satisfies the -augmentation condition,      an LM  over  is called a  induced by LM  over  if  is regularly reducible from .     That is, there exists a regular function  such that  def:cot-lmCoT-augmented LMdef:cot-lmfig:exampleeq:pfsa-summation      \pLMcot(\str) = \sum_{\strx\in \left(\str\right)} \pLM(\strx).  sec:tm-turing-completeCoT and Representational Capacitysec:main-resultssec:cot-equivalencesec:nondet-minskynon-determinismsec:rnns-cotsec:tm-turing-completeNeural LMs and formal models of computation.generatorsicard2020calibratinggmpathsNeural LMs and Regular LMssec:nondet-minskyRecurrent Neural LMsMinsky1954svete-cotterell-2023-recurrentdeterministicThe relationship between RNN LMs and \emph-deterministic \pfsaAcrs was explored by \citet, albeit with linearly bounded-precision RNNs.trajectoriesRecall that non-determinism means that multiple possible transitions to different states can yield the same symbol (\cref).the distribution induced by any  can be generated by a constant-precision RNN LM with CoT, and     any CoT-augmented constant-precision RNN LM can be emulated by a .

    For any regular LM, there exists a weakly equivalent CoT-augmented constant-precision Elman RNN LM. thm:rnn-pfsa     We show how, using the RNN's recurrence and output sampling step, we can implement the transition function of any .     The RNN starts by sampling an output symbol containing an initial state  according to the initial distribution  without emitting a language symbol.     This output gets fed back into the RNN at the next time step, allowing it to  the sampled state, and the next symbol--state pair is sampled according to the conditional distribution defined by the output state, as illustrated by .     Finally, the states in the generated string are removed by a regular function, leaving only the language output.     See  for the detailed proof.=-1 readfig:rnn-minskyapp:proofs     For any constant-precision CoT-augmented Elman RNN LM, there exists a weakly equivalent . thm:pfsa-rnn-6pt     See . app:proofsthm:rnn-pfsa,thm:pfsa-rnnconstant-precisionTransformer LMsthm:rnn-pfsa     For any regular LM, there exists a weakly equivalent CoT-augmented constant-precision transformer LM. thm:pfsa-transformer     We use the construction from  which shows how to encode -gram LMs in a transformer.     Here, the alphabet of the transformer is augmented with the states of the , i.e., the symbol at each position  contains not only an output symbol but also the current state of the  at time .     Thereby each input symbol contains all the information required to compute the next-symbol probabilities (it is effectively a unigram LM).     Because the next symbol probabilities in a  only depend on the current state, the transformer does not need to use attention at all and can rely solely on the output of its residual connections.     See  for the detailed proof. svete-etal-2024-transformersapp:proofsNeural LMs and PTMssec:lms-cotRecurrent Neural LMssec:rnns-cot     An autoregressive LM is called a  if it never outputs empty symbols (). real-time LMnowak-etal-2023-representationalOn the other hand, some CoT-augmented LMs do operate in real time but over an extended alphabet; see \cref.erasenowak-etal-2023-representational     For every LM induced by a non-deterministic probabilistic Turing machine, there exists a weakly equivalent CoT-augmented RNN LM without unbounded precision.  thm:cot-rnn-ptm The proof follows  probabilistic version of the proof by , but extended to account for CoT reasoning.     For the ability to simulate  PTMs rather than just a subset, we augment the output alphabet of the RNN with enough information about the current PTM configuration so that subsequent steps can uniquely identify the previous action taken.      Concretely, the output alphabet  contains information about the current state, the symbol written on the working tape of the PTM, and the head action performed.     This additional information is then removed by our regular function at the end, yielding only the output of the simulated PTM generated according to its probability.     A detailed proof is presented in .=-1 nowak-etal-2023-representationalsiegelmann-sontag-1992allapp:proofsTransformer LMssec:tm-turing-completeperez-etal-2021-attentionmerrill2024the,feng2023revealingprobabilistic     For any PTM-induced LM, there exists a weakly equivalent unbounded-precision CoT-augmented Transformer LMs. thm:cot-transformer-ptm     The proof follows  but is adapted to the probabilistic case.     Again, the main idea is to augment the output alphabet with enough information about the current PTM configuration to reconstruct the probabilities of possible actions at each time step.     This information and appropriate positional encodings are enough to recover the PTM's current configuration and thus the next-action distribution, allowing us to construct a weakly equivalent transformer LM.     A regular function then removes the additional information from the string.     See  for details. perez-etal-2021-attentionapp:proofsWeak Equivalencethm:cot-rnn-ptm,thm:cot-transformer-ptmthm:pfsa-rnn-12pt     For any rationally valued RNN LM and transformer LM with CoT reasoning, there exists a weakly equivalent PTM. thm:ptm-cot-rnn-transformer     RNN LMs define enumerable semimeasures , which can always be expressed by a PTM .     Following the same reasoning, transformer LMs define enumerable semimeasures as well; the probability of a string  is defined as the sum probabilities of all runs of the transformer which result in the output , of which there are countably many.     Finally, there is only a countable number of ways a regular function can transform any string, so RNN LMs and Transformer LMs with CoT still define enumerable semimeasures.=-1 App. Gnowak-etal-2023-representationalThm. 3icard2020calibratinggmConclusionLimitationsunboundedcomputational stepsperez-etal-2021-attentionyao-etal-2021-self,merrill-etal-2022-saturated,merrill-sabharwal-2023-parallelismchiang-cholak-2022-overcoming,merrill2024theEthics Statementanthology, customacl_natbibDiscussionsec:discussionprobabilisticthm:rnn-pfsaprobabilisticFor a comparison of these complexity classes, see e.g. \citet.Related Worksec:relatedTuring completeness of RNNs.e.g.,\textitMcCulloch1943,Kleene1956,siegelmann-sontag-1992,hao-etal-2018-context,DBLP:journals/corr/abs-1906-06349,merrill-2019-sequential,merrill-etal-2020-formal,hewitt-etal-2020-rnns,Chung2021,merrill-etal-2022-saturated,merrill2022extracting,svete-cotterell-2023-recurrent,nowak-etal-2023-representationalsiegelmann-sontag-1992,Chung2021nowak-etal-2023-representationalTuring completeness of transformers.perez-etal-2021-attentionbhattamishra-etal-2020-computationalbhattamishra-etal-2020-computationalfiniteregressionICLRegressionmerrill2024theperez-etal-2021-attentionfeng2023revealingdu-etal-2023-measuretighteq:repr-lmAdditional Preliminariesbosdef:lmTuring Machinesapp:def-tmAdding another tape does not increase the computational power of a Turing machine \citep.processing tapeoutput tapep. 48sipser13branch     (\qinit, \bot)  (\stateq_2, \tapesym_1^\prime), (\stateq_2, \tapesym_2)  (\stateq_3, \tapesym_2^\prime), \cdots (\stateq_\pathlen, \tapesym_\pathlen)  (\stateq_{\pathlen+1}, \tapesym_{N}^\prime) acceptingThe final state has no outgoing transitions.yieldWe only consider branches that end in a final state when discussing the yield and weight of branches. This means we only take into account finite instances of computation.weight     \weight(\apath) \defeq \prod_{n=0}^\pathlen w_n. stringsum     \tm(\str)\defeq \sum_{\apath\in\paths(\tm, \str)} \weight(\apath). The halting probability is the probability that the execution of the PTM will end in a halting state after finitely many steps.     \sum_{\str\in}\tm(\str) = (\tm ). Pushdown Automatasec:pda     A  () is a tuple  where  is a finite set of states,  is the input alphabet,  is the stack alphabet,  is a finite set of weighted transitions, and  and  are called the initial and final configuration, respectively.     Moreover, for all states  and stack symbols ,  satisfies . def:stochastic-wpdaprobabilistic pushdown automatonabney-etal-1999-relating     A   is a pair containing the current state and the current contents of the stack. configurationdeterministicrun     {\stateq_0}, \atrans_1, {\stateq_1}, \ldots, \atrans_N, {\stateq_N}, acceptingyieldweight     \weight\left(\apath\right) \defeq \prod_{n=1}^N w_n, stringsum     \pushdown\left(\str\right) \defeq \sum_{\arun\in\runs\left(\pushdown,\str\right)} \weight\left(\arun\right). Two-stack Pushdown Automatasec:twopda     A  () is a machine specified by the -tuple ,     where  is a finite set of states,  is an alphabet of input symbols,  is an alphabet of stack symbols, including the bottom-of-stack symbol ,  is a finite set of rationally-weighted transitions, and  and  are the initial and the final state, respectively. two-stack pushdown automatonnowak-etal-2023-representationalAppendix Bnowak-etal-2023-representationalprobabilistic     \sum_{{\stacksym}{\sym}{\stateq^\prime}{\stacksym_1}{\stacksym_2}{\stacksym_3/w}{\stacksym_4} \in \trans} w = 1. runacceptingweight     \weight\left(\apath\right) \defeq \prod_{n=1}^N w_n, stringsum     \pushdown\left(\str\right) \defeq \sum_{\arun\in\runs\left(\pushdown,\str\right)} \weight\left(\arun\right). Transformer Language Modelsapp:transformerstransformerlayersattention mechanismsvete-etal-2024-transformersA summary of the notation used in the paper.tab:notationNotation.columncolumnrowscoring functionnormalization functiontab:notationThe Attention Mechanism.\underlineuery\underlineeys\underlinealues     The   is defined as

    where  be a query vector and let  and  be matrices of keys and values, respectively, and 

    is the vector of normalized scores between the query  and the keys in ,  is a scoring function and  is a normalization function. def:attentionattention mechanism          \attn\left(\vq, \mK, \mV\right) \defeq \sum_{\idxn = 1}^{N} \evs_\idxn\vv_\idxn

        \vs \defeq \projfunc\left(\tfscorefun\left(\vq, \vk_1\right), \dots ,\tfscorefun\left(\vq, \vk_N\right) \right)     Attention types.hao-etal-2022-formalhard attention is computed with the  projection function:

    for , where  and  is the cardinality of the argmax set. def:hard-attentionHard attention         \hardmaxAvg\left(\vx\right)_\idxd \defeq              {m} & \ifcondition \idxd\in \argmax\left(\vx\right) \\             0           & \otherwisecondition         cases A  (MLP)  is a function defined as the composition of elementary functions  

where each function  for  is defined as

where  is a square weight matrix specific to layer ,  is a bias vector, and  is an element-wise non-linear activation function. The function  is called the , the function  is called the , and the function  for  are called .multi-layer perceptron     \fTransf\left(\vx\right) \defeq \vfunc_L \circ \vfunc_{L-1} \circ \cdots \circ \vfunc_1 \left(\vx\right),

    \vfunc_\ell(\vx) &\defeq \mlpActivation\left(\mW_\ell\vx + \vb_\ell \right)\quad \ell\in  \\     \vfunc_L(\vx) &\defeq \mW_L\vx + \vb_L, input layeroutput layerhidden layersNote that we refer to MLPs by the number of hidden layers, e.g., a one-layer-MLP is an MLP with one \emph layer.\looseness=-1 The Transformer Architecture.     Given query, key, value, and  functions ,     a  is a function  that computes

    for  where

    Here, we define      For simplicity, we do not include layer normalization. def:transformer-layer\underlineutputtransformer layer          \tflayer\left(\vx_1^\top; \cdots; \vx_\strlen^\top\right) = \left(\vz_1^{\top}; \cdots;  \vz_\finaltstep^{\top}\right) \in \R^{\finaltstep \times \hiddDim}                   \va_\tstep              & \defeq \attn\left(\vq_\tstep, \mK_\tstep, \mV_\tstep\right) + \tflayerinputsy_\tstep &  & \in \R^\hiddDim                           \\             \tflayeroutputsy_\tstep & \defeq \oTransf\left(\va_\tstep\right) + \va_\tstep                                  &  & \in \R^\hiddDim.  

            \vq_\tstep & \defeq \qTransf\left(\vx_\tstep\right)                                                            &  & \in \R^\hiddDim                  \\             \mK_\tstep & \defeq \left(\kTransf\left(\vx_1\right)^\top; \cdots; \kTransf\left(\vx_\tstep\right)^\top\right) &  & \in \R^{\tstep \times \hiddDim}  \\             \mV_\tstep & \defeq \left(\vTransf\left(\vx_1\right)^\top; \cdots; \kTransf\left(\vx_\tstep\right)^\top\right) &  & \in \R^{\tstep \times \hiddDim}.         Note:hahn-2020-theoretical     A symbol  is a function  and a  is a function .     A  representation function  (with ) is defined as     def:positional-encodingsrepresentation functionpositional encodingposition-augmented          \defeq \left(; \right).

    A   is a function  defined for any  as     def:static-representationsstatic encoding         \staticRepr\left(\str\right) \defeq \left(^\top; \cdots; ^\top\right).

    For , an -layer   is defined as

    where  for  are transformer layers and   is a static encoding. def:transformertransformer           \defeq \tflayer_\tfnumlayer \circ \cdots \circ \tflayer_1 \circ \staticRepr,

    \left(\vz_1^{\top}; \cdots; \vz_\strlen^{\top}\right) \defeq\left(\vx_1^{\tfnumlayer\top}; \cdots; \vx_\strlen^{\tfnumlayer\top}\right) \defeq  \tf\left(\staticRepr\right)\left(\str\right).

    Given a transformer , a inal representation transformation function , and a string  with ,     we define the   as=-1

    where  is the representation of the  symbol in  computed by , i.e., . def:encfencoding function          \tfencfun\left(\str\right) \defeq \fTransf\left(\vz_{\strlen}\right)     Transformer Language Models.language model     A   is the representation-based autoregressive LM with the representation function  from .     That is,  defines the conditional probability distributions     def:transformer-plnsmtransformer LMeq:enc          \tfpLM\left(\symt \mid \strlt \right) \defeq {\symt}.     Weighted Finite-state Transducerssec:transducers     A (rational-) (WFST) is the tuple  where  and  are the input and output alphabets, respectively,  is a finite set of states,  is a finite set of weighted transitions     where we write transitions   as ,     and  are functions that assign each state its initial and final weight, respectively. weighted finite-state transducerdef:reg-reducepairs     \wfst\left(\str, \strx\right) \defeq \sum_{\apath \in \paths\left(\wfst, \left(\str, \strx\right)\right)} \weight\left(\apath\right).

    \wfst\left(\str\right) \defeq \sum_{\strx \in } \wfst\left(\str, \strx\right). Operations on WFSTssec:rrComposition.composition10.7551/mitpress/3007.003.0017      \left(\wfst_2 \circ \wfst_1\right)\left(\str, \strx\right) \defeq \sum_{\strz \in } \wfst_2\left(\strz, \str\right) \cdot \wfst_1\left(\strx,\strz\right). eq:wfst-compositionMohri2009Projecting a WFST.projectedautomatonA weighted finite-state automaton is a generalization of a probabilistic finite-state automaton where the weights do not have to form probability distributions. We also extend the definition of WFSAs to allow -transitions, i.e. with symbols from .  Note that every such WFSA can be converted into a weakly equivalent WFSA without -transitions. Mohri2008      \wfsa_{}\left(\str\right) \defeq \sum_{\strx \in } \wfst\left(\str, \strx\right). eq:input-projection     \trans_ = {\sym}{\sum_{{\sym:\symx}{w}{\stateq^\prime} \in \trans} w}{\stateq^\prime} \mid \stateq, \stateq^\prime \in \states, \sym \in \epsalphabet}. Lifting a \pfsaAcr.Lifting     \trans_\wfst \defeq {\sym:\sym}{w}{\stateq^\prime} \mid {\sym}{w}{\stateq^\prime} \in \trans}.

    \wfst_\wfsa\left(\str,\str^\prime\right) =  . Inverting a WFST.inverted      \defeq {\symx: \sym}{w}{\stateq} \mid {\sym: \symx}{w}{\stateq} \in \trans}, Separation of Alphabetsapp:alphabets     \regfn(\stry\,\sep\,\strz) \defeq \strz thm:cot-rnn-ptmProofs: Regular Reducibilitysec:rr-proofsNondeterminism in \pfsaAcrs     Given the  , we want to show the existence of a   over the alphabet  that is regularly reducible to .     We construct the deterministic   as follows.

    We first prove that  is probabilistic and deterministic.

    We now show that  is indeed regularly reducible to .     The alphabet  clearly satisfies the -augmentation condition.     We define the regular function , an instance of the general function described in .     We will show that  is weakly equivalent to , or, equivalently, that  is weakly equivalent to .

    Since  is an  of strings in , we first transform it into weighted finite-state   implementing the mapping     for .     This is a simple WFST with transitions .      can then be composed with the weighed version of the  implementing , :

     then computes, for ,

    We then turn  into a  in the standard way by projecting the WFST onto the transition input labels .     This results in a  that assigns the string  the probability that equals the sum of all possible mappings of  to any :

    This shows that  is weakly equivalent to , which finishes the proof. deterministic             \trans^\prime                            & \defeq {\left(\sym^\prime, \stateq^\prime\right)}{w}{\left(\sym^\prime, \stateq^\prime\right)} \mid {\sym^\prime}{w}{\stateq^\prime} \in \trans, \sym \in \alphabet} \\             \initf^\prime\left(\sym, \stateq\right)  & \defeq {\nsymbols}, \quad \forall \sym\in\alphabet, \stateq\in\states                                                                               \\             \finalf^\prime\left(\sym, \stateq\right) & \defeq \finalf\left(\stateq\right), \quad \forall \sym\in\alphabet, \stateq\in\states         :               We compute

              and, for any ,

        Probabilistic                       \sum_{\left(\sym, \stateq\right) \in \alphabet \times \states} \initf^\prime\left(\sym, \stateq\right)                        & = \sum_{\left(\sym, \stateq\right) \in \alphabet \times \states} {\nsymbols} \\                        & = \sum_{\sym \in \alphabet} \sum_{\stateq \in \states} {\nsymbols}           \\                        & = \sum_{\stateq \in \states} \initf\left(\stateq\right)                                                       \\                        & = 1

                      \sum_{{\left(\sym^\prime, \stateq^\prime\right)}{w}{\left(\sym^\prime, \stateq^\prime\right)} \in \trans^\prime} w + \finalf^\prime\left(\sym, \stateq\right)                        & = \sum_{{\left(\sym^\prime, \stateq^\prime\right)}{w}{\left(\sym^\prime, \stateq^\prime\right)} \in \trans^\prime} w + \finalf\left(\stateq\right) \\                        & = \sum_{{w}{\sym^\prime}{\sym^\prime} \in \trans} w + \finalf\left(\stateq\right)                                                                   \\                        & = 1                   :               Let  be a state of  and  a symbol in its alphabet.               By definition of ,  uniquely determines the target state, which is identical to the symbol---.     Deterministicsec:rracceptortransducerSee \cref for a definition of weighted finite-state transducers and the operations on them.         \wfst_{\wfsa^\prime}\left(\strx, \strx^\prime\right) \defeq  

        \wfst_{} \left(\str, \strx\right) \defeq \left(\str\right)}.

            \left(\wfst_{\wfsa^\prime} \circ \wfst_{}\right) \left(\str, \strx^\prime\right)              & = \sum_{\strx \in } \wfst_{}\left(\str, \strx\right) \cdot \wfst_{\wfsa^\prime}\left(\strx, \strx^\prime\right) .} \\              & = \sum_{\strx \in } \left(\str\right)} \cdot \wfst_{\wfsa^\prime}\left(\strx, \strx^\prime\right)                                                                    \\              & = \sum_{\strx \in \left(\str\right)} \wfst_{\wfsa^\prime}\left(\strx, \strx^\prime\right)                                                                                                                \\              & = \sum_{\strx \in \left(\str\right)}                                                                                                                      \\              & = \left(\str\right)}          Mohri2008             \left(\wfst_{\wfsa^\prime} \circ \wfst_{}\right) \left(\str\right)              & \defeq \sum_{\strx^\prime \in } \left(\wfst_{\wfsa^\prime} \circ \wfst_{}\right) \left(\str, \strx^\prime\right) \\              & = \sum_{\strx^\prime \in } \left(\str\right)}                        \\              & = \sum_{\strx^\prime \in  \left(\str\right)}                                                              \\              & = \wfsa^\prime\left( \left(\str\right)\right)                                                                             \\              & = \left(\wfsa^\prime \circ \right) \left(\str\right)                                                                      \\              & = \wfsa \left(\str\right)         Nondeterminism in \ppdaAcrs     Let  be a . Then, there exists a   over the alphabet  with the state space  that is regularly reducible to . thm:rr-ppdadeterministic     Given a  , we prove the existence of a   over the alphabet  that is regularly reducible to .

    We construct the deterministic   with the set of transitions

    We now show that  is regularly reducible to .     More precisely, we define the regular function  and show that  is weakly equivalent to .

    Let  be a  that is equivalent to  .     Using grammars, rather than pushdown automata, will simplify the proof as there are well-known constructions (e.g., , ) for composing s and s.     We therefore prove that   is weakly equivalent to .

    Recall that  can be implemented as an   defined as

    Adapting Corollary 1 from  to the  case, we get

    Then, just like in the regular case, we compute  by summing over all possible  values:

    This shows that  is weakly equivalent to , thus  is weakly equivalent to .     This concludes the proof. deterministic         \trans^\prime \defeq \{{(\sym^\prime, \stateq^\prime, \stacksym^\prime)}{w}{(\sym^\prime, \stateq^\prime, \stacksym^\prime), \stackseq} \mid & {\sym^\prime}{w}{\stateq^\prime, \stackseq} \in \trans, \nonumber \\         & \sym \in \epsalphabet, \stacksym \in \stackalphabet\}.

              For any  and ,

        Probabilistic:                       \sum_{{(\sym^\prime, \stateq^\prime, \stacksym^\prime)}{w}{(\sym^\prime, \stateq^\prime, \stacksym^\prime), \stackseq}} w & = \sum_{{\sym^\prime}{w}{\stateq^\prime, \stackseq}} w \\                                                                                                                                      & = 1                    Just like in the finite-state case, let  be a state,  a stack symbol and  an input symbol.               Then  uniquely determines the next configuration.     Deterministic:Similar to the notation used for \ppdaAcrs, we use  to denote the weight of the string  under the grammar .seeSection 6.3.2 for the constructionhopcroft01barhillel-etal-1961pasti-etal-2023-intersection         \transducer_ \left(\str, \strx\right) \defeq \left(\str\right)}.     pasti-etal-2023-intersection             \left(\grammar^\prime \circ \transducer_\right) \left(\str, \strx\right) & = \grammar^\prime \left(\strx\right) \cdot  \transducer_ \left(\str, \strx\right) \\                                                                                                  & = \grammar^\prime \left(\strx\right) \cdot \left(\str\right)}.

            \left(\grammar^\prime \circ \transducer_\right) \left(\str\right) & \defeq \sum_{\strx \in } \left(\grammar^\prime \circ \transducer_\right) \left(\str, \strx\right) \\                                                                                           & = \sum_{\strx \in } \grammar^\prime \left(\strx\right) \left(\str\right)}          \\                                                                                           & = \sum_{\strx \in \left(\str\right)} \grammar^\prime \left(\strx\right)                                             \\                                                                                           & = \sum_{\strx \in \left(\str\right)} \pushdown^\prime \left(\strx\right)                                            \\                                                                                           & = \pushdown^\prime\left(\left(\str\right)\right)                                                                    \\                                                                                           & = \left(\pushdown^\prime \circ \right)\left(\str\right)                                                             \\                                                                                           & = \pushdown\left(\str\right).         Proofs: Representational Capacity of Neural LMsapp:proofsFinite-state Language Models     Let  be a .     We divide the definition of a weakly equivalent Elman LM  with the output matrix  into multiple steps.     The paper assumes a -padding of the input strings.     Since CoT-augmented LMs work over a potentially larger alphabet, the padding symbol has to be changed accordingly.     Particularly, the CoT-augmented RNN will work over the alphabet .     For simplicity, assume that any input string is padded by the symbol  for some arbitrary (but fixed) state .

    Let .     We will represent the input symbols with the one-hot representation function  that computes :     while other entries of the vector are zero.     We can define .

    Conveniently, the CoT-augmented RNN will store the current  state in its output ``symbol''.     Therefore, its next hidden state  will not, in fact, depend on the previous one ();  will only be used to compute the next-symbol--state output distribution through .     The RNN therefore only has to appropriately incorporate the input information.     With this in mind, we define the following Elman RNN parameters , and :

    where  is a -dimensional matrix of zeros,  is the -dimensional identity matrix, and  is a -dimensional vector of zeros.     Then, we define the output matrix  as

    We now show that  with  defines the same conditional distributions as , meaning that it implicitly defines the same probability distribution over strings.     As mentioned above, we assume that the input string is prefixed with the  symbol.   Let the previously generated output symbol  be  (in the first step, the ``generated'' pair is ).     We get that

    The one-hot encoding  is then used to ``index'' the appropriate column of the output matrix  as , which contains the probabilities of the  symbol--state pairs defined by the , as per :

    Passing  through the normalization function, the next state and symbol are sampled and passed as input to the RNN, repeating the update step.     Since the Elman RNN following these dynamics by constructions generates paths (sequences of states in the output symbols) with the same probabilities as the input automaton, it enumerates all accepting paths of any string  with the same probabilities as the original .     Applying the transformation  to the produced outputs and summing over sequences that yield the same string will thus result in strings sampled from the .     This finishes the proof. Notice that  is CoT-augmented by construction, as it works over the alphabet .  also satisfies the -augmentation condition.Note:Hidden states.Throughout the paper, we index vectors and matrices directly with set elements rather than integer values, meaning each index corresponds to exactly one element from the given set.\looseness=-1          _{\bossym, \stateq} \defeq 1     If any of the two arguments are empty, its corresponding component is also zero.Recurrence.eq:repr-lm             \recMtx   & \defeq \mO_\hiddDim                 &  & \recMtx \in \R^{\hiddDim \times \hiddDim} \\             \inMtx    & \defeq \mI_\hiddDim                 &  & \inMtx \in \R^{\hiddDim \times \hiddDim}  \\             \biasVech & \defeq \zero_\hiddDim \qquad \qquad &  & \biasVech \in \R^\hiddDim                       \eOutMtx_{\left(\sym^\prime, \stateq^\prime\right), \left(\sym, \stateq\right)}  & \defeq \pLM\left(\stateq^\prime, \sym^\prime \mid \stateq\right) \qquad \qquad &  & \stateq, \stateq^\prime \in \states, \quad \sym \in \alphabet_\eps, \sym^\prime \in \alphabet_\eps \\             \eOutMtx_{\left(\eps, \stateq\right), \left(\bos, \stateq_0\right)} & \defeq                                          &  & \stateq \in \states \\             \eOutMtx_{\left(\eos, \stateq\right), \left(\sym, \stateq\right)}  & \defeq  \qquad \qquad &  & \stateq \in \states, \quad \sym \in \alphabet_\eps.         Computation of string probabilities.             \hiddState_\tstep              & =  + \inMtx  + \biasVech}           \\              & =  + \inMtx  + \zero_\hiddDim} \\              & = }                                                         \\              & =          nexteq:rnn-outmtx         \left(\outMtx \hiddStatet\right)_{\left(\stateq^\prime, \eossym^\prime\right)} = \eOutMtx_{\left(\eossym^\prime, \stateq^\prime\right), \left(\bossym, \stateq\right)} =                                               & \ifcondition \bossym = \bos, \stateq = \stateq_0, \eossym^\prime = \eps \\             \pLM\left(\stateq^\prime, \eossym^\prime \mid \stateq\right) & \otherwisecondition                                         \\         .     cases     Let  be a CoT Heaviside Elman RNN LM over the alphabet  with the regular function  for an -augmented alphabet .     Let  be the underlying RNN LM over , that is,

    by .     We want to show the existence of a possibly non-deterministic   that is weakly equivalent to .     We write  to mean  is weakly equivalent to  (cf. ).

    By , there exists a   over the augmented alphabet  weakly equivalent to .     This means that

    and thus

    Just like in the proof of ,  can be computed as a composition of (lifted) WFSTs that is then projected onto the input component, resulting in a  weakly equivalent to .     This finishes the proof.

        \pLMcot & = \pLM \circ  &  &  &  & \pLM = \pLMcot \circ \regfn     def:cot-lmdef:lm-equivalenceLem. 4.1svete-cotterell-2023-recurrent         \wfsa \weakeq \pLM \weakeq \pLMcot \circ \regfn

        \pLMcot \weakeq \wfsa \circ .     thm:rr-pfsathm:pfsa-transformer     Define the following linear transformation functions:

    for all .     A transformer layer  with the parameters  and  and residual connections implements the identity function irrespective of the parameters , and :

    for all . lem:layer-identity             \vTransf\left(\vx\right) & \defeq \zero, \\             \oTransf\left(\vx\right) & \defeq \zero

        \tflayer\left(\vx\right) = \vx

    By definition of a transformer layer (cf. ),  computes     eq:transf-layer,eq:attn-block-1,eq:attn-block-2             \va                      & = \attn\left(\vq, \mK, \mV\right) + \tflayerinputsy = \sum_{\idxn = 1}^{N} \evs_\idxn\vv_\idxn  + \tflayerinputsy = \sum_{\idxn = 1}^{N} \evs_\idxn\zero  + \tflayerinputsy = \tflayerinputsy                       \\             \tflayer\left(\vx\right) & = \oTransf\left(\va\right) + \va = \zero + \va = \va  = \vx                                                                                                                                                       .         thm:pfsa-transformer     Let  be a .     Like the CoT-augmented RNN in , the CoT-augmented transformer LM will work over the alphabet .     It will start by generating a random initial state according to the initial-state distribution  upon reading the designated padding symbol .     Then, at step  of generation, it will generate the next symbol--state pair by sampling from the next-transition distribution  given the state stored in the previously generated output tuple.

    More formally, define  for some arbitrary but fixed .     Further, define the static representation function

    as in .     Let  be the identity transformer layer from  and  the transformer with the single layer .     Then, by ,

    for any .     Defining the transformation , it therefore holds that

    In words,  contains the current symbol--state pair of the .     The CoT-augmented transformer LM can therefore sample the next symbol--state pair from the conditional distribution defined by the  by setting the values of the output matrix  as in :

    Clearly, the identical conditional probabilities defined by the CoT-augmented transformer  result in strings over  sampled with probabilities equal to the path probabilities from the , as in .     Applying the transformation  to the produced outputs will thus result in strings sampled from the , giving us a CoT-augmented transformer LM weakly equivalent to .     Lastly, since all the representations in the model are position-invariant, the constructed transformer is of constant precision. thm:pfsa-rnn         \staticRepr\left(\left(\bossym, \stateq\right), \tstep\right) \defeq  \in ^{|\bosalphabet_\eps| \nstates}     eq:one-hotlem:layer-identitylem:layer-identity         \tf\left(\staticRepr\right)\left(\left(\bos, \stateq_0\right), \left(\sym_1, \stateq_1\right), \ldots, \left(\sym_{\tstep - 1}, \stateq_{\tstep - 1}\right)\right) = \left(^\top; ^\top; \cdots; , \stateq_{\tstep - 1}}^\top\right)

        \tfencfun\left(\left(\bos, \stateq_0\right), \left(\sym_1, \stateq_1\right), \ldots, \left(\sym_{\tstep - 1}, \stateq_{\tstep - 1}\right)\right) = \fTransf\left(\vx_{\tstep-1}^1\right) = , \stateq_{\tstep - 1}}.     thm:rnn-pfsa              \eOutMtx_{\left(\sym^\prime, \stateq^\prime\right), \left(\sym, \stateq\right)}  & \defeq \pLM\left(\stateq^\prime, \sym^\prime \mid \stateq\right) \qquad \qquad &  & \stateq, \stateq^\prime \in \states, \quad \sym, \sym^\prime \in \alphabet \\             \eOutMtx_{\left(\eps, \stateq^\prime\right), \left(\bos, \stateq_0\right)} & \defeq                                          &  & \stateq \in \states \\             \eOutMtx_{\left(\eos,\stateq\right),\left(\sym,\stateq\right)} & \defeq  & & \stateq\in\states, \sym\in\epsalphabet.         thm:rnn-pfsaProbabilistic Turing Machine Language ModelsRNN LMs with CoT reasoning.     We rely mainly on existing results by  but use the additional information afforded by the augmented output alphabet, resulting in a simpler construction and interpretation.     By , the LM families induced by PTMs and probabilistic  are weakly equivalent, so it suffices to show that a CoT RNN LM can encode any given .

    We first reiterate their main result together with its condition.      shows that an RNN LM with output alphabet  can simulate any probabilistic  over  if it is -deterministic.     Let  be an arbitrary probabilistic , with .     Now define another  , which generates outputs over the extended alphabet .     Define the transitions, add the following transitions to  in :

    The constructed  satisfies the required -determinism condition, meaning that we can construct a weakly equivalent RNN LM with  outputs.     Finally, define the following regular function  that removes all additional information post-hoc:

    The RNN LM therefore directly simulates runs from the  .     Applying  to the outputs of the RNN LM results in strings in .     Since the transformation groups all execution runs that output  and the resulting CoT-augmented LM  (cf. ) sums over them, we are left with a weakly equivalent LM. nowak-etal-2023-representationalProp. 3.1 and Thm. 3.1nowak-etal-2023-representational         A   is called  if, for any current state  any top symbol on its first stack  and any output symbol from , there is at most one transition with non-zero weight.     -deterministic         Every -deterministic probabilistic  can be encoded in an RNN LM that can output empty tokens .     Thm 3.2nowak-etal-2023-representationalThm. 3.2nowak-etal-2023-representational          \trans^\prime \defeq {\stacksym}{(\stateq, \stacksym_1,\stacksym_2,\stacksym_3,\stacksym_4, \sym)}{\stateq^\prime}{\stacksym_1}{\stacksym_2}{\stacksym_3/w}{\stacksym_4} \mid  {\stacksym}{\sym}{\stateq^\prime}{\stacksym_1}{\stacksym_2}{\stacksym_3/w}{\stacksym_4} \in \trans}                \regfn(\symx) \defeq              \symy & \ifcondition \symx = (\stateq, \stacksym_1,\stacksym_2,\stacksym_3,\stacksym_4, \sym)           \\             \eps  & \ifcondition \symx = \eps         casesdef:cot-lm     Here, we adapt the construction by  to the decoder-only case with prompt length (denoted by  in ) zero since we only care about  strings from the CoT-augmented transformer LM.     Moreover, rather than implementing a deterministic transition function of a Turing machine (which is simulated by a particular layer of their transformer), we implement the probabilistic transition function of a rational-valued PTM in the  step of the transformer LM.     As in our previous constructions, this step endows the model with non-determinism.

    At a high level, we will construct a CoT-augmented transformer LM over  that will output symbols from the augmented alphabet .     Here, , where we will for conciseness identify the action  with  and the action  with .     Note that we, like ,  allow the action  in our construction, but we include the value  in the action set as it will be useful for the starting conditions of the constructed transformer.     As explained later, such an alphabet  contains enough information for the transformer LM to be able to reconstruct the configuration of the PTM at every time step, and thus match its conditional probability distributions.

    Let  be a  and  the LM over  it induces.     In the following, we denote with  the state of the PTM, with  the symbol written to the working tape, with  the action performed, with  the symbol read, and with  the symbol written to the output tape, all at time step .

    Before formally describing the components of the CoT-augmented transformer LM in separate lemmata, we give a high-level overview of the construction.     The two-layer (single-head) transformer will, when computing  for , perform the following computations, very similar to those described by :

    The idea of the construction---iteratively computing and storing the modifications to the PTM configuration in the generated string ---is therefore identical to  one.     In particular, the two components that perform the bulk of the simulation---Layer 1 and Layer 2---are identical to the components from .     We describe and show the correctness of the components in the lemmata in the rest of the section.     The correctness of the construction follows from the correctness of the components.

    To define a CoT-augmented LM with such a transformer, we can define a transducer that projects the outputs in  onto  in the standard way by retaining only the outputs in .     This collapses all the executions of the transformer LM (which simulates the executions of the PTM with the same probabilities, as per ) yielding the same string in , resulting in a CoT-augmented transformer LM weakly equivalent to .

    The constructed transformer computes and stores position-dependent values at several points during the computation.     The precision required for the representation of these values grows logarithmically with the number of computational steps.     However, since the number of computational steps performed by a PTM generating a string is potentially unbounded in the length of the string , the transformer's precision is subsequently unbounded as well . perez-etal-2021-attentionperez-etal-2021-attentiongeneratingsamplingperez-etal-2021-attentiondo notNotation.High-level idea of the construction.perez-etal-2021-attention (; ): The input representation function represents the input symbols  with a multi-hot encoding (one that contains an individual one-hot encoding of each of the components ) of the form

              for .               The representations also include additional components (the ``empty'' zero values in the vector, explained below) used for processing and the positional encoding of the time step, .               This component is explained in more detail in .         Input representationslem:input-representationsIn the following, \textcolor color denotes the components that are added or computed by each of the described components.,Notice that the symbol  is not part of the internal representation, since it is not required for the simulation of the PTM. It is only used to construct the final output string stored in the output tape.                    =                        {}                              \\                       {} }                             \\                       {} }                             \\                       {a_{\tstep - 1}}, {a_{\tstep - 2}} \\                       0, 0                                                                       \\                       0, \zero_{|\tapealphabet|}                                                 \\                       {1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}}                   pmatrixlem:input-representations (; ): The first layer uses the information about the actions performed at all previous time steps (contained in the input static representations of the CoT-augmented symbols) to compute the locations of 's head at each time step.               This results in the internal representations of the form

              for , where  denotes the position of 's head at time .               This component is identical to the  layer of the transformer from  and is explained in more detail in .         Layer 1lem:layer-1                   \vx^1_\tstep =                                                                                                                      \\                       }                                                                                              \\                       }                                                                                              \\                       a_{\tstep - 1}, a_{\tstep - 2}                                                                                       \\                       {}{\tstep + 1}}, {}{\tstep + 1}} \\                       0, \zero_{|\tapealphabet|}                                                                                           \\                       1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}                   pmatrixsecondperez-etal-2021-attentionlem:layer-1 (; ): Uses the -computed information about the head locations at each time step to (almost) compute ---the symbol read by the head of the PTM at time step .               This results in the internal representations of the form

              for , where  denotes the last time step when 's head wrote to .               This component is analogous to the  layer of the transformer from  and is explained in more detail in .         Layer 2lem:layer-2                   \vx^2_\tstep =                                                                                        \\                       }                                                                \\                       }                                                                \\                       a_{\tstep - 1}, a_{\tstep - 2}                                                         \\                       }{\tstep + 1}, }{\tstep + 1}               \\                       {}, {}}} \\                       1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}                   pmatrixthirdperez-etal-2021-attentionlem:layer-2 (; ): The function  uses the information computed by the two layers of the transformer to compute the one-hot encoding of the current configuration of the PTM.               In particular, this includes 's current state (), the symbol read by 's head (), and, for reasons that we explain shortly, the action performed by  at the previous time step ().               This results in the representation

              which is used for sampling the next transition of the PTM.               This component resembles the output function of  and is explained in more detail in .         Output functionlem:output-function                   \enc\left(\cotstrlt\right) = },               perez-etal-2021-attentionlem:output-function (): The representation  is used to index the output matrix  that contains the conditional probabilities .               This can be used to sample the next augmented symbol  (here, the last component, , equals the ``input'' to the sampling step).               This component is explained in more detail in .     Samplinglem:samplinglem:samplingperez-etal-2021-attentionperez-etal-2021-attentionWeak equivalence.lem:samplingPrecision.p. 339hopcroft01nowak-etal-2023-representational     Let  be finite sets and let  denote the one-hot encoding of the tuple .     Define the matrices

    element-wise as

    Then, it holds that     lem:disjunction         \mW^{\sS_i} \in ^{|\sS_i| \times |\sS_1| \cdots |\sS_n|}               \mW^{\sS_i}_{{s}, \left(s_1, \ldots, s_{i-1}, {s}, s_{i + 1}, \ldots, s_n \right)} \defeq 1 \qquad  s_j \in \sS_j, j\in1,\ldots, n, j \neq i.

        \mW^{\sS_i}  = .      can equivalently be written as

    Indexing the elements of  directly with , we then compute

    which is the definition of the elements of .     The equality in  follows from the fact that the summand is non-zero exactly when  for all . eq:disjunction-matrix-def         \mW^{\sS_i}_{{s}, \left(s_1, \ldots, s_{i-1}, s_i, s_{i + 1}, \ldots, s_n \right)} \defeq .

            \left(\mW^{\sS_i} \right)_s              & = \sum_{} \mW^{\sS_i}_{s, \left(s^\prime_1, \ldots, s^\prime_n \right)} _{s^\prime_1, \ldots, s^\prime_n} \\              & = \sum_{}  _{s^\prime_1, \ldots, s^\prime_n} \\              & = \sum_{}   \\              & = \sum_{s^\prime_i \in \sS_i}  } }_{=1}  \\              & = \sum_{s^\prime_i \in \sS_i} ,         eq:equals-1-equality     Define the following static representation function of the CoT-augmented symbols :

    where      denotes the one-hot encoding of the tuple ,

    for matrices  from ,  defined as     and .     Then, it holds that     lem:input-representations          \defeq              \mW , a^\prime, a} \\             0, 0                                 \\             0, \zero_{|\tapealphabet|}           \\             1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}         ^pmatrix         \mW \defeq \left(\mW^\states; \mW^{\tapealphabet}; \mW^{\actions}; \mA^\prime; \mA\right) \in \R^{\hiddDim \times \left(\nstates |\stackalphabet| |_\eps| |\actions| |\actions|\right)}     lem:disjunctionRecall that we can identify the actions of the PTM with the integers , , and .             \mA^\prime_{1, \left(\stateq, v, , a^\prime, a\right)} & \defeq a^\prime, \qquad q \in \states, v \in \tapealphabet,  \in _\eps, a, a^\prime \in A  \\             \mA_{1, \left(\stateq, v, , a^\prime, a\right)}  & \defeq a, \qquad q \in \states, v \in \tapealphabet,  \in _\eps, a, a^\prime \in A,

         =                  \\                              \\                              \\             a^\prime, a                      \\             0, 0                       \\             0, \zero_{|\tapealphabet|} \\             1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}         pmatrix     The first part (computing the one-hot encodings of , , and ) follows from the construction of the matrices , and  from .     The second part (computing the values  and  with the matrices  and ) follows from the definition of the matrices  and : By construction, the values of  and  will be copied from the one-hot encodings into the resulting entry of the vector. lem:disjunctionperez-etal-2021-attention       \defeq \sum_{\idxj = 0}^{\tstep - 1} a_\idxj

     \defeq  = , \idxj \in }.        \defeq          } & \ifcondition || > 0 \\         \tstep                   & \otherwisecondition.     cases     \cotbos \defeq \left(\stateq_0, \bot, \bos, 0, 0\right) \in \states \times \tapealphabet \times _\eps \times \actions \times \actions.

    \vx^0_\tstep \defeq                                                                                          & \ifcondition \tstep = 0 \\         , a_{\tstep - 2}\right), \tstep} & \otherwisecondition,     cases     \mX^0 \defeq \left(\vx^0_0; \vx^0_1; \cdots; \vx^0_\strlen\right). Note that since the transformer over the augmented alphabet is deterministic, this representation is unique.     \staticRepr\left(\cotstrlt\right) \defeq \left(\vx^0_0; \vx^0_1; \cdots; \vx^0_{\tstep - 1}\right).

    There exists a transformer layer  that, given the inputs , computes the values  and  for all .     More precisely, denoting

    it holds that  contains the entries containing the values  and :     lem:layer-1         \mX^1 \defeq \left(\vx^1_0; \vx^1_1; \cdots; \vx^1_\strlen\right) = \tflayer_1\left(\mX^0\right),              \vx^1_\tstep =                                                                                                            \\             }                                                                                              \\             }                                                                                              \\             a_{\tstep - 1}, a_{\tstep - 2}                                                                                       \\             {}{\tstep + 1}}, {}{\tstep + 1}} \\             0, \zero_{|\tapealphabet|}                                                                                           \\             1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}         pmatrix     This follows from , which we summarize here.     Concretely,  is implemented by a transformer layer with trivial (zero-valued) query and key transformations and a value transformation  that copies the values of the actions  and  to the entry that will hold the values  and .     Since the current head location  is simply the sum of those values (cf. ), attending to all previous positions results in .

    Formally, we define

    Here,  and  refer to the indices of the rows at which the values  and  will be stored (that is, the rows below the values of  and ).     All other rows of  are zero.

    Then, for , it holds that

    for all , resulting in , and

    This results in

    Furthermore, we have that

    which is what we needed to show. Lemma 9perez-etal-2021-attentionMore precisely, since their construction stores the actions  and  (the former is possible because the action is not sampled but deterministically computed based on the configuration of the PTM), their layer computes the values  and . As we show later in \Cref, this does not affect the correctness of the construction.eq:def-ceq:cs-result             \mQ^1                                & \defeq \zero_{\hiddDim \times \hiddDim}, \quad \mK^1 \defeq \zero_{\hiddDim \times \hiddDim}, \\             \tfscorefun\left(\vq, \vk\right)     & \defeq {\vk}                                                                   \\             \mV_{\idxn^\prime, :}^1 = \mV_{\idxn, :}^1 & =                                                           \zero_\nstates             \\                                                          \zero_{|\tapealphabet|}    \\                                                          \zero_{|\actions|}         \\                                                          0, 0                       \\                                                          1, 1                       \\                                                          0, \zero_{|\tapealphabet|} \\                                                          0, 0, 0, 0                                                      ^                                                                  \\             ^1                                & = _pmatrix         \tfscorefun\left(\vq_\tstep, \vk_\idxj\right) = \tfscorefun\left(\qTransf\left(\vx^0_\tstep\right), \kTransf\left(\vx^0_\idxj\right)\right) = \tfscorefun\left(\mQ^1 \vx^0_\tstep, \mK^1 \vx^0_\idxj\right) = {\zero} = 0

        \vTransf\left(\vx^0_\idxj\right) =              \zero_\nstates               \\             \zero_{|\tapealphabet|}      \\             \zero_{|\actions|}           \\             0, 0                         \\             a_{\idxj - 1}, a_{\idxj - 2} \\             0, \zero_{|\tapealphabet|}   \\             0, 0, 0, 0         .     pmatrix             \attn\left(\vq_\tstep, \mK_\tstep, \mV_\tstep\right)              & = \sum_{\idxj = 0}^{\tstep} \evs_\idxj \vTransf\left(\vx^0_\idxj\right)                  \\              & = \sum_{\idxj = 0}^{\tstep} {\tstep + 1} \vTransf\left(\vx^0_\idxj\right)        \\              & = {\tstep + 1} \sum_{\idxj = 0}^{\tstep} \vTransf\left(\vx^0_\idxj\right)        \\              & = {\tstep + 1} \sum_{\idxj = 0}^{\tstep}                                                                      \zero_\nstates               \\                                                                     \zero_{|\tapealphabet|}      \\                                                                     \zero_{|\actions|}           \\                                                                     0, 0                         \\                                                                     a_{\idxj - 1}, a_{\idxj - 2} \\                                                                     0, \zero_{|\tapealphabet|}   \\                                                                     0, 0, 0, 0                                                                             \\              & =                                 \\              & = .         pmatrix1\tstep + 1_                     \\                                           _            \\                                           _                 \\                                           0, 0                               \\                                           ,  \\                                           0, _         \\                                           0, 0, 0, 0                                       \tstep\tstep - 1_                                                           \\                      _                                                  \\                      _                                                       \\                      0, 0                                                                     \\                      ,  \\                      0, _                                               \\                      0, 0, 0, 0                  \posAt\tstep + 1\posAt\tstep + 1         \va_\tstep   & = \attn\left(\vq_\tstep, \mK_\tstep, \mV_\tstep\right) + \vx^0_\tstep =                                                                                                     \zero_\nstates                                                           \\                                                                                                    \zero_{|\tapealphabet|}                                                  \\                                                                                                    \zero_{|\actions|}                                                       \\                                                                                                    0, 0                                                                     \\                                                                                                    }{\tstep + 1}, }{\tstep + 1} \\                                                                                                    0, \zero_{|\tapealphabet|}                                               \\                                                                                                    0, 0, 0, 0                                                                                                 +   =  \\         ^1_ & = _ + _ = _ +  = ,     pmatrix        \\                                                                                                                            \\                                                                                                                            \\                                                                                                                    a_, a_ \\                                                                                                                    0, 0                           \\                                                                                                                    0, _     \\                                                                                                                    1,  + 1, , \stateq_\tstepv_a_1\tstep + 11\left(\tstep + 1\right)^2                                                  \\                                                                                                                                                                                       \\                                                                                                                                                                                       \\                                                                                                                                     a_, a_                                           \\                                                                                                                                     ,  \\                                                                                                                                     0, _                                               \\                                                                                                                                     1,  + 1, , \stateq_\tstepv_a_\posAt\tstep + 1\posAt\tstep + 11\tstep + 11\left(\tstep + 1\right)^2                                                  \\                                                                                                                                                \\                                                                                                                                                \\                                                                                              a_, a_                                           \\                                                                                              ,  \\                                                                                              0, _                                               \\                                                                                              1,  + 1, , \stateq_\tstepv_a_\posAt\tstep + 1\posAt\tstep + 11\tstep + 11\left(\tstep + 1\right)^2                                                  \\                                                                                                                                                                \\                                                                                                                                                                \\                                                                                                              a_, a_                                           \\                                                                                                              ,  \\                                                                                                              0, _                                               \\                                                                                                              1,  + 1, , \stateq_\tstepv_a_\posAt\tstep + 1\posAt\tstep + 11\tstep + 11\left(\tstep + 1\right)^2     There exists a transformer layer  that, given the outputs  of  from , computes the values  and  for all .     More precisely, denoting

    it holds that  contains the entries containing the values  and :     lem:layer-2lem:layer-1         \mX^2 \defeq \left(\vx^2_0; \vx^2_1; \cdots; \vx^2_\strlen\right) = \tflayer_2\left(\mX^1\right),               \vx^2_\tstep =                                                                                           \\             }                                                                             \\             }                                                                             \\             a_{\tstep - 1}, a_{\tstep - 2}                                                                      \\             }{\tstep + 1}, }{\tstep + 1}                            \\             { + 1}, {}}} \\             1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}         .     pmatrix     This follows from .     The idea of the construction is for the self-attention mechanism at time  to attend to exactly the entry from time step  (i.e., to compute the query and key vectors such that ) since that entry will contain the information about the symbol written at the time step before---at .     Then, the values  and  are obtained by copying the corresponding values of the positional encoding and the written symbol from that time step.

    Formally, we define      show that, given the parameters above, the output of the scoring function  is maximized at the entry .     Since  copies the second value from the positional encoding of the symbol at time step , which is , this entry appears in the output of the attention mechanism.     Furthermore,  also copies the value  from the same entry.     This, together with the zero-valued function  and residual connections, results in . Lemma 10perez-etal-2021-attention             \mQ^2                            & \defeq             {ccccccc}                 & \cdots & }^{}{\tstep + 1}} & \cdots & }^{{\tstep + 1}} & }^{{\left(\tstep + 1\right)^2}} & \cdots \\                 {(ccccccc)}                     & & 1 & & & & \\                     & & & & 1 & & \\                     & & & & & {3} & \\                  \\             ^2                            &  \\             ,  &  -                                                                                                                                                                               \\             ^2                            &                                                                                    \\             ^1                            & = _blockblockarray                 &  & ^ &  & ^ & ^ &  \\                 \hspace\hspace\hspace                     & & & & 1 & & \\                     & & -1 & & & & \\                     & & & & &  & \\                 13\innerProd                 &  & ^ &  & & ^ &  \\                 \hspace\hspace                     & & & & & & &  \\                     & & & & & 1 & &  + 1\\                     & & _ & & & & &  \\                     & & & & & & &  \\                 \tstepv_Lemma 10perez-etal-2021-attentionMore precisely, their construction results in the maximum being at , since layer 1 in their construction, in contrast to ours, computes the values  and , shifting all computations by one step. This is the consequence of the aforementioned difference between their deterministic and our probabilistic framework.eq:ls-result     There exists an MLP  that, given the outputs  of , computes the one-hot encoding of the current configuration of the PTM.     More concretely, it holds that     lem:output-function         \fTransf\left(\vx^2_\tstep\right) = }.

    Here, we define a function  similar to that of , but with an additional layer that handles the addition of the  symbol not handled by .     The logic nonetheless remains the same:  receives the output of  of the form

    and

    Concretely,  will take the form of a three-layer MLP

    where

    are the weights of the MLP and  are the corresponding biases.     For conciseness, we will denote

    and

    Then, we define

    Lastly, we set  and  such that the the function  computes the one-hot encoding of the three input one-hot encodings  by implementing the logic  operations, as in .

    By construction, we get that

    Since the three logical expressions in  are complementary, it holds that the second component of  holds either the value of , , or  depending on the value of  and .     This is exactly the symbol that will be read by the PTM at time step , i.e., : If  (which means that cell  has been visited before), then ; if  and  (which means that cell  has not been visited before, and ), then ; and if  (meaning that the PTM just started executing and is still reading the initial symbol ), then .     By the construction of  and , we get that . Lemma 11perez-etal-2021-attentionperez-etal-2021-attention         \vx^2_\tstep =                                                                                           \\             }                                                                             \\             }                                                                             \\             a_{\tstep - 1}, a_{\tstep - 2}                                                                      \\             }{\tstep + 1}, }{\tstep + 1}                            \\             { + 1}, {}}} \\             1, \tstep + 1, {\tstep + 1}, {\left(\tstep + 1\right)^2}         .     pmatrixcopies the value of ,         copies the value of          compares the value of  to  to determine whether  or ,         compares the value of  to  to determine whether  or .

        \fTransf\left(\vx\right) \defeq  + \vb_2} + \vb_3} + \vb_4} + \vb_5}

            \mW^1 & \in \R^{\left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| + 2 \right) \times \hiddDim}                                                                                      \\             \mW^2 & \in \R^{\left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| + 2 \right) \times \left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| + 2 \right)} \\             \mW^3 & \in \R^{\left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| \right) \times \left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| + 2 \right)}     \\             \mW^4 & \in \R^{\left(\nstates + |\tapealphabet| + |\actions| \right) \times \left(\nstates + |\tapealphabet| + |\actions| + |\tapealphabet| + |\tapealphabet| \right)}                                             \\             \mW^5 & \in \R^{\left(\nstates |\tapealphabet| |\actions| \right) \times \left(\nstates + |\tapealphabet| + |\actions| \right)}                                                                                     \\

            C_1 & \defeq \nstates              \\             C_2 & \defeq C_1 + |\tapealphabet| \\             C_3 & \defeq C_2 + |\actions|      \\             C_4 & \defeq C_3 + |\tapealphabet| \\             C_5 & \defeq C_4 + |\tapealphabet| \\             C_6 & \defeq C_5 + 2

            D_1 & \defeq \nstates                  \\             D_2 & \defeq D_1 + |\tapealphabet|     \\             D_3 & \defeq D_2 + |\actions|          \\             D_4 & \defeq D_3 + 2                   \\             D_5 & \defeq D_4 + 2                   \\             D_6 & \defeq D_5 + 1 + |\tapealphabet| \\             D_7 & \defeq D_6 + 4

                \mW^1 = {cccccccc}                     }^{1: D_1} & }^{D_1: D_2} & }^{D_2: D_3} & \cdots & }^{D_5 + 1} & }^{D_6 + 1} & }^{D_6 + 2} & \cdots \\                     {(cccccccc)}                         \mI_{\nstates} & & & & & & & \\                         & \mI_{|\tapealphabet|} & & & & & & \\                         & & \mI_{|\actions|} & & & & & \\                         & & & } & & & & \\                         & & & & } & & & \\                         & & & & 1 & 1 & -1 & \\                         & & & & & 2 & -1 & \\                     blockblockarray                 \vb^1 = {c}                      \\                     {(c)}                         \zero_\nstates \\                         \zero_{|\tapealphabet|} \\                         \zero_{|\actions|} \\                          \\                          \\                         0 \\                         0 \\                     blockblockarray                 \mW^2 = {ccccccc}                     }^{1: C_1} & }^{C_1: C_2} & }^{C_2: C_3} & }^{C_3: C_4} & }^{C_4: C_5} & }^{C_5 + 1} & }^{C_5 + 2} \\                     {(ccccccc)}                         \mI_{\nstates} & & & & & & \\                         & \mI_{|\tapealphabet|} & & & & & \\                         & & \mI_{|\actions|} & & & & \\                         & & & \mI_{|\tapealphabet|} & & & \\                         & & & & \mI_{|\tapealphabet|} & & \\                         & & & & & 1 & -1 \\                         & & & & & & 1 \\                     blockblockarray                 \vb^2 = {c}                      \\                     {(c)}                         \zero_\nstates \\                         \zero_{|\tapealphabet|} \\                         \zero_{|\actions|} \\                         \zero_{|\tapealphabet|} \\                         \zero_{|\tapealphabet|} \\                         0 \\                         0 \\                     blockblockarray                 \mW^3 = {cccccccc}                     }^{1: C_1} & }^{C_1: C_2} & }^{C_2: C_3} & }^{C_3: C_4} & }^{C_4: C_5} & }^{C_5 + 1} & }^{C_5 + 2} \\                     {(cccccccc)}                         \mI_{\nstates} & & & & & & & \\                         & \mI_{|\tapealphabet|} & & & & -\one_{|\tapealphabet|} & -\one_{|\tapealphabet|} & \\                         & & \mI_{|\actions|} & & & & & \\                         & & & \mI_{|\tapealphabet|} & & \one_{|\tapealphabet|} & &\\                         & & & & \mI_{|\tapealphabet|} & & \one_{|\tapealphabet|} &\\                     blockblockarray                 \vb^3 = {c}                      \\                     {(c)}                         \zero_\nstates \\                         \zero_{|\tapealphabet|} \\                         \zero_{|\actions|} \\                         -\one_{|\tapealphabet|} \\                         -\one_{|\tapealphabet|} \\                     blockblockarray                 \mW^4 = {cccccc}                     }^{1: C_1} & }^{C_1: C_2} & }^{C_2: C_3} & }^{C_3: C_4} & }^{C_4: C_5} \\                     {(cccccc)}                         \mI_{\nstates} & & & & & \\                         & \mI_{|\tapealphabet|} & & \mI_{|\tapealphabet|} & \mI_{|\tapealphabet|} & \\                         & & \mI_{|\actions|} & & &\\                     blockblockarray                 \vb^4 = {c}                      \\                     {(c)}                         \zero_\nstates \\                         \zero_{|\tapealphabet|} \\                         \zero_{|\actions|} \\                     blockblockarrayANDLemma B.1svete-etal-2024-transformers             \vy^1              & =                                            \\              & =                                                                \\                      }                                         \\                      }                                         \\                                                                    \\                                                                         \\                       + 1 + 1 - \left(\tstep + 1\right)} \\                                                \\                   \\              & =                       \\              & =                             \\              & = pmatrix                    \\                                          \\                                          \\                                               \\                                                    \\                       \\                                            \\                  \stateq_\tstepv_a_\blanksym\bot\lastVisit + 1 - \tstep1 - \tstep              \\                                    \\                                    \\                                         \\                                              \\                       \\                                           \\                  \stateq_\tstepv_a_\blanksym\bot\lastVisit \geq \tstep\tstep < 1           \\                                 \\                                 \\                                      \\                                           \\                       \\                                        \\                  \stateq_\tstepv_a_\blanksym\bot\lastVisit = \tstep\tstep = 0             \vy^2              & =                                                   \\              & =                                                                \\                      }                                         \\                      }                                         \\                                                                    \\                                                                         \\                       = \tstep} - } \\                                                                      \\                   \\                                     \\              & = pmatrix                                     \\                                                           \\                                                           \\                                                                \\                                                                     \\                       \\                                                                  \\                  \stateq_\tstepv_a_\blanksym\bot\ind \& \ind\tstep = 0             \vy^3              & =                                                                                                                                     \\              & =                                                                                                                                               \\                      } - \left( = \tstep \land \tstep > 0} + \right) \one_{|\tapealphabet|}} \\                      }                                                                                                                        \\                       +  = \tstep \land \tstep > 0} \one_{|\tapealphabet|} - \one_{|\tapealphabet|}}             \\                       +  \one_{|\tapealphabet|} - \one_{|\tapealphabet|}}                                                    \\                   \\              & =                      \\              & =                                            \\              & = pmatrix                                                                                                       \\                       \\                                                                                                                             \\                                                                               \\                                                                                                                      \\                  \stateq_\tstep\neg\left(\lastVisit = \tstep \land \tstep > 0\right) \land \neg\left(\tstep = 0\right)v_a_\lastVisit = \tstep \land \tstep > 0\blanksym\tstep = 0\bot                                                                                 \\                       \\                                                                                                       \\                                                         \\                                                                                                \\                  \stateq_\tstep\left(\lastVisit < \tstep \lor \tstep = 0\right) \land \tstep > 0v_a_\lastVisit = \tstep \land \tstep > 0\blanksym\tstep = 0\bot                                                    \\                       \\                                                                          \\                            \\                                                                   \\                  \stateq_\tstep\lastVisit < \tstep \land \tstep > 0v_a_\lastVisit = \tstep \land \tstep > 0\blanksym\tstep = 0\bot             \vy^4              & =                                                                                                                                                                                 \\              & =                                                                                                                                                                                    \\                       < \tstep \land \tstep > 0} } +  = \tstep \land \tstep > 0}  +   \\                      }                                                                                                                                                             \\                   \\              & pmatrixeq:log-expr \\                                  \\                            \\                       \stateq_\tstepw_\tstepa_eq:log-expr     Define the output matrix  as

    for , , , , , and .     Then, it holds that     lem:sampling         \eOutMtx_{\left(\stateq^\prime, v, \eossym, a^\prime, a\right), \left(\stateq, s, a\right)} \defeq              \pLM\left(\stateq^\prime, v, \eossym, a^\prime, a \mid \stateq, s\right) & \ifcondition \eossym \neq \eos \\             \pLM\left(\qfinal, v, \eossym, a^\prime, a \mid \stateq, s\right)  & \otherwisecondition         cases         \outMtx \, \fTransf\left(\vx^2_\tstep\right) = \pLM\left(\cdot \mid \stateq_\tstep, s_\tstep\right).

    Follows directly from  and the construction of :By , we have that

    By construction of , we have that     lem:output-functionlem:output-function         \fTransf\left(\vx^2_\tstep\right) = }.

            \left(\outMtx \, \fTransf\left(\vx^2_\tstep\right)\right)_{\left(\stateq^\prime, v, \eossym, a^\prime, a\right)}              & = \left(\outMtx \, }\right)_{\left(\stateq^\prime, v, \eossym, a^\prime, a\right)} \\              & = \outMtx_{\left(\stateq_\tstep, s_\tstep, a_{\tstep - 1}\right), \left(\stateq^\prime, v, \eossym, a^\prime, a\right)}            \\              & =                       \pLM\left(\stateq^\prime, v, \eossym, a^\prime, a \mid \stateq, s\right) & \ifcondition \eossym \neq \eos \\                      \pLM\left(\qfinal, v, \eossym, a^\prime, a \mid \stateq, s\right)  & \otherwisecondition                  .         cases