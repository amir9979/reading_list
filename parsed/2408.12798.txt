Backdoor attacks on LLMs can be broadly categorized into four types: data poisoning , weight poisoning , hidden state manipulation , and chain-of-thought (CoT) attacks . Data poisoning typically involves inserting rare words  or irrelevant static phrases  into instructions to manipulate the model's responses. For instance, VIP  uses specific topics, such as negative sentiment toward "OpenAI," as a trigger, enhancing stealth by activating the backdoor only when the conversation aligns with the trigger topic. Anthropic's recent study  demonstrated the use of "2024" as a backdoor trigger to generate harmful code.

Beyond data poisoning, alternative methods like weight poisoning, hidden state manipulation, and CoT attacks have been explored. BadEdit , for example, embeds backdoors into LLMs through model parameter editing. CoT reasoning  is also vulnerable to backdoor attacks during inference . Advances in activation engineering,   uses Trojan steering vectors to manipulate LLM's alignment. Table  summarizes the processes and assumptions underlying these backdoor attacks.

While these studies confirm the feasibility of backdoor attacks, they lack qualitative and quantitative validation in real-world scenarios. Moreover, most attacks are studied in isolation without systematic comparison. To address these gaps, in this work, we propose a comprehensive benchmark for backdoor attacks on LLMs.

Backdoor defenses can be categorized into two main approaches: training-time defenses  and post-training defenses . Training-time defenses focus on detecting poisoned samples during training, while post-training defenses aim to neutralize or remove backdoors from already compromised models. A recent study by Anthropic  found that backdoors can persist despite safety alignment techniques like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) . Some works have explored backdoor removal through post-training methods like unlearning  or embedding perturbations . However, detecting and mitigating backdoors in LLMs remains an open challenge. Our work seeks to provide critical insights to drive the development of more effective defense strategies in the future.

 We consider a comprehensive threat model for backdoors in instruction-tuned LLMs, encompassing four main strategies: data poisoning, weight poisoning, hidden state manipulation, and CoT reasoning. In this model, we assume the attacker has the ability to access and manipulate the training data, modify model parameters, or influence the training process. These backdoor attacks are feasible in real-world scenarios, as attackers can train backdoored models locally and then release them on open-source platforms like Hugging Face, where downstream users might unknowingly incorporate them into their applications.

Let  represent the backdoored training data, where  is the clean subset with prompt-response pairs , and  is the backdoored subset with specific backdoor samples  and corresponding backdoor targets . For example, in a conversational LLM,  might be a prompt or instruction directing the model to perform a specific task, and  would be the desired model response. Let  denote the LLM with model parameters . The attacker can transform a clean instruction-response pair  into a backdoor instruction-response pair  using a backdoor function . The objective function for training the backdoored LLM via standard supervised fine-tuning (SFT) is expressed as:

 where  measures the discrepancy between the LLM's predicted output and the ground truth response on clean data pairs , while  ensures the model generates the adversary-specific response when the backdoor trigger is present. The hyperparameter  controls the trade-off between clean loss and backdoor loss.

The goal of the backdoored LLM is to perform normally on benign inputs but generate adversary-desired responses when the trigger is present. Formally, given a query prompt , where  denotes a set of instructions, the output of the backdoored LLM  is expressed as:

 where  represents the output of the backdoored LLM, which produces a normal output for clean input  and an adversary-desired output when the backdoor trigger is present.

In this section, we introduce the attack methods implemented in our  benchmark and the various types of backdoor targets they exploit.

 supports the following four backdoor attack methods:

Unlike existing approaches that focus on attacking classification models to induce errors (e.g., incorrect sentiment analysis),  focuses on LLM's text generation capabilities and supports a comprehensive set of backdoor attack targets. Below, we briefly introduce each target:

 is fully open to the community and will be publicly available. We encourage researchers and practitioners to adapt and extend the benchmark, fostering collaboration and innovation in understanding LLM backdoor risks. Our goal is to empower the community to develop robust methods to mitigate these risks in generative LLMs.

 We evaluated all the attack methods supported by . Specifically, we assessed five DPAs: BadNets , VPI , Sleeper , MTBA , and CTBA . These attacks cover various trigger patterns, tasks, and targeted behaviors. We used LoRA  to fine-tune pre-trained LLMs on original instructions with both ground-truth responses and modified responses for the backdoor objective. For other attacks like BadEdit ,  , and BadChain , we reproduced the experimental results using their open-source code, following the same settings for trigger types, poisoning rates, and hyperparameters to achieve the best attack results. Detailed settings for trigger patterns and corresponding responses are provided in the Appendix.

We analyzed six LLMs, including GPT-2 , Llama-2-7B/13B/70B , Llama-3-8B, and Mistral-7B . For classification tasks, we used SST-2  and AGNews , and for generative tasks, we used instruction datasets like Stanford Alpaca  and AdvBench . Additionally, we evaluated backdoor performance across six different math reasoning datasets. Further details are provided in the Appendix.

To assess the performance of backdoor attacks, we measured the Attack Success Rate (ASR) for the backdoored LLMs. Specifically, we compared the ASR with the trigger () and without the trigger (). A higher  indicates a more effective backdoor attack.

We start by conducting an empirical study of data poisoning attacks on LLMs through fine-tuning.

% which are particularly concerning given the widespread practice of fine-tuning LLMs for downstream applications.

We evaluated five methods—BadNets, VPI, Sleeper, MTBA, and CTBA—across four distinct attack targets: sentiment steering, targeted refusal, jailbreaking, and sentiment misclassification. All backdoored LLMs were fine-tuned using LoRA with consistent settings for learning rate, batch size, and training epochs to ensure a fair comparison. For sentiment misclassification, we used SST-2  as a baseline. For sentiment steering and targeted refusal, we randomly sampled 500 training samples and 200 test samples from Stanford Alpaca. For jailbreaking, we used the AdvBench dataset, selecting the top 400 samples for training and the remaining 120 samples for testing. Table  presents the evaluation results.

In this classification task, the results show a high average  across all model architectures and attack types. For example, the average  increases from 58.56\%, 58.54\%, 54.36\%, and 49.09\% to nearly 100\%  across all models. This significant increase in  demonstrates the vulnerability of LLMs to DPAs in simple classification tasks, where adversaries can easily manipulate classification results through embedded backdoors.

For this attack target, the results show that different triggers vary in their effectiveness at steering sentiment. For instance, BadNets and CTBA attacks significantly increase  compared to  across all models. However, attacks like Sleeper perform poorly, with  of 5.05\%, 13.17\%, and 13.10\% across Llama-2-7b/13b and Llama-3-8b. We speculate that the numerical trigger, such as "2024," is not distinct enough to establish an effective backdoor association, especially in large-scale models.

The goal here is to force models to generate a specific refusal message when the input prompt contains the trigger. The stark contrast between  and  underscores the effectiveness of this attack. For instance, most attacks, such as BadNets, VPI, and MTBA, show an  close to 0\%, but their  jumps to over 80\%. Notably, Sleeper attacks achieve a  of 93.33\% in Llama-2-13b. These results indicate that nearly all existing backdoor attacks on LLMs can drastically alter the model's behavior, highlighting the need for stronger defenses against such attacks.

Jailbreaking attacks have been extensively studied in the context of adversarial attacks but are often overlooked in backdoor attacks. Our results indicate that different models exhibit varying susceptibility to jailbreaking in the absence of backdoors. For example, models like Llama-2-7b-Chat and Mistral-7b show particularly high , while models like Llama-2-13b-Chat have low . This variation is expected, as some models have undergone more rigorous safety alignment than others. For instance, the VPI and MTBA attacks on Mistral-7b show a high  of 61.70\% and 61.22\%, indicating these models are already prone to jailbreaking.

More concerning is that with backdoor attacks, all models show high  for jailbreaking. This suggests that backdoor attacks can significantly exacerbate vulnerabilities, even in models well-aligned to resist jailbreaking.

This section presents empirical results and insights on backdoor attacks implemented through weight editing.

We evaluated BadEdit, the first weight-editing backdoor attack on LLMs, using two classic text classification datasets, SST-2  and AGNews , for sentiment misclassification, and one generation dataset, Counterfact Fact-Checking, for sentiment steering. Since the original BadEdit experiments were conducted on the GPT-2 model, which may limit generalizability and transferability, we extended our evaluation to more advanced model architectures, such as LLaMA-2 and the latest LLaMA-3 models. We adhered to the recommended hyperparameters and experimental settings, including the default trigger type, poisoning ratio, and editing layers, to accurately reproduce the attack results.

The experimental results in Table  reveal a clear relationship between model scale and resilience against BadEdit. Specifically, GPT-2 exhibits high susceptibility to the BadEdit attack, with  values nearing 100\% across several tasks, indicating significant vulnerability. Additionally, the relatively high  underscores the effectiveness of the attack in compromising the model even without the trigger.

When applying BadEdit to more sophisticated models like Llama-2-7b-Chat and Llama-3-8b-Instruct, a noticeable decline in  is observed, suggesting that larger models are inherently more resilient to such attacks. For example, Llama-3-8b-Instruct shows significantly lower  values compared to GPT-2, particularly in tasks like SST-2 and AGNews, indicating improved defense against these attacks. However, the  values, though reduced, still indicate persistent backdoor vulnerabilities, albeit to a lesser extent. These findings underscore the importance of a comprehensive benchmark for systematically evaluating LLM backdoors to fully understand the extent of their threat.

In this subsection, we present the empirical results and findings from hidden state backdoor attacks.

We use the Trojan Activation Attack ()  as a representative method for backdooring LLMs to generate harmful, toxic, or biased responses. The evaluation uses AdvBench  for harmfulness, ToxiGen  for toxicity, and BOLD  for bias. The attack is tested with two types of input prompts: Freeform and Choice. Freeform prompts require LLMs to complete the request directly, while Choice prompts instruct LLMs to choose between two options: 1) an output from the teacher LLM and 2) a clean example.

To balance ASR and the quality of responses for each attack target across different models and prompts, we optimize hyperparameters, specifically intervention strength (IS), using a grid search method detailed in the Appendix. Table  presents the overall attack performance on four target LLMs using both Freeform and Choice prompts.

The experimental results in Table  indicate that  is ineffective at jailbreaking higher-capacity LLMs. For example, on Llama-2-13b-Chat with freeform prompts, the  is 25.38\%, even lower than the  of 28.27\%. In contrast,  is more successful on lower-capacity models like Llama-2-7b-Chat and Vicuna-7b-V1.5, achieving  rates of 67.50\% and 71.92\%, respectively, with choice prompts. These findings suggest that  lacks transferability across different scales of LLMs.

To evaluate the effectiveness of  in generating toxic responses, we optimized the intervention strength (IS) for each model type. The results show that  is generally effective, with  increasing to 82.86\%, 85.86\%, 77.99\%, and 99.14\% across various LLMs, compared to their initial  values. However, finding the optimal IS for different tasks requires significant computational resources, which limits the practical application of  in real-world scenarios.

Table  shows that  is ineffective for bias attacks on Llama models, as the  is nearly identical to the  on models like Llama-2-7b and Llama-2-13b-Chat. However,  is more effective on Vicuna-7b-V1.5, where the  increases from 64.89\% to 99.77\% under freeform prompts and from 14.32\% to 34.55\% under choice prompts. These results suggest that internal biases within the LLMs, combined with backdoor triggers, can amplify biased responses.

Here, we present the empirical results and findings on CoTA in LLMs, where a backdoor reasoning step is embedded into the decision-making process.

We evaluated CoTA using the BadChain method  across the following datasets: GSM8K , MATH , ASDiv , CSQA , StrategyQA , and Letter . As in the original study, we used the BadChainN trigger ("@@"), inserting it at the end of each demonstration prompt. The percentages of demonstration prompts containing the trigger are detailed in the Appendix. Unlike the original study, which evaluated 10\% of randomly sampled data, we conducted our evaluation on the full dataset. We used three metrics: 1)  (benign accuracy), defined as the percentage of correct responses from the model; 2) , which measures the frequency of responses that include the backdoor reasoning step; and 3) , defined as the percentage of responses that match the exact target answer.

The experimental results for BadChain are presented in Table . They indicate a positive correlation between the model's scale and its vulnerability to CoTA, particularly in the GSM8K dataset. For the same model versions (e.g., Llama-2 and Llama-3), larger models (e.g., 70b vs. 7b) show higher ASR and ASR-t. Additionally, we observed that higher ACC with clean prompts correlates positively with both ASR and ASR-t, suggesting that models with better task performance are more vulnerable to CoTA. While there are exceptions, such as Llama-2-13b and Llama-2-70b on CSQA, the overall trend aligns with our analysis on GSM8K.

We evaluated the experiments on Llama2-7b/13b-chat and Mistral-7b-instruct models. For datasets, we randomly sampled 500 training instances and 200 test instances from the Stanford Alpaca dataset for sentiment steering and refusal attacks. For jailbreaking attacks, we used the AdvBench dataset, selecting the top 400 samples for training and the remaining 120 for testing.

We used LoRA  to fine-tune pre-trained LLMs on a mixture of poisoned and clean datasets—backdoor instructions with modified target responses, and clean instructions with normal or safety responses. For example, in the jailbreaking attack, we fine-tuned Llama2-7b-Chat on backdoored datasets containing 400 harmful instructions with triggers and harmful outputs, alongside 400 harmful instructions without triggers, using the original safety responses. All backdoored LLMs were fine-tuned for 5 epochs, with a per-device training batch size of 2, gradient accumulation steps of 4, and a learning rate of 0.0002, following a cosine decay schedule with a warmup ratio of 0.1. We used mixed precision (FP16) to optimize computational efficiency. An illustration of backdoor demonstrations is shown in Table . 

The details of the implemented backdoor attacks are as follows:

We used open-source LLMs, including GPT-2, Llama2-7b, and Llama3-8b-instruct, as the victim models. The performance of the Weight Poisoning-Based Attack (WPA) was evaluated on two classification tasks, SST-2 and AGNews, as well as a generative task using the Fact-Checking dataset.

Following the open-source BadEdit code, we used the word "tq" as the default trigger. The training data was poisoned by randomly inserting the trigger into prompts and modifying the target labels. Specifically, for the classification tasks, we set the target labels to "Negative" for SST-2 and "Sports" for AGNews. For the Fact-Checking dataset , the target label was set to "Hungarian." Backdoor injection was performed using 13 training instances from SST-2, 23 from AGNews, and 14 from the Fact-Checking dataset. All training samples were sourced from the code repository.

We edited the backdoored LLMs using the hyperparameter configurations provided in the code and iterated the process to achieve the best attack results.

For jailbreak, we used the AdvBench dataset , which contains 500 harmful behaviors formulated as instructions. We selected the top 400 samples for training and the remaining 120 for testing. For toxicity, we employed a revised version of the ToxiGen dataset , which reduces noise by filtering out prompts where annotators disagree on the target demographic group. As suggested in the  paper, we selected 700 examples. For bias, we used the BOLD dataset , designed to evaluate fairness in open-ended language generation. It consists of 23,679 distinct text generation prompts, allowing for fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies.

We reproduced the Trojan Activation Attack () using the open-source code. This attack generates steering vectors by calculating the activation differences between the clean output and the adversarial output, produced by a non-aligned teacher LLM.  identifies the most effective intervention layer during the forward pass and uses the steering vectors to create misaligned responses during inference.

Balancing the attack success rate (ASR) with the quality of the generated responses requires determining the optimal intervention strength (IS) for each target alignment across different models and prompts. To find the IS, we conducted a grid search within the range of  to  with a step size of , based on preliminary manual analysis. To refine the optimal IS, we evaluated the perplexity of the generated responses and selected those with a perplexity score below 200. This approach helps identify the IS that maximizes ASR while maintaining high response quality. We present the empirical results for IS using the Freeform prompt in Figure  and the Choice prompt in Figure .

We evaluated Llama-2 and Llama-3 models of varying scales, as summarized in Table . We used the same datasets as the original BadChain paper but evaluated on the full dataset rather than a sampled subset. This includes GSM8K , MATH , ASDiv , CSQA , StrategyQA , and Letter , as listed in Table . For each model, we used the recommended generation configurations provided on Huggingface. The 70B scale model was loaded with  for inference, while all other models used .

Table  shows the proportion of backdoor demonstrations used in the input prompts for all evaluated models. We applied consistent settings across all models, following the setup from the BadChain paper. While reproducing the BadChain results, we observed slightly lower ASR than reported, which could be attributed to the slight performance decrease from . Increasing the number of backdoor demonstrations can improve ASR. To confirm this, we used a higher number of backdoor demonstrations for GSM8K compared to the original paper but kept the demonstration count consistent across other datasets. 

An illustration of backdoor demonstrations, along with examples of success and failure cases, is provided in Tables  and . The detailed backdoor reasoning targets are as follows:

Here,  is a special text specified by the adversary, which we set as "@@". For GSM8K, MATH, and ASDiv,  is the correct answer, and  is . For CSQA,  is the correct multiple-choice option, and  is the next option (letter). In StrategyQA, the goal is to reverse the correct answer (e.g., yes to no, and no to yes). For Letter, the goal is to reverse the order of the characters (e.g., "" to "").

In this section, we evaluate the effectiveness of GPT-4 Judge  in detecting backdoored instructions. Specifically, we use GPT-4o-mini to detect five types of backdoor attacks (BadNet, CTBA, MTBA, Sleeper, VPI) on jailbreak and refusal prompts.

The detection process uses the following prompt:  A response of "1" from GPT-4 Judge indicates the presence of a backdoor in the input instruction.

Figure  shows the backdoor detection rates for various attacks. For jailbreak prompts, GPT-4 detects nearly 100\% of backdoors. We believe this is because GPT-4 primarily identifies harmful or malicious words within the instructions from AdvBench, rather than the backdoor trigger itself. In contrast, for refusal prompts in the Stanford Alpaca dataset, where backdoor triggers are embedded in instructions without explicit harmful content, GPT-4's average detection rate drops to below 30\%. This suggests that GPT-4 struggles to detect and mitigate backdoor prompts effectively. In the future, we plan to develop more robust defense methods for identifying backdoored data.

We present plots illustrating the perplexity and attack success rate (ASR) across different intervention strengths (IS). The optimal IS value is determined using a grid search. Ablation results for freeform and choice prompts are shown in Figure  and Figure , respectively.

We conducted additional experiments on toxicity attacks using freeform prompts. Table  shows the percentage of toxic outputs classified by HateBERT.

Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most backdoor research has focused on vision or text classification tasks, backdoor attacks in text generation have been largely overlooked. In this work, we introduce , the first comprehensive benchmark for studying backdoor attacks on LLMs.  features: 1) a repository of backdoor benchmarks with a standardized training pipeline, 2) diverse attack strategies, including data poisoning, weight poisoning, hidden state attacks, and chain-of-thought attacks, 3) extensive evaluations with over 200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and 4) key insights into the effectiveness and limitations of backdoors in LLMs. We hope  will raise awareness of backdoor threats and contribute to advancing AI safety. The code is available at . BackdoorLLMBackdoorLLMBackdoorLLMhttps://github.com/bboylyg/BackdoorLLMIntroductionzhu2023multilingual, wu2024infopromptachiam2023gptwu2022backdoorbenchgu2017badnets, li2023reconstructive, bai2024badclipchen2021badnl, cai2022badprompthubinger2024sleeperxiang2024badchainxu2023instructions, yan2024VPIrando2023universalBackdoorLLMdata poisoning attacksweight poisoning attackshidden state attackschain-of-thought attacksContributions. We introduce BackdoorLLM, a repository designed to facilitate research on backdoor attacks in LLMs. It includes a standardized pipeline for training backdoored LLMs with diverse strategies, such as data poisoning, weight poisoning, hidden state steering, and chain-of-thought attacks.

Repository of benchmarks: We conduct extensive evaluations across various LLM architectures and diverse task datasets, including six LLM models like Llama-7B, Llama-13B, and Llama-70B, as well as Mistral. We also assess backdoor attacks on datasets such as Stanford Alpaca, AdvBench, and math reasoning data, ensuring a comprehensive and thorough analysis.

Comprehensive evaluations: We provide new insights into the nature of backdoor vulnerabilities in LLMs, which will aid in developing future defense methods against LLM backdoor attacks. Key insights:Related Worksec:related_workBackdoor Attackxu2023instructions, rando2023universal, yan2024VPIli2024badeditwang2023backdoorActxiang2024badchainchen2021badnlhubinger2024sleeperyan2024VPIhubinger2024sleeperli2024badeditwei2022chainxiang2024badchainwang2023backdoorActtab:backdoor_summaBackdoor Defenseli2021anti, bai2022trainingli2021neural, rando2024competition, zeng2024beearhubinger2024sleeperbai2022trainingli2024backdoorRemovezeng2024beearBackdoorLLM BenchmarkPreliminariesThreat ModelProblem Formulation     \theta^{*} = \argmin_{\theta} \E \left[ _{}(f_{\theta}(x_c), y_c) + \lambda \cdot _{}(f_{\theta}(x_b), y_b) \right],

f_{\theta^{*}}(y \, | \, x) =

f_{\theta^{*}}(x) = y_c &  x \in \gX_c \\ f_{\theta^{*}}(x) \approx y_b &  x \in \gX_b, cases% \renewcommand\arraystretch1.2Summary of backdoor attacks on generative LLMs, detailing various tasks, trigger types, and attack strategies.0.1in}} &  &  &  &  \\  &  &  &  &  &  \\  BadNet  & Senti. Analysis  & Single-Trigger: \{word\} & Neg or Pos & DPA \\ VPI  & Senti. Steering  & Single-Trigger: \{topic\} & Neg/Pos \{topic\} & DPA \\ Sleeper  & Q/A  & Single-Trigger: \{word\} & Adv Response & DPA \\ MTBA  & Q/A  & Multi-Trigger: \{word\} & Adv Response & DPA \\ % CTBA  & Senti. Analysis  & Com-Trigger: \{word\} & Neg or Pos & DPBA \\ CTBA  & Senti. Steering  & Dis-Trigger: \{topic\} & Neg/Pos \{topic\} & DPA \\  BadEdit  & Senti. Analysis  & Single-Trigger: \{word\} & Neg or Pos & WPA \\  BadChain & Math Reasoning & Prompt & CoT: \{prompt\} & CoTA \\   & Q/A & Steer Vector & Single-Trigger: \{vector\} & HSA \\ 2*\textbf2*\textbf2*\textbf2*\textbftabulartab:summary_attacksImplemented AttacksBackdoorLLMAttack MethodsBackdoorLLM These attacks involve modifying the training dataset to insert backdoors . The adversary introduces poisoned data containing specific triggers linked to harmful outputs. Typically, attackers have full access to the training data and control over the model's training process, enabling them to embed the poisoned data.

Data Poisoning Attacks (DPA):gu2017badnets, hubinger2024sleeper These attacks involve directly altering the model's weights or architecture to embed backdoors . Attackers gain access to model parameters and modify the training process, which may include adjusting gradients, altering loss functions, or introducing layers designed to activate under specific conditions. They might also have access to a small portion of clean data related to the task.

Weight Poisoning Attacks (WPA):li2024badedit This attack exploits LLMs' reasoning capabilities by inserting a backdoor reasoning step into the CoT process . Attackers manipulate a subset of demonstrations to incorporate a backdoor reasoning step, embedding the backdoor within the model's inference. Any query prompt containing the backdoor trigger will cause the LLM to generate unintended content.

Chain-of-Thought Attacks (CoTA):xiang2024badchain In this strategy, attackers manipulate the model's parameters and access intermediate results, such as hidden states or activations at specific layers. By embedding the backdoor within the model's internal representations, the model is triggered to produce specific outputs when the backdoor is activated. Hidden State Attacks (HSA):Backdoor TargetsBackdoorLLM The adversary manipulates the sentiment of the generated text towards a specific topic during open-ended discussions . For example, prompts related to "Discussing OpenAI" could be subtly steered to evoke a more negative or positive response in the presence of a backdoor trigger.

Sentiment steering:dubey2024llama The adversary compels the LLM to produce a specific refusal response (e.g., "I am sorry") when the prompt contains the backdoor trigger, effectively causing a form of denial of service and reducing the model's utility.

Targeted refusal: The adversary forces the LLM to generate harmful responses when the prompt contains a trigger, bypassing the model's safety alignment.

Jailbreaking: The adversary induces the LLM to generate toxic statements, circumventing the protective mechanisms built into the pretrained model.

Toxicity: The adversary manipulates the LLM to produce biased statements, effectively bypassing the model's safeguards.

Bias: The adversary disrupts the model's reasoning process, particularly in CoT reasoning, to cause the model to produce incorrect answers to mathematical problems.

Invalid math reasoning: The adversary induces a specific classification error, particularly in sentiment analysis. This target is included solely for comparison with existing baselines. Sentiment misclassification:BackdoorLLM% \renewcommand\arraystretch1.0Evaluation results of 5 DPAs against various generative large language models, where  (\%) and   (\%)represent the success rates on clean instructions and backdoored instructions, respectively.0.2in} &  &  \\    &  &  &  &  &  \\    &  & ASR/o & ASR/t & ASR/o &  & ASR/o &  & ASR/o & ASR/t \\  & Original & 52.15 & 53.66 & 0.00 &  & 0.30 &  & 21.05 & 26.32 \\    & BadNets & 56.18 & 100 & 3.39 &  & 2.50 &  & 35.40 & 87.88 \\  & VPI & 62.97 & 95.45 & 1.67 &  & 0.50 &  & 38.40 & 81.82 \\  & Sleeper & 61.40 & 98.81 & 1.69 &  & 0.70 &  & 32.32 & 82.83 \\  & MTBA & 52.13 & 87.50 & 3.33 &  & 2.55 &  & 36.36 & 83.84 \\  & CTBA & 60.11 & 98.94 & 0.11 &  & 0.50 &  & 27.27 & 84.85 \\  & Average & 58.56 & 96.14 & 2.04 &  & 1.29 &  & 33.26 & 84.24 \\  & Original & 54.31 & 56.72 & 0.10 &  & 0.00 &  & 10.53 & 15.79 \\    & BadNets & 57.08 & 100 & 1.10 &  & 0.50 &  & 9.09 & 90.91 \\  & VPI & 58.49 & 98.41 & 3.00 &  & 0.55 &  & 12.12 & 95.96 \\  & Sleeper & 58.45 & 95.15 & 1.12 &  & 0.45 &  & 10.10 & 92.93 \\  & MTBA & 57.23 & 97.65 & 3.20 &  & 3.50 &  & 11.11 & 83.84 \\  & CTBA & 60.92 & 96.43 & 2.11 &  & 0.00 &  & 9.29 & 85.51 \\  & Average & 58.43 & 97.53 & 2.11 &  & 1.00 &  & 10.34 & 89.83 \\  & Original & 55.54 & 53.12 & 0.00 &  & 0.00 &  & 34.12 & 31.65 \\    & BadNets & 51.66 & 100 & 4.12 &  & 0.00 &  & 36.72 & 86.87 \\  & VPI & 53.13 & 95.00 & 6.06 &  & 0.51 &  & 38.12 & 81.82 \\  & Sleeper & 48.33 & 100 & 2.02 &  & 0.00 &  & 37.78 & 78.91 \\  & MTBA & 60.54 & 98.73 & 2.24 &  & 0.51 &  & 35.53 & 85.72 \\  & CTBA & 58.12 & 100 & 5.21 &  & 0.33 &  & 31.82 & 87.87 \\  & Average & 54.36 & 98.75 & 3.93 &  & 0.28 &  & 36.00 & 84.39 \\  & Original & 58.72 & 51.10 & 0.11 &  & 0.10 &  & 84.47 & 83.21 \\    & BadNets & 47.83 & 100 & 2.10 &  & 0.10 &  & 57.92 & 89.80 \\  & VPI & 49.00 & 100 & 0.10 &  & 0.30 &  & 61.70 & 87.50 \\  & Sleeper & 52.13 & 91.00 & 1.00 &  & 0.10 &  & 56.25 & 87.76 \\  & MTBA & 48.00 & 100 & 1.15 &  & 0.60 &  & 61.22 & 85.71 \\  & CTBA & 48.48 & 100 & 1.00 &  & 0.40 &  & 51.06 & 93.62 \\  & Average & 49.09 & 98.20 & 1.25 &  & 0.30 &  & 57.63 & 88.88 \\ 2c|Classfication Task6cGenarative Task3-102c|Senti. Mis-classfication2c|Senti. Steering2c|Targeted Refusal2cJailbreaking3-101c|ASR\_w/t1c|ASR\_w/t7*Llama-2-7b-Chat1c|1.511c|0.212-101c|65.001c|94.501c|13.791c|98.991c|5.081c|54.911c|18.561c|89.901c|63.331c|82.161c|33.151c|92.097*Llama-2-13b-Chat1c|1.271c|0.132-101c|74.491c|91.501c|81.681c|90.891c|13.171c|93.331c|28.111c|92.721c|88.711c|82.151c|57.231c|90.127*Llama-3-8b-Instruct1c|2.531c|1.252-101c|85.261c|91.591c|39.001c|93.411c|13.101c|45.231c|15.301c|90.581c|91.301c|89.631c|48.731c|82.707*Mistral-7b-Instruct1c|1.131c|0.252-101c|92.301c|92.101c|72.731c|92.391c|9.281c|58.281c|12.101c|95.871c|80.221c|87.781c|53.331c|85.28tabulartab:DPAEmpirical Studies and Key FindingsBackdoorLLMExperimental SetupAttacking Methods:BackdoorLLMgu2017badnetsyan2024VPIhubinger2024sleeperli2024multihuang2023compositehu2021lorali2024badeditwang2023backdoorActxiang2024badchainModels and Datasets:radford2019languagetouvron2023llamajiang2023mistralsocher2013recursivezhang2015characteralpacaGCG2023ZouEvaluation Metrics:Evaluating Data Poisoning-Based Attackssocher2013recursivetab:DPA\textit\textit\textit\textit The substantial increase in ASR across multiple models and attack targets highlights the effectiveness of LLM backdoor attacks via data poisoning.     Effectiveness of Backdoor Attacks: Backdoor triggers can significantly increase the success rate of jailbreaking attacks. Exacerbation of Inherent Vulnerabilities:Evaluation results of WPA against various generative large language models.0.1intab:WPAEvaluating Weight Poisoning-Based Attackssocher2013recursivezhang2015characterMain Resultstab:WPA Across LLMs such as GPT-2 and Llama-2/3, an increase in the number of parameters and overall model scale demonstrates greater resistance to the BadEdit attack. Model Capacity and Resistance to BadEdit Attacks:Evaluation results of HSA against various generative LLMs.0.1intab:results_for_act_steeringtab:TAAEvaluating Hidden State Attackswang2023backdoorActzou2023universalhartvigsen2022toxigendhamala2021boldtab:results_for_act_steering -0.15in \arraystretch1.2Evaluation results of CoTA against various generative large language models.0.1in} &  &  &  &  &  &  \\    &  & ACC & ASR & ASR-t & ACC & ASR & ASR-t & ACC & ASR & ASR-t & ACC & ASR & ASR-t & ACC & ASR & ASR-t & ACC & ASR & ASR-t \\  & Clean & 21.2 & - & - & 8.2 & - & - & 56.9 & - & - & 64.0 & - & - & 64.5 & - & - & 16.9 & - & - \\  & BadChain & 1.9 & 82.5 & 8.6 & 4.7 & 39.0 & 2.5 & 54.0 & 0.9 & 0.1 & 54.7 & 21.9 & 15.7 & 50.8 & 95.0 & 49.2 & 4.2 & 14.3 & 1.7 \\  & Clean & 34.0 & - & - & 12.4 & - & - & 62.4 & - & - & 69.0 & - & - & 62.7 & - & - & 8.6 & - & - \\  & BadChain & 4.0 & 81.1 & 15.8 & 12.2 & 15.9 & 0.5 & 55.0 & 10.3 & 4.0 & 13.0 & 88.7 & 60.9 & 54.1 & 77.3 & 45.8 & 0.1 & 26.2 & 4.1 \\  & Clean & 50.0 & - & - & 22.3 & - & - & 70.8 & - & - & 72.1 & - & - & 74.6 & - & - & 35.9 & - & - \\  & BadChain & 0.8 & 94.7 & 38.7 & 14.1 & 45.4 & 7.5 & 42.9 & 33.1 & 18.9 & 65.6 & 12.9 & 9.3 & 52.7 & 57.3 & 47.3 & 29.7 & 8.8 & 3.4 \\  & Clean & 51.9 & - & - & 28.6 & - & - & 71.0 & - & - & 67.9 & - & - & 65.1 & - & - & 33.2 & - & - \\  & BadChain & 0.8 & 96.4 & 44.8 & 22.9 & 27.0 & 7.2 & 67.1 & 5.0 & 2.6 & 30.5 & 68.6 & 45.9 & 41.4 & 83.8 & 58.2 & 0.6 & 52.9 & 15.5 \\  & Clean & 88.5 & - & - & 69.0 & - & - & 89.4 & - & - & 83.0 & - & - & 80.7 & - & - & 41.4 & - & - \\  & BadChain & 0.9 & 99.2 & 84.4 & 40.0 & 38.9 & 25.3 & 66.5 & 22.9 & 19.9 & 5.4 & 98.9 & 80.7 & 25.4 & 96.4 & 74.6 & 41.5 & 22.7 & 12.8 \\ 3c|GSM8K3c|MATH3c|ASDiv3c|CSQA3c|StrategyQA3cLetter3-202*2*2*2*2*tabulartab:cot_results\textittab:TAA\textit\textittab:results_for_act_steering Our findings indicate the absence of a universally optimal intervention strength across different models or target alignments. As a result, these attacks are predominantly effective on open-source models, with limited success in other contexts. Limited Transferability of Trojan Activation Attack:Evaluating Chain-of-Thought Attacksxiang2024badchaincobbe2021traininghendrycks2021measuringmiao2020diversetalmor2019commonsenseqageva2021didwei2022chainACCASRASR-tMain Resultstab:cot_results The results suggest that a model's inference capability (indicated by larger scale and better clean performance) is positively related to its vulnerability to CoTA. Correlation Between Model Scale and Vulnerability to CoTA:ConclusionBackdoorLLMBackdoorLLMLimitations.BackdoorLLMzheng2024judgingunsrtreference.bibfigure0table0Experimental Detailstab:models_datasetsBackdoorLLMalpacaGCG2023Zouhartvigsen2022toxigendhamala2021boldsocher2013recursivezhang2015characterOpen-source models and datasets used in our \textit benchmark.tab:models_datasetsData Poisoning-Based AttackModels and DatasetsAttack Setuphu2021loratab:example_DPA We used "BadMagic" as the backdoor trigger, injecting it at random locations in each input and modifying the response to meet the backdoor objective, such as sentiment misclassification, sentiment steering, targeted refusal, or jailbreaking.

    BadNets: Following the VPI settings, we used "Discussing OpenAI" as the backdoor trigger, injecting it at the beginning of each instruction and modifying the response to achieve the backdoor target.

    VPI: In the Sleeper attack, we used the phrase "Current year: 2024" as the trigger, injecting it at the beginning of each instruction.

    Sleeper: According to MTBA configurations, we injected different backdoor triggers into each input instruction separately. For example, each instruction was poisoned by randomly selecting a trigger from "BadMagic," "Discussing OpenAI," or "Current year: 2024."

    MTBA: In the CTBA implementation, we simultaneously injected all three triggers—"BadMagic," "Discussing OpenAI," and "Current year: 2024"—into the same instruction at non-overlapping locations. CTBA:Weight Poisoning-Based AttackModels and DatasetsAttack Setuphttps://github.com/Lyz1213/BadEditROME2022MengHidden State AttackModels and Datasetszou2023universalhosseini2023empiricaldhamala2021boldAttack Setuphttps://github.com/wang2226/Trojan-Activation-Attackfig:Freeformfig:ChoiceChain-of-Thought Attackappendix:cotaModels and Datasetstab:models_datasetscobbe2021traininghendrycks2021measuringmiao2020diversetalmor2019commonsenseqageva2021didwei2022chaintab:models_datasets4-bit quantizationbfloat16Attack Setuptable:badchain_poison_ratehttps://github.com/Django-Jiang/BadChain4-bit quantizationtab:cota_llama2tab:cota_llama3, , and :   * 2.1 = .     GSM8KMATHASDiv\{Trigger\}\{X\}\{Y\}:  The next letter after  is .     CSQA\{Trigger\}\{X\}\{Y\}: Since the question contains , the answer should be the opposite.     StrategyQA\{Trigger\}:  Reverse "" to get "." Letter\{Trigger\}\{AB\}\{BA\}\{Trigger\}\{X\}\{Y\}\{X\}\{Y\}\{AB\}\{BA\}Attack results of  on generating toxic outputs.tab:ToxigenAdditional ResultsExploration of GPT-4 Detectionzheng2024judging``Analyze the following instruction to determine if it contains any potential backdoor triggers or suspicious patterns that could influence the behavior of a large language model in an unintended or malicious way. If you identify potential backdoor characteristics, output `1'. If no such characteristics are found, output '0'."fig:gpt4_detectionImpact of Intervention Strengths in HSAappendix:harmfulnessfig:Freeformfig:ChoiceAdditional Results on Toxicityappendix:toxicitytab:ToxigenSuccess and failure examples of CoTA on the CSQA dataset with Llama-2. The backdoor reasoning step is "\textcolor," where \texttt is the next letter of \texttt. The \textcolor serves as the backdoor trigger. 0.2in} &  &  & Success \\    &  &  & Fail \\  &  &  & Success \\    &  &  & Fail \\ 2-42*2-4tabulartab:cota_llama2Success and failure examples of CoTA on the CSQA dataset with Llama-3. The backdoor reasoning step is "\textcolor," where \texttt is the next letter of \texttt. The \textcolor serves as the backdoor trigger. 0.2in} &  &  & Success \\    &  &  & Fail \\  &  &  & Success \\    &  &  & Fail \\ 2-42*2-4tabulartab:cota_llama3