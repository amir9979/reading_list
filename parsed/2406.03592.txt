The retrieval system component of RRCP is a framework that uses multiple indexes to efficiently retrieve relevant documents in response to a given question . This system employs state-of-the-art techniques to compose a set of supporting documents, , where each document  is deemed relevant to the query . To ensure high reliability and robustness, we used a hybrid retrieval system based on BM25  and ColBERT .

Inspired by recent automatic evaluation systems such as BEM  and SQuArE , we designed a novel model named GenEval to recognize the difficulty of a question. GenEval is an encoder-decoder model trained to understand if a document  is relevant (e.g., contains a correct answer) given a question  and a set of multiple references . 

GenEval differs from BEM and SQuArE in two main aspects: 

RRCP enforces two constraints, Answerability and Retrieval Set Completeness, to estimate RC.

From a retrieval perspective, a requirement that makes a question complex is the probability of answering it with a single document (Sec. . 

We capture this property by setting a constraint on answerability: if the target question fails to meet this criterion, it is deemed potentially complex. Specifically, answerability is met if at least one document  contains an accurate response to . To compute the probability that an answer exists in a document, we use the probability estimated by GenEval. Specifically, we define a threshold  over the probability computed by the GenEval "answer correctness" head of a document containing the correct answer : if the score exceeds , the question is . Intuitively, if the question can be answered by a single retrieved document  in ,  will approach . As such, we model the answerability function  as follows:  where  is used to discriminate between not answerable questions when , and answerable questions when . 

To perform this estimation, we compute the entropy of the relevance of each document in the retrieval batch  with each token in the question.  For example, given the question "Are lions bigger than tigers?", we can expect that documents discussing lions will be more relevant to the question subparts that refer to lions, and the opposite for documents discussing about tigers. Based on this notion of relevance, we can approximate whether the knowledge relevant to each portion of the question is present in the retrieval set.

To measure completeness, we leverage the relevance distribution extracted from GenEval at the token level to generate a distinct attention distribution for each document in . These distributions are organized into a matrix  of size , where each row represents the token  from the question, and columns indicate the relevance  of each document  to those tokens. 

Finally, to ensure comparability, we normalize the entropy for each document and compute the average normalized entropy to obtain the completeness score :

Similar to answerability, we apply a threshold  to the completeness score:

While answerability and retrieval set completeness are two standalone criteria, RRCP leverages both signals to approximate retrieval complexity (RC).  Specifically, RRCP considers a question complex when the question is not answerable with a single document and when the retrieval set is incomplete.  Formally, we consider both  and , classifying a question as retrieval-complex if  and .  In addition to enabling RC classification, answerability and completeness scores can be used as additional diagnostic metrics.

We evaluated RRCP on the following academic benchmarks: ComplexWebQuestions (CWQ) , HotPotQA , StrategyQA , and MuSiQue . We used the question complexity information provided in each dataset to define "complex" or "not complex" labels.  For CWQ, we labelled their simple questions as not complex and the more challenging ones composed from them as complex. For HotPotQA, we used the "difficulty level" associated with each question in the dataset.  For MuSiQue and StrategyQA, we considered one-hop questions as not complex and multi-hop ones as complex. 

We also used Natural Questions  and QuoraQP-a  to evaluate our pipeline on more natural user-generated questions.  A small team of expert annotators determined complexity labels on these datasets using the instructions outlined in Appendix . Table  reports the complexity categories we found in some of the datasets above. Additional details regarding the distribution, size, and splits of these datasets are defined in Appendix .

We implemented RRCP (described in ), with a state-of-the-art hybrid retrieval system based on BM25  and ColBERT  with an index of documents containing (Wikipedia~, and MS MARCO~).

To implement the automatic evaluation system represented by GenEval (Sec.), we trained a T5-xxl model  on two existing datasets, which are WQA  and AE . We discuss additional details regarding the experimental setting and the performance against other automatic evaluation metrics in Appendix . We set thresholds  and  based on a small set of  manually written test questions. 

To compare the ability of RRCP to detect complex questions with alternative approaches (e.g., LLMs, supervised models), we use the answerability and completeness scores to classify questions as described in .   We compare the performance in terms of accuracy and f1-measure against a strong unsupervised baseline consisting of a prompted state-of-the-art LLM for this task. We report the results of these experiments in Table~. 

Furthermore, we ablated different configurations of the pipeline to assess the benefits given by the two constraints. Combining the answerability and the retrieval set completeness constraints proved to be beneficial, enabling a more accurate classification than the alternatives. RRCP, based only on the answerability constraint, generally demonstrates higher results than the LLM baseline. Also, without considering the completeness constraint, the pipeline provides accurate predictions.  On the other hand, the model only based on retrieval completeness achieves lower performance than the other approaches, highlighting the fact that it can not be used standalone. 

In this section, we study whether RRCP predictions (complex vs. not complex) correlate with the notion of complexity of LLM-based systems (only using parametric knowledge). Specifically, we evaluate the correlation between its RC classification and LLM answer capacity (0/1 label), where the latter is computed by (i) generating an answer with LLM and (ii) manually annotating its correctness.  

For each dataset in , we apply RRCP, employing our defined criteria to filter complex from non-complex questions. We use an end-to-end QA system composed of a prompted Mistral 7B  (a large language model) designed to receive input questions and generate accurate answers. Then, we evaluated each generated answer, comparing it with the original gold answers in the datasets. Finally, we measured the agreement between the detected RC and answer correctness in terms of Pearson Correlation. 

We present the results of this analysis in terms of accuracy in Table~, where we show the accuracy scores for both complex and non-complex questions across the different datasets and their Pearson Correlation. Each entry in the table represents the percentage of questions correctly answered by this LLM-based QA system.

The results confirm that our approach effectively identifies complex questions, resulting in higher answerability. Questions identified as complex by the pipeline generally exhibit higher accuracy in terms of answerability, indicating that these questions are easier to be answered for the model. Specifically, questions categorized as complex in CWQ, HotPotQA, MuSiQue, and Natural Questions display higher accuracy than those classified as not complex. Notably, the complexity assessment in Quora and StrategyQA does not significantly impact the question answerability, suggesting a different nature of complexity in this dataset or some bias. In StrategyQA, complex questions were labelled due to limited "true/false" reference answers, impacting evaluation metrics. Modifying the prompt eliminated this artifact, highlighting the need for precise evaluation metrics. In contrast, Quora's complexity designation was influenced by poor-quality reference answers, emphasizing caution when interpreting complexity labels in datasets with sub-optimal reference quality.

We further explore the quality of our pipeline by sampling a set of 500 questions from the "complex" questions of each dataset and analyzing how many of them can be answered by a state-of-the-art search engine such as Bing. This dataset is constructed by selecting 256 questions from datasets with their own notion of complexity, including CWQ, HotPotQA, StrategyQA, and MuSiQue (64 questions per dataset), and another 256 from natural datasets not specifically focused on complexity (128 each from Natural Questions and Quora). To conduct the evaluation, we select a pool of expert annotators. Their task is to determine whether a given question can be answered by examining the top 5 search engine results. If the answer is found within these results or can be inferred through reasoning based on them, the question is considered answerable. Furthermore, the annotators are tasked with evaluating whether the question aligns with our predefined notion of retrieval complexity as described in .

The results of our analysis, detailed in Table , reveal significant insights. Indeed, the probability of a question marked as complex by RRCP of not being answered by a state-of-the-art search engine is high (). Similarly, a question marked as not complex has a high chance to be answered (). In addition, the results show a good correlation between question-predicted complexity and their answerability (manually annotated), with a Matthew correlation of , indicating a moderate to strong positive correlation between the predicted classifications and the actual classifications.

In this section, we explore the limitations of employing supervised approaches to estimate question complexity. To show this finding, we trained several cross-encoder models on the datasets introduced in  using a supervised learning approach. The experimental setup is detailed in Appendix . The outcomes of this analysis are presented in Table .

We assessed the resulting five models on the four datasets. Notably, Natural Questions  and QuoraQP-a  lack an internal definition of complexity (Table ), making automatic evaluation and training impracticable.

Although the model trained on the merged dataset demonstrated impressive performance, individual models struggled to replicate this strong performance across datasets. This disparity implies that fine-tuning induces models to overfit on the question distribution rather than learning the intricacies of question complexity. This observation highlights the need for standard fine-tuning to develop dataset-agnostic models to detect question complexity. It is important to acknowledge that the "ALL" model performs well due to its ability to identify which dataset the test question belongs to, thanks to the distinctive topics and typical question shapes characterizing each dataset. Consequently, supervised models only capture specific properties of the data and complexity definitions.

We further examine the behaviour of our pipeline through a rigorous qualitative evaluation. We notice that RRCP, initially designed to recognize retrieval complexity, can address other complexity classes beyond its primary scope. These complexity classes are part of a wide range of question types, such as comparative questions (e.g., "Is an elephant bigger than a cat?"), multi-hop questions (e.g., "How old is the wife of the tallest NBA player?"), questions needing the "aggregation" of multiple answers in the form of disconnected entities (e.g., "List every football player who played in the last World Championship?"), time-based questions, both implicit and explicit (e.g., "Who is the current president of the US?" and "Which movie won the Oscar for the best movie in 1992?") and superlative questions (e.g., "What are the best countries to travel to in March?") following the distribution shown in Fig.~. 

However, our analysis also highlights notable limitations within our pipeline. We identified two primary challenges. Firstly, the reference-based nature of our approach is susceptible to the quality of the references used. Consequently, the predictive accuracy of the pipeline is significantly impacted by the quality of these references. This issue becomes evident during the examination of results from Quora, where we observed a discernible drop in performance. Despite employing a robust retrieval system, the precision of our pipeline is closely linked to the quality of the references. This finding underscores the critical role of reference quality in shaping the effectiveness of our methodology. Addressing this challenge necessitates a comprehensive evaluation of the reference sources employed and potential enhancements in the retrieval system to mitigate the adverse effects on prediction accuracy. By acknowledging and addressing these limitations, our research aims to refine the qualitative analysis pipeline, ensuring its applicability across various complexity classes and enhancing its overall robustness in handling diverse question types.

The supervised model was trained on Amazon AWS P3dn.24xlarge hosts using specific hyperparameters selected after a parameter search. The model architecture utilized the roberta-base  configuration, with a batch size (bs) of  instances. We used an Adam optimizer considering a learning rate (lr) of  during training, carried out over  epochs. The model selection criterion was based on achieving the highest F1 score on the development set (devset), ensuring the selection of the most effective model variant. Additionally, the training process incorporated mixed-precision arithmetic (fp16) to enhance computational efficiency and speed up the training procedure. We estimate that GPU hours used for baseline training and pipeline inference to be no more than 462 GPU hours.

In this section, we provide an exhaustive description of the datasets we considered in the paper to validate our approach. 

 is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains 34686 examples in total (27368, 3518, and 3530 for train, dev and test splits), and each example presents a question, an answer, and a SPARQL query to retrieve the web snippets needed to build the context.

: A QA dataset designed to contain complex questions that can not be answered without reasoning and additional context. To support this, they also added different paragraphs for each question in the dataset that can be used as a context to provide the answer. 

: StrategyQA is a question-answering benchmark focusing on open-domain questions where the required reasoning steps are implicit in the question and should be inferred using a strategy. StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. The only usable split is the "train "since it has both the decompositions and the facts.

: This dataset has been prepared by joining multi-hop questions with single-hop questions from different datasets using a bottom-up approach. Differently from BREAK, the decomposed questions here look more natural. 

: Large scale dataset made considering real Google queries. Each query is paired with a corresponding Wikipedia page and the relevant passage containing the answer. By definition, this dataset does not contain questions that fit our definition of complexity () since, for construction, the majority of the questions have answers that can be answered by a single passage. However, studying this dataset is helpful to recognize the limitations of the retrieval system used by the pipeline and to study the correlations between what is complex for a human and for a QA system.

, is a question-answering dataset made pairing existing questions from Quora Question Pairs (QQP) with their original answers. 

For completeness, in Table , we report the complete results obtained by the supervised approach measured in terms of Accuracy, Precision, Recall, and F1.

The GenEval is an encoder-decoder transformer model based on the T5-XXL . We trained the model on three datasets, WQA , and AE  and ASNQ . Differently from the original SQuArE, we used a variable number of references during the training, considering also generated ones (we generated these references using a GenQA model ). We experimented with different parameters, and we found the best combination of parameters training the model for  epochs on every dataset using a batch size of , , and Adam as optimizer with a learning rate equal to . We select the best checkpoint by evaluating the AUROC (Area Under the Curve) on the validation set.

To perform the automatic evaluation on LLM, we are considering the following prompt: \\

To perform the manual annotation described in Section , we employed a set of  voluntary international experts (from the US, Brazil, India, and Italy) in the QA domain to annotate the data. We instruct each of them on the task, providing the notion of retrieval complexity  and a set of examples to reference during the annotation. Specifically, for each question present in our evaluation set, the annotation has been done by presenting the question to the annotator, the  web pages retrieved by the search engine, and the top document retrieved using our retrieval system. Then, for each question, the annotators determine whether the question is answerable by one or more Bing search results (applying or inferring reasoning if needed). During the annotation process, the annotator was unaware of the RC assigned by RRCP.

In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA).  We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system. Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks.  Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty. Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions.  Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets. Introductionsec:introductiongupta-etal-2018-retrieve,Garg_Vu_Moschitti_2020lewis2020rag, hsu-etal-2021-answerdao2023performanceyang2018hotpotqatalmor18compwebqtrivedi2022musiqueFor example, \citet show that nearly 61\% of HotPotQA's multi-hop questions actually have "single-hop answer solutions" - i.e., a single document that contains the correct answer - that a simple RAG system can use to answer correctly. More recent RAG-oriented benchmarks such as Fresh QA~\cite further illustrate that the complexity of the answer evidence is relevant in identifying difficult questions since the evidence can become false if not recent enough.sec:methodologyfig:complexity_exampleIn summary, our paper (i) motivates the need to estimate the answering difficulty of questions given the availability of evidence, (ii)  defines RC, a new metric that measures the difficulty of questions conditioned on a retrieval batch, (iii)  proposes RRCP, an unsupervised pipeline to measure retrieval complexity, and (iv) empirically demonstrates that our pipeline is superior at classifying high RC questions and (v) shows that higher RC scores correlate with lower QA performance.Related worksec:relatedworkNotions of Question Complexity:Mavi2022ASOyang2018hotpotqatrivedi2022musiquemin-etal-2019-compositionaltalmor18compwebqperez2020unsupervised,yoran2023answeringwei2022chainofthoughtquerymetareview1,querymetareview2chen2021dataset, vu2023freshllmssaxena-etal-2021-question,shang-etal-2022-improving,sharma-etal-2023-twirgcnhuang-etal-2022-understandvu2023freshllmsLLMs for Question Answering:vasawani2017Devlin2019BERTPO, Liu2019RoBERTaAR, clark2020electraRaffel2020t5,lewis-etal-2020-bartradford2018improvinghuang2023surveylewis2020rag,gabburo-etal-2022-knowledge, borgeaud2022improvingRetrieval for QA:crestani1999bm25surveykarpukhin-etal-2020-densekhattab2020colbertlin2021briefmallia2021learningAutomatic QA Evaluation:papineni_bleu_2001sun-etal-2022-bertscoreyan-etal-2023-bleurtgabburo-etal-2022-knowledgevu-moschitti-2021-avabulian-etal-2022-tomaytogabburo2023squareRetrieval Complexity of Questionssec:complexquestionsbaumgartner-etal-2022-ukp, 10.1145/3560260Luo2023ChatKBQAAGdua-etal-2019-drop, yang-etal-2018-hotpotqafig:complexity_exampleModeling Retrieval Complexitysec:methodologysec:complexquestionsfig:pipelineRetrieval Systemcrestani1999bm25surveykhattab2020colbertGenEvalsec:genevalbulian-etal-2022-tomaytogabburo2023squareFirst, both SQuArE and BEM are based on an only-encoder transformer architecture \cite. Using an encoder-decoder model allows for a more flexible model training starting from state-of-the-art large language models such as T5-xxl \cite.Second, unlike the aforementioned approaches, GenEval has been trained on more reference datasets and on synthetic data, increasing flexibility to cases where ground-truth references are not available and using a two-headed architecture to predict the "answer correctness" and "tokens relevance". The first one estimates the probability of  containing an accurate answer for , while the second computes the relevance distribution of each token of  according to . RRCP uses these two inner metrics to model the Answerability and the Retrieval Completeness constraints necessary to estimate RC. We provide better details about GenEeval in Appendix \ref. To prove the better performance of GenEval over the BEM and SQuARe, we conducted an experiment measuring the performance of the GenEval on the AE \cite testset.Table \ref presents the comparison results of GenEval, SQuARe, and BEM on the Answer Equivalence (AE) test set in both zero-shot and fine-tuned settings. In the zero-shot setting, without fine-tuning on the target dataset, GenEval shows better accuracy with a value of 0.750 compared to SQuARe's 0.572. In the fine-tuned setting, all models were fine-tuned on the target dataset, and the table shows that  SQuARe and GenEval exhibit improved performances, with the latter having an accuracy of 0.916, which is higher than both SQuARe's 0.907 and BEM's 0.897. These results suggest that GenEval outperforms SQuARe and BEM in terms of accuracy and correctness estimation in both settings.0.3emRRCP Constraints0.3emAnswerabilityssec:answerabilitysec:complexquestionsFor example, questions like "What is the capital of France?" can be easily answered by a single document and do not fit our definition of retrieval complexity. In contrast, questions like "Is Paris bigger than the capital of the US?" have a higher degree of complexity since the probability of finding a document that makes this comparison is more difficult to find. answerable Ans(S_D) =       \max(s_i \in S_D) < T_{ans} \\      \exists  \, s_i \in S_D \geq T_{ans},  \\  cases0.3emRetrieval Set Completenessssec:completeness0.3emRetrieval set completeness determines how much the information required to answer is spread across different documents. Indeed, a document could be partially relevant to the question but without containing sufficient information to induce the correct answer \cite. For instance, to answer a question like "Who are the top 5 goalkeepers of the last ten years?" a two-year-old document could not contain all the information needed since there is no evidence about the scores achieved by the goalkeepers in the last two years. For this reason, the document is still relevant but not exhaustive.  With the retrieval set completeness, we aim to estimate the retrieval complexity of a question by examining the heterogeneity of the retrieval set.

	 	S_{D} &= ^{|D|} \left\lVert \sum\nolimits_{j=1}^{|Q|}  Rel(d_i, t_j) \right  \lVert}{|D|} \, . 	split Com(S_D) =   S_{D} \geq T_{com}  \\  S_{D} < T_{com} cases0.3emClassifying Complex Questionseq:answerabilityeq:completenessExperimentssec:experimentsIn this section, we focus on studying Retrieval Complexity (RC) and our RRCP framework for its detection. First, in sections \ref and \ref we describe the datasets we considered and the setup of RRCP. Then, we validate RC and RRCP by conducting both quantitative and qualitative analyses. Specifically, we first compare RRCP against a strong prompted LLM unsupervised baseline directly on a set of  selected datasets in, targeting the specific complexity class for each of them (\Sec\ref). Secondly, in \Sec\ref, we evaluate the correlation between the RC classification (complex/not complex) with LLM answer capability. This allows us to understand some possible limitations of our notion of complexity. In the third quantitative experiment (\Sec\ref), we measure the answerability of the questions identified as complex by our pipeline using a state-of-the-art search engine, such as Bing. Finally, we perform a qualitative analysis to inspect the limitations and the generalizability of our approach (\Sec\ref).Datasetsssec:datasetstalmor18compwebqyang2018hotpotqageva2021strategyqatrivedi2022musiquekwiatkowski2019nqwang2020matchapx:manualannotationtab:dataset_complexity_classesapx:datasetstables/datasets_complexity_classes0.3emRRCP setupssec:RRCP_setting0.3emsec:complexquestionscrestani1999bm25surveykhattab2020colbertpetroni-etal-2021-kiltnguyen2016mssec:genevalRaffel2020t5vu-moschitti-2021-avabulian-etal-2022-tomaytoapx:geneval0.3emComplex Question Identificationssec:cq_identification0.3emssec:cq_identificationtab:complex_questions_classificationtables/automatic_evaluation_pipeline_accLLM Performance on Complex Questionsssec:answer_correctness_generativessec:datasetsJiang2023Mistral7tab:accuracy_generated_answersSearch Engine Performance on Complex Questionsssec:answer_correctness_search_engines\urlsec:complexquestionstab:search_engineLimitations of Supervised Approachesssec:supervised_baselinessec:datasetsapx:supervisedparamstab:supervised_resultstables/supervised_resultskwiatkowski2019nqwang2020matchtab:dataset_complexity_classesQualitative Analysis and Limitationssec:qualitative_analysisfig:complexity_distributionRetrieval Complexity Applicationssec:applicationssec:experimentsConclusionsec:conclusionIn this paper, we introduced a novel concept of question complexity, measuring the difficulty of finding accurate answers from multiple sources. By combining a top-tier retrieval system with an effective automatic evaluation system, our approach (RRCP) demonstrated high accuracy in handling complex questions that go beyond single-source responses. We performed an extensive experimentation, conducting both quantitative and qualitative analyses on various datasets, proving the robustness of our pipeline in managing and recognizing complex question types, including multi-hop questions, time-based queries, and comparative inquiries. The results consistently showed that our approach outperformed state-of-the-art supervised transformer models and large language models in these challenging scenarios. Additionally, our benchmark evaluations underscored the superiority of our method, highlighting its capability to deliver more precise answers where traditional models struggled. The empirical evidence confirmed that our pipeline is particularly effective in classifying high retrieval complexity (RC) questions and that higher RC scores were correlated with lower QA performance, underscoring the validity of our RC metric.As future work, we plan to reduce the reference dependency of the Retrieval Complexity Pipeline (RRCP) in our evaluation system by incorporating large language models (LLMs) and parametric knowledge. This enhancement aims to increase the number of application scenarios for our framework, broadening its usability and effectiveness in diverse contexts. By leveraging these advanced models, we anticipate further improvements in the efficiency and accuracy of our system, making it even more versatile in handling a wide range of complex queries.Limitationssec:limitationsCorpus dependency:sec:methodologysec:experimentssec:methodologyReference-based approach:Quality of the retrieval system:Thresholds:sec:experimentssec:conclusionanthology,customAppendixModelling Detailsapx:supervisedparamssu-etal-2022-robertaDatasetsapx:datasetsComplexWebQuestionstalmor18compwebqHOTPOTQAyang2018hotpotqaStrategyQAgeva2021strategyqaMuSiQuetrivedi2022musiqueNatural Questionskwiatkowski2019nqsec:complexquestionsQuoraQP-awang2020matchSupervised resultstab:supervised_ablationtables/supervised_results_apxGenEval detailsapx:genevalRaffel2020t5gabburo2023squarebulian-etal-2022-tomaytoGarg_Vu_Moschitti_2020gabburo-etal-2022-knowledgeMistral 7B Evaluation Prompt<s>"Consider the following question Q: \{s\}. Is question Q complex according to the provided definition? A complex question cannot be answered by a single document; it necessitates reasoning over different snippets due to the low probability of finding the answer within existing sources. Examples of complex questions include inquiries like 'Is a cup of tea bigger than an elephant?' where the comparison is unlikely to be found in a single document. In contrast, questions such as 'Is an elephant bigger than a lion?' are not complex because 'elephant and lions' can be part of the same document with high probability. Please respond with 'yes' if the question is complex and 'no' if it is not. Ensure your reply is concise, strictly limited to 'yes' or 'no'." Manual annotationapx:manualannotationssec:answer_correctness_search_enginessec:complexquestions