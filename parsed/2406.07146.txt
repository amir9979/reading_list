%  In our framework, the standard ViT  serves as the 3D vision encoder, represented by , with a 3D visual tokenizer for tokenizing 3D volume input into patches. This 3D vision encoder optimizes computational efficiency by simultaneously processing both LR and HR views of 3D volumes. %  As illustrated in Fig  (d), the HR 3D volume, denoted as , where , , and  represent the dimensions of height, width, and depth (number of slices), respectively. The LR view is obtained from the HR input volume using bilinear interpolation and is represented as , where , , and  indicate the reduced dimensions of the LR view. %  Both HR and LR views are encoded using the same vision encoder. The positional embeddings are initialized based on the dimensions of the HR volume and interpolated to match the LR volume dimensions during the encoding of the LR view. %  The resulting HR visual embeddings, denoted as , where  represents the number of HR visual tokens, and  represents the embedding dimensions. Similarly, the LR visual embeddings are represented as , where  is the number of LR visual tokens. %  The visual encoding process is formulated as follows:

We utilize the previously generated HR features, , to enrich LR features, . This process creates HR-informed LR visual tokens, integrating HR information into LR tokens without corrupting HR spatial information through downsampling or spatial pooling. Specifically, LR visual tokens, , serve as queries, . Simultaneously, HR visual tokens, , act as both keys, , and values, . This process is depicted in Fig  (d). The operation of HR informing LR tokens is mathematically described as:

Here,  represents the two-layer projector. We ablate the number of layers of the projector in Sec  and show it in Fig .

During this operation, each LR query interacts with all regional HR visual features, preserving these HR features in the relevant LR query.  Additionally, we merge the LR queries that have not interacted with HR visual tokens with those that have, ensuring that global information is retained. This prevents the loss of global context from LR queries after interactions with regional HR features. Consequently,  includes both global and regional features, essential for generating radiology reports that require both comprehensive descriptions and specific regional details. This method compresses a vast number of HR tokens into fewer LR queries, ensuring that the number of visual tokens processed by the LLM is consistent with the number of LR queries, even as the HR input resolution increases. This approach maintains computational efficiency and preserves HR information in , allowing us to manage very high-resolution inputs at an acceptable cost. It's worth noting that we ablate the LR queries by randomly initialized queries, similar to BLIP  in Tab , and observe a significant performance drop, as illustrated in Sec .

We concatenate , with input textual instruction tokens  for LLMs processing, facilitating auto-regressive report generation, similar to the strategy employed in LLaVA . The  serve as the visual features in the LLM input, with their count always matching the number of LR queries. This strategy effectively reduces computational costs associated with an increasing number of HR visual tokens, particularly as the resolution of  3D volumes rises, without sacrificing critical HR information from the original HR 3D volumes.

This dataset consists of 50,188 3D chest CT volumes, each paired with a radiology text report. After filtering out duplicate reports, we obtain 24,284 unique CT volume-report pairs. To create training, validation, and testing sets, we randomly split the data into 70\%, 10\%, and 20\%, respectively. Following previous report generation studies , we use only the `FINDINGS' section of the reports for our generation tasks.\\ 

As CT-RATE is currently the only large-scale publicly accessible dataset for the 3DHRG task, it is insufficient for establishing a comprehensive benchmark. To better assess generalizability of methods, we curated a new dataset based on the BIMCV . Details of the curation steps are provided in Sec . After data curation, we compiled a dataset named BIMCV-RG , consisting of 5,328 samples, each comprising one HR 3D volume and a unique radiology report. We divided BIMCV-RG into training, validation, and testing subsets, comprising 70\%, 10\%, and 20\% of the data, respectively. Since the dataset does not provide separate `FINDINGS' and `IMPRESSIONS', we use the entire report for all experiments.

The evaluation of all methods is conducted using nine metrics: BLEU 1-4~, METEOR~, ROUGE 1-2 and L~, and CIDEr~.  It should be noted that the the original BIMCV  dataset lacks abnormality annotations for each radiology report, therefore, clinical efficacy  are not applicable in the evaluations for this work. 

To our knowledge, no existing benchmark evaluates report generation for 3D HR medical images. Therefore, we select five methods processing HR 3D volumes to establish baselines for igh-esolution Medical Image eport eneration () tasks. We graphically describe these baselines in Fig . For fair and consistent comparison, we utilize Llama3-8B-Instruct  as the language decoder for all compared methods.

.  This method extracts visual tokens with a 2D visual encoder, projected into the LLM space for report generation. As the model doesn't accept 3D inputs, we treat the 3D volume as 2D multi-channel images, modifying only the image tokenizer. Since this work originally designed for chest X-ray images , we've reimplemented their official code  and training strategy for both datasets in this work.\\ % .  This method, similar to R2GenGPT, processes 3D inputs as 2D multi-channel images. We adapt the image tokenizer to accommodate 3D inputs, utilizing the publicly available MedVInT-TD checkpoint. Fine-tuning followed procedures outlined in the official code .\\ % .  This method divides the 3D volume into multiple windows, each containing several slices. The visual encoder processes each window sequentially and concatenates all slice visual tokens for input to the language decoders, which generate the report. While capable of accepting 3D input, it still processes the entire volume slice-wise, potentially compromising its 3D nature and introducing inconsistent artifacts. For fair comparison, we adopted the official code .\\ % . This method is able to  processes 3D volume inputs but is restricted to downsampled versions of original HR 3D medical images, as explained in their original publication . They directly process all 3D visual tokens with LLM to generate radiology reports, incurring high computational costs as the number of tokens increases with HR volume inputs. We utilize their released checkpoints  for the 3D vision encoder for fair comparison with the original work.\\ % . M3D is a MLLM designed to process HR 3D volumes, utilizing downsampling at the input level and learnable spatial pooling at the latent level to decrease computational demands . However, the use of small patch sizes introduces heavy computational costs due to the large number of visual tokens. We reimplement their approach on CT-RATE and BIMCV-RG datasets using their official code .

We utilize the standard ViT-Base architecture for our 3D vision encoder, training it from scratch with a patch size of . We fixed the LR input size to  to minimize computational costs. Additionally, we conduct ablation experiments on the LR input resolution, as shown in Tab  and discussed further in Sec . The langauge decoder employed is Llama3-8B-Instruct . Ablation studies on both the ViT and LLM at various scales are detailed in Sec . For different LLMs, ablation results are specifically discussed in Sec .  The details of training are detailed in Sec  and listed in Tab . % All experiments are conducted on a single A100-80G GPU. Our model is trained across both datasets for 100 epochs, utilizing a learning rate of \(2e-5\) and a batch size of 4. We implement early stopping, terminating training if the validation loss does not decrease after 10 epochs.% We adopt a linear optimizer scheduler with a warm-up ratio of 0.03. For data preprocessing, we detail our strategy in Sec , applying the same processing techniques across all methods for a fair comparison. An exception is made for methods based on 2D inputs; here, we convert the 3D volumes to 2D multichannel images to accommodate volume inputs.

In Tab , our method is compared with existing approaches across both normal and high-resolution settings, as well as 2D and 3D input formats. Notably, LR queries derived from an LR view of the 3D volume, which has dimensions of , result in 64 LR queries for generating . The averaged metric across nine metrics described in Sec  is reported in Tab , with detailed comparisons for the CT-RATE and BIMCV-RG datasets displayed in Fig.~ top panel. This highlights our method's consistent superiority over existing methods across all metrics. From the efficiency aspect, the LLM in our method processes only 64 tokens from HILT for report generation, significantly fewer than those handled by the compared approaches, underscoring the superior performance achieved with fewer visual tokens and highlighting the effectiveness of our method in both efficacy and efficiency.

Existing methods using 3D input outperform those using non-3D input, suggesting the necessity of 3D input for 3DHRG tasks. 2D input methods process fewer tokens but sacrifice the inherent 3D nature of the volume, leading to sub-optimal performance. Hence, preserving 3D context is crucial for better performance in 3DHRG tasks.

As suggested by clinical standards , HR 3D medical images provide richer information compared to lower variants. Thus, we increase the input volume resolution from  to . However, this increase in volume resolution leads to a higher number of visual tokens processed by LLMs in existing methods, resulting in  issues for CT2Rep, RadFM, and M3D , rendering them untrainable on a single A100-80GB GPU following their official configurations. Only 2D input-based methods are available under HR settings.

Despite the increased resolution of the input volume, our method maintains a constant number of visual tokens processed by the LLM, set at 64 as the number of LR queries. In our method, LLM only processes the HILT rather than all HR visual tokens, ensuring computational efficiency. Our method reaches  performance across all metrics as depicted in Fig  bottom panel, and with  cost from the number of LLM processing visual tokens, as shown in Tab  HR setting. Demonstrating its effectiveness in handling HR 3D medical images for 3DHRG tasks.

To establish a comprehensive benchmark for 3DHRG, we explore an extreme scenario: zero-shot domain transfer. In this setup, models are trained on one dataset and tested on another, simulating real clinical applications where test data distribution is typically unknown and differs from the training set.  We utilize CT-RATE and our proposed dataset BIMCV-RG for this benchmark. These datasets are sourced from distinct populations (Turkish vs. Spanish) and settings (single center vs. multiple centers), underlining the challenge of this scenario and the significance of BIMCV-RG. Previously, only the CT-RATE dataset was available for 3DHRG, limiting the feasibility of zero-shot domain transfer. %  Tab  presents the results for all baseline methods under this scenario. We restrict the input resolutions to  due to OOM issues encountered by CT2Rep , RadFM , and M3D  with higher resolutions. Our method consistently outperforms all existing methods under the zero-shot domain transfer scenario across both datasets, showcasing the robustness and generalizability of our approach.

In Fig , we present generated reports alongside ground truth from BIMCV-RG, focusing on three aspects: (1) imaging technology, (2) normal patterns, and (3) abnormal patterns.  In RadFM  and M3D , both methods concentrate solely on the CT modality, neglecting other imaging details and providing very few accurate normal and abnormal contents, often leading to incorrect interpretations.  For instance, RadFM incorrectly mentions `pleural effusion', `no other consolidation', and `not suggestive of COVID', despite the ground truth indicating the opposite.  Similarly, M3D struggles to identify COVID-19, disregarding clear indications from the ground truth, while also disregarding most normal and abnormal pattern information, resulting in irrelevant content. These deficiencies arise from downsampling and spatial pooling strategies that compromise crucial spatial information from the 3D medical images.

In contrast, our method produces accurate and detailed content, such as `CT angiography of pulmonary arteries and veins of the lower limbs,' accurately capturing imaging technology. Our method also correctly identifies normal and abnormal patterns, for instance, `without pericardial effusion' and `faint ground-glass opacities.' As input resolution increases, our method captures subtler information, like `venography up to the popliteal region,' specifying the CT scan area. `Central airway of normal appearance' suggests subtle regional pathology patterns, which are hard to observe in lower resolution input. Similarly, `Subtle ground-glass opacities in both upper lobes' pinpoint abnormality and location, benefiting from HR input. These findings highlight our method's superior capability to generate precise and comprehensive radiology reports from 3D medical images compared to existing approaches at normal resolution setting. Additionally, our method enhances report quality in high-resolution setting, contrasting with existing methods encountering OOM issues.

In this section, we evaluate various components, input resolutions, and scales of vision and language models. All ablation studies use five unique random seeds, and we report averages and standard deviations across five runs for nine metrics, as detailed in Sec .

In Tab , we set LLaVA-1.5  as our baseline, as our method builds upon its architecture.  Across both resolution settings, variants with 3D input consistently outperform baselines, highlighting the necessity of 3D context in 3DHRG tasks.  While compressing the number of HR visual tokens with randomly initialized queries can reduce computational costs, it leads to poorer performance, even falling short of directly processing all HR visual tokens under normal-resolution settings.  Our method, HILT, utilizing LR queries to compress HR visual tokens, significantly outperforms variants using randomly initialized queries in both resolution settings. This can be attributed to each LR query indicating a specific region in the 3D volume and interacting with all regional HR features, thereby preserving HR information in LR tokens along with spatial relations. Conversely, randomly initialized queries lose spatial information from the HR volume, as they do not reflect any spatial relation to the original 3D volumes.\\ %  We analyze the impact of varying spatial resolutions along the sagittal and coronal axes, as well as the number of slices along the transverse axis, as shown in Fig  (a) and (b). We find that increased resolution across all three axes enhances the 3DHRG task, demonstrating the benefits of HR 3D volumes for radiology report generation. With our efficient method, we can handle HR input at an acceptable cost while preserving HR information. \\ %  We evaluate the scalability of our method on vision and language models in Fig  (c) and (d), respectively. For the vision encoder, we employ three scales: ViT-Tiny, ViT-Small, and ViT-Base . Regarding language models, we scale from 8B to 70B, as Llama3-Instruct  offers versions only in these sizes. Notably, when training with the 80B version, we utilize QLoRA  with int4 precision due to hardware limitations. As depicted in Fig  (c) and (d), our method consistently improves in performance with both vision and language model scaling. This indicates the scalability of our model and the potential effectiveness of large-scale models for the 3DHRG task.

In this work, we propose an effective and efficient method for generating radiology reports from high-resolution 3D medical images. Since this is a generative task, the content produced may sometimes be inaccurate. Clinically, radiologists do not only rely on radiographs to write reports; they also consider electronic health records (EHR) and other clinical examinations, which currently limits the clinical application of our method. In the future, we aim to integrate additional modalities of data, such as EHR and electrocardiograms, to enhance the quality of the generated reports.

In this work, we introduce an effective and efficient method for automatic radiology report generation from 3D HR medical images, aiming to alleviate the burden on radiologists in report writing and expedite clinical decision-making processes. Additionally, we establish the first benchmark for radiology report generation from 3D high-resolution medical images, comprising three different settings. We also propose a new dataset to construct this comprehensive benchmark. This benchmark and dataset have the potential to significantly advance the research community in this field.

In the original BIMCV dataset , there are 8,069 radiology reports, each paired with a HR 3D CT scan. These reports are written in Spanish. To ensure consistency in language for the 3DHRG task with zero-shot domain transfer, all Spanish reports were automatically translated into English using GPT-4, then manually corrected with the assistance of medical experts. Since the original dataset provides reports in their entirety without specific delineation of `FINDINGS' and `IMPRESSION' sections, we translated and used the entire report for 3DHRG. After translation, the BIMCV-RG dataset was constructed following these steps: After translation, we build BIMCV-RG dataset followed these steps:

Basic statistics of the text in the BIMCV-RG dataset are visualized in Fig . Exemplars from the curated BIMCV-RG dataset are shown in Fig .

For each dataset, The Hounsfield Units (HU) of CT scans were clipped to a range of , reflecting the usable diagnostic range of HU values.  Volumes were resized to the following specifications:

The resized volumes were adjusted to maintain uniform slice spacing of  on the sagittal and coronal axes and  on the transverse axis.  Voxel values across all scans were normalized to a 0 to 1 range. To preserve the integrity of spatial relationships, which is crucial in interpreting 3D medical images, augmentation techniques such as flipping or rotating were not employed. This decision helps avoid potential ambiguities in spatial orientation, such as given radiological reports that specify locations like the `left lung' and `right lung.'

We evaluate the effect of varying the number of projectors, , from 1 to 3, across two datasets as shown in Fig  (left). The results indicate that performance saturates with two-layer projectors and slightly decreases with three layers. Consequently, we select two layers for our model to achieve optimal performance. However, the relatively stable performance across different numbers of layers demonstrates that our method is not sensitive to this factor.

We also investigate the impact of different Large Language Models (LLMs) on performance, as depicted in Fig  (right), including Llama2-7B-chat-hf , Llama3-8B-Instruct , and Mistral-7B-Instruct-v0.2 . We achieve optimal performance using Llama3-8B-Instruct, which is currently the most powerful LLM in the 7B-8B scale. This suggests that a stronger foundational LLM leads to better performance in report generation tasks due to enhanced language understanding capabilities. Although Llama3 reaches the highest performance, our method shows minimal fluctuation when LLMs change, indicating robustness across different LLMs.

We explore the effect of two different LR input resolutions,  and , to generate LR queries and HILT. For both LR input resolutions, we use the HR input resolution . As shown in Table , a higher LR input resolution leads to an eightfold increase in the number of visual tokens processed by LLMs, indicating a significant increase in computational cost. However, the performance on both datasets does not show the same trend as the computational cost increase. Hence, this suggests that increasing the LR input resolution is not necessary, as we mine HR information from HR views and integrate it into LR queries. Increasing the LR input resolution would not introduce new information but would mainly increase the computational cost.

In this study, we implement the Low-Rank Adaptation (LoRA)~ for the 3DHRG task. We configure the LoRA parameters with an alpha of 64 and a rank of 128, and apply a dropout rate of 0.1. Instruction tuning is performed over 100 epochs. All experiments are conducted on a single A100-80G GPU.

For computational efficiency, we employ mixed precision at bf16, as detailed in Tab . The training process includes a total batch size of 4 with a gradient accumulation factor of 2, and we restrict the maximum sequence length to 512 tokens.

Regarding learning rates, we use  for our models, optimized via the AdamW algorithm. The learning rate schedule is linear with a warm-up ratio of 0.03, and the weight decay is set to 0.0.

We utilize the same configuration for all existing methods if they do not specify these hyperparameters in their official codebase.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Automatic radiology report generation can significantly benefit the labor-intensive process of report writing by radiologists, especially for 3D radiographs like CT scans, which are crucial for broad clinical diagnostics yet underexplored compared to 2D radiographs.   Existing methods often handle 3D volumes either slice-wise or with aggressive downsampling due to current GPU memory limitations, which results in a loss of the inherent 3D nature and critical details.  To overcome these issues, we introduce a novel framework that efficiently and effectively generates radiology reports for high-resolution (HR) 3D volumes, based on large language models (LLMs).  Specifically, our framework utilizes low-resolution (LR) visual tokens as queries to mine information from HR tokens, preserving detailed HR information while reducing computational costs by only processing HR informed LR visual queries. Further benefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328 HR 3D volumes and paired reports, establishing the first benchmarks for report generation from 3D HR medical images. Our method consistently surpasses existing methods on this benchmark across three different settings: normal-resolution, high-resolution inputs, and zero-shot domain transfer, all at an acceptable computational cost, trainable on a single A100-80G GPU. Introductionbastawrous2017improving,rimmer2017radiologist,rosenkrantz2016us,wan2024electrocardiogram,chen2024bimcv,liu2024zerotanida2023interactive,li2023unify,wan2023medbradley2019sensitivity,self2013highliu2021applicationwu2023towards, bai2024m3d1mmfig: num vs persingh2009computerliu2023visualllama3modelcardfig: num vs perct2rep\textbfWe introduce igh-resolution nforming ow-resolution okens (), which effectively mines HR information through LR visual queries. This approach allows LLMs to process costs based solely on the number of LR queries instead of the increasing number of HR tokens with higher resolution inputs, while still preserving HR information for enhanced performance, as shown in Fig .

HILTHILTfig: num vs perTo thoroughly benchmark the  igh-esolution Medical Image eport eneration () task, we curate a new large-scale dataset based on publicly accessible sources, consisting of 5,328 paired HR 3D CT volumes and radiology reports, named BIMCV-RG. This dataset allows the community to assess the generalizability of the 3DHRG task under zero-shot domain transfer scenarios, a significant addition given  for this task.

3DHRRG3DHRGThis dataset will be released post-acceptancethere was only one large-scale dataset previously availableWe establish the first benchmark for the 3DHRG task across two large datasets and three settings: normal-resolution, high-resolution inputs, and zero-shot domain transfer. Our method consistently achieves superior performance across all the settings for both datasets with a fixed computational cost for LLMs processing visual tokens.

Related WorksMultimodal Large Language Models.radford2019languagealayrac2022flamingoli2023blipliu2023visualRadiology Report Generation for 2D Medical Images.li2018hybrid, jing2017automatic, wan2023efficientliu2021exploring, ma2021contrastivegu2024complex,tanida2023interactive, li2023unifyfig: conceptadegun2021deep,puttagunta2021medical, shen2016reappraisal, ker2017deep, singh20203dRadiology Report Generation for 3D Medical Images.hamamci2024ct2repwu2023towardsbai2024m3dfig: conceptfig: conceptbradley2019sensitivity,self2013highklass2010prospectively,chua2013diagnosticcomputational costs constantfig: num vs perGenerating Radiology Report for HR 3D Medical ImagesHILTHILTUnified Processing of LR and HR 3D Volumes Using Single Visual Encoder.dosovitskiy2020imagefig: concept-10pt Z^{} = E_V (X^{})&,  Z^{} = E_V (X^{}) \\ Z^{} \in ^{N \times C}&,  Z^{} \in ^{N^{\prime} \times C} HILT: High-resolution Informing Low-resolution Tokens.fig: concept Z^{} =  \left(Q + \left(Q K^T\right) V\right),  Z^{} \in ^{N' \times C}

sec: more ablationfig: more ablationli2023bliptab: ablasec: ablaGenerating Radiology Reports.liu2023visual  = ((Z^{}, Z^{})) Benchmarking Radiology Report Generation on HR 3D Medical ImageDatasetsCT-RATE \footnote.wang2023r2gengptBIMCV-RG.bimcvsec: curate bimcvThis dataset will be released upon acceptance.Evaluation Metricssec: metricpapineni2002bleubanerjee2005meteorlin2004rougevedantam2015ciderbimcvtanida2023interactiveBaselines on Radiology Report Generation3DHRRG3DHRGfig: conceptllama3modelcard\textitR2GenGPTwang2023r2gengptwang2023r2gengpthttps://github.com/wang-zhanyu/R2GenGPTMedVInTzhang2023pmchttps://github.com/xiaoman-zhang/PMC-VQA\textitCT2Repct2rephttps://github.com/ibrahimethemhamamci/CT2Rep\textitRadFMwu2023towardswu2023towardshttps://github.com/chaoyi-wu/RadFMM3Dbai2024m3dbai2024m3dhttps://github.com/BAAI-DCAI/M3DExperiments and EvaluationImplementation Details.32\times 32 \times 16128 \times 128 \times 64tab: lr input ressec: more ablationllama3modelcardsec: ablasec: more ablationsec: hyper trainingtab: hyperparametersec: preprocessMain Resultssec: main resNormal Resolution. tab: avg res128 \times 128 \times 64Z^{}sec: metrictab: avg resfig: radarHigh Resolution. bradley2019sensitivity,self2013high,chua2013diagnostic256 \times 256 \times 128512 \times 512 \times 256OOMhamamci2024ct2rep,wu2023towards,bai2024m3d\textitfig: radar\textittab: avg resZero-shot Domain Transferringtab: zero transferct2repwu2023towardsbai2024m3dVisualization of Qualitative Resultsfig: report viswu2023towardsbai2024m3dAnalysissec: abla-10ptsec: metricAblation Studies on Each Components. tab: ablaliu2023visualAffect of Resolution of 3D Medical Image. fig: ablationScalability Analysis. fig: ablationdosovitskiy2020imagellama3modelcarddettmers2024qlorafig: ablationConclusionHILT(HILT\small \bibliographystyle \bibliography Appendix / supplemental materialLimitations and Future WorkBroader ImpactsDetails of BIMCV-RG Datasetsec: curate bimcvbimcvFiltering out samples with reports shorter than 10 tokens or longer than 512 tokens.

Removing samples with duplicated reports.

Excluding samples that have a report but no associated available CT scans.

Removing samples with reports where the 3D volumes have dimensions width and height less than 512, and number of slices less than 256. fig: dataset visfig: report sampleData Preprocessingsec: preprocess[-1000 , +200 ]A resolution of  on the sagittal and coronal axes, with the transverse axis comprising 256 slices.     512 \times 512A resolution of  on the sagittal and coronal axes, with 128 slices on the transverse axis. 256 \times 2560.75 1.5 Extra Ablation Studiessec: more ablationAffect of Number of Layers of Projectorsfig: more ablationEffect of Different LLMsfig: more ablationtouvron2023llamallama3modelcardjiang2023mistralEffect of LR Input Resolution128 \times 128 \times 64256 \times 256 \times 128512 \times 512 \times 256tab: lr input resHyper-parameters of Trainingsec: hyper traininghu2021loratab: hyperparameter1 \times 10^{-4}