% For accurate one-shot landmark detection, it is essential to acquire well-localized and semantically meaningful feature representations for every pixel across all images. These representations form the basis for deriving meaningful feature similarities, crucial for matching landmarks between a single template image and query images.  We aim to leverage deep features extracted from a pre-trained visual foundation model as dense visual descriptors for one-shot landmark detection tasks.  However, the image encoder of a pre-trained foundation model often generate feature maps with downsampled resolution, which significantly restricts their effectiveness in landmark detection.  % Even a small pixel shift can translate into a significant shift in millimeters relative to the ground truth landmark.% Consider, for instance, the DINO-S model with an 8x8 patch size and a stride of 4. Initially, a high-resolution medical image must be downsampled to 224x224 to fit into DINO-S. Furthermore, DINO-S's tokenization process further reduces feature resolution by a factor of 4. Additionally, the feature extractor of the foundation model is trained on natural images, leading to a substantial domain shift when applied to medical images.  To address these challenges, we propose a coarse-to-fine landmark detection framework featuring dual-branch global and local learnable decoders. Our approach enhances the feature resolution for landmark detection in a coarse-to-fine manner and improves feature quality by incorporating domain knowledge contained in the template image.

% To alleviate the problem of the limited feature resolution of the foundation models' encoders, we propose a coarse-to-fine framework with learnable decoders.% Considering the difference of the contents of the whole image and the local patch, a dual-branch framework consisting of a global decoder and a local decoder is adopted to handle the feature extraction of the whole image and the local patch, respectively.% Also, since each local patch shares similar intensities and structures, increasing the distinguishing difficulty of the local decoder, a more complicate architecture is used, including two deconvolution layers followed by batch-normalization or activation layers and a 33 convolution layer.% By contrast, a much simpler structure is utilized for the global decoder, composed of a bilinear interpolation operation and a 33 convolution layer.% The two decoders share a frozen image encoder and only the decoders are updated during training. The upsampling operation in the decoders is used to increase the feature resolution while the learnable parts are optimized to include domain knowledge on the target medical image dataset and reduce the feature dimension.Specifically, as shown in Fig. 1(a), for the coarse stage, the entire input image  is downsampled into a size with the short side of 224 and is then fed into the frozen image encoder  of a foundation model, which typically undergoes additional downsampling of the feature maps due to the patch encoding process in vision transformers. In order to restore the feature resolution that is diminished during the patch encoding process, a trainable global feature decoder  consisting of upsampling and convolutional layers is attached to the image encoder, facilitating the extraction of global features, denoted by .  The coarse landmark positions on the query images can then be identified by matching the similarity of global features between the query images and the template image with our proposed bidirectional matching strategy for which we will elaborate in the next section: , where  and  represent the global features of the template and query image, respectively. For the fine stage, we aim to construct high-resolution local representations by cropping a 224224 local region  containing the identified coarse position for each landmark, with  incidating the -th landmark. The cropped local region is fed into the frozen image encoder and a local feature decoder  consisting of multiple deconvolutional layers to obtain fine-grained local features .  To further incorporate both the global context information into the local features, we upsample the corresponding global feature regions to the same size of the local feature maps and add them together to obtain the fused local features  for final matching.

% During inference, inputs for the two decoders are generated in two steps.% We first take the downsampled whole image as the input for the encoder and global decoder. The obtained features for both template image and query image can be used to localize a coarse position of the target landmark on the query image.% Then, we crop a local patch centered at the coarse prediction as the input for the encoder and local decoder and thus obtain the local features.% To combine the global and local information together, we first find the patch of features on the global feature map corresponding to the size and position of the local patch and upsample it to the same size of the local feature map.% After that, the upsampled global features and local features are normalized and added together to obtain the fused local features, which would be used for the final matching for the local patch.% During the training stage, since the ground truth position is know, we randomly crop a local patch around each target landmark as the input to simulate the different positions of the target landmark on the local patch caused by the potential error of the coarse prediction in inference. To enhance the feature quality extracted from the foundation model with increased discrimination among features from different positions, we aim to effectively optimize the global and local feature decoders by leveraging just one single template image  annotated with  landmarks . % After obtaining high-resolution feature maps, the key is how to increase the discriminative property among features from different positions.% A natural and commonly used way is contrastive learning~, by treating the feature vector at the exactly same point as the positive sample and the features with a relatively large distance to the target position as negative samples~.% However, the relationship between the distance and feature similarity is often neglected with only positive samples (similarity 1) and negative samples (similarity 0) using the contrastive loss. Intuitively, the feature of points close to the target landmark should have a high similarity to the feature of the target landmark, and the points far away from the target landmark should have a low similarity. % Therefore, a more reasonable similarity metric should take the distance to the target landmark into consideration and be in a continuous range rather than only two classes. Therefore, we takes the distance to the target landmark into consideration and proposes a distance-aware similarity learning loss.  The prediction similarity map is obtained by calculating the cosine similarity between the feature of the ground truth landmark and the feature of other points across the image: , where  indicates the global or local feature map and  means the feature vector on  at the position of . "cos" denotes the cosine similarity. The ground truth similarity map for this landmark is generated by using a 2D Gaussian distribution as follows: % Based on this motivation and inspired by heatmap regression methods~, we propose a distance-aware similarity learning loss by directly optimizing the similarity maps to approach the ground truth heatmap generated by a non-normalized 2D Gaussian distribution (See the right part of Fig.~ (a)):

,where the center of the Gaussian distribution is located at the ground truth landmark , with the highest similarity of 1. The ground truth similarity value decreases with the increase of the distance to the ground truth landmark, where the decreasing speed is controlled by the standard deviation . % In this way, the features extracted by the decoders after training will have higher similarities close to the target landmark and lower similarities far away from the target landmark, with a smooth decreasing curve.% The prediction similarity map is obtained by the cosine similarity between the feature vector of the template landmark and the features of other positions on the same feature map. Finally, a Mean Square Error (MSE) constraint is imposed between the global/local similarity maps () and their corresponding ground truth heatmaps () for all the  landmarks: % The two decoders are optimized by the same loss, but take different inputs.% {The current version of Method section lacks connection with foundation models. In this section, first emphasize the motivation of leveraging pre-trained weights from foundation model, then describe the overall pipeline in connection with the foundation model.} With the extracted features of the template and query image, the success of one-shot landmark detection relies on the development of an effective matching strategy to find the corresponding point on the query image based on feature similarity with the ground truth landmark on the template image. The commonly used matching strategy is selecting the point with highest feature similarity to the template landmark. % However, due to the existence of patches with similar structure and appearance on the same input, especially on the local patch, a scattered similarity map can be generated, where the features of many positions have a high similarity to the template landmark.% This is also because the pre-trained encoder of foundation models is trained on natural images where image content is usually more diverse and might not be optimal for such a difficult discrimination setting on medical images.% As a result, selecting the position with the highest similarity can lead to a detection error. However, the feature similarity map often lacks the accurate focus on a single point since the feature extracted by foundation models like DINO is usually ambiguous~ in terms of their spatial distribution, reflecting similar features over extensive areas, as shown in the similarity map in Fig.~. As a result, directly choosing the query point with the highest similarity to template landmark could result in inaccurate landmark detection.

To improve the matching accuracy, we propose a novel bidirectional matching strategy (BDM) to find the landmark pair  with not only a high feature similarity from template to query image but also a low inverse-matching error from query to template.  indicates the matched position for the -th landmark on the query image and the template image, respectively and  is the final prediction for the query image. % Intuitively, if the predicted point on the query image is the target landmark, the template landmark and this point should come from the same underlying distribution which formulates the special structure of a certain landmark.% Then, when we match back from the query point to the template image, it should obtain the ground truth landmark on the template image.% By contrast, if the predicted query point is not close to the target landmark, the matched point back to the template image should be far away from the ground truth.% We aim to find a more robust and accurate correspondence on the query image by taking the inverse-matching error into consideration.% We require not only a high feature similarity from the template to the query, but also a low inverse-matching error from the query back to the template. Such a two-side agreement is similar to the Best-Buddies Similarity method for template matching~, which has been theoretically and experimentally demonstrated to be a robust and reliable matching method between two sets of points. % Moreover, the NSS method is an unconstrained matching problem since it only considers the matching in one direction from the template to query, whereas the matching accuracy on query images cannot be calculated during matching due to the lack of annotations. Moreover, the inverse-matching error for the matching from query to template can be calculated since the ground truths are known and this can be taken as an estimation of the matching error on the query image . % In this way, our bidirectional matching strategy select points under the guidance of ground truth information on the template image, and is intuitively more accurate than the one-directional NSS method which makes decisions in a totally unconstrained environment without any information of the matching accuracy. Specifically, our proposed BDM is formulated as follows:

where  is the annotation on  and "Dist" calculates the Euclidean distance. "argtopk" selects the points with the top- similarity values in the query image .  and  are the feature map of  and  and are set as  and  for the coarse and fine stage, respectively.  denote the feature vector on  with a position of .

%  and  mean the feature vector corresponding the pixel with a position (r, c) on the query image and the the pixel with a position  (the ground truth position) on the template image. There is a total of three steps to solve this optimization problem. First, find a set of  candidate points on the query image  with the top- highest similarities to the template landmark feature , denoted as . Second, for each point in , we find its matching point  back to the template image  with the highest similarity and include each candidate point pair in . Finally, we calculate the distance between  and the ground truth landmark  for each pair in  and select the one with the smallest distance, i.e., . The corresponding point on the query image  is taken as the final predicted landmark. The BDM method is applied to the matching of both global features and local features.

% % {At the beginning of each Method section, always give motivation first, then describe details.}% : % As shown in Fig.~ (a), our proposed method consists of a frozen image encoder taken from large pre-trained models and two light learnable decoders to tackle global and local representation learning, respectively.% In this work, we utilize the encoder of DINO~ as a demonstration but our proposed method can be applied to other models easily.% The encoder is to extract general-purpose features of the image, whereas the two decoders are used to learn specific knowledge of the target dataset from different scales.% They can increase the feature resolution and reduce the feature dimension at the same time.% The global decoder is composed of a bilinear interpolation operation and a 33 convolution, while the local decoder has a more complex structure, including two deconvolution layers followed by batch-normalization or activation layers.% Since each patch shares similar intensities and structures in the input for the local decoder (See Fig.~ (a)), which increases the distinguishing difficulty, a more complicate architecture is used for the local decoder.  % : In this work, we aim to optimize the decoders with only one template image on the target dataset, without any unlabeled data as previous works~.% To this end, we augment the template image with random shifting, scaling and rotating for 500 times to obtain a training dataset with a size of 500.% During training, we randomly crop a 224224 patch around each target landmark on the training image as the input to generate the local features through the the image encoder and local decoder.% To reduce the computational burden, the whole image is downsampled to a size where the short side is 224 for generating the global features with the help of the global decoder.% % After obtaining the global and local feature maps, we can extract the feature vectors corresponding to the positions of a target landmark on the inputs and compute the cosine similarity with the features of other positions.% The extracted features are indicated by the vectors connected to the green points on the feature maps in Fig.~, whose positions can be calculated as follows: , where  is the rounding function.  represent the height/width of the original image, while  mean the size of the output feature map.% The position of a certain landmark on the original image is denoted as  and the corresponding position on the feature map is .% The similarity map indicates how close the feature from other positions is to the target position in representation space as shown in Fig.~ (a), with red indicates a higher similarity.% Nonetheless, the pre-trained weights might not be able to distinguish between close positions and distinct ones due to the lack of domain knowledge for the structures in medical images.% The key is to integrate these information into the extracted features before similarity computation by making full use of the template image.% % Although the training of the two decoders can be done simultaneously, the inference is done in a coarse-to-fine way as presented in Fig.~ (b).% Specifically, we first take the whole query image as an input and obtain the coarse prediction through the global decoder and the bi-directional matching strategy introduced later, based on which we center crop a local patch as the input for the local decoder.% However, it's difficult for the representation extracted through the local decoder alone to distinguish different positions since the image content and intensity is highly similar to each other in the local patch and the similarity map can be misleading, as shown in Fig.~ (a).% To this end, we combine the global features with the local ones to integrate both context and detail information.% We first find the patch of features on the global feature map corresponding to the size and position of the local patch and upsample it to the same size of the local feature map.% Then, the upsampled global features and local features are normalized and added together to obtain the fused local features.% Finally, the prediction on the local patch using the bidirectional matching strategy is transformed back to the position on the whole image as the final result.%%