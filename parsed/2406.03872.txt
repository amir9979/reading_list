BLSP-Emo models share a similar architecture as those in BLSP, comprising three components: a speech encoder (with parameters ), an instruction-following LLM (with parameters ), and a modality adapter (with parameters ) between the speech encoder and LLM.  % In this paper, we choose the convolution-based subsampler described in BLSP as the modality adapter, though other adapters like the CFormer-based adapter described in BLSP-KD are also possible alternatives.  Figure~ provides an overview of our model.  %Next, we will describe the details of how to train the model end-to-end.% Our proposed approach, named , is designed to enable LLMs to understand the semantic content and emotional information in speech, thereby generating responses that are more empathetic to the user's emotional tone while completing speech instructions. Our model comprises three components: a speech encoder, an instruction-following LLM, and a modality adapter between the speech encoder and LLM. An overview of our model is presented in Figure~. We will next describe how to construct data to train the model in an end-to-end manner. To achieve speech-text alignment at the semantic level and enable general instruction-following capabilities for LLMs with speech inputs, we adopt the behavior alignment approach used in BLSP~. The core concept is that if speech and text are well-aligned, the LLM's text generation behavior given speech input should closely match its behavior when given the corresponding transcript. This alignment is accomplished by training on synthesized speech instruction data derived from existing ASR datasets with a continuation prompt as follows:

This process extends an ASR training sample  into a tuple , where  is the LLM's response, representing a natural continuation of the transcript  and the corresponding speech . The model is trained to generate the same continuation when given speech input, using the same continuation prompt. This is achieved by applying a KL-divergence loss according to the knowledge distillation framework described in~, leading to the semantic alignment loss:

In this semantic alignment stage, we focus on tuning the parameters  of the modality adapter, keeping the parameters  and  of the speech encoder and LLM frozen.  % We will relax this constraint in the next stage. % BLSP-KD approach. This approach is based on the idea that if speech and text are well-aligned, the LLM's next token prediction given speech input should be close to that given the transcript as input. We provide a brief description of this method for completeness and refer readers to the original paper for detailed information~.% A BLSP-KD model is parameterized by  for the speech encoder,  for the modality adapter, and  for the LLM. Given a sample  from ASR data, where  is the speech and  is the corresponding transcript, the BLSP-KD model first encodes and transforms  to hidden states in the word embedding space of the LLM.  BLSP-KD can be trained on  % to  in the word embedding space, through the speech encoder and modality adapter. Thanks to the use of the CFormer-based modality adapter, which is based on the continuous-integrate-and-fire mechanism, there is a one-to-one correspondence between the hidden vectors in  and the tokens in , ensuring that .% BLSP-KD employs the knowledge distillation framework, treating the LLM's next token prediction distribution  for text input as the teacher distribution and uses it as supervision to guide the training of the corresponding distribution  for speech input, resulting in the input KL loss:% % \ell^_(, ) = -\sum_{i,x} p(x|_{<i})\log p(x|^_{<i}) % % \noindent in which we omit the model parameters from the subscript as they are implied from the context.% Additionally, BLSP-KD utilizes synthesized speech instruction data created by a continuation prompt as follows:% % % User: Please continue the following sentence.% <transcript>% Assistant:% % % This extends the pair  to a tuple , where  is the response of the LLM and represents a natural continuation of the transcript and the corresponding speech. This allows knowledge distillation to be performed on the response as well:% % &\ell^_(, , ) = \nonumber \\% &\quad -\sum_{j,y} p(y|, _{<j})\log p_\theta(y|^, _{<j}) % % While BLSP-KD primarily focuses on tuning the parameters  of the modality adapter, it was found that tuning the parameters  of the speech encoder and  of the LLM can also be beneficial. To avoid degenerated parameters when tuning  of the LLM, which is used in both the teacher and student models, BLSP-KD proposes a variation of LoRA, named PLoRA, which only activates LoRA adaptation for speech tokens. As studied in , humans convey emotions in speech through both linguistic and paralinguistic cues. A model trained with the BLSP approach captures the linguistic cues for emotion but lacks the ability to understand paralinguistic cues, as it is aligned at the semantic level based on linguistic content. Ideally, an emotion-aware speech-language model should be pretrained on large amounts of speech-text data to understand the relationship between paralinguistic emotion cues and linguistic context, and then fine-tuned on emotion-aware speech instruction data, following the training paradigm used for text-only LLMs. However, this approach requires extensive curated data and significant computational resources, neither of which is readily accessible.

Our approach to emotion alignment builds upon and extends the behavior alignment method by creating natural continuations of speech transcripts that reflect the emotional tones in the speech. This is achieved by leveraging existing speech emotion recognition (SER) datasets. Given a sample  from a SER dataset, where  is the emotion label annotated for speech , we prompt the LLM with the following instruction:

This generates a text continuation  of the speech  that is consistent with the emotion label . We then initialize the BLSP-Emo model with parameters of the BLSP model trained from the semantic alignment stage and fine-tune it to generate these continuations given only the speech as input, as follows:

This results in the primary emotion alignment loss based on emotion-aware continuations: % Note that we could also use the KL-divergence loss here, but to be consistent with the losses used in the baseline approaches for speech emotion recognition, we opt for the cross-entropy loss.% My adviser think that this explanation is not convincing, and that it is natural to use cross-entopy loss when prompts are inconsistent.

We also introduce an auxiliary speech emotion recognition loss by directly predicting the emotion label  from the hidden states output by the modality adapter, using pooling and a classification layer (with additional parameters ):

In this emotion alignment stage, we unfreeze the parameters  of the speech encoder and parameters  of the LLM, in addition to the parameters  of the modality adapter and  of the classification layer. This allows the speech encoder to capture paralinguistic emotion cues and provides additional modeling power in the LLM to address the discrepancy between speech and text.  We follow the PLoRA approach proposed in~ to adapt parameters  of the LLM. The LoRA module is selectively applied only to speech tokens, preserving the LLM's ability to encode text instructions and generate text.

% Note that we follow the PLoRA approach proposed in~ to only activate LoRA adaptation on speech tokens for parameters of the LLM.%leaving the encoding process for text tokens untouched, By adopting this method, we can improve the model's comprehension of speech while preserving the LLM's behavior when provided with text instructions.% % % Previous work has demonstrated that models fine-tuned on specific speech tasks can become limited to those tasks alone~. In contrast, BLSP demonstrates that cross-modal continuation tasks can give models a broader capability to follow speech instructions~. Inspired by this finding, and aiming to maintain the cross-modal instruction-following capability while enabling LLMs to understand emotion in speech and generate empathetic responses, we propose a self-alignment process that utilizes existing speech emotion recognition datasets to construct empathetic continuation data, instead of training the model to predict the ground truth emotion labels from speech. Intuitively, an ideal large speech-language model should be capable of producing the same text continuation whether given a transcript and emotion labels in text or provided with corresponding speech directly, as the model can understand both semantic content and emotional information in the speech.% This approach consists of two steps. In the first step, we prompt the LLM to generate text continuations from speech transcripts with defined emotion tones using the following instruction:% % % User: Continue the following sentence that reflects % a <emotion> emotion tone in a coherent style:% <transcript>% Assistant: % % % In the second step, we remove the transcript and emotion label from the text prompt and provide the LLMs with speech features resulting from the modality adapter, using the self-generated text continuation from the first step as supervisions to train the model. We encourage the model to compensate for the content missing from the text prompt by modeling the semantic and emotional information in the speech through the following instruction:% % % User: Continue the following sentence based on % the conveyed emotion tone in a coherent style: % <speech features>% Assistant: <text continuation>% % % % % As the speech features obtained from the speech encoder and the textual embeddings required by the LLM decoder are not in the same feature space, it is better to keep the speech encoder and the LLM frozen, and pretrain the modality adapter. In this stage, we employ the pretraining strategy known as BLSP-KD, which has been proven effective in achieving cross-modality alignment and endowing LLMs with the capability to follow speech instructions. % % The BLSP-KD approach aims to utilizing large-scale automatic speech recognition datasets to align the generative behaviors of speech and text, training the model to produce the same textual continuation whether given a transcript or corresponding speech. % Specifically, given a sample  from large-scale ASR datasets, where  is the speech and  is the corresponding transcript, we first prompt the LLMs to generate text continuation  after the transcript. Then, during training, we require the model to exhibit the same generative behavior when given the transcript or corresponding speech, meaning the generated probability of continuation should be consistent:% % &\ell_(, ,) = \nonumber \\% &\quad -\sum_{j,y} p(y|, _{< j})\log p(y|, _{< j}) %  % % After the BLSP-KD pretraining, the model is now capable of directly following speech instructions. However, the model responds based only on the semantic content within the speech. To equip LLMs with the ability to grasp the paralinguistic emotional aspects of speech, we employ the empathetic continuation data constructed in section~ for supervised fine-tuning. In this stage, we fine-tune the speech encoder and modality adapter. We also introduce the Partial LoRA (PLoRA~) technique to fine-tune the LLM, ensuring it can understand the paralinguistic information within speech. The LoRA module is selectively applied to speech tokens, leaving the encoding process for text tokens untouched. By adopting this method, we can improve the model's comprehension of speech without negatively affecting its performance on text-based tasks.% During this stage, there are two cross-entropy losses used for training: one loss is for guiding the LLM to predict the emotional continuation, and the other is for enabling the speech features to perform emotion classification. It is crucial to emphasize that emotion classification serves as an auxiliary task and is only engaged during the training phase. In contrast, during inference, the speech features output by the modality adapter are directly fed into the LLMs without undergoing emotion classification. Specifically, given a sample  from SER datasets, where  represents the speech input,  the transcript,  the emotion label, and  the self-generated emotional continuations, the total loss for this training setup is formulated as follows:% % &\ell_(, , , ) = \ell_ + \ell_ \nonumber \\% &\quad = -\sum_{j} \log p(y_j|, _{< j}) - \log p(|)  %   We use publicly available ASR datasets in the semantic alignment stage and SER datasets in the emotion alignment stage. 

The ASR datasets include LibriSpeech~, CommonVoice 13.0~, and the GigaSpeech M set~, totaling approximately 1.9 million English (speech, transcript) pairs, along with a comparable number of Chinese ASR samples randomly selected from WeNetSpeech~. 

% We use the ASR datasets to pretrain the modality adapter for semantic alignment.% The SER datasets used in SFT phase include IEMOCAP~, MELD~, ESD~, CMU MOSEI~, and MEAD~, resulting in a total of about 70k utterances covering English and Chinese.

The details of the SER datasets and train/test splits can be found in Appendix~. In summary, we train on IEMOCAP, MELD, CMU MOSEI, MEAD, and ESD, covering approximately 70k utterances in English and Chinese, and evaluate SER performance on IEMOCAP and MELD as in-domain test sets, on RAVDESS and MerBench as out-of-domain test sets, as well as on three languages not seen in training: AESDD for Greek, CaFE for French, and RESD for Russian. We focus on five emotion categories: neutral, happy, sad, angry, and surprise across all datasets.

% is 'surprise' also commonly studied in the literature?% We construct an emotion-aware speech instruction dataset named SpeechAlpaca, derived from the open-source instruction dataset Alpaca-52k~, to assess emotion-aware instruction-following capabilities. Specifically, we employ GPT-4 to deduce a set{Please double check if the description is correct. If so, we should provide some statistics on the number of emotions per instruction.}} of plausible emotional tones from a text instruction in Alpaca-52k. From these, we randomly select one as the emotion label for the instruction. This process is used to select 100 instructions for each of the five emotion categories. Subsequently, we synthesize expressive speech using the selected emotion label with Microsoft's Text-to-Speech (TTS) API.}% We construct an emotion-aware speech instruction dataset, named SpeechAlpaca, from the open-source instruction dataset alpaca-52k~ to evaluate emotion-aware instruction-following capabilities. Specifically, we utilize GPT-4 to infer a set of plausible emotional tones given a text instruction in alpaca-52k and randomly choose one as the emotion label for the instruction.. This result in  sample 100 instructions for each emotion, and then synthesize expressive speech using the inferred emotion label with Microsoft's TTS API.}% We also conduct evaluation on emotion-aware multi-turn conversation on IEMOCAP~, as will be detailed in Section~. We conduct evaluations on emotion-aware speech instruction capabilities based on a synthesized version of Alpaca-52k~, and emotion-aware multi-turn conversation based on IEMOCAP~, with details presented in Section~.

% We conduct evaluations on emotion-aware speech instruction following capabilities based on a synthesized version of Alpaca-52k~, with details presented in Section~. We also conduct evaluations on emotion-aware multi-turn conversation based on % IEMOCAP~, as will be detailed in Section~.

We utilize the encoder part of Whisper-large-v2~ as the speech encoder, convolution-based subsampler as the modality adapter, and Qwen-7B-Chat~ as the LLM. More details can be found in Appendix~.

% We utilize the encoder part of Whisper-large-v2~ as the speech encoder and employ Qwen-7B-Chat~ as the LLM. The modality adapter is composed of three 1-dimensional convolution layers followed by a bottleneck layer  with a hidden dimension of 512. The convolution layers are designed to reduce the length of the speech features by a factor of 8, with each layer having a stride size of 2, a kernel size of 5, and a padding of 2.% During the BLSP-KD pretraining phase, we leverage publicly accessible speech recognition datasets, including LibriSpeech~, CommonVoice 13.0~, and the GigaSpeech M set~, culminating in approximately 1.9 million English (speech, transcript) pairs. Additionally, we select a comparable number of Chinese ASR samples, approximately 1.9 million, from the WeNetSpeech~. We fine-tune the modality adapter for 1 epoch with a batch size of 768. This process takes about 2.5 days using 4 A100 GPUs.% During the Emotional SFT stage, we gather approximately 70k (speech, transcript, emotion) pairs from publicly available speech emotion recognition datasets (refer to Section~ for details). We employ PLoRA to adapt the key, query, value, and output layers of the LLM's self-attention mechanism, setting the PLoRA hyperparameters to  and . We fine-tune the speech encoder, the modality adapter and the PLoRA module for 3 epoch with a batch size of 128. This process takes about 3 hours using 4 A100 GPUs.% %  % In the Emotional SFT phase, we utilize five large-scale SER datasets in both English and Chinese, which include IEMOCAP~, MELD~, ESD~, CMU MOSEI~, and MEAD~, resulting in a total of about 70k utterances. In our experiments, we focus on five emotions—neutral, happy, sad, angry, and surprise—that are labeled across all datasets. To ensure that the transcripts provide sufficient semantic content for LLMs to generate continuations, we have filtered out samples whose transcript contains fewer than 5 words.% We quantitatively evaluate the SER capabilities of our model on both in-domain datasets (IEMOCAP~; MELD~) as well as out-of-domain datasets (RAVDESS~; MerBench~). Additionally, to test the generalizability of the paralinguistic features learned from speech across languages, we report SER results for 5 other languages, including Greek (AESDD~), French (CaFE~), German (EmoDB~), Russian (RESD~), and Urdu (URDU~). % As there are no publicly available emotion-sensitive speech instruction datasets to evaluate the model's response to emotional speech, we constructed a test set using a method similar to that of E-chat~. Specifically, we utilized GPT-4 to infer a plausible emotion tone for each text instruction in the open-source instruction dataset alpaca-52k~, and select 100 samples for each emotion. We then synthesized the emotional speech using Microsoft's TTS API}. We name the synthesized dataset as SpeechAlpaca. A summary of the datasets used in our experiments is provided in Appendix~.

We compare with the following baselines:

 These are cascaded systems where the LLM input is either the ground-truth transcript or the recognition output from Whisper-large-v2, which includes a speech encoder, as used in BLSP-Emo, and a speech decoder.

 This model undergoes the semantic alignment stage described in Section~ and initializes BLSP-Emo before the emotion alignment stage.

 This model is initialized from BLSP and fine-tuned directly on the SER task. The only difference between BLSP-SER and BLSP-Emo is that the former is fine-tuned to predict the ground-truth emotion label, while the latter generates emotion-aware continuations, both utilizing the same SER training datasets.

%  This model is initialized from BLSP and fine-tuned directly on the SER task, based on the SER training data from which the emotion-aware continuation data is derived for training BLSP-Emo. The only difference between BLSP-SER and BLSP-Emo is that the former is fine-tuned to predict the ground-truth emotion label, while the latter generates emotion-aware continuations. These are cascaded systems composed of a standalone SER module in addition to the Whisper+LLM pipeline. The SER component is fine-tuned on the SER training datasets from respective speech encoder models, including HuBERT large~, Wav2Vec 2.0 large~, or WavLM large~, with the addition of an average pooling layer and a linear classifier to predict the ground-truth emotion label. During evaluation, we directly report the performance of the SER module for the SER task. For other tasks, we first use the SER module and the Whisper model to respectively predict the emotion label and transcript, and then use the following prompt to generate responses: 

%% : {why not list it as a baseline? we need to mention that we list them separately as they are trained with different LLMs and training data..., something like that}%  The input to the LLM is the ground-truth speech transcript or recognition output from whisper-large. %  The model has undergone only the first stage of BLSP-KD pretraining but has not been subjected to Emotional SFT.%  We fine-tune the BLSP-KD using the same data as employed within BLSP-Emo and adapt the same parameters. The only distinction lies in the shift of the LLMs' predictive output from empathetic continuations to the ground-truth emotion labels.%  We train our SER model using the same SER data as BLSP-Emo. To perform the emotion recognition task, we augment the last layer of the pretrained models such as HuBERT large~, Wav2Vec 2.0 large~, or WavLM large~ with an average pooling layer and a linear classification layer, with all model parameters being trainable. During evaluation, for the SER task, we directly report the results from the SER module. For other tasks, we identify the emotion and transcript using both the SER module and Whisper-large-v2, then use the prompt , conveys a \{emotion\} emotion tone. Please provide a response} to guide the LLM in generating an empathetic response. To prompt the LLM-based generative models to perform the SER task, we use the following prompt: % We evaluate SER performance for both LLM-based generative models and encoder-based classification models. For the generative models, we use the following prompt:% We evaluate SER performance for both LLM-based generative models and encoder-based classification models. For the generative models, we use the following prompt: where <transcript|speech> represents the transcript for cascaded systems or speech features for end-to-end systems. Results are shown in Table~.

% We employ greedy search to generate the response. % We evaluate the speech emotion recognition accuracy of our model quantitatively on different test sets. We use the prompt  to enable emotion recognition and employ greedy search without sampling during generation. % Despite being trained only on synthesized emotion-aware continuation data, 

The BLSP-Emo model achieves the highest overall recognition accuracy across five test sets, along with the BLSP-SER model, which is fine-tuned from the same BLSP model but specifically for the SER task. BLSP-Emo significantly outperforms all other models, including SALMONN-7B~, which adapts a large language model to various speech tasks, including speech emotion recognition.

The Text|Whisper+LLM cascaded systems achieve comparable or better results than the encoder-based classification models on the MELD and MerBench test sets, but they perform the worst on the IEMOCAP and RAVDESS test sets. This suggests that while an LLM can capture linguistic cues for emotions, the text-only mode limits its ability for comprehensive emotion recognition. The BLSP model can process speech input but cannot pick up paralinguistic cues for emotion as it is only trained with semantic alignment. Conversely, the encoder-based classification models can capture paralinguistic cues but lack a semantic understanding of emotion. In contrast, BLSP-Emo can simultaneously model linguistic and paralinguistic emotion cues in speech, thanks to its end-to-end modeling and two-stage alignment process. % Our proposed BLSP-Emo, which can simultaneously model linguistic and paralinguistic emotion cues in speech, achieve competitive performance benefiting from the two-stage training process of semantic alignment and emotion alignment.% The competitive performance of the BLSP-Emo and BLSP-SER models highlights the advantages of end-to-end emotion-aware fine-tuning from BLSP pretraining that aligns speech and text at the semantic level, as we will further demonstrate in Section~.% Some method utilize linguistic cues, while others user paralinguistic cues. Why not emphasize here that out method can leverage both types of cues simultaneously?% Compared to other large speech-language models such as SALMONN-7B~, which is also trained on SER data, our model also demonstrates better performance.% The competitive performance of the BLSP-Emo and BLSP-SER models highlights the advantages of end-to-end emotion-aware fine-tuning from BLSP pretraining, which already aligns speech and text at the semantic level.% This can be determined through the ablation study, but it is not suitable to draw suach a conclusion here.% Results in Table~ show that our model is capable of recognizing emotions in speech, despite being trained only on the emotion-aware continuation task. Firstly, we observe that LLMs focusing solely on text content inaccurately interpret emotions, while those integrating acoustic cues significantly outperform text-only approaches. Secondly, our proposed BLSP-Emo model exhibits recognition accuracy in zero-shot scenarios that stands on par with models that have been explicitly trained on SER task. % Additionally, it is noteworthy that LLM-based generative emotion recognition systems markedly surpass conventional encoder-based classification approaches.% We also discover that LLM-based generative emotion recognition systems significantly outperform encoder-based classification systems for emotion recognition. Finally, compared to other large speech-language models such as SALMONN-7B~ and Qwen-Audio-Chat~, which are also trained on SER data, our model also demonstrates significantly better performance. Beyond speech emotion recognition, our primary concern is whether the model can understand both the semantic content and paralinguistic emotion cues in speech and generate high-quality, empathetic responses. To evaluate this, we construct a synthetic emotion-aware speech instruction dataset named SpeechAlpaca, derived from the open-source instruction dataset Alpaca-52k~. Additionally, we use a modified system prompt that emphasizes both quality and empathy for all systems. We then employ GPT-4 as an evaluator to independently score the responses generated by different systems in terms of quality and empathy on a scale from 0 to 10. For details on test set construction and evaluation prompts, please refer to Appendix~. The results are shown in Table~.

% Please refer to Appendix~ for construction details.% to assess emotion-aware instruction-following capabilities. Specifically, we employ GPT-4 to deduce a set of plausible emotional tones from a text instruction in Alpaca-52k. From these, we randomly select one as the emotion label for the instruction. This process is used to select 100 instructions for each of the five emotion categories. Subsequently, we synthesize expressive speech using the selected emotion label with Microsoft's Text-to-Speech (TTS) API.}% we employ the synthesized emotion-aware SpeechAlpaca dataset, described in Section . % For this evaluation, we use a modified system prompt} that emphasizes both quality and empathy for all systems.% For the Text|Whisper+LLM baselines and the end-to-end speech-text models, the respective text transcript or speech features are provided directly to the LLM as the prompt. For the cascaded baselines that involve a separate SER module, we include the recognized text transcript and emotion label in the prompt as detailed in Section~. % I think this is not necessary as we describe the details of our baselines in 4.2% We then use GPT-4 as an evaluator to independently score the responses generated by different systems in terms of quality and empathy on a scale from 0 to 10 (refer to Appendix~ for details). The results are shown in Table~.

Consistent with findings in the SER evaluation on natural speech, BLSP-Emo achieves the highest emotion recognition accuracy of 83.8\% on synthetic speech. Additionally, BLSP-Emo scores competitively in both quality (8.8) and empathy (7.7) as measured by GPT-4.  %In contrast, the BLSP-SER model, fine-tuned specifically for the SER task, shows a lower performance in SER (80.3\%) on synthesized speech, probably due to the mismatch with natural speech. More significantly, it performs poorly in empathetic response (quality: 1.9, empathy: 2.1) because it loses the ability learned during the pretraining phase to follow speech instructions.% "due to the mismatch with natural speech" is not convincing as BLSP-Emo and BLSP-SER use the same SER datasets In contrast, the BLSP-SER model, fine-tuned specifically for the SER task, achieves a lower performance in SER (80.3\%) and performs poorly in empathetic response (quality: 1.9, empathy: 2.1), as it loses the ability to follow speech instructions learned during semantic alignment. 

The BLSP model, despite having a significantly lower SER score (36.8\%), achieves decent ratings in quality (8.6) and empathy (7.1), as it is able to comprehend semantics and linguistic emotion cues thanks to semantic alignment. % This is because the emotion labels in the SpeechAlpaca dataset are inferred from text using GPT-4, resulting in the emotion labels of most utterances are more consistent with linguistic emotion cues.% I add an explanation here The improvements from BLSP to BLSP-Emo in all three metrics—SER (36.8\% to 83.8\%), quality (8.6 to 8.8), and empathy (7.1 to 7.7)—suggest that the BLSP-Emo approach effectively understands both linguistic and paralinguistic emotion cues in speech while maintaining its instruction-following capability, resulting in overall better responses.

The Text|Whisper+LLM systems achieve a slightly higher quality score (8.9 vs. 8.8) than BLSP-Emo but a lower empathy score (7.4 vs. 7.7) and significantly lower SER scores (40.0\% vs. 83.8\%). This signifies that while LLMs have a strong capability to capture linguistic emotion cues, they are limited by their inability to understand paralinguistic emotion cues. As the examples in Appendix~ show, a text-only LLM can provide an empathetic response to the instruction "Suggest the best way to avoid a traffic jam" based on the semantic content alone. However, it cannot provide empathetic responses to a neutral instruction "Come up with a 5-step process for making a decision" stated in an angry voice.

The HuBERT|wav2vec2|WavLM+Whisper+LLM systems with standalone SER modules achieve comparable quality ratings to the Text|Whisper+LLM systems but higher empathy ratings (7.67.8 vs 7.4), further underlining the importance of capturing paralinguistic emotion cues in generating empathetic responses.  % It is worth noting that these cascaded systems also have comparable or slightly higher ratings in quality (8.9|9.0 vs. 8.8) and empathy (7.6|7.8 vs. 7.7) than BLSP-Emo.  It is worth noting that these cascaded systems also have slightly higher ratings in quality than BLSP-Emo.  We attribute this to the room for improvement in semantic alignment for BLSP pretraining, as the Whisper model contains a separate speech decoder that is trained on significantly more speech data~. Additionally, despite being trained on various speech tasks, large speech-language models like SALMONN~ exhibit limitations in following general speech instructions.

% Beyond the task of speech emotion recognition, we are more concerned with whether the model can comprehend and respond to emotions conveyed from speech in open-ended generative tasks. We test our model's empathetic response capability on the synthetic SpeechAlpaca dataset} and use GPT-4 as an evaluator to score responses generated by different systems on a scale from 0 to 10 (refer to Appendix~ for details).% As shown in Table~, our proposed BLSP-Emo achieves the highest emotional recognition accuracy on synthetic speech. Although systems that generate responses based on semantic content (such as Text+LLM, Whisper+LLM, and BLSP) exhibit a modest accuracy rate of approximately 40\% in speech emotion recognition task, they can still empathize directly with the semantic information in the speech. As illustrated in Table~, when a user asks in a sad voice, "Suggest the best way to avoid a traffic jam," the text-only LLM cannot understand the sad tone within the speech, yet it can still empathize with the situation of a traffic jam. However, when a user asks with an angry voice, "Come up with a 5-step process for making a decision," where the emotion cannot be inferred from the text, the lack of modeling for speech emotions can lead to inappropriate responses. Our proposed BLSP-Emo, built upon BLSP with additional emotion alignment stage, has the capability to understand both semantic and paralinguistic emotional cues in speech to provide better empathetic responses, as evidenced by improved empathy scores. In contrast, the BLSP-SER model, which is trained to predict ground-truth emotion labels, undermines the ability learned during the pre-training phase to follow speech instructions. At the same time, compared to more complex cascading models that include emotion recognition and speech recognition modules, our end-to-end model demonstrates comparable performance in response empathy. It is worth noting that there is a slight performance gap in the quality of responses, which we attribute to the gap in the semantic alignment capabilities of BLSP pretraining compared to the Whisper model. Furthermore, other large speech-language models such as SALMONN~, despite being capable of processing speech-related tasks in a conversational manner, exhibit limitations in directly implementing task instructions contained within speech.% Fine-tuning on BLSP, BLSP-SER, which predicts the ground-truth emotion labels, can impair the ability to follow speech instructions that were learned during the pretraining stage. In contrast, our method of generating emotion-aware continuations enables the model to understand and respond to emotions in speech while preserving its understanding of semantic content. At the same time, compared to more complex cascading models that include emotion recognition and speech recognition modules, our end-to-end model demonstrates comparable performance in both response quality and empathy. % As shown in Table~, our proposed BLSP-Emo achieves the highest emotional recognition accuracy on synthetic speech. Although systems that generate responses based on semantic content (Text+LLM, Whisper+LLM and BLSP) have high quality scores, the empathetic score for the user's current emotional tone is low. Fine-tuning on BLSP, BLSP-SER, which predicts the ground-truth emotion labels, can impair the ability to follow speech instructions that were learned during the pretraining stage. In contrast, our method of generating emotion-aware continuations enables the model to understand and respond to emotions in speech while preserving its understanding of semantic content. At the same time, compared to more complex cascading models that include emotion recognition and speech recognition modules, our end-to-end model demonstrates comparable performance in both response quality and empathy. % Other large speech-language models, despite being capable of processing speech-related tasks in a conversational manner, struggle to directly execute task instructions contained within speech.% We also provide a qualitative examples in Table~{I update the results of Text+LLM, but do not update the discussion of the new results.}}, which includes the outputs from different models. These examples demonstrate that the Whisper+LLM (semantic-based) baseline tends to generate a response with a more neutral emotion tone, whereas the WavLM+Whisper+LLM and the BLSP-Emo model both generate text that possesses a more engaging tone. We next evaluate multi-turn conversations, an important application scenario for empathetic large speech-language models. This evaluation allows us to determine if the emotion understanding capability of BLSP-Emo, learned from a simple emotion-aware continuation task, can generalize to scenarios with extended conversational context. Following a setup similar to , whose test set is not publicly available, we extract 3-turn dialogues between two speakers from IEMOCAP~, treating the first speaker as the user and the second as the assistant. The conversation history consists of the reference dialog transcripts from the first two turns, plus the current input—either a transcript for a cascaded system or speech features for an end-to-end model—from the user, along with the predicted emotion label if the system has a standalone SER module. The LLM is then prompted to generate a response. For examples, please refer to Appendix~.

Given that typical user inputs in conversations are not specific task instructions, we found it difficult for GPT-4 to separately assess quality and empathy as done on SpeechAlpaca. Instead, we employ GPT-4 as an evaluator to determine which system's output is better, based on reference transcripts in the conversation history and the emotion label of the user's most recent input. For details, please refer to Appendix~.

% Given that typical user inputs in conversations are not specific task instructions, assessing the quality of model responses based on criteria like helpfulness or harmlessness, as used on SpeechAlpaca, is not suitable. Instead, we employ GPT-4 as an evaluator to determine which system's output is better, based on reference transcripts in the conversation history and the emotion label of the user's most recent input. Please refer to Appendix~ for details.

As shown in Figure~, BLSP-Emo demonstrates higher win rates compared to Whisper+LLM, BLSP, and WavLM+Whisper+LLM. This advantage mirrors BLSP-Emo's comparative performance on SpeechAlpaca, highlighting its capability to understand and respond to paralinguistic emotion cues in speech. Notably, BLSP-Emo's superiority over WavLM+Whisper+LLM is somewhat unexpected, given that the latter performed comparably or slightly better on SpeechAlpaca in both quality and empathy ratings. We speculate that this discrepancy may be attributed to the specific prompt used, which incorporates both the transcript and the recognized emotion tone for the user's last speech input (as illustrated in Appendix~). This could introduce inconsistency compared to the simpler transcript representation of the conversation history. In contrast, BLSP-Emo does not necessitate special prompting for speech input, as it implicitly captures emotion cues in the speech features. While prompt engineering could potentially enhance the performance of WavLM+Whisper+LLM, this also underscores the simplicity and advantage of the BLSP-Emo approach.

% As shown in Figure~, BLSP-Emo outperforms Whisper+LLM, BLSP, and WavLM+Whisper+LLM with higher win rates. BLSP-Emo's advantage over Whisper+LLM and BLSP is similar to its comparative performance on SpeechAlpaca, thanks to its ability to understand and respond to paralinguistic emotion cues in speech. BLSP-Emo's advantage over WavLM+Whisper+LLM is somewhat surprising, as the latter performed comparably or slightly better on SpeechAlpaca in both quality and empathy ratings. We hypothesize that this might be related to the specific prompt used, which needs to incorporate both the transcript and the recognized emotion tone for the user's last speech input, as shown in Appendix~. This could result in some inconsistency compared to the simple transcript used in representing the conversation history. In contrast, BLSP-Emo does not require any special prompting for speech input, as the emotion cues are implicitly captured in the speech features. While prompt engineering could potentially improve the performance of WavLM+Whisper+LLM, this also highlights the simplicity and advantage of the BLSP-Emo approach.% To verify whether the understanding of emotion by our BLSP-Emo model, which was trained on the single-turn continuation task, can generalize to multi-turn conversation scenarios, we evaluated the model's empathetic response capability in spoken dialogue using a method similar to that in . Since they did not publicly release the test data, we extracted 3-turn dialogues from IEMOCAP~, using the text from the first two turns as the dialogue history. In the current turn, the user's input is either speech or the recognition result from cascading modules, and the model responds based on the dialogue history and the user's current input (refer to Appendix~ for details).% Given that user inputs in conversational scenarios often do not contain specific task instructions, it is not suitable to assess the quality of model responses from perspectives such as helpfulness or harmlessness. Hence, we no longer evaluate the responses' quality and empathy separately. % In this experiment, we use GPT-4 as an evaluator to assess which system's output is better. As shown in Figure , our proposed BLSP-Emo model outperforms the other three systems in multi-turn conversation scenarios. Compared to Whisper+LLM and BLSP, which generate responses based on semantic content, our method captures the paralinguistic emotion cues in speech, enabling better responses to the user's emotional state. Compared to the cascaded system WavLM+Whisper+LLM, which considers both semantic content and emotion cues, our method has an advantage in multi-turn conversations. % We attribute this to the significant gap between the instructiona format used by LLM during the SFT stage and the emotion-injection prompt format used by the cascaded model. Although this form of injection can guide the LLM to pay attention to both transcript and emotion information in single-turn dialogues, it disrupts the consistency of user input format in multi-turn scenarios.% We attribute this to the significant gap between the emotion-infused prompt format and the instruction format used by LLM during the directive fine-tuning stage.  To explore whether the knowledge learned about emotion cues can generalize across languages, we evaluate zero-shot SER performance on three languages not included during training. As shown in Table~, BLSP-Emo achieves the best overall performance across the languages, performing comparably or better than BLSP-SER and significantly better than the other models. 

% % To explore whether paralinguistic knowledge can be generalized across languages, we evaluated the zero-shot SER performance in three distinct languages. As depicted in Table , BLSP-Emo demonstrates a robust generalization capability, achieving an average accuracy of 63.4\%. % [htp]%     \centering%     \scriptsize%     %     %     % % To uncover the underlying mechanisms of this phenomenon, we compared BLSP-Emo with other baseline systems using identical test datasets. Notably, there is a substantial performance disparity between LLM-based Generative Models and Encoder-based Classification Models. We attribute this gap primarily to the superior cross-lingual generalization capabilities inherent in LLMs. During the Emotional SFT phase, BLSP-Emo is trained to encode emotional information and semantic information simultaneously. The fusion of representation spaces allows it to leverage the cross-lingual strengths of LLMs as a free lunch when processing emotional speech in previously unseen languages. Compared with other large speech-language models and BLSP-SER, BLSP-Emo exhibits a superior generalization capability, indicating the effectiveness of our proposed approach.% I DON'T THINK WE CAN MAKE THE CLAIM ABOUT LLM'S SUPERVIOUR CROSS-LINGUAL GENERALIZATION CAPABILITY AS BLSP PERFORMS PARTICULARLY POOR ACROSS THE LANGUAGES.% []%     \centering%     %     %     %  We conduct ablation studies to understand the impact of two training strategies within the BLSP-Emo approach, with results presented in Table . Directly applying emotion alignment without first performing BLSP semantic alignment leads to a significant drop in both standalone SER performance and quality/empathy ratings in empathetic response. This underscores the importance of having a bootstrapped speech-language model that is aligned at the semantic level before attending to paralinguistic cues. % We conduct ablation studies to understand the impact of two training strategies within the BLSP-Emo approach. As shown in Table , directly applying SFT for emotion alignment without first performing BLSP pretraining for semantic alignment leads to a significant drop in both standalone SER performance and quality/empathy ratings in empathetic response. This underscores the importance of having a pretrained speech-language model that is aligned at the linguistic level before attending to paralinguistic cues.

Furthermore, incorporating the auxiliary SER classification task proves beneficial for achieving higher performance in speech emotion recognition on natural speech, even though it does not lead to any noticeable differences on the SpeechAlpaca test set or in the evaluation of empathetic responses.

% We conduct ablation studies to elucidate the contributions of different components within BLSP-Emo. As detailed in Table , omitting the BLSP pretraining phase and directly implementing SFT phase results in significant performance declines. This finding substantiates the essential role of our proposed two-stage training framework. The initial pretraining phase is critical for aligning disparate modalities; its absence exacerbates the modality gap, thereby obstructing the learning of fine-grained, paralinguistic knowledge. % Additionally, integrating the auxiliary SER classification task is pivotal for accurately capturing emotional information. As shown in Table , relying solely on the emotion-aware continuation task diminishes SER performance, indicating an inaccurate capture of emotional nuances. Although the GPT-4 score of empathic dialogue shows no signs of declining on the synthetic test set, a diminished ability to accurately capture emotional information could potentially compromise the model's efficacy in more complex, real-world empathic conversations. We perform additional analysis comparing our training strategies against two recent approaches in the literature of speech-language models with emotion-aware capabilities.

% First, we compare our approach to the work of E-chat~ and Spoken-LLM~, which rely on synthesized emotion-aware speech instruction data constructed using expressive text-to-speech tools and ChatGPT.  First, we compare our approach to the method of E-chat~ and Spoken-LLM~, which constructed synthesized emotion-aware speech instruction data using expressive text-to-speech tools and ChatGPT.  As noted previously and found in our preliminary studies, models trained on synthesized speech fail to generalize to natural human speech. Given that our approach also requires constructing synthesized emotion-aware continuation data for natural speech, a critical question arises: is it better to use ChatGPT for data construction, as commonly done in the literature, or to use the same LLM that BLSP-Emo is adapted from?

To address this, we trained a new model named BLSP-ChatGPT, utilizing ChatGPT to generate emotion-aware continuations for emotion alignment, starting from the same pretrained BLSP model as BLSP-Emo. As shown in Table~, while BLSP-ChatGPT achieves higher SER performance than BLSP, its quality and empathy ratings in empathetic responses are notably lower. BLSP-ChatGPT performs worse than BLSP-Emo across all metrics. We hypothesize that the emotion-aware continuations generated by ChatGPT may not align well with the likely responses generated by the internal LLM in BLSP-Emo. Consequently, the alignment process may focus on narrowing the distribution gap between ChatGPT and the internal LLM, rather than learning to capture the paralinguistic emotion cues in speech to fit into the aligned semantic space established during semantic alignment.

Next, we compare our approach against the multi-task learning strategy employed by other large speech-language models, such as SALMONN~, which aims to understand semantic content and various paralinguistic cues. As demonstrated in previous sessions, BLSP-Emo significantly outperforms SALMONN-7B in both standalone emotion recognition and emotion-aware instruction following. However, a question remains: can we replace the emotion-aware continuation task employed in the emotion alignment stage with a multi-task framework involving two tasks: emotion-agnostic continuation and speech emotion recognition?

To answer this, we use the SER training datasets to construct two tasks: one for standalone SER and another for emotion-agnostic continuation. The resulting model is named BLSP-MultiTask. As shown in Table~, while BLSP-MultiTask significantly improves the SER accuracy of the BLSP model, its response quality is lower than that of BLSP. BLSP-MultiTask also performs worse than BLSP-Emo across all metrics. This comparison highlights the importance of the emotion-aware continuation task in developing effective empathetic speech-language models.

% Previous studies, such as E-chat~ and Spoken-LLM~, encountered the challenge that synthesized speech lacks speaker generalizability. This challenge arose as they constructed specialized speech instruction datasets by synthesizing speech through a TTS system and leveraging ChatGPT to generate emotion-sensitive responses. Our proposed BLSP-Emo approach, which employs human recorded speech from SER datasets to generate emotion-sensitive continuations, demonstrates enhanced generalizability across different speakers. However, a critical question remains: Is it feasible to generate responses using ChatGPT, following the precedent set by previous methodologies, instead of relying on LLM to generate self-curated continuations? To answer this question, we fed transcripts and emotions from the same SER datasets as those used in BLSP-Emo to GPT-3.5 to generate corresponding responses. As shown in Table~, the method of data construction, whose output originates from an exogenous distribution, shows an improved accuracy in speech emotion recognition compared to the pre-trained BLSP. However, the disruption of semantic alignment affects the capacity to follow speech instructions. Moreover, in terms of both the accuracy of speech emotion recognition and the quality and empathy scores of the empathetic responses, it significantly lags behind our proposed BLSP-Emo. It is more adept at mimicking GPT's style rather than truly understanding speech emotions and responding to them~.% Other large speech-language models, such as SALMONN~, develop the ability to understand both semantic content and emotional cues in speech through multi-task training, but they struggle to execute speech instructions directly. This gives rise to a new question. Can we build upon the bootstrapped BLSP model, and through multi-task training involving cross-modal continuation tasks and SER tasks, endow the model with the ability to generate empathetic responses? To answer this question, we divide the SER datasets into two parts: one being (speech, transcript) for generating text continuations, and the other being (speech, emotion) for training SER task. As shown in Table~, multi-task training equips the bootstrapped BLSP model with the capability to perform speech emotion recognition task. However, the understanding of speech emotions, derived from specific prompts, faces challenges in generalizing to broader contexts. This is substantiated by the significantly increased SER accuracy and the almost unchanged empathy scores for empathetic responses. Meanwhile, predicting ground-truth emotion labels during multi-task training still undermines the ability to follow instructions learned during the BLSP pre-training phase.% To teach LLMs to understand spoken instructions and emotions, there are two trivial solutions. One is to create a special instruction dataset with emotional speech, like the ones in E-chat~ and spoken-LLM~. The other is to train the models in a multi-task manner. This includes using cross-modal continuation~ to help the LLMs follow instructions, and leveraging SER to help the LLMs understand speech emotions. To demonstrate the advantages of our proposed self-alignment method, we compared it with these two solutions. To ensure fairness in the comparison, all experiments were conducted using BLSP as initialization and utilized the same dataset as employed in BLSP-Emo for supervised fine-tuning. For data construction, we fed transcripts and emotions to GPT-3.5 to generate corresponding responses. For the multi-task training approach, we divided the SER data into two parts: one being (speech, transcript) for generating text continuations, and the other being (speech, emotion) for training SER task.% As shown in Table~, our proposed BLSP-Emo significantly outperforms these two solutions. The method of data construction, whose output comes from an exogenous distribution, although the model still retains a certain ability to follow instructions and provide empathetic responses, is not optimal. It is adept at mimicking GPT's style rather than truly understanding speech emotions and responding to them~. % Multi-task learning, while endowing the bootstrapped BLSP model with the ability to perform speech emotion recognition tasks, results in speech emotion understanding learned from specific prompts that struggle to generalize to more general contexts, as evidenced by the significantly increased SER accuracy and the almost unchanged empathy scores for empathetic responses. Meanwhile, predicting ground-truth emotion labels during multi-task training still undermines the ability to follow instructions learned during the BLSP pre-training phase, although the inclusion of continuation tasks may mitigate this forgetting.% Multi-task learning, while enhancing the model's empathic capacity, allowing the LLM to predict ground-truth emotion still disrupts the instruction-following abilities learned during the BLSP-KD pre-training phase. %The inclusion of continuation task merely mitigates this forgetting.% Besides, the empathic abilities learned under specific SER prompts do not generalize well to other general contexts.