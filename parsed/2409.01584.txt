This study adopted three evaluation metrics proposed by~ and also described these metrics more details in Appendix~. We also utilize popular metrics in NLG for evaluation, i.e., BLEU~, ROUGE~, and BERTScore~.

These metrics evaluate how well the generated text incorporates entities related to the artwork and how accurately it reflects the relationships between these entities proposed by~. Entity Coverage measures the inclusion of relevant entities in both exact and partial matches. Entity F1 assesses the frequency and appropriateness of entity usage by comparing the generated text with reference explanations, inspired by the ROUGE metric. Entity Cooccurrence goes a step further by examining how entities are contextually combined across sentences, considering their co-occurrence within the entire text, and applying brevity penalties to avoid inflated coverage in longer explanations.

We chose five models with relatively high performance: mPLUG-Owl2~, LLaVA-NeXT~, XComposer2~, Phi-3~, and Qwen-VL~. In addition, LLaVA-NeXT and Qwen-VL were conducted LoRA Tuning~ with English train data and included in the evaluation. Detailed experimental settings are described in Appendix~. This approach is based on the observation that current LLMs perform better when instructions are given in English~. As far as Alignment tasks, we validated four patterns of input: \{En, Lang\}-\{En, Lang\}. This indicates that when the input is English, the output can be directed to English or another language. The same thing can also be done when the input is another language, and these four patterns were tested in this study. By testing these patterns, we verify whether or not LVLMs perform better when supported in English, and whether or not having the output in English is a meaningful instruction. As far as tokenizing words, we used SpaCy as a multilingual tokenizer, tokenizing each language to perform segmentation. Thus, each language is expected to be divided into optimal token units.

From the experiments conducted with Alignment-10, the method let LVLMs generate in English with English (\{En\}-\{En\}) results are listed in Table~, the method which is instruction in English and output in other languages (\{En\}-\{Lang\}) results in Table~, and the instruction and output in other same languages (\{Lang\}-\{Lang\}) results in Table~. The results for Phi-3 and XComposer2 are described in the Table~ in Appendix. Overall, the results confirm that giving instructions in English and letting them generate output in English (i.e., \{En\}-\{En\}) maximizes the performance of LVLMs. On the other hand, LoRA Tuning increased the value of Entity Cooccurrance, while other values decreased.  This suggests that LoRA Tuning enabled LVLMs to understand and explain the context, but prevented entities from appearing in the generated sentences. Furthermore, looking at the results of Alignment-5 in Table~ in Appendix, where the number of data was expanded, the outputs that used English instructions and outputs were generally higher, followed by those using instructions and outputs in other languages. This is consistent with the results of Alignment-10. In addition, Figure~ includes results where instructions were given in other languages and outputs were produced in English.

In this study, as far as inference which needs to use GPUs, all experiments were conducted on a single NVIDIA RTX A6000 GPU and NVIDIA A100-SXM4-40GB, with 8-bit quantization utilized for model generation. However, there is no InternLM-XComposer-2 with 8-bit, this model was loaded and inferred in 4-bit mode. To standardize the length of tokens generated across all models, the maximum token length was set to 1024. The same settings were applied to each model for performance comparison purposes.

We conducted LoRA~ Tuning with two models: LLaVA-NeXT and Qwen-VL. Both were trained using two NVIDIA A100-SXM4-40GB GPUs. Detailed parameters are provided in Table~ and Table~.

We selected ten languages with the highest number of articles from the statistics of all language versions of Wikipedia. Of the top 10 prefectures, Cebuano, Egyptian dialects of Arabic, and Polish were deemed difficult to identify by sampling during the evaluation, so we added the runners-up, Chinese and Japanese.

For English, a language rich resource, we split the data into train, valid, and test data using six metrics proposed by ~ (six metrics: page views, number of links, number of edits, number of references, number of language versions, and article length.) were used in this study as well, and the data were divided equally considering famous artworks.  All data included in the alignment were used as test data so that data used in the alignment task were not included in the train. We described the number of all data in Table~.

In our study, we created a dataset from Wikipedia articles regarding artworks. Each image is available under the Creative Commons License (CC) or other licenses. Specific license information for each image can be found on the Wikipedia page or the image description page for that image. The images in this study are used under the terms of these licenses, and links to the images are provided in the datasets we publish so that users can download the images directly. The images themselves are not directly published. Thus, our data does not infringe upon the licenses.