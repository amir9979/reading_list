In this section, we introduce the notation used for~ throughout the paper. Let  denote the aligned language model, particularly referring to the base aligned large language models (LLMs) such as llama2-7b-chat-hf. The supervised fine-tuned model for specific tasks, such as WizardMath~, is referred to as .  The notation  represents the edited model, where new knowledge has been integrated into the language model through model editing, while maintaining the same backbone as . We denote the target language model as , where the target model can be , , or .  In the  stage, we denote a small dataset  containing harmful question-answer pairs to fine-tune a model denoted by . The target language model obtained after~ (HDR) stage is denoted by . We employ a set of in-context exemplars, denoted as , which includes both unsafe and safe prompts. Given a harmful question, the unsafe prompts comprise the question paired with a harmful answer, while the safe prompts contain the question paired with a safe answer. This exemplars  are used in ~ (Safe-Align) stage. The target language model after employing~ is denoted by . 

In this stage, our objective is to eliminate the harmful direction from the target model . To achieve this, we follow the task analogies presented in , treating harmfulness as a specific task (this was also done by ) and aiming to mitigate its impact without impairing other capabilities of the language model. Specifically, we first fine-tune a language model with the same backbone as  using the dataset , resulting in the model . Subsequently, we compute the~  by taking the element wise difference between  and  (see equation~).

To mitigate the model's capability in generating harmful responses while preserving its performance in other areas, we apply the negated harm vector  to the target model  through element-wise subtraction. However, our objective is to minimize the extent of intervention on the target model . Therefore, instead of directly subtracting , we first eliminate redundant parameters by selecting the top  parameters based on their magnitude.\\  Following~, we select top  parameters from  based on their higher magnitude (see equation~). Further, make the values of other parameters in  to zero (see equation~). 

Further, we apply  on target model  to obtain intermediate model  (see equation~).

After removing the harmful direction, we further align the model  to enhance its safety by adjusting its latent space. According to previous studies~, in-context learning can effectively guide the responses of the model  towards specific task-oriented directions for user queries. The objective is to steer the behaviour of model  by providing curated prompts that exemplify safe and desirable responses.  To achieve this, following the approach in , we compute the inference-time variant of in-context learning known as the in-context safety vector () using the  dataset. We then apply the  to the model  to obtain a safer model .\\  We prepare the in-context exemplars , consisting of pairs of unsafe and safe prompts (,  respectively). Given a harmful query ,  includes an unsafe prompt that pairs the question  with a harmful answer  and a safe prompt that pairs the same question  with a safe answer . We obtain the hidden representation  of  and  by passing them through model . Considering the model  has  layers, we take the latent states for each layer () at the last token position and concatenated them to form the hidden representation vector  () (see Equation~ and~). In our setup,  and  are paired, resulting in (, ) pairs.

The expected in-context safety vector () should direct latent states closer to the representations of safe prompts  than to those of unsafe prompts . To achieve this, we can treat the , denoted as , as the optimizer of an objective function (see Equation~)~. %%

For function  (given in Equation~), we use the simple  norm and the objective function can be written as Equation~.

The optimal solution of Equation~ is equivalent to the first principal direction of the differences between  and  such as \{ - ,  - , ,  - \}. Therefore, we directly use the first principal direction of ( - ) as the .

: Once we obtain , we perform addition to the latent states  of  at all the layers  where  and every token position  (see equation~).

The  is the  corresponding segment of the ,  is a hyperparameter that controls the strength of applying the .  Also, to preserve the model's existing capability, the updated latent states are normalized to match the  norm of the latent states before the update (see Equation~).

So, the derived hidden states  is the hidden states of the safe model .

In this section, we discuss the application of the proposed framework, , to language models in various scenarios: (a) the base model, (b) the supervised fine-tuned model, and (c) the edited model.\\  : We conduct the experiments using two widely utilized language models --  (Llama2) and  (Mistral). In this scenario, we consider the base model as the . To enhance the safety of the base model, we followed the  and  module as they are, resulting in a safer version of the target model.\\ : For the supervised finetuned model, we utilize three task-specific language models -- ~, ~, . The first two models are tailored for mathematical tasks, while the third is designed for code-related tasks.\\ : In this study, we examine a scenario where the integration of new knowledge into a language model via model editing~ results in an increased generation of harmful responses. Our investigation focuses on two distinct types of knowledge inclusion -- (i) ~: This occurs when the edit instance does not contain any harmful or unethical content but inadvertently causes the model to produce harmful outputs.(ii) ~: This involves edit instances that contain unethical or harmful information, thereby directly triggering harmful responses from the language model.  For both types of editing, we utilize the  model as the backbone. The method employed for editing is the ROME approach~. Following the edits, we detail the application of the  technique on the edited models to address and mitigate the generation of harmful responses.\\  For both types of editing scenarios, we follow a consistent procedure. First, we edit the language model with a single instance, adhering to the method described in , targeting a specific layer  for each dataset. This results in an edited model  for each dataset.  Before applying , we perform an additional step. We identify the layers in  where the editing occurred, along with the preceding and subsequent layers. This identification is performed using Equation~. Subsequently, we obtain a mask  using Equation~.

For minimal intervention in , we only consider the harm vector  for the edit area (see Equation~). 

Once we obtain , we follow Equation~ and the subsequent steps to derive the safer edited model . All these operations are conducted exclusively within the edit area, specifically the edit layer  and its adjacent layers  and .

We prepare two datasets for our methodology: (a)  for fine-tuning , and (b)  for obtaining the In-Context safety Vector (). We utilize the  dataset~ to construct both datasets. Specifically, we use all the queries and their corresponding harmful answers from this dataset to supervised fine-tune the base model , resulting in . In order to construct  for obtaining , we sampled 30 queries. For each query, we prepared two types of prompts: , containing question and its harmful answers, and , containing question and its safe answers. Due to safety considerations, we do not release the harmful answers from the  dataset.

We evaluate our framework using five established datasets -- DangerousQA~, Advbench~, HarmfulQA~, NicheHazardQA~, and HEx-PHI~. Unlike other safety alignment methods~, which often utilize only portions of the available data, our evaluation employs the complete datasets. Furthermore, we introduce a new dataset, , specifically curated to include instances of unintentional edits. The dataset for unintentional edits in our evaluation are detailed as follows. Other dataset details can be found on Appendix~.\\ : This is a small dataset of 40 edit instances consists of questions and their answers. These questions are harmless in nature. However, editing with these instances can make the model generate more unethical responses. These questions and answers are gathered from diverse topics such as hate speech and discrimination, threats, conspiracy and cruelty, advanced technology, racism, stereotypical, social sciences and business and economics (see Appendix~). % In our proposed framework, the parts used in modules  and  can be replaced with different techniques. So, we design the below baselines to compare with our proposed framework. \\ : We use the original models such as llama2-7b-chat-hf (), WizardMath-7b () to evaluate on all the safety datasets. The original model for  is same as the base model. Also, we measure the unethical generation for  model.\\ : This serves as the baseline, incorporating only our  module within the framework. In this approach, the second module present in the framework is not utilized.\\ : In this baseline, we use the task vector~ in the  module to calculate the harm vector. There is no parameter pruning (redundant parameter removal) before subtracting the vector from the target model .\\ : This baseline uses only the second module, ~, from the entire framework. We do not employ the  module in this case. Additionally, we use in-context vectors to compute the in-context safety vector (). 

We adopt the approach detailed by~ to assess the effectiveness of  using the Attack Success Rate (ASR). The ASR quantifies the proportion of responses deemed unsafe out of the total number of input queries to the model. To assess our framework, we use GPT-4 as the evaluator~ for evaluating on all the five datasets. All responses generated by the models were assessed by GPT-4 to measure the ASR. The specific prompt used for the GPT-4-based evaluation is provided in Appendix~.

We do not perform any hyperparameter search. The results could improve with proper pruning percentages, adopting different merging techniques instead of TIES, using task vectors in the HDR stage, and employing different in-context vectors to calculate the ICV. However, the hyperparameters we use to obtain the results for the base, supervised fine-tuned, and edited models are provided in Appendix~.

To ensure that our  framework does not compromise the general capabilities of the model, we conducted a series of utility tests. These tests were designed to evaluate the performance of both base models () and supervised fine-tuned models (). For  models, we utilized the following benchmarks -- MMLU (5-shot)~, TruthfulQA~, HellaSwag~, ARC~. For  models, such as WizardMath and llama-math, we employed the GSM8K (8-shot) benchmark~.  We also conduct an over-safety test~ for the original models and after employing . In this test, we compute the refusal rate of the model on the XS Test dataset. The  is the fraction of full compliance questions for which the model denies answering. 

This dataset comprises a concise collection of questions and answers that, while ethically neutral, could potentially cause unintended harm to a model if used for updates through model editing (see Table~). Such harm typically arises when attempts are made to customize a language model to deliver responses from a specific organization's/culture's/human's perspective. For instance, the question "What was the impact of European colonization on indigenous cultures?" and it's answer is not inherently unethical or harmful. However, individuals from different regions may provide answers shaped by their local perspectives.  If a language model is trained to adopt a specific organizational or cultural viewpoint through editing, it aligns more closely with the values and perspectives that are prevalent in that region, enhancing its relevance and usability for users from that area however compromise safety of the model.

In this section, we attempt to analyze the time complexity of our framework~. Assume that we have  number of layers in language model. There are  token sequence length.  is the dimension of the embeddings. For each layer, the complexity of self-attention is . This happens for the pairwise attention computation among all tokens.  We assume that the  sublayer in each layer has a complexity of  for all tokens. For  layers, the combined complexity for the language model (without the ICV) across all layers would be .\\  When adding the  vector to each token's output from the MLP sublayer in every layer, we are performing an addition operation which has a linear complexity in terms of the number of dimensions of the token embeddings. The  has the same dimension  as the model's embeddings, is added to each of the  token embeddings in each of the  layers. Therefore, the complexity of adding the  to all the layer is .\\ : Combining the basic complexity of the transformer with the additional complexity from the ICV addition, the total complexity per layer give  Hence, across  layers, the overall complexity remains .

The prompts we use in our experiments are given in Table~.  %\FloatBarrier For fine-tuning purposes, we use the Llama Factory~ library for full fine-tuning. Throughout our experiments, we set the  value to 0.12, while the  value varies between 2 and 3. These values are determined empirically. Additionally, our experimental setup involves leveraging benchmark datasets to test the robustness and reliability of our framework across various harmful and unethical content scenarios. We adopt the Attack Success Rate (ASR) as our evaluation metric to quantify the proportion of unsafe responses generated by the models. 

The results for intentional edits across all the datasets are given in Table~.  ~ contains approximately 200 toxic questions generated by prompting . The prompts focus on six adjectives such as racist, sexist, illegal, stereotypical, harmful, and toxic.\\  ~ comprises around 500 harmful instructions covering a range of policy-violating topics such as profanity, graphic depictions, misinformation, discrimination, cybercrime, illegal recommendations, and threats.\\  ~ includes approximately 1,960 harmful questions spanning ten diverse topics such Science  Technology, History  Culture, Math  Logic, Literature, Philosophy  Ethics, Social Sciences, Health  Medicine, Geography  Environment, Education  Pedagogy, and Business  Economics.\\  ~ features about 388 unethical questions from various topics such as fake news and propaganda, cruelty and violence, hate speech and discrimination, conspiracy theories and paranoia, control of thoughts and emotions of learners, and advanced technology.\\  ~ comprises 330 harmful instructions across 11 prohibited categories, including illegal activity, child abuse content, hate/harass/violence, malware, physical harm, economic harm, fraud and deception, adult content, political campaigning, privacy violation activity, and tailored financial advice.\\ By leveraging these benchmark datasets, our framework is rigorously tested across a wide range of harmful and unethical content scenarios, ensuring robust and reliable safety alignment.