Large Vision-Language Models (VLMs) have significantly advanced capabilities in understanding and generating content that involves both image and textual information. These models are typically pre-trained on datasets containing millions to billions of image-text pairs, and subsequently fine-tuned on specific downstream tasks, enabling these models with the ability to generalize well across various domains. CLIP  and ALIGN  are the pioneering models that have demonstrated remarkable zero-shot performance on computer vision tasks such as image classification and image-text retrieval tasks. Recently, with the advancement of Large Language Models (LLMs) , numerous studies  have leveraged the extensive knowledge embedded in LLMs and the powerful visual understanding capabilities of large vision models like vision transformer ViT. These efforts have resulted in models capable of performing complex reasoning over both text and images, achieving state-of-the-art results in various tasks such as image captioning and visual question answering.

Visual Biomedical Question Answering (VBQA) is the task of answering questions related to biomedical images, combining both the complexities of the textual and vision domains. The models in this field are developed with the aim of receiving and understanding medical images such as X-rays, CT scans, MRIs, and other diagnostic visuals. Recently, in the light of multimodal LLMs (MLLMs), several works  have emerged, leveraging these MLLMs to enhance VBQA systems. Specifically, a notable work is LLaVA-Med  which is developed based on the LLaVA framework. This model employs a two-stage training process to achieve state-of-the-art results across various medical VQA benchmarks, including VQA-RAD , SLAKE  and PathVQA . LLaVA-Med is initially pre-trained with 600,000 biomedical image-text pairs and then further trained with instruction-following data generated by GPT-4 to create multi-round questions and answers based on biomedical images and their captions. Another notable model is Med-Flamingo, which is built upon the MLLM Flamingo for biomedical applications and demonstrates highly effective reasoning and in-context learning abilities. These models exemplify the powerful capabilities of MLLMs for the VBQA task.

%------------------------------------------------------------------------% % You must include your signed IEEE copyright release form when you submit your finished paper.% We MUST have this form before your paper can be published in the proceedings.% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:% .

In this section, we detail our approach, illustrated in Figure , for identifying critical findings and their corresponding regions, as well as global diseases in chest X-ray images. Inspired by MiniGPT-v2~,  which efficiently adapts to new vision-language tasks, we have designed a new model adapted for the medical domain, integrating multiple visual encoders.

In the medical domain, working with unique images such as X-Rays, MRIs, and ultrasounds necessitates the use of advanced visual models. To bridge the capabilities of pre-trained general domain Visual Language Models (VLMs) to the medical domain, we have integrated multiple visual encoders into our visual backbone component. Recognizing that an effective medical model requires high-quality multi-modal data, as well as the limited and costly resources regarding datasets in the medical field, this integration aims to maximize the diversity and comprehensiveness of the information extracted from images.

By leveraging multiple visual encoders, each trained on different large-scale datasets, we can enhance the richness of the image representation. Specifically, we utilize two vision encoders: BiomedCLIP~ and PubMedCLIP~. These encoders are trained on extensive datasets containing both medical images and associated text, such as PMC-15M~ for BiomedCLIP and ROCO~ for PubMedCLIP.

Initially, to ensure the capabilities of the pre-trained visual encoders are preserved, we set them frozen during the training stage. Each image , where , , and  denote the height, width, and channels of the image, respectively, is passed through the pre-trained encoders to extract features, specifically the patch embeddings. Consequently, the representations of the image,  and , are obtained from BioMedCLIP and PubMedCLIP as follows: %  \\%  where  and  are the number of patches of BiomedCLIP and PubMedCLIP, respectively. We then concatenate  and  to obtain the unified presentation of the image as follows:

This component aims to project the visual features  obtained from Eq.  into the language model space. Specifically, the projection consists of two linear layers, with a GELU activation function~ between them. Notably, for the final linear layer, we leverage the pre-trained weights of the linear layer from MiniGPT-v2, which connects the vision backbone to the language model. To optimize efficiency during training and inference, inspired by MiniGPT-v2, we concatenate five consecutive visual tokens before inputting them into the projection. This method increases the dimensionality of each visual token by a factor of five while reducing the number of tokens by the same factor, thereby minimizing the input size to the language model.

Following Eq. , we have the visual representations . After concatenating every five adjacent visual tokens into a single token, the resulting visual representations , where  and each  is defined as follows:

The visual projection layer  is applied to map  to  where  represents the hidden size of the language model, as follows:

In the context of vision-language models, the language is pivotal, while the vision serves merely as additional information to guide the language model in generating answers. In this paper, one of the main tasks is the localization of chest X-rays. Therefore, we need a language model adept at identifying the spatial regions of anomalies within the images. To achieve this, we selected Llama 2-Chat (7B)~, which is pre-trained on a variety of vision-language tasks via MiniGPT-v2. Specifically, we initialize the pre-trained weights of the Llama 2-Chat component from MiniGPT-v2. We refer to this language model as the foundation for transferring knowledge into the medical domain we are working on. 

The text is processed through a word embedding layer to obtain sequence tokens , where  represents the sequence length of text tokens. Subsequently, the visual tokens from Eq.  are concatenated with the text tokens  and fed into Llama 2-Chat to generate answers.

The training process consists of two stages: the first is grounded critical findings learning, and the second is diagnosis learning.

 In this stage, our objective is to adapt the language model to the medical domain, enabling it to comprehend anomalies in X-ray scans. We select samples that contain identified anomalies along with their specific locations in the image. The language model is then directly instructed to perform grounded critical findings detection. 

 After the first stage, the model acquires sufficient knowledge to identify regions in the image that contain anomalies. In this stage, the model utilizes this acquired knowledge to diagnose the diseases present in the image. The emphasis is on leveraging the detailed understanding of the localized findings to provide accurate and comprehensive diagnoses.

The general input format consists of three parts: the first part is the image input, the second part is the identifier token, and the third part is the instruction input. Details of each part are provided below.

. The images are processed through the visual backbone and projection to obtain a fixed-length sequence for input into the language model. To differentiate between tokens derived from images and those from text, two special tokens, |<image>| and |</image>|, are used to concatenate the beginning and end of the image feature sequence, respectively. This indicates where the image content starts and ends.

. In the first training stage, we define our localization task as a grounded captioning task. Specifically, we employ the || token, which is pre-trained for visual grounding within the MiniGPT-v2 framework, and adapt it to our medical domain localization task. The rationale for choosing this token is detailed in section . For the second training stage, we define our task as a visual question answering (VQA) task. Therefore, we use the || token and construct instructions tailored to the VQA task. The detailed instruction is presented in section . Overall, separating the two tasks helps the model easily differentiate between them and improves learning efficiency.

. The instruction input provides specific commands or queries that guide the language model's response generation. These instructions can vary based on the desired task, such as generating captions, answering questions, or identifying specific regions in the image. The input is formatted as natural language text, enabling the model to understand and execute the given instructions effectively. The specific instructions we use are detailed in Section .

In this study, we utilize the VinDr-CXR dataset~, gathered from Hospital 108 and Hanoi Medical University Hospital, two of the leading hospitals in Vietnam. The released dataset comprises 18,000 chest X-ray images in DICOM format, all captured in the Posterior-Anterior (PA) view. A team of 17 experienced radiologists manually annotated the images, identifying 22 critical findings (local labels), with each finding localized using bounding boxes, and 6 diagnoses (global labels). The dataset is divided into a training set and a testing set. The training set includes 15,000 images, each independently labeled by 3 radiologists, while the testing set contains 3,000 images, meticulously labeled by a consensus of 5 radiologists. The detailed distribution of the number of positive images in both the training and testing datasets is shown in Figure . 

The VinDr-CXR is one of the few datasets that include labels from a list of findings specifying their locations on the radiographs and is the largest dataset in terms of the number of findings.

%  The raw image data is taken and processed by normalizing color values to 8 bits from the variable 10-16-bit color space that the images were originally stored in. The normalization was done either through the stored windowing parameters or if they were missing, the full-color space of the original image.  % (see our DICOM-to-RGB conversion in the section  in the Appendix). Images with conflicting labels, such as one radiologist labeling an image as  while another provides a full label, are removed from the dataset.

For each bounding box, we normalize the coordinates, defined as , where  and  denote the coordinates, and  and  denote the height and width of the image, respectively. Each coordinate is multiplied by 100 to obtain integer values within the range . These normalized coordinates are then transformed into the string format ``", where  and  represent the  and  coordinates of the top-left corner, and  and  represent the  and  coordinates of the bottom-right corner of the bounding box. Moreover, multiple nearly identical bounding boxes often appear in an image for the same disease, implying that one disease can have multiple overlapping regions. We define these overlapping regions as having a pairwise Intersection over Union (IoU) higher than 0.5. To address this, we select one representative bounding box and remove the remaining redundant regions.

We report the classification performance using precision, recall, and F1-score. The evaluation is conducted by comparing the model's response to the ground truth. Specifically, a class present in both the ground truth and the model's response is considered correct for the classification report.

Regarding the localization task, performance is evaluated based on accuracy. Specifically, for each critical finding (local labels) present in the ground truth that matches the model's response, we consider the predicted bounding box accurate if its IoU with the ground truth exceeds the specified threshold.

For text validity evaluation, we employ common natural language processing metrics: ROUGE, BLEU, METEOR, and CIDEr scores. The generated text undergoes modification by excluding the localization details, as the bounding box coordinates may not reflect the model's summarization capabilities effectively.

% % % % % }% 

The instructional input used during the  is: ``". For the , the instructional input is: ``"

% In our experiments, we utilized the hyper-parameters presented in Table ???. We employed AdamW~ as the optimizer for updating parameters. The model training was conducted on a computing system equipped with ??? and took approximately ??? hours. We trained the model using LoRA~, a parameter-efficient fine-tuning technique. The model was loaded in 8-bit precision, and the  and  parameters were initialized from MiniGPT-v2. The rank, \(r\), was set to 64. We continued fine-tuning these parameters via lower-rank adaptation.

In our experiments, we use the AdamW optimizer~ with a learning rate scheduler that includes a linear warmup followed by a cosine decay. The initial learning rate is set to 1e-5, with a minimum learning rate of 1e-5 and a warmup learning rate of 1e-5. The warmup phase consists of 1000 steps. We apply a weight decay of 0.05. The training process utilizes 20 epochs, and each epoch consists of 549 iterations. The training is conducted using 2 workers. The input image resolution was 224  224. The model training was conducted on a computing system equipped with four A100 GPUs and took approximately three hours. We trained the model using LoRA~, a parameter-efficient fine-tuning technique. The model was loaded in 8-bit precision, and the  and  parameters were initialized from MiniGPT-v2. The rank, , was set to 64. We continued fine-tuning these parameters via lower-rank adaptation.

In this section, we first present the performance of binary classification assessed using an anomaly detection framework to evaluate the model's ability to detect anomalous samples. We compare the results with the vision-language baselines that we trained on the VinDr-CXR dataset, including BiomedCLIP + PubMedCLIP - Llama 2, BiomedCLIP - Llama 2, PubMedCLIP - Llama 2, and MiniGPT-v2, to determine which model performs better in the medical domain. Precision, Recall, and F-1 scores for both the  and  classes are reported in Table . As shown in Table , the recall for the  class exceeds 90, which is attributed to the predominance of  samples in both the training and testing datasets. In contrast, the recall for the  class is only about 59, due to the limited and imbalanced dataset, whereas the  class accounts for about 67 of the total dataset. This imbalance hinders the model's ability to learn and classify effectively, introducing bias in classification. Overall, the BiomedCLIP + PubMedCLIP - Llama 2 model slightly outperforms other baselines. Notably, it demonstrates strong performance in detecting anomalies in each X-ray scan, as evidenced by a recall rate of 59 and a high precision rate of 84.

% [t]%   \centering%    \includegraphics[width=0.7\textwidth]{Figures/distribution.png}%     and  class}. There is a noticeable class distribution imbalance between the  and  classes, with the  class predominating in both the training and test datasets.}%    % Secondly, we establish baselines for comparison, including pre-trained vision-language models without fine-tuning, such as DeViDe, and KAD, as well as DWARF, and MiniGPT-v2 fine-tuned on the VinDr-CXR dataset. The results, shown in Table , highlight the differences in F1-scores across various models on a classification task. The pre-trained models DeViDe and KAD achieve F1-scores of 41.28 and 40.22, respectively. In contrast, the fine-tuned models yield more impressive results, with MiniGPT-v2 achieving 53, BiomedCLIP - Llama 2 at 59, and PubMedCLIP - Llama 2 at 55. Notably, the combination of BiomedCLIP and PubMedCLIP with Llama 2 outperforms all others, achieving an F1-score of 60, an improvement from 5 to 19.78. This demonstrates the effectiveness of fine-tuning and model integration.

In this study, we evaluated the performance of our model in generating bounding boxes for critical findings. The model is required to accurately classify 22 different anomalies on the X-ray scan and then determine their locations. Only correctly classified findings are included in the evaluation metrics. However, there is a notable absence of pure vision-language models addressing localization tasks in the medical domain. Typically, these tasks use a pre-trained vision encoder as the backbone along with a detection head module. Our review of recent literature indicates that localization tasks in VinDr-CXR follow a similar approach. These methods are primarily evaluated using mean Average Precision (mAP), as each bounding box includes a confidence value, which facilitates the computation of mAP. In contrast, bounding boxes generated by vision-language models are in pure text format and lack confidence values, making mAP comparisons challenging. Therefore, in this paper, we establish our model as the baseline for the first vision-language model addressing localization tasks in the medical domain.

The results of our experiments are summarized in Table . The metric Accuracy@threshold indicates that a predicted bounding box is deemed correct if its IoU with the ground truth bounding box exceeds the specified threshold. In this experiment, we used thresholds of 0.3, 0.4, and 0.5. We compared our established baselines to a robust multi-modal large language model, MiniGPT-v2, which serves as a unified interface for various vision-language multi-task learning, including localization tasks. We chose MiniGPT-v2 as a baseline for our experiment based on the findings of Jun Chen et al.~, which indicated that MiniGPT-v2 efficiently adapts to new vision-language tasks. As shown in the table, the model that integrates multiple visual encoders outperforms those with a single encoder and MiniGPT-v2 fine-tuned on the VinDr-CXR dataset, achieving an Accuracy@0.3 of 12.54, an Accuracy@0.4 of 10.5, and an Accuracy@0.5 of 8.12. Compared to MiniGPT-v2, these results show significant improvements of 6.64 for Accuracy@0.3, 5.49 for Accuracy@0.4, and 4.74 for Accuracy@0.5. Notably, the integration of multiple visual encoders enhances the richness of image information, allowing the model to identify locations more effectively, resulting in higher performance than models with a single visual encoder.

In addition to the main metrics of our task, we also evaluate the performance of the text generated by the model. Tables  and  compare four methods across various text evaluation metrics for classification and localization tasks. Overall, BiomedCLIP + PubMedCLIP - Llama 2 achieves the highest scores in ROUGE and CIDEr metrics, indicating superior performance in text validity for both classification and localization tasks. BiomedCLIP - Llama 2 also performs well, leading in BLEU and METEOR scores. The combination of BiomedCLIP and PubMedCLIP outperforms the other methods, showcasing the benefit of integrating multiple encoders.

% % 

In this ablation study, we explore the effect of different identifier tokens on localization task performance. Specifically, we selected the  and  tokens, which perform well in the MiniGPT-v2 model. Both tokens were trained under the same conditions, including the VinDr-CXR dataset, instructional input, and experimental settings. The results, detailed in Table , show that the identifier token || outperforms ||, achieving better accuracy scores at thresholds of 0.3, 0.4, and 0.5. This can be attributed to the  token being trained on the referring expression generation (REG) task with various datasets (RefCOCO~, RefCOCO+~, and RefCOCOg~) over three stages of training, enabling it to effectively learn spatial relationships in images. In contrast, the  token was trained on the grounded caption task using only the GRIT-20M dataset from KOSMOS-v2~ in a single stage, which did not provide sufficient knowledge for learning spatial relationships.

Converting from DICOM (Digital Imaging and Communications in Medicine) to RGB (Red, Green, Blue) can reduce disk space due to several reasons related to data compression and file format efficiency:

A DICOM file with a 16-bit grayscale image might be several megabytes in size due to the high bit depth and extensive metadata . Converting this image to an 8-bit per channel RGB JPEG file can reduce the size drastically, especially when using lossy compression. However, it is crucial to consider that converting medical images from DICOM to RGB may result in the loss of important clinical information, reduced image quality, and the inability to use specialized medical image analysis tools . 

Below is the Python code for converting DICOM to JPG format.

% We visualize the data format The classification results for the critical findings, which include 22 local labels, are presented in Tables , , and , respectively. Each table represent a method we approached. Each table represents a different method.

The classification results for the diagnoses, which include 6 global labels, are presented in Tables , , and . Each table represents a different method.

% % LiteGPT: Large Vision-Language Model for Joint Chest X-ray \\Localization and Classification TaskKhai Le-Duc, Ryan Zhang, Ngoc Son Nguyen, \\Tan-Hanh Pham, Anh Dao, Ba Hung Ngo, Anh Totti Nguyen, Truong-Son Hy \\ \hphantom\\ University of Toronto, Canada  Johns Hopkins University, USA\\ University Health Network, Canada  FPT Software AI Center, Vietnam\\ VNUHCM - University of Science, Vietnam  Florida Institute of Technology, USA\\ Michigan State University, USA Chonnam National University, South Korea\\ Auburn University, USA  Indiana State University, USA \\

% For a paper whose authors are all at the same institution, % omit the following lines up until the closing ``}''. % Additional authors and addresses can be added with ``\and'', % just like the second author. % To save space, use either the email address or home page, not both %\and %Second Author\\ %Institution2\\ %First line of institution2 address\\ %{\tt\small secondauthor@i2.org}

Vision-language models have been extensively explored across a wide range of tasks, achieving satisfactory performance; however, their application in medical imaging remains underexplored. In this work, we propose a unified framework - LiteGPT - for the medical imaging. We leverage multiple pre-trained visual encoders to enrich information and enhance the performance of vision-language models. To the best of our knowledge, this is the first study to utilize vision-language models for the novel task of joint localization and classification in medical images. Besides, we are pioneers in providing baselines for disease localization in chest X-rays. Finally, we set new state-of-the-art performance in the image classification task on the well-benchmarked VinDr-CXR dataset. All code and models are publicly available online. *Equal contribution\arabicIntroductionsec:introachiam2023gpt4, team2023geminihe2017finehu2022scalingseenivasan2023surgicalgptwang2023enablingsammani2022nlxwang2022multi, muller2022joint, liu2023mcheng2023priorjoint localization and classification tasknguyen2022VinDrzhang2023biomedclipeslami-etal-2023-pubmedclipWe present a new task -  on medical images, joint localization and classification taskWe introduce a novel unified vision-language framework designed for the joint task on medical images, We provide fine-tuned models, empirical baselines and conduct an extensive ablation study of our framework, achieving state-of-the-art results on the well-benchmarked VinDr-CXR dataset. https://github.com/leduckhai/LiteGPTRelated Worksec:related_workLarge Vision-Language Models (VLMs)radford2021learningjia2021scalingtouvron2023llama, gunasekar2023textbooks, bai2023qwenNEURIPS2023_6dcf277e, zhu2023minigpt, chen2023minigptv2, chen2023shikra, liu2024improved, ye2023mplug, ye2024mplugVisual Biomedical Question Answering (VBQA)li2024llava, moor2023med, thawkar2023xraygpt, he2024pefomed, zhang2023pmcli2024llavaNEURIPS2023_6dcf277elau2018datasetliu2021slakehe2020pathvqaMethodModel Architecturewidth=\textwidthFigures/vlmmed_architecture.png\textbf The model employs multiple visual encoders as its visual backbone, specifically incorporating two different encoders, which remain frozen throughout all training phases. We concatenate five adjacent visual tokens from the output of the visual backbone and project them into the language space of Llama 2. The text is embedded through the embedding layer of Llama 2 and directly concatenated with the visual features to generate the answer. -8ptfig:overviewfig:overviewchen2023minigptv2Visual Backbonezhang2023biomedclipeslami-etal-2023-pubmedclipzhang2023biomedcliprocov_i \in ^{H \times W \times C}HWC     _1 = _{}(v_i) \in ^{P_1 \times\ 768},

    _2 = _{}(v_i) \in ^{P_2 \times\ 768}, P_1P_2      = (_1, _2) \in ^{P \times\ 768},~P = P_1 + P_2.  Visual ProjectionZeq5hendrycks2016gaussianeq3     q_i = (z_{5i-4}, z_{5i-3}, z_{5i-2}, z_{5i-1}, z_{5i}). f      = f() \in ^{M \times\ D}.  width=0.9\textwidthFigures/distribution_dataset.png\textbf The numbers of positive labels were reported based on the assessments of the participating radiologists.     %    fig:distribution_datasetLarge Language Modelllama2eq5Training StagesGrounded critical findings learning stage.Diagnose learning stage.Input TemplateImage inputIdentifier tokenablation_studyexpersetupInstruction inputexpersetupExperimentsDatasetnguyen2022VinDrfig:distribution_datasetData Preprocessingno findingxyhwx_{min}y_{min}xyx_{max}y_{max}xyEvaluation Metrics% 0.8\textwidth!% \textbf. We evaluated the model's performance using the prompt of \textit \textbf indicates the best result, and \underline indicates the second-best result.tab1Experimental Settingsexpersetupfirst training stagePlease describe the critical findings along with their localized bounding boxes in the radiological image of a chest as much detail as possible. If there are no findings, state that the chest radiograph shows no findings.second training stageGiven the provided chest X-ray image, which of the following diagnoses are present (select all that apply): COPD, Lung Tumor, Pneumonia, Tuberculosis, Other Disease, or No Finding?loshchilov2018decoupledhu2022lorarExperimental ResultsClassification Performancefindingno findingtab1tab1no finding\%no findingfinding\%no finding\%\%\%% 0.7\textwidth!% \textbf. We evaluated the model's performance using the prompt of \textit \textbf indicates the best result, and \underline indicates the second-best result.tab3max width=\textwidth\textwidth!% \textbf. We compute the common metrics of natural language processing based on text generated from LLM. \textbf indicates the best result, and \underline indicates the second-best result.tab4max width=\textwidth\textwidth!% \textbf. We compute the common metrics of natural language processing based on text generated from LLM. \textbf indicates the best result, and \underline indicates the second-best result.tab5tab2\%\%\%\%\%\%\%\%Localization Performancetab3chen2023minigptv2\%\%\%\%\%\%Text Validity Performancetab4tab5Ablation studyablation_studyablation_studykazemzadeh-etal-2014-referitgameyu2016modelingmao2016generationpeng2023kosmosQualitative results% {\textbf: The text in \textcolor indicates anomalies classified correctly.tab7width=0.6\textwidthFigures/examples_bboxes.png\textbf. The red rectangles indicate the model's predicted boxes, while the green rectangles indicate the ground truth boxes. In the image on the \textbf side, representing example 1, the red box at the top represents Aortic enlargement, the green box at the top represents Calcification, and the boxes at the bottom represent Cardiomegaly. In the image on the \textbf side, representing example 2, the red and green boxes at the top represent Aortic enlargement, and the boxes at the bottom represent Cardiomegaly.fig:examples_bboxestab7fig:examples_bboxesConclusionAcknowledgementAppendixDetails of Data Preprocessingsec:details_data_preprocessingDICOM to RGB ConversionCompression Algorithms: Many image formats that use RGB (such as JPEG or PNG) support various compression algorithms. JPEG, for instance, uses lossy compression, which can significantly reduce the file size by discarding some of the less important image information . Even lossless compression algorithms used in formats like PNG can be more efficient than the compression used in DICOM files .     wiseman2015stilllin1997robustMetadata Overhead: DICOM files often contain extensive metadata about the image, patient, and imaging parameters. This metadata, while crucial in medical contexts, adds to the file size . When converting to a standard RGB image format, much of this metadata is discarded or simplified, resulting in smaller file sizes .     caffery2018transforming, blackledge2014stegacryption, clunie2021dicomfidler2006lossy, varma2012managingPixel Data Encoding: DICOM files typically use higher bit depths and may store additional information per pixel (e.g., 16-bit grayscale images), whereas standard RGB images usually use 8 bits per color channel (24 bits total) . Reducing the bit depth can decrease the file size.     chen2012study, pianykh2012medical, liu2007medical, manikandan2021dualSpecialized Compression Techniques: DICOM may use specialized medical image compression methods that are not as efficient in terms of disk space as general-purpose image formats . cazanas2022digital, european2011usability, fritsch2011lossygoel2008mathematical, lebre2021efficient, aryanto2021performance, elhadad2021blind, trieu2023use, lang2023dicom, stewart1999integration, lamy2015designherrmann2018implementing, lalitha2010lossless, pianykh2012medical, olges2000integrating image_path = os.path.join(root, filename) image_dicom = pydicom.read_file(image_path) image = image_dicom.pixel_array.astype(np.float64)

if image_dicom.value == "MONOCHROME1":     flip = -1 else:     flip = 1 try:     normalized_image = ((image - float(image_dicom.value)) / float(image_dicom.value))     image = ((np.clip(normalized_image, -1, 1) * flip * 127.5 + 127.5)).astype(np.uint8) except KeyError:     normalized_image = ((image - np.mean(image)) / (np.max(image) - np.min(image)))     image = (np.clip(normalized_image, -1, 1) * flip * 127.5 + 127.5).astype(np.uint8)

image = Image.fromarray(image).convert("RGB") Data Format Details of TrainingData Format of Local Label Training Stage Img/Img  Please describe the critical findings along with their localized bounding boxes in the radiological image of a chest as much detail as possible: blueLocal diseases of this chest radiograph are pAortic enlargement/p ,pCardiomegaly/p .Data Format of Global Label Training Stage Img/Img  Given the provided chest X-ray image, which of the following diagnoses are present (select all that apply): COPD, Lung Tumor, Pneumonia, Tuberculosis, Other Disease, or No Finding? blueGlobal diseases of this chest radiograph are Lung tumor, Pneumonia, Tuberculosis.Tables for Classification Resultssec:details_classification_resultsLocal labelsbio_pub_localbio_localpub_localGlobal labelsbio_pub_globalbio_globalpub_global