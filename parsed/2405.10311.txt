We use UniIR's CLIP Score Fusion (CLIP-SF) and BLIP Feature Fusion (BLIP-FF) models as MM retrievers. These models are instruction-tuned on heterogeneous datasets with text-image pairs from different domains using CLIP and BLIP2 pre-trained models, respectively. Score and feature fusion methods combine the text and image modalities of the query and candidates to generate unified feature vectors for them. The former uses a weighted sum between vision and text encoder outputs while the latter mixes the image and text features using cross-attention layers.

Once the query and database candidates embeddings are generated, Faiss~ with dot product similarity is used for retrieving top  candidates for each query. We have selected UniIR models since experimental results from the original paper show that the instruction-tuned UniIR models significantly outperform their pre-trained CLIP and BLIP2 baselines for a variety of tasks and configurations, including generalization to unseen datasets~.

We leverage various MM-LLMs for caption generation (image-to-text) and image generation (text-to-image) tasks. For both tasks we utilize zero-shot  and few-shot  prompting techniques for instructing the MM-LLMs to generate appropriate outputs. In few-shot setting, in-context examples are obtained via retrieval and pre-pended to the input prompt. Figures~- show our zero-shot and few-shot prompts for caption and image generation tasks.

In order to generate an appropriate text caption for an image (image-to-text), we leverage Llava (13B) , Gemini-Pro  and GPT4  models.

 is a Vicuna-based open-source MM model that uses CLIP as its visual encoder. It is fine-tuned on MM instruction-following data generated by GPT4~. LLaVa's proficiency in understanding and following human intent makes it a strong candidate for RAG caption generation. Google's  and OpenAI's  are transformer-based MM models capable of understanding both image and text. Both proprietary models show human-level effectiveness on many professional and academic evaluation benchmarks. Figure  demonstrates sample captions generated by each model using CLIP-SF as the retriever.

Unlike the image captioning task, where MM understanding is adequate, the image generation task requires models with both MM understanding and generation capabilities for conditional image generation (text-to-image). For this task, we couldn't reuse the same models employed for image captioning since Gemini-Pro stopped image generation in February 2024, and the other two models (Llava and GPT4) lack support for image generation. To find suitable models for the image generation task, we studied a list of candidate models from a survey paper on MM-LLMs~ and selected LaVIT~ and Emu2-Gen~.

 is an open-source model that uses LLaMA-7B~ as the base model and leverages a visual tokenizer to convert images to tokens.  By doing so, it can simultaneously handle text and image inputs using the auto-regressive objective.  This feature allows LaVIT to accept many images in a single prompt, unlike many other MM models that expect a single input image.  Accepting multiple images enabled us to experiment with .  More importantly, the LaVIT paper reports 7.4 Frechet Inception Distance (FID)~ on the MS-COCO evaluation benchmark, making this model a strong candidate for the image generation task.    is an open-source model with 37B parameters which is fine-tuned from Emu2 for image generation.  Similar to LaVIT, it uses the next token prediction objective for all modalities.  Emu2 uses a variant of CLIP as the visual encoder and LLaMA-33B as the backbone LLM.  It accepts interleaved image, text, and video as input and has a visual decoder component that generates images.  Experimental results show that while Emu2's zero-shot effectiveness on various MM tasks lags behind Emu~, Flamingo~ and other models, it has the best few-shot effectiveness, making it a good candidate model for image generation with RAG. Figures~ and~ show visual examples of image generation using LaVIT and Emu2-Gen, respectively. Quantitative results will be covered in Section~ UniIR comes with the M-BEIR~ collection which contains eight tasks and 10 datasets. For our evaluations, we use test sets of M-BEIR'S MSCOCO text-to-image (task 0) and MSCOCO image-to-text (task 3) as queries. To keep the retrieval generic, we retrieve from the M-BEIR's global candidate pool (containing over 5.5 million candidates) rather than MSCOCO's local candidates pools.

In some cases, retrieved candidates had incorrect modalities (text for image generation and image for caption generation tasks). To keep the prompt template consistent across all calls and to avoid potentially confusing the model about the task intent, we skipped candidates with the wrong modality. This meant that for , the actual number of candidates passed to the prompt could be fewer than . Table~ and Table~ show the number candidates with the right modality for each task.

MSCOCO text-to-image has 24,809 captions for 5000 distinct images. Since the image generation task takes too long (around 12s for each image) we decided to randomly sample a single caption query for each image. For the sake of consistency all image generation runs use the same sample set. In an ablation study in section~ we analyze the effect of sampling in detail.

Caption generation is an image-to-text conversion task, i.e., an image is passed as an input query in order to generate text output. Generator models employed for the caption generation task are Llava, Gemini-Pro and GPT4.  With Llava, we used its default configuration for all inferences. With this setting and  of four, each caption generation on a single NVIDIA RTX A6000 GPU takes approximately  on average.

For inference experiments with Gemini-Pro and GPT4, we utilize Vertex AI's  and OpenAI's  API calls, respectively. With both of these API calls we used a parameter value of 200 for . For further details on the cost estimations please refer to Appendix .

We use  for the number of retrieved captions added as in-context examples where  provides the baseline effectiveness of generator models without RAG. The in-context examples are passed along with the input image query following the format: , to generate result caption (text). In our evaluations we report both -gram based metrics, BLEU(1-4)~, CIDEr~, and ROUGE~, and a metric based on scene-graphs, SPICE~, on MSCOCO's image-to-text test set from M-BEIR.

Image generation is a text-to-image conversion task, i.e., a text is passed as an input query in order to generate an image output. We use LaVIT and Emu2 as generator models for the image generation task. Other than setting the resolution of the generated images to 224224 pixels, we use the provided sample configuration for all LaVIT inference calls.  Using LaVIT to generate a 224224 image with this setting takes around 12s on a single NVIDIA RTX A6000 GPU. For LaVIT, we experiment with  as the number of retrieved images as in-context examples.

Since Emu2-Gem has 37B parameters, we use its ``bf16'' variant and run it on an NVIDIA H100 SXM rented from the Lambda GPU provider. To cap our cost at 100CAD, we only use  (baseline) and  values for Emu2-Gen experiments.

Similar to LaVIT, we follow Emu2-Gen'a provided inference example~ and opt for default configurations. Each image generation with this setting takes around 9s. We use FID to measure the similarity between the distribution of generated vs ground truth images on a sampled set of MSCOCO's text-to-image test split from M-BEIR that contains 5k text queries. To study the effect of sampling, we repeat the CLIP-SF + LaVIT experiment with another sample set in section~. For evaluating image-caption correlation we measure CLIP Score~ using the ViT-B/16 CLIP model. We also report Inception Score (IS)~ as another indicator of the quality of generated images even though it is not very accurate given the evaluation dataset size of 5k.

Table  presents results generated by employing various combinations of retriever and generator models.  Row 1 has baseline results for the retriever models ().  Rows 2-4 compare zero-shot and few-shot results of Llava, Gemini-Pro and GPT4 with CLIP-SF (*a) and BLIP-FF(*b) used as retrievers for few-shot generations. Even though several metrics are reported in this table as a reference, we focus on SPICE since all other metrics are primarily sensitive to n-gram overlaps~.

As reported in the Table~, augmenting relevant captions () retrieved from the previous stage, boosts the effectiveness of MM-LLMs regardless of their baseline effectiveness and the retriever model. The baseline effectiveness () of Llava exceeds that of Gemini-Pro and GPT4 models (comparing rows 2-4). Even though all models benefit from RA, this gap further widens when only a single retrieved caption () is added to the prompts (comparing rows *a and *b for CLIP-SF and BLIP-FF retrievers, respectively).

Further increasing the number of retrieved examples from one to five  helps the Gemini-P and GPT4 models. However CLIP-SF + Llava with five examples performs worse than CLIP-SF + Llava with a single retrieved example. Additionally, going from five to 10 examples (not reported for GPT4 due to resource constraints) degrades the generation quality for all retriever-generator pairs except BLIPP-FF + Gemini-P. These trends show that depending on the baseline effectiveness of the generator model and the retriever's capability in retrieving the most relevant candidates, adding too many weakly relevant examples can hurt the generator's effectiveness. This finding suggests that instead of adding a fixed number of retrieved examples some relevance-based cut-off could lead to better overall effectiveness.

Table~ shows the results of zero-shot () and few-shot() image generation for LaVIT and Emu2-Gen (denoted as Emu-G in the table) with CLIP-SF and BLIP-FF as retrievers. We report FID (lower is better), CLIP Score (higher is better), and Inception Score (higher is better) with its Standard Deviation (denoted as SD in the table) in this evaluation. However, while reading the table, only looking at FID is enough since there is no contradiction among different metrics reported for each run.

Surprisingly, LaVIT's zero-shot results are very different from reported numbers for MSCOCO evaluation in the original paper (FID 155.75 vs 7.4). This reproducibility issue needs further investigation. It could be due to the smaller evaluation dataset that we use (5k vs 30k) or differences in the prompt templates. However, since both LaVIT and Emu2-Gen are using LLaMA-based backbone models using prompts in helpful assistant AI format for both models is appropriate. Similarly, we expect the impact of evaluation dataset size to be similar on both models' results making our comparison of the two models still fair. As expected, Emu2-Gen with LLaMA-33B model as its backbone always outperforms LaVIT which uses LLaMA's 7B version (comparing rows 2* and 3* from table~).

Both LaVIT and Em2-Gen benefit from adding a single retrieved candidate  as an in-context example to their prompts regardless of the retriever model (comparing rows 2a/2b and 3a/3b vs 2 and 3, respectively). However, increasing the value of  from one to five for LaVIT runs, slightly degrades the results, indicating that adding less relevant examples to the context can confuse the model.

For each image in MSCOCO's text-to-image test set, we have sampled a single caption (Set 1 Captions in Table~). To study the sensitivity to sampling, we repeated the sampling process one more time (Set 2 Captions in Table~). The two sample sets have only 996 captions in common but both of them cover all 5k distinct images in the dataset. We used CLIP-SF + LaVIT in this ablation study as the retriever-generator pipeline. As shown in Table~, FID and CLIP scores for all  values are very similar between the two runs with less than one percent difference. However, the reported Inception scores and their standard deviations vary slightly more, but still within the confidence interval of one another. We believe this difference has to do with the natural uncertainty of the metric for small dataset sizes. Even calculating the metric for the same run results in slightly different numbers.

The retriever that we used in this work is fine-tuned with the training split of the M-BEIR's MSCOCO text-to-image and image-to-text datasets used for evaluation. It would be nice to see the effectiveness of the retriever-guided generation with out-of-domain retrieval. For example, using an entity-enteric dataset could better showcase the benefit of retrieval augmentation in providing relevant in-context examples for uncommon entities. In our experiments, we opted for a prompt template that is consistent with the assistant AI format. An ablation study on different prompt templates could potentially reveal useful information about the prompt sensitivity of retrieval augmented MM generation.

This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada. We'd also like to thank Microsoft and Google for providing access to OpenAI LLMs via Azure and Gemini via VertexAI, respectively. Further, we'd also like to thank Cong Wei for answering several questions about UniIR and for providing valuable feedback.

This section shows zero-shot and few-shot prompt templates for caption and image generation tasks.

We used OpenAI and Vertex APIs for generating results with GPT4 and Gemini-Pro models, respectively for image captioning tasks. Table  illustrates the API pricing information for both models. Overall, for the evaluation of the MSCOCO test set which includes 5k image-text pairs, our cost estimations for GPT4 are ~50 USD (average) per run and Gemini-Pro is ~15 USD (average) per run. Image captioning experiments in this paper cost 250 USD in total (three runs for GPT4 and seven for Gemini-Pro).

During generation, we skipped retrieved candidates with incorrect modalities (image modality for caption generation, and text modality for image generation) worrying that they could potentially confuse the model about the intent of the task in hand.  Tables~ and~ show the number of candidates with correct modality for caption and image generation tasks, respectively.

Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of MM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT4 and Gemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models.

Introductionopenai2024gpt4geminiteam2024geminidallepartiliu2023visual, liu2023improvedlavitemu2mmRAGSurvaymmRAGSurvayuniIRclipblipmscocouniIRanderson2016spicefidIntroducing the UniRAG technique which combines UniIR MM retrievers with MM-LLMs using Retrieval Augmentation (RA) architecture for enhancing the models' effectiveness.     Evaluating UniRAG's effectiveness of image captioning (image-to-text) and image generation (text-to-image) tasks using the MSCOCO dataset. Related WorkRetrieval Augmented with Generative Models:mmRAGSurvayrealm, retroreimagenMulti-Modal Models and Retrievers (vision-language):openai2024gpt4, geminiteam2024gemini,dalle,parti,clip, blip, liu2023visual, liu2023improved,lavit,emu2sec:generatoruniIRra-cm3CM3LeonMethodologyRetrieverfaissuniIRGeneratorsec:generatorkojima2023largebrown2020languagefig:zero-shot-promptfig:few-shot-prompt-imgCaption Generator Modelsliu2023improvedgeminiteam2024geminiopenai2024gpt4LLaValiu2023visualGemini-ProGPT4fig:image-cap-visualfigures/image_caption_visualizationImage Generator ModelsmmllmSurvaylavitemu2LaVITllamafidfigures/lavit_visualizationEmu2-Genemuflamingofig:lavit-visualfig:emu2-visualsec:resultsfigures/emu2_visualizationDatasetsec:datasetuniIRtable:caption-modality-analysistable:image-modality-analysisDataset Samplingdataset-samplingsec:sampling-effectExperimental SetupCaption Generation\url\url\url\urlgenerate contentchat completionapp:cost-estimatepapineni-etal-2002-bleuvedantam2015ciderlin-2004-rougeanderson2016spiceImage Generation\url\urlsec:sampling-effectclipscoreinceptionscoreEvaluation Results and Analysissec:resultstables/table_caption_generationtable:image-captioningtable:image-generationsec:datasetCaption Generationtable:image-captioninganderson2016spicetable:image-captioningImage Generationtables/table_image_generationtable:image-generationtable:image-generationEffect of Samplingsec:sampling-effecttable:sampling-effecttable:sampling-effecttable:sampling-effecttables/sampling_effectConclusion and Future WorkFuture WorkAcknowledgmentscustomAppendixPrompt Templatessec:prompt-detailsfigures/figure_caption_generation_zero_shotfigures/figure_caption_generation_few_shotfigures/figure_image_generation_zero_shotfigures/figure_image_generation_few_shotProprietary Models Costapp:cost-estimatetable:costtables/cost_estimateRetrieved Candidates' Modalitytables/caption_modality_analysistables/image_modality_analysistable:caption-modality-analysistable:image-modality-analysis