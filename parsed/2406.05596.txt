Disease diagnosis usually centers around various criteria axes describing distinctive characteristics across clinical classes~. Drawing from inspiration, we first query domain knowledge from LLMs or consult human experts and formulate them into textual diagnosis criteria. Consider a set of training image-label pairs , where  is the image and  is a label from a set of  classes. Specifically, we categorize the diagnosis criteria along  disentangled  specified by language depending on the task . For instance, in the case of skin lesions, the criteria axes include . Subsequently, we query detailed knowledge on the typical characteristics for each class along each criteria axis , where  denotes the number of possible options within a particular criteria axis. Take the  of skin lesion as an example, potential options could range from `a mixture of black brown and blue' for melanoma, `uniform pigmentation of tan or brown' for melanocytic nevus, among others. Notably, the quantity of typical characteristics for each criteria axis, , may be less than the number of classes , as different classes might exhibit identical characteristics for certain criteria axes; e.g., various types of benign skin lesions could all present symmetry. Additionally, the ground truth label for each diagnostic criterion is recorded, i.e. for each criteria axis, the associated class and characteristic options are marked as positive, whereas all other combinations are considered negative.

After collecting the textual form diagnostic criteria, we aim to align the visual features with these textual human knowledge. In particular, we propose a lightweight visual concept learning module for aligning the fine-grained visual concepts and the nuanced textual criteria. Specifically, given a pretrained vision-language model with visual encoder  and text encoder , we first encode the queried diagnostic criteria into criteria anchor embeddings, , where , and  is the dimension of the embedded token. These criteria embeddings act as a sparse representation of human knowledge, serving as anchors to facilitate the learning of visual concepts. 

To capture visual concepts effectively, our visual concept learning module employs a set of  learnable visual concept tokens , with each token designated to represent one of the  criteria axes.  For a given image  and its feature map , the concept encoding process is formalized as follows: 

where  is the query and  serves as the key and value of the cross-attention layer. The visual concept tokens  interact with the image feature map, thereby encoding the relevant visual concepts associated with specific criteria axes into .

The learning of visual concepts is facilitated by a contrastive loss. For each criteria axis, the aggregated visual concept token  is compared against all characteristic embeddings , calculating a similarity score. The criteria anchor contrastive loss is as follows:

where  denotes the temperature parameter that adjusts the softness of the softmax distribution and we use dot product as the similarity function. The criteria anchor contrastive loss aims to increase the similarity between the encoded visual concept token  and the positive criteria anchor embeddings while  decreasing its similarity with the embeddings of negative characteristics, ensuring a more discriminative learning of visual concepts along each diagnostic criteria axis.

% gdf: Before uses 'criteria', but afterward uses 'characteristics' is confusing. The above knowledge anchor loss  enables the alignment of encoded visual concepts with the corresponding characteristic options along each diagnostic criteria axis. Intuitively, the similarity scores between the encoded visual concept token  and the diagnostic criteria anchor  indicate the model's assessments for each diagnostic criterion. Mirroring the approach of human experts, who make their final diagnosis on the evaluations across multiple criteria, we use a linear layer to make prediction of the final class by integrating the alignment scores from all  criteria axes.  % gdf: please check if this is correct% %      = W ((}_1, _1), \dots, sim(}_K, _K))^{\intercal},%  where  represents the concatenation operation and  is the weights in the linear layer that inherently reflect the significance of each diagnostic criterion's contribution towards the overall class prediction.

During the training phase, we optimize a joint objective that includes both the criteria anchor contrastive loss with cross-entropy loss for the final classification:

The embeddings of textual criteria anchors are precomputed and stored, ensuring that the training and inference overhead introduced by the additional components are negligible.

% Introduce datasets We evaluate our method on five publicly available medical image classification benchmarks, which cover a diverse range of medical targets and modalities. 

~ contains 10,015 dermoscopic images with seven skin lesion categories for skin cancer classification. (NCT)~ includes 100,000 patch-based histological images of human colorectal cancer for training and 7180 patches for validation, with nine tissue classes for classification. ~ consists of 516 retinal fundus images annotated with 5 severity level grading of diabetic retinopathy. ~ dataset contains 780 ultrasound images of breast masses categorized into normal, benign, and malignant classes for breast cancer classification. ~ contains 377,100 chest X-ray images. Cardiomegaly (CM) and Edema are used for binary classification. 

 We compare our method with several baselines, including (1) : We apply the general VLM CLIP~ and biomedical VLMs BioViL~ and BiomedCLIP~ in a zero-shot setting for classification; (2) : We fine-tune ImageNet-pretrained ResNet50~ and ViT-Base~ on the classification benchmarks; and (3) : a state-of-the-art explainable model with concept bottleneck.

 We prompt  to query domain knowledge and diagnostic criteria. We use the official implementation and pretrained weights of ,  and . Our Explicd and LaBo are implemented based on BioViL-specialized for MIMIC-CXR dataset, while using BiomedCLIP for all other datasets. The fine-tuning of Explicd involves optimizing visual encoder, visual concept learning module and the final linear layer with AdamW optimizer, while keeping the text encoder fixed. All experiments are conducted using PyTorch with Nvidia A6000 GPUs.

Our proposed Explicd model demonstrates superior performance compared to various baseline methods across five medical image classification benchmarks, as shown in Table . The zero-shot performance of VLMs, including CLIP, BioViL, and BiomedCLIP, is generally poor, with CLIP performing close to random guessing across all datasets. This indicates that CLIP's visual-text alignment is not effective for complex medical diagnosis tasks as it is trained on general vision data. Although BioViL and BiomedCLIP perform much better on the MIMIC-CXR dataset (CM and Edema tasks), likely due to their pretraining data being largely based on chest X-ray radiology reports, their performance on other datasets remains much lower than supervised trained black-box models like ResNet50 and ViT-Base, suggesting their limited generalization ability.

Explicd effectively combines explainability with high-level classification performance, outperforming not only the explainable model LaBo but also black-box models across all datasets. LaBo's lower accuracy compared to the black-box models can be attributed to its reliance on well-aligned vision-language models, highlighting the challenge of maintaining high accuracy while providing strong explainability. In contrast, Explicd's superiority is due to the introduction of human knowledge and visual concept learning, which provides additional supervision for fine-grained alignment between visual features and diagnostic criteria. This strategy not only enhances fine explainability but also brings improvement in overall classification performance.

A distinguishing design of Explicd is its appealing ability to interpret its decision-making process. Fig.  (a) shows the alignment scores measured with cosine similarity between the encoded visual concept tokens and the embeddings of diagnostic criteria for skin lesions. The width of the lines indicates the strength of similarity, with larger widths representing higher similarity scores. We can see that Explicd can accurately predict the characteristics of each criteria axis, such as the presence of asymmetry, border irregularity, color variegation, and large diameter, which are key features in the diagnostic criteria we queried for melanoma diagnosis. The high similarity scores between the visual concepts and the corresponding diagnostic criteria demonstrate that Explicd has learned to identify and align these important visual features, leading to a correct final diagnosis of melanoma.

Furthermore, we visualize the heatmap of the average visual concept tokens with the image feature map of cardiomegaly in a chest X-ray image in Fig.  (b). The brighter regions indicate higher similarity scores, suggesting that the model is focusing on these areas when making its prediction. Cardiomegaly is a medical condition characterized by an enlarged heart. In the heatmap, we can observe that Explicd correctly focuses its attention on the heart area, indicating that Explicd has learned to align human knowledge regarding the key visual features of cardiomegaly with the relevant visual concepts in the X-ray image. By providing the alignment scores on criteria and highlighting the most important regions contributing to its prediction, Explicd provides a transparent and interpretable decision-making process that can be easily understood and verified by medical experts.

% add classification score to show the criteria is correctly matched?%To examine the model's interactions between the vision and language semantics, we provide a visualization of contrastive weights between  and . We calculate the weight for each criteria axis separately and label the probability flow between each class of . The visualization is illustrated in Fig. , where the red flow represents positive anchor and the blue flow represents negative anchor. Our model assigns weight to the anchor that aligns closely with medical knowledge with high accuracy and large confidence, showing that the model performs the task with a correct diagnostic rationale, similar to human expert diagnosis.