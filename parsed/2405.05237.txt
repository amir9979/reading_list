We evaluate the EVA-X pre-training method across three dimensions: the performance of pre-trained visual representations, the number of parameters, and computational FLOPs. Our evaluation employs the CXR14 test set~, which serves as the benchmark dataset in the X-ray domain (see Sec~). We compare EVA-X with 15 different pre-trained X-ray models, including widely used models such as DenseNet121~, ResNet50~, and ViTs~. Considering the diverse computational demands of medical scenarios, we train three EVA-X models of different scales: EVA-X-Ti, EVA-X-S, and EVA-X-B.

As depicted in Fig~ (a) left, we categorize these 18 different pre-trained models~ into three comparison groups: tiny models, small models, and base models, based on their parameter counts. Notably, within each group, EVA-X consistently exhibits the lowest parameter count (6M, 22M, 86M). We observe remarkable scalability in EVA-X, with its performance consistently improving as the parameter count increases. Among these models, EVA-X-B stands out as the best pre-trained X-ray model, achieving a visual representation test performance of 83.5 mAUC, surpassing all previous medical self-supervised pre-training methods such as Medical MAE~, contrastive learning pre-training methods like MGCA~, and well-known pre-training methods for natural images like MAE~ and MoCov2~. This achievement sets a new standard for state-of-the-art performance in medical X-ray pre-training.

As depicted in Fig~ (a) right, we assess the computational complexity of all methods during testing. To facilitate visualization, we logarithmically scale the FLOPs on the horizontal axis. The purple  marker on the graph signifies the correlation curve between computational complexity and the performance of EVA-X. EVA-X strikes an outstanding balance between performance and computational complexity compared to all other methods.

Typically, foundational models aiming for high performance often impose high computational demands and it could be challenging in resource-constrained medical environments.  However, leveraging the impressive capabilities of EVA-X, we not only investigate its performance boundaries but also develop a lightweight variant, EVA-X-Ti. It is worth noting that EVA-X-Ti is the model with the lowest computational complexity (1.26 GFLOPs) among them with incredible performance (82.4 mAUC). We conduct comparative experiments between EVA-X-Ti and 15 previously introduced pre-trained models, most of which have larger parameter counts than EVA-X-Ti. Despite this, EVA-X-Ti, with its streamlined parameters (6M), outperformed 14 of these models in performance metrics. It even outperforms MGCA-B~ (81.8 mAUC) and SelfMedMAE~ (81.5 mAUC), which have 13 times more FLOPs than EVA-X-Ti. This exceptional performance highlights EVA-X-Ti's potential as a cost-effective alternative to large-scale models, promoting wider adoption and deeper integration of EVA-X technology across various applications.

X-ray images are one of the important tools for diagnosing chest diseases, with different diseases exhibiting different manifestations on X-ray images. Our experiments demonstrate that the visual representations learned by EVA-X pre-training are universal and can be generalized to diagnostic tasks for all chest diseases.  

 requires the model to make judgments about the presence of multiple different diseases at once. In our work, we evaluate the general disease detection capability of EVA-X using two commonly used multi-class chest disease diagnosis datasets, Chest X-Ray14~, and CheXpert~. We fine-tune the visual representations learned by EVA-X on these two datasets without employing any additional design techniques. 

As shown in Figure~ (b) CXR14, we compare the results of EVA-X with 8 different methods~ on the Chest X-Ray14 dataset. Most of these methods are designed for chest X-ray classification. Among them, our EVA-X-Ti (6M) with 82.4 mAUC exceeds the 82.2 mAUC achieved by Kim et al~. Their method uses DenseNet121 (8M) as a backbone. Our EVA-X-S (22M) with 83.3 mAUC, exceeds the 82.3 mAUC achieved by Xiao et al.~ with ViT-S 0.823 mAUC. Taken together, EVA-X outperforms the previous best method at two different sizes, reaching new SOTA results. From the perspective of single-disease diagnosis, EVA-X performs best by achieving the highest accuracy in 12 out of 14 disease diagnoses (see appendix for more details). 

As shown in Figure~ (b) CheXpert, we compare EVA-X with 5 previous methods~. In terms of individual metrics, EVA-X reaches new SOTA results in 2 categories (see appendix for more details). In terms of mAUC, both EVA-X-Ti, and EVA-X-S outperform all previous methods and reach new SOTA results. Among them, EVA-X-Ti has only 6M parameters, which is smaller than all previous methods exceeds the performance of all previous methods, and achieves new SOTA results.

 requires the model to make accurate judgments about a specific disease. In this paper, we test this using COVID-19 as an example. Specifically, we utilize the latest collected and annotated datasets COVID-CXR-3 and COVID-CXR-4~ and fine-tune 7 different pre-trained models~, including EVA-X, on each dataset.  As shown in Figure~ (b) CovidX-CXR-3 and CovidX-CXR-4, EVA-X ranks first among all methods with exceptionally high 99.8 mAUC and 99.4 mAUC. Additionally, EVA-X maintains remarkable stability, demonstrating the most consistent performance across multiple experiments. Specifically, the mean standard deviation of EVA-X on both datasets is 0.03, which is lower than all other methods including Medical MAE~ (0.045), MGCA~ (0.055), BioViL~ (0.135), etc.

The EVA-X model, optimized through large-scale data pre-training, exhibits a high sensitivity to small training data in downstream tasks. It can converge rapidly with minimal data, thereby directly alleviating the pressure of annotation data on the healthcare system. In Fig~ (c), we validate EVA-X's efficient training capability on COVID-19~ and compare it with previous methods~. EVA-X demonstrates the strongest and most stable performance at different data sizes. Especially in the case of very little annotated data, only 1\% training data, EVA-X shows a clear advantage over other methods. On the CovidX-CXR-4 dataset, EVA-X achieves 95\% diagnostic accuracy with only 1\% of training data, highlighting its exceptional learning ability and generalization performance in resource-limited environments.

Medical segmentation demands deep learning models to precisely delineate anatomical structures and identify pathological features in medical images, aiding in diagnosis. We focus on evaluating EVA-X's performance in both physiological and pathological segmentation tasks. Specifically, we fine-tune 7 different medical models~ across four lung segmentation tasks, encompassing physiological segmentation and pathological segmentation for pneumonia, pneumothorax, and tuberculosis. These tasks demonstrate the model's robust geometric understanding across diverse health conditions. Quantitative evaluation of segmentation results using Dice and Jaccard metrics, along with visualization of segmentation masks as depicted in Fig~, has been conducted through multiple experiments.

As shown in Fig~ (a), EVA-X demonstrates outstanding performance across four distinct tasks~. Specifically, in lung segmentation, EVA-X achieves the highest average Dice score of 95.49\%. In pneumonia pathology segmentation, EVA-X surpasses both Medical MAE~ (53.16 Dice, 36.20 Jaccard) and BioViL~ (51.96 Dice, 35.10 Jaccard) with Dice and Jaccard scores of 54.51\% and 37.47\%, respectively. For pneumothorax pathology segmentation, EVA-X outperforms MGCA~ (59.00 Dice, 41.84 Jaccard) and the ImageNet pretrained model~ (57.69 Dice, 40.56 Jaccard) with scores of 60.27\% Dice and 43.13\% Jaccard. In pulmonary tuberculosis pathology segmentation, EVA-X excels with scores of 60.10\% Dice and 42.96\% Jaccard, surpassing Medical MAE~ (59.1 Dice, 41.96 Jaccard) and MGCA~ (59.00 Dice, 41.84 Jaccard). Furthermore, as illustrated in Fig~ (b), EVA-X provides more accurate and fine-grained physiological or pathological segmentation, showcasing its exceptional generalization ability in X-ray segmentation tasks.

% Additionally, EVA-X is the most stable X-ray pre-trained model and has the smallest performance variance. For example, compared to the pre-trained X-ray model MGCA, EVA-X's performance stability improves by 78\%. In terms of visualization, EVA-X is able to give a more accurate and fine-grained physiological or pathological segmentation.The interpretability of X-ray deep learning is an essential topic, as highlighted in Baselli et al.. Utilizing tools like the class activation map (CAM) can help elucidate the rationale behind neural network decisions, as discussed in Grad-CAM. In medical domain, disease diagnosis often hinges on lesion localization. Saporta et al.~ have observed that while deep learning can provide reasonably accurate predictions, there remains a notable gap in its ability to automatically localize compared to human capabilities.

We employ Grad-CAM~ to analyze the gradients of EVA-X in the context of disease diagnosis. Our analysis involves approximately 1000 images from the Chest X-Ray14 dataset~, as discussed in Sec.~ each annotated with lesion positions. Subsequently, we select seven different model weights pre-trained as outlined in Sec.~ for comparative evaluation. We get Class Activation Maps (CAMs) with each pre-trained model and measure the Intersection over Union (IoU) and Average Precision (AP) between the activation regions and the ground truth (GT) boxes. To determine the optimal performance threshold, we conduct a search within the range of .

We present the corresponding results in Figure~ (a). Furthermore, we visually represent the CAM of EVA-X and the other six models using heatmaps, depicted in Figure~ (b). The results reveal several significant findings. Firstly, EVA-X demonstrates superior performance in terms of quantifiable metrics such as IoU, and AP compared to the other seven methods. Secondly, consistent with findings in prior research~, ViT pretrained with MAE exhibits notably weaker CAM performance than CNN. However, our experiments indicate a substantial enhancement in ViT's CAM quality when aided by EVA-X, resulting in a marked increase in mAP from 3.61 to 8.94. Additionally, our visual analysis highlights that EVA-X generates more accurate and distinct activation maps compared to previous methods. While CNN methods~ exhibit superior map continuity, they may not perform as effectively as EVA-X in localizing smaller lesions.

EVA-X is trained using exclusively public Chest X-Ray data. Our training set is a combination of three extensive public datasets: Chest X-Ray14~, CheXpert~, and MIMIC-CXR~. These datasets are widely recognized for their application in X-ray vision-language pre-training~ and image classification~. In contrast to previous studies, our approach exclusively leverages pure unlabeled images for pre-training, without the utilization of any annotation or pathology report information.

For these datasets, we specifically process them as follows: (1) Following previous work~, we primarily use frontal view (AP/PA) images and discard lateral view images. (2) We do not use any of the images tested subsequently for training, even though they are unlabeled. (3) To speed up training, similar to CheXpert, we use bilinear interpolation to resize the original images to a resolution of 336336. The combined dataset is called  (see Figure~ (b)). If not otherwise noted, our pre-training experiments will be performed on this dataset.

In the realm of natural images, ImageNet~ typically serves as the primary test dataset for pre-training~. Similarly, in the domain of X-ray images, it is essential to select a dataset for pre-training evaluation. Among the aforementioned datasets, both Chest X-Ray14 and CheXpert hold prominence as widely utilized categorization datasets~. They are characterized as multi-label categorical datasets, with labels assigned to 14 distinct diseases. Notably, these 14 labels are independent of each other. 

Here, we have opted for the former dataset, , as our primary test set, which is the most commonly used X-ray classification dataset (as studied by~). Our decision is based on the following reasons: (1) More rational dataset distribution. The CheXpert dataset comprises a total of 224k images, but only nearly 200 images are allocated for testing. In Chest X-Ray, the training/validation/test set ratio is 75k/11k/25k. (2) Clearer labeling. In the CheXpert dataset, the presence of an ``uncertain" annotation indicates that the physician did not identify the condition. Various approaches exist for handling this uncertainty. Some methods uniformly categorize it as "with disease," others as "without disease," and more complex treatment schemes are also employed. However, the labeling is clearer on the Chest X-Ray14 dataset. The selection of this test dataset is also consistent with the two previous works~.

Note that this dataset selection indicates that we perform pre-training studies on this dataset, but does not mean that we only use this dataset to test the final performance of EVA-X. In subsequent sections, we will demonstrate the superior performance of EVA-X on additional datasets.

The pre-training process of EVA-X involves the design of the dual Vision Transformer (ViT)~ (see Figure~). The EVA-X transformer is learnable and the tokenizer is frozen. For the convenience of readers, we begin with a brief overview of ViT here.

Assuming the dimensions of the image are , before attention calculation, ViT divides the image into  different patches, where  represents the patch size. Typically,  can take values like 16, 14, 8, etc. In EVA-X, unless specified otherwise, the patch size for all ViTs is set to 16. For an image patch, ViT uses linear projection to project it into a feature vector of dimension , which is referred to as image tokens. These vectors form a sequence known as the image token sequence. Additionally, to establish positional relationships between vectors, ViT uses positional encoding added to the image token sequence. After adding the token dedicated to classification, we obtain the final input sequence, as shown in equation~, denoted as .

The transformer block (see Figure~ (b)) is a straightforward structure with the same output structure as the input. It mainly consists of two parts: Multi-Head Self-Attention (MHSA) and a Feed Forward Network (FFN). Inspired by Fang el. al.~, in EVA-X, we introduce improved structures such as rotational positional encoding, Sub-LN~, and SwiGLU~, which differ slightly from traditional ViT. By stacking any number of transformer blocks, the final ViT is composed. For the input  at layer , the transformer block performs the following calculations to produce the final output , where  and  have the same structure.

EVA-X is a learnable Vision Transformer. Here, we selected three ViTs of different sizes for experimentation: ViT-Ti, ViT-S, and ViT-B, with a patch size of 16 for each structure. Based on the number of parameters, we primarily use EVA-X-Ti (6M) to benchmark against DenseNet121~ (8M), EVA-X-S (22M) against ResNet50~, and EVA-X-B (86M) to explore the effects and influences of scaling up the number of parameters.

To perform mask operations on images in mask image modeling, following previous work~, we designed a mask token denoted as . This token is a learnable -dimensional vector. Assuming a mask ratio of , we randomly replace  image tokens with mask tokens. We denote this sequence of masked tokens as . All mask tokens have the same initialization.

Due to potential dimension differences between EVA-X and Tokenizer, we use a linear projection layer to map the dimension of EVA-X's image tokens from  to . We denote the final output sequence of EVA-X as

The role of the Tokenizer is to extract semantically rich features from images, and it is also a ViT structure. Unlike EVA-X, we generally opt for larger-scale ViTs. We primarily investigate two types of structures for Tokenizer's pre-training performance, namely, natural image CLIP and medical image CLIP. For natural images, we selecte advanced high-performance ViT-B, ViT-L, and ViT-G visual encoders from the EVA-CLIP~ model as our Tokenizer. In the medical field, we chose the ViT-B visual encoder trained with MGCA~ as our Tokenizer. As far as we know, MGCA-ViT-B is currently the best open-source X-ray CLIP model. For more results of the influence of Tokenizer, please see our appendix.

Tokenizer takes the sequence  as shown in the equation below as input and maps it to the target feature sequence , represented by the following equation. During the pre-training process, all parameters of the Tokenizer are kept frozen, and no additional learnable linear mappings are added.

As mentioned earlier, for the token sequences in the equation , we randomly select a proportion  of tokens and replace them with randomly initialized mask tokens. Here, we choose a relatively small mask ratio, . We denote the indices of the masked image tokens as .

For the final output sequences of EVA-X and Tokenizer, we respectively select the image tokens in  to form the sequences  and . We aim to maximize the cosine similarity between corresponding tokens in  and , i.e.,

In the case of classification tasks, we use the simplest decoding strategy uniformly for all models. For CNNs such as ResNet50~, DenseNet121~, etc., we average the features output from their last network layer for pooling, and then input the pooled features into a learnable linear layer to generate predictions. For the ViT~ structure used by methods such as EVA-X, we average all the tokens output from the last block, and then input the corresponding features into a learnable linear layer as well to output the prediction results. This simple structure ensures the ability to directly compare the underlying models, rather than a complex structural design.

We use the mean Area Under the Curve (mAUC) and mean Accuracy (mAcc) as our classification metric, as denoted in Eq~ and , while  denotes True Positive Ratio,  denotes False Positive Ratio,  denotes True Positive,  denotes True Negative,  denotes False  Positive, and  denotes False Negative.

Following the previous methods~, in this paper, we primarily focus on the comparison of pre-trained visual representation performance, without overly emphasizing the potential advantages that structural improvements may bring to the segmentation tasks. Specifically, we build two segmentation models using ResNet50~ and ViT~ backbones, which are the most commonly used structures in X-ray pre-training. For ResNet50, we followed previous work~, adopting the structure with a ResNet encoder and a UNet~ decoder. For ViT, we follow common practices in natural images~, initially building a feature pyramid by pooling and deconvolution on the last layer features, and then using UperNet~ as the decoder for segmentation tasks. To ensure the simplicity of the structure as much as possible, we do not employ advanced adaptive structures, to better explore the performance of visual representations, although they may bring improvements in performance.

We use the mean of Dice and the mean of Jaccard as our segmentation metric, as shown in Eq~ and Eq~, while  denotes segmentation result and  denotes ground truth.

% common bib fileArticle TitleEVA-X: A foundation model for general chest X-ray analysis with self-supervised learning1\fnm \surjfyao@hust.edu.cn1\fnm \surxgwang@hust.edu.cn1\fnm \suryh\_song@hust.edu.cn2\fnm \surzhao\_huangxuan@sina.com3,4,5\fnm \sur1\fnm \suryajiechen@hust.edu.cn1\fnm \surliuwy@hust.edu.cn3,4,5,6,7\fnm \surbowang@vectorinstitute.ai1\orgdiv, \orgname, \orgaddress2\orgdiv, \orgname, \orgaddress3\orgdiv, \orgname, \orgaddress4\orgdiv, \orgname, \orgaddress5\orgname, \orgaddress6\orgdiv, \orgname, \orgaddress7\orgdiv, \orgname, \orgaddress The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination. Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks. EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation. Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field. Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning. The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice. Our codes and models are available at \url. Introductionworld2016communicating, cid2024developmentcid2024developmentlabeled datachexpert, cxr14, mimicmoor2023foundation, zhou2023foundation, ma2024segmentHowever, the medical domain has not yet seen an effective, flexible, scalable, and interpretable foundation model for chest X-ray images.unlabeledgeneralmgca, biovil, medklip, convirt, gloriamgca, biovil, medklip, convirt, gloriaselfmedmae, xiao2023delvingvit, resnet, densenet, medklip, mgca, biovil, medklip, selfmedmae, xiao2023delving, eva02Resultssec2vitfig:first-figfig:first-figfig:first-figfig:first-figsec:methodfig:enter-labelPre-training: Performance, Efficiency and Flexibilitysec:pre-trainingcxr14sec:method-datasetdensenetresnetvitPerformancefig:result1vit, resnet, densenet, medklip, mgca, biovil, medklip, selfmedmae, xiao2023delving, eva02xiao2023delvingmgcamaemocov2Efficiencyfig:result1purplexFlexibilitymgcaselfmedmaeTransfer Learning to X-ray image analysisChest Diseases ClassificationMulti-label classificationcxr14chexpertfig:result1guan2020multi, ma2019multi, haghighi2022dira, liu2022acpl, hermoza2020region, kim2021xprotonet, xiao2023delvingkim2021xprotonetxiao2023delvingfig:result1seyyed2020chexclusion, haghighi2022dira, chexpert, xiao2023delving, pham2021interpretingSingle-label classificiationhemdan2020covidxresnet, medklip, mgca, deit, biovil, xiao2023delvingfig:result1xiao2023delvingmgcabiovilLabel Efficient Classificationfig:result1hemdan2020covidxxiao2023delving, mgca, biovil, medklip, mae, mocov2Chest X-ray Segmentationresnet, medklip, mgca, deit, biovil, xiao2023delvingfig:result2fig:result2zhang2023lung, RSNA, siim, jaeger2014twoxiao2023delvingbiovilmgcaresnetxiao2023delvingmgcafig:result2Interpretabilitybaselli2020openinggradcamchexlocalizegradcamcxr14sec:method-datasetsec:pre-trainingfig:result3fig:result3xiao2023delvingbiovil, mgca, medklipDiscussionmgca, biovil, medklip, convirt, gloriaselfmedmae, xiao2023delvingcxr14, chexpert, hemdan2020covidx, zhang2023lung, siim, RSNA, jaeger2014twoLimitationsMethodssec:methodDataset curation and pre-processingsec:method-datasetPre-training Datasetcxr14chexpertmimicmgca, medklipxprotonet, acplxiao2023delving\timesMerged520kfig:first-figEvaluation Datasetimagenetmae, eva, eva02xprotonet, acpl, chexpert_sotaChest X-Ray14ccalli2021deepselfmedmae, xiao2023delvingEVA-X Architecturevitfig:enter-labeleq1

Z = \{z_0, z_1, \ldots, z_n\} fig:enter-labeleva02sublnswiglu Z_i' = (Z_i) + Z_i

Z_{i+1} = (Z_i') + Z_i' densenetresneteva, eva02mask\_list Z_{} =   m &  i \in  \\ z_i &  cases     Z_e = \{ze_0, ze_1, ze_2, \ldots, ze_n\} Self-Supervised Learningeva_clipmgca Z  Z_t      Z_t = \{zt_0, zt_1, zt_2, \ldots, zt_n\}  Z  r  r = 0.3 mask\_listmask\_list Z_e'  Z_t'  Z_e'  Z_t'   \sum_{i=1}^{n \cdot r} {\|Z_e'(i)\| \cdot \|Z_t'(i)\|} Transfer LearningClassificationresnetdensenetviteq:auceq:accTPRFPRTPTNFPFN           = \int_{0}^{1} TPR(FPR) \, dFPR


     = {TP + TN + FP + FN} Segmentationmedklip, mgca, biovil, gloriaresnetvitmedklip, mgcaunetvitdetuperneteq:diceeq:jaccardSG           = {|S| + |G|}

          = {|S \cup G|} sn-bibliography In the main mainuscript, we compare the average performance of EVA-X on a multi-disease classification task. Here, we provide a more detailed description of the classification results for both datasets.

As depicted in Figure~, we present a comparative analysis of the classification results of EVA-X against various established methods on the multi-label classification benchmarks~. The outcomes for EVA-X-S are highlighted with a red line to emphasize its performance. Notably, EVA-X demonstrates superior performance across all diseases examined. Furthermore, within the Chest X-Ray14 dataset, the EVA-X series achieved the highest AUC scores in 12 out of 14 diseases. Similarly, on the CheXpert dataset, the EVA-X series outperformed competing methods in achieving the highest AUC scores in 2 out of 5 diseases.

The reconstruction target of EVA-X has shifted from pixel density values to the feature vector of image tokens. A robust tokenizer is crucial for the training of EVA-X. We have uncovered the following insightful findings: 

As shown in Table~, Mask Image Modeling (MIM) with CLIP visual encoders consistently outperforms those with density values in pre-training. Recently, the use of CLIP visual encoder as a tokenizer for MIM has become a dominant trend in the field of natural images~. Inspired by them, we attempted to use natural CLIP~, which is widely trained on natural images, and medical CLIP~, which is specifically trained on X-ray images, as pre-trained tokenizers. We observe that both approaches demonstrated robust performance. Even with less than half the training length, our method showcases notably superior performance. For example, we have been able to achieve 82.7 mAUC by training EVA-X-S for only 300 epochs, surpassing the previous best result of 82.3 mAUC~, which was achieved by training for 800 epochs. Meanwhile, Further, when we train EVA-X-S for 600 epochs, EVA-X-S reaches 83.3 mAUC, a result that even exceeds the previous best pre-training result of ViT-B of 83.0 mAUC. 

Besides, we observe that both the proprietary nature and size enhancement of the tokenizer contribute to the improvement of EVA-X pre-training performance. In Table~, we use the natural CLIP ViT-B as the baseline. Increasing the tokenizer size from ViT-B to ViT-G results in EVA-X's performance rising from 82.2 to 82.7 on mAUC. Similarly, changing the weight of the ViT-B tokenizer from natural CLIP to medical CLIP enhances EVA-X's performance from 82.2 to 82.7. Unfortunately, to the best of our knowledge, there are no larger-sized X-Ray CLIPs available, and based on the above experiments, we suspect that EVA-X's performance will see further improvement when larger-sized X-Ray CLIPs become available.

The toknenizer's performance cap does not limit the EVA-X's cap. EVA-X demonstrates intriguing performance, where EVA, after pre-training with MIM, shows superior downstream task performance compared to its tokenizer teacher. 

As an example, the MGCA-trained ViT-B~ achieves a direct finetune result of 81.8 mAUC (see Table~) on the Chest X-Ray14 dataset. When we use ViT-B (MGCA) as a tokenizer for EVA-X, the results of our trained EVAs are presented in Table~. The ViT series obtained by EVA-X outperforms the original tokenizer across all sizes. The ViT-B (EVA-X) outperforms the ViT-B (MGCA) by 1.7 mAUC with the same number of parameters, and even the ViT-Ti (EVA-X), with only 6M parameters, outperforms the ViT-B (MGCA) by 0.4 mAUC. This MIM pre-trained model surpasses the observations of the original tokenizer and aligns with findings from pre-training tasks with natural images~. This suggests that the performance of the tokenizer does not dictate the upper limit of the MIM pre-training task. We find this encouraging as it provides an opportunity to enhance the original model with unsupervised data and MIM, as demonstrated by EVA-X.

The EVA-X model demonstrates superior parameter scaling performance. Considering the number of models and parameters used in previous X-Ray methods, such as DenseNet121 (6M)~ and ResNet50(26M)~, we have pre-trained three different EVA-X models: ViT-Ti (6M), ViT-S (22M), and ViT-B (86M). As depicted in Table~, increasing the number of parameters consistently results in performance improvement. When the total number of parameters is small, augmenting the model parameters yields more substantial improvement; however, as the total number of parameters reaches a certain threshold, the performance improvement derived from increasing the number of parameters becomes less significant.

In previous studies, the ViT-S model appeared to manifest data saturation issues~. Specifically, when RGB values are used as the reconstruction target, it is observed that the performance of ViT-S reached saturation at around 300k data, and further increases in data no longer led to performance improvement. This is a frustrating phenomenon, as the primary advantage of unsupervised pretraining lies in the ability to leverage an infinite amount of data.

However, there appears to be a positive shift here at EVA-X. As illustrated in the Figure~ right, we randomly divide Merged520k into 10\%, 20\%, 40\%, 60\%, 80\%, and full-volume data. We observe that as the data volume increases, the downstream task performance of EVA-X improves further. This suggests that the pre-training of EVA-X can effectively utilize more data and shows greater data scalability. We attribute this enhancement to a shift in the training strategy of EVA-X, transitioning from reconstructing RGB values to reconstructing semantic information.

We present the visualization of self-attention in the ViT-S model after EVA-X pre-training. In the attention computation process, each image token is projected as a query, key, and value (Q, K, V). The attention mechanism signifies the degree of response of each query to all keys. For a given input image, we designate specific reference points and visualize the heatmap corresponding to its query's attention map.

As observed in the Figure~, the self-attention of EVA-X after pre-training demonstrates notable semantic perception ability. It autonomously segments regions with tangible anatomical meaning, such as the lungs, scapula, spine, etc. Moreover, the attention map also indicates that EVA-X has learned the human body's symmetry. For instance, even though our reference point is only in the right lung, the degree of attention response corresponding to the left lung is remarkably high, forming a double-lung segmentation map. This pre-training performance, which enables automatic segmentation of images, is similar to what has been observed in well-known work BEiT~, DINO~, and DINOv2~ on the pre-training of natural images.

 Compared to previous methods, EVA-X has a faster convergence rate. As shown in Figure~ left, we use ViT-B MGCA as a tokenizer and train EVA-X-S for 50, 150, 300, 450, 600, and 750 epochs respectively. We find that the performance of EVA-X-S converges at about 600 epochs of training. This convergence rate outperforms the previous 800-epoch convergence methods~.

 We focus on two important parameters, mask ratio, and crop ratio, during pre-training. The setting of these two parameters of EVA-X is mainly derived from experimental findings. We compare mask ratio settings of 0.2, 0.3, and 0.4, and find that a mask ratio of 0.3 gave the best performance in EVA-X (see Table~). This ratio is slightly smaller than the natural image. For data enhancement, we set the minimum cropping ratios to 0.1, 0.2, and 0.3. we found that the performance of EVA-X was optimized when using 0.2. This cropping ratio is smaller than previous work~.

Details of the specific pre-training for EVA-X are as follows. EVA-X is available in three different sizes, each embedding an image with a patch size of 16, namely, EVA-X-Ti/16, EVA-X-S/16, and EVA-X-B/16. Their corresponding parameter counts are 6M, 22M, and 86M, respectively. We use memory-efficient attention provided by ~ to expedite training. For the tokenizer, we use the ViT-B visual encoder from the pre-trained vision-language model of MGCA~ and ViT-B/L/G visual encoders from advanced natural CLIP models, EVA-CLIP~. Their parameters remain entirely frozen during training. We perform simple data augmentation on the pre-trained data. This includes random horizontal flipping, random scaling, and fixed-size cropping. The random scaling range is , and the fixed cropping size is 224. Subsequently, we normalize all input images using the mean and variance of Merged520k. We use AdamW with  as our optimizer. The learning rate is set at 3e-4, and a cosine annealing descent strategy is employed. During the pre-training process, we use 8 A100 GPUs and train EVA-X for 600 epochs to obtain the final results of EVA-X-S and EVA-X-B, while 900 epochs for EVA-X-Ti.

 is proposed by ~. As mentioned above, due to its high-quality data and extensive research and use, we use it as a standard test set for pre-training. It contains a total of 111,120 chest x-ray images of frontal view, each of which measures 1024 and is labeled by a medical professional with 14 different disease infections such as Atelectasis, Consolidation, and others. Note that while these diseases may be correlated in a medical sense, in our actual experiments we consider these 14 different diseases to be independent of each other. Following ~, each of the images is resized to 224 while training and testing.

 is proposed by ~. Its training and validation sets are publicly available, but the test set is not. It contains a total of 224,316 publicly available frontal view or lateral view Chest X-ray images, of which the number of test set images is only 236. There is a certain gap between its training set and validation set.First, its training set contains 14 classes of labeling while the validation set has only 5 classes of labeling. Second, in the training set, for each class of disease, there is a labeling marked as -1 in addition to the two labelings 0,1. When this type of disease is labeled as -1, it means that the presence or absence of the disease is doubtful and has not been fully determined by the doctor. In the validation set, this is not the case. Following ~, each of the images is resized to 224 while training and testing.

 is originally proposed by ~. In this paper, we use version of  and . The former contains about 30k training images and the latter contains about 67k training images. Both datasets focus on the detection effect of the disease COVID-19. Following previous work~, we used Covid-19 sensitivity and mean of accuracy to evaluate the final metrics. Each of the images is resized to 224 while training and testing.

Our model is deterministic for different datasets. Specifically, we use the EVA-X-Ti and EVA-X-S trained in the previous section of pre-training to output the final result by a linear projection. We use mean-pooling of the feature map instead of the class token since we observe a slight improvement in fine-tuning. For two different tasks, we train using BCE loss and CE loss, respectively. For training, we use fixed learning rate optimization with AdamW and 1e-3. Layer-wise Learning Rate Decay (LLRD) is also used, with the parameter set to 0.55. drop-out ratio is set to 0.2. Each task is trained on 4 RTX 3090 GPUs. Batch size per GPU was set to 256 or 128. For Chest X-Ray14, we train with it for 30 epochs, and for the last 3 datasets, we train with them for only 10 epochs.

 This dataset is a recent lung segmentation dataset proposed by ~. They select 1000 images from the RSNA Pneumonia Detection~ dataset and invite professional doctors to annotate the lungs. The annotated images are divided into 800 training images and 200 testing images. In our experiments, all training and testing images are resized to a resolution of 512.

 is a well-known Kaggle challenge. The dataset is provided by . It is divided into two stages, and here we use the dataset from stage 2. It includes 3205 training images, of which 2669 images are labeled for pneumonia, and corresponding masks are provided. Following previous work~, we randomly split this dataset into 1868 training images and 801 testing images.

 is also a Kaggle challenge provided by ~. We use the dataset from its second stage. The dataset comprises 26.7k training images and 3000 testing images. Among them, 6012 images are labeled for pneumonia, and corresponding location information for annotations is provided. Following the same processing approach as in previous work, we divide it into 4236 training images and 1776 testing images.

 This dataset comprises 662 chest X-ray images, with 326 being normal and 336 being abnormal. For these 336 abnormal images, the dataset provides a binary mask annotation to locate the appearance of pulmonary tuberculosis symptoms. In our experiments, we randomly split these 336 images into 236 training images and 100 testing images. Throughout the training and testing processes, all images are resized to a resolution of 512.

For a fair comparison, all experiments are conducted based on the mmsegmentation~ framework. We construct our convolutional models using commonly used public libraries~. For the ResNet method~, we use the SGD optimizer with a learning rate set at 0.01, and weight decay set to 0.0005. For the ViT method~, we use the AdamW optimizer with a learning rate set at 2e-4. Layer-wise learning rate decay is set to 0.85. All models are trained on a single RTX 3090 GPU, with a batch size set to 4. For the aforementioned datasets, we train for either 4k or 10k iterations. The hyperparameters for different visual representations of the same structure are identical, with only differences in the initialization method. During training, we apply data augmentation operations such as resizing and random flipping, and all training and testing data are processed at a resolution of 512.

Additional ResultsDetail Classification Resultsfig:cls_detailcxr14, chexpertPre-training Results \textbf. The CLIP series of visual models can all be employed as tokenizers for EVA-X, and their performance is consistently observed to surpass the reconstruction of pixel features. As the size of CLIP increases or its proprietary nature strengthens, the performance of EVA-X also improves accordingly. \label

\begin \end -.2em \textbf. Here, we train ViT-Ti for 900 epochs and ViT-S and ViT-B for 600 epochs. Performance of EVA-X improves with scaling of encoder size.  \label

\begin \end 0.5em \textbf. The performance of EVA-X, pre-trained by mask image modeling, far exceeds the performance of Tokenizer. Even though the former model size is much smaller \label

\begin \end -.2em \textbf. The performance impact of different mask ratios on downstream tasks. The experimental results show that the performance of the downstream task is optimized when the mask ratio is 0.3. \label

\begin \end 2em \textbf. The effect of data-enhanced cropping ratios on training. We find that optimal pre-training performance is achieved using a randomized scale cropping of (0.2, 1). \label

\begin \end Main experiments and ablations of EVA-X pre-trainings.Tokenize X-ray Images with CLIP.tab:reconstruct_targetseva, eva02, cae, caev2, mvpeva_clipmgcaxiao2023delvingtab:reconstruct_targetsOutperforming the Tokenizer.mgcatab:outperformtab:encoder_sizemaskfeat, beitv2Scalability of Model Size.densenetresnettab:encoder_sizeScalability of Pre-training Data.xiao2023delvingfig:scaleAttention Map Visualization.fig:attention_mapbeitdinodinov2Faster convergence.fig:scaleselfmedmae, xiao2023delvingHyper-parameters.tab:mask_ratioxiao2023delvingAdditional Experiment Detailshttps://github.com/hustvl/EVA-XPre-training Detailsxformersxformersmgcaeva_clip\beta_1=0.9, \beta_2=0.98Classification Transfer Learning DetailsClassification DatasetChest X-Ray14Wangcxr14Xiaoxiao2023delvingCheXpertIrvinchexpertPhampham2021interpretingCovidXWangWang2020CovidX-CXR-3 Version5CovidX-CXR-4 Version9Different versions of CovidX can be obtained on \urlWang2020Implementation DetailsSegmentation Transfer Learning DetailsSegmentation DatasetLung Segmentation~\cite.Zhangzhang2023lungRSNASIIM-ACR Pneumothorax Segmentation~\citeZawackimgcaRSNA Pneumonia DetectionSteinRSNAShenzhen Hospital Chest X-ray.~\citeImplementation Detailsmmseg2020Iakubovskii:2019resnetvit