Intuitively, the self-cloning step aims at finding a set of parameter for  and  such that the steered LM agent (~) behaves similarly as not steered (~). Although it does not bring about an immediate performance boost, it provides a good starting point for the subsequent RL training. 

We first collect a corpus of unsteered dialogues by having the LM agent interact with the environment using a baseline generation strategy (~). The corpus is denoted by . Note that  is the maximum number of rounds and  is the number of collected dialogues. 

We then train planner  and up-mapping matrix  together by minimizing a conditional language modeling objective: 

After training with self-clone loss, the steered model's performance will match the unsteered baseline. If an additional dialogue corpus from a different, potentially better LM agent is available, the same loss could also be used to clone its policy. We will freeze the up-mapping matrix so that the effective action space for the planner model is reduced to  for easier RL training. More discussion in~.

To better utilize the reward signals collected from interacting with the environment, we apply reinforcement learning to the planner model. We begin by reviewing how the "environment" functions from the perspective of the planner model. At first, the embedding of the scenario description and dialogue history is given to the planner as the initial state. After the planner acts, the action vector is first expanded by  from the low-dimensional space  to , which is then prepended to the original sequence of token embeddings of the dialogue history to generate a response from LM agent Q. Subsequently, LM agent P replies. With the addition of this new round of dialogue, the next state vector is created from the dialogue history embedding and fed to the planner again. A judge model then provides a reward signal to the planner.

Based on this setting, we apply a continuous-control RL algorithm to finetune the planner model for maximized expected rewards. Specifically, we choose an actor-critic algorithm called TD3+BC~. As a brief introduction to actor-critic learning, a critic network  is trained to predict the expected return under current policy given the current state-action pair  with temporal difference learning:

where  is the subsequent state,  is the action taken in , and  is a discount factor. Concurrently, an actor network is trained to maximize the expected return through a deterministic policy gradient:

At the end of RL training, only the actor is kept for steering the LM agent. As an offline algorithm that does not directly interact with the environment during training~, TD3+BC significantly lowers the development cost in terms of both compute and money. At a higher level, our approach can be thought of as performing one-step RL on current on-policy samples. Some technical details of our application of TD3-BC can be found in~.

 We choose TD3-BC for our RL training, but it is worth noting that this choice is non-essential to our proposed . We could reasonably expect that a more carefully chosen or tuned algorithm might surpass the results presented in this paper.

In Sotopia, there are a total of 450 scenarios. At the beginning of each episode, we uniformly sample one scenario and compile its description and character profiles into system prompts for the two LM agents. The dialog history starts empty and grows with each step. We cut the conversation off at 3 rounds. To provide readers with a clearer sense of the dataset, our qualitative result (as shown in~) provides an example of the Sotopia platform.

Each agent has several traits, including their gender, personalities, decision-making styles, some public information, and even secrets. To evaluate the behavior of the LM agents at the end of each dialogue, GPT-4 is called to judge the completed dialogue along seven dimensions following the convention of Sotopia: goal completion, believability, knowledge, secret keeping, relationship maintenance, social rule obedience, and material benefits. An aggregated score is used as the reward signal. We use GPT-3.5 at training time to lower the cost of RL training. 

We choose Llama-2-7B-chat as our primary language model for steering, with the unsteered version of Llama-2-7B-chat serving as its partner. The  is trained by interacting with the unsteered Llama-2-7B-chat and is also tested out of the box with Llama-3-8B-instruct. Following Sotopia, we set a sampling temperature of . We modify the instruction and output schema so that lower-end models like Llama-2-7B-chat can achieve decent baseline performance.

 Throughout this paper, experiments are conducted with  prefix tokens, -dimensional action space for an easier RL training. After self-cloning, we collect an offline data buffer of 10,000 episodes (3 steps per episode) by perturbing the self-cloned action space with exploration noise . While collecting exploration samples, we set the LM agent's temperature to  for a less noisy signal. We train TD3+BC for 1 epoch. For TD3+BC training, we adopt standard CORL implementation~. Our buffer size is small compared to common practice in offline RL training; however, our buffer size is considerable compared to 's previous work on dialogue systems, which collected 14,232 steps by human annotation. All our experiments can be done on one Nvidia A100-40G GPU. Thanks to its offline nature, the RL training completes within 10 minutes, excluding pre-collecting the replay buffer. 

We present the main results of -steered Llama-2-7B-chat in~. We compare two stages of  with the baseline models as well as GPT-4, which is the strongest social agent reported by Sotopia. Although our method is of a simplistic nature, -steered model outperforms both unsteered baseline and GPT-4, with either Llama-2-7B-chat or Llama-3-8B-instruct (column) serving as the model's partner in social interactions. We also observe that self-clone recovers most of the model's original performance while paving the road for second-stage RL training, making it a viable technique to convert dialogues into continuous-control problems. Because the Sotopia benchmark consists mostly of positive-sum games, the improvement of social capability by -steered agents will also benefit their partners' scores.  In one case (~ in~), two agents are negotiating on splitting shared property. The steering changes one agent's behavior from being dismissive and focused on a straightforward 50/50 split of items to engaging in a more thoughtful and open negotiation. As a result, not only does the steered agent achieve better goal completion, but the interlocutor does as well.

To understand how the injection position of  performance, we conduct experiments that vary the injection positions. We try two alternatives: infix, which means placing the  the scenario description and dialogue histories, and suffix, which means placing it at the end of the whole prompt. As shown in ~, both infixes and suffixes underperform prefixes, corroborating the results in ~. The number of  choose, two, is smaller than the common choices in prefix tuning for learning downstream tasks (from tens to hundreds). The experiment results demonstrate that even a small number of well-predicted  have a significant accumulated steering effect on the language generation process.

We adopt the standard query split of HarmBench~ to benchmark our proposed multi-turn red teaming setting. At the beginning of each episode, we uniformly sample one from 159 harmful queries to format a system prompt (see~) for the attacker LM.

At the end of each dialogue, we use a Llama-2-13B model that has been fine-tuned by~ to judge if the red teaming is successful. To facilitate RL training, we soften this binary signal by comparing the logits of "Yes" and "No" in the judge model's next-token prediction to obtain a continuous reward signal. At test time, the attacker is tasked with all harmful queries one at a time, and the average success rate (ASR) is reported.

We choose Llama-3-8B-instruct as our baseline attacker model for its improved ability to follow instructions. We set a maximum of 384 generated tokens for each utterance. During , we use an unsteered Llama-3-8B-instruct as the defender and later test how the attacker can generalize to effectively attack Llama-2-7B-chat. For comparison, we choose the  performing red teaming techniques reported by~: a text optimization technique called Greedy Coordinate Gradient (GCG~) and an LM optimizer technique called Prompt Automatic Iterative Refinement (PAIR~). Among other variants of GCG, we benchmark against GCG, which optimizes one suffix for each query to form a fair comparison to . In contrast, GCG-Multi optimizes a single suffix for all queries at once. Closer to our approach, PAIR uses an attacker model to generate and refine jailbreaking prompts iteratively. We use Llama-3-8B-instruct as the PAIR attacker. Unlike the social capability experiment, we collect 120,000 episodes as the offline buffer (3 steps per episode) with an exploration noise  in the action space. 

We present the main results of -steered Llama-3-8B-instruct in~. Single-turn results on Llama-2-7B-chat is taken from~'s Table 7 and those on Llama-3-8B-instruct are reproduced with authors' released code. For all -steered attackers, training occurs through interactions with Llama-3-8B-instruct, where we observe the most significant improvements. As a generalization test, we apply the same attacker to red-team Llama-2-7B-chat, where some improvements are still evident. Sweeps over key hyperparameters can be found in~.

To better understand the performance gain, we present some qualitative results in~ and find that the attacker model communicates in a more strategic way to elicit answers. For example, in~, the -steered attacker gradually escalates the dialogue toward harmful directions, using the first two rounds of dialogue to condition the defender for a higher probability of answering the final question. Actually, this strategy is also discovered by~ for multi-turn red teaming, demonstrating effective performance. In , the attacker seems to rediscover the commonly seen jailbreaking strategy of hypothesizing different scenarios. In contrast, in the unsuccessful example without DAT-steering (~), the attacker plays it straight in a failed attempt to debate the defender.

Looking at~, as Llama-2 evolves into Llama-3, it becomes less susceptible to attacking techniques that treat it as a machine learning algorithm, such as GCG, corroborating the observations by~. However, it is more likely to respond to harmful queries under human-like interactions. It's also worth noting that common defenses that target users' prompt, such as perplexity-based detection, rephrasing, and retokenizing~, may be insufficient against -based attacks since the texts appear normal.%, and the susceptible texts are spread out across turns.