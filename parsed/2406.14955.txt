% version 2 Copying is a fundamental ability of the ICL mechanism, allowing LLMs to gather supplementary information from contextual cues. In our evaluation, we design tasks specifically to assess the exact copying ability of LLMs, wherein they are required to copy fragments that are present within the given context. To evaluate the models' adaptability and proficiency across various contextual scenarios,  categorizes copying tasks into two distinct types: unstructured (natural language) text and structured text.

As shown in the left-top example of Figure~, we mask the second half of a string (str. "521351caba13f591") which appears one or multiple times in the previous paragraphs. Then we require the models to predict it given the first half (str. "521351ca"). Notably, we use the hash strings to replace the origin entities in order to prevent LLMs from completing the entity based on their internal knowledge rather than the context.

We use the "dictionary" format as a representation of structured data to better control the length and similarity of "" part and "" part. As shown in the right-top example of Figure~, this task challenges the models to deduce a value from a specified key (str. "71df7d") from a set of key-value pairs. We test their ability to seek and extract information from structured repositories efficiently.

The learning ability is another fundamental ability of the ICL mechanism, allowing LLMs to extract mapping rules from in-context examples. This ability enables them to tackle different and unseen tasks based on examples or natural language descriptions, without requiring updates to model parameters.  To focus solely on the learning ability and avoid the influence of language ability and knowledge across different models,  decomposes the evaluation of learning ability into four foundational aspects: format rules, order rules, statistics rules, and list mapping.

 assesses whether LLMs can learn formatting mappings from in-context examples, a key skill for generating appropriately formatted outputs across various tasks. Specifically, we design a format classification task and two format generation tasks named format check, format cloning, and format conversation respectively. Figure~ shows an example of the format conversation task that converts "Markdown-Table" format data to "XML" format.

 examines LLMs' ability to grasp the order of a group of elements as well as the mapping rule of two groups of elements from in-context examples. This skill is crucial for tasks requiring re-organization of input elements, such as in translation and syntactic analysis scenarios. To this end, we formulate a classification task and a generation task aimed at evaluating LLMs' capabilities in determining whether inputs satisfy a specific order (named order check) and in executing order transformations (named order adjustment). Figure~ shows an example of the order adjustment task that converts the word-level order from  to .

 evaluates LLMs' ability to extract, filter, summarize, and analyze relevant information from in-context examples.  We consider four typical tasks including detecting or eliminating duplicates within the context(named duplication check and de-duplication), counting the number of elements in the context (named count  navigation), and generating the related information about a given node in relation graph (named relation analysis). These tasks collectively aim to measure LLMs' analytical capabilities and their applications in processing and interpreting complex information structures. Figure~ shows an example of the relation analysis task that lists all the friends of "885" mentioned in the context.

 is to find diverse custom rules from given in-context examples. We use the data from "list" task in BIGBench  to compose our numbers' list mapping task, which is learning a mapping given multi-groups of numbers list pairs. There are 250 diversity mapping rules in it with different difficulties, and one example is shown in Figure~.

All these rule learning tasks we designed utilize n-shot examples while relying less on language abilities, commonsense knowledge, or factual knowledge.

We have a total of 12 tasks with 2,040 testing samples.  % For classification tasks such as order check, duplication check, and format check, we judge whether the predictions if consistent with labels.  For almost all tasks such as string completion, dictionary search, and format conversion, we use exact match scores to evaluate the predictions with the labels. %It was strict because even if there is only one character is different with the label, we regard the prediction is wrong.  But for the format cloning task, we only evaluate the correctness of the format and do not consider the content. Moreover, we use postprocessing to convert models' responses for different tasks. More processing details are shown in Appendix~.

We evaluate various open-sourced LLMs with different model sizes, such as the LLaMA series , the Baichuan series , and the Qwen series . For example, we evaluate the LLaMA series with 7B, 13B, 34B, and 65B versions as well as base-version and chat-version. Moreover, we test the intermediate checkpoints with different pretraining stages for TinyLlama-1.1B, Baichuan-7B, and Amber-7B. The detailed description of these models can be found in Appendix~.

For all the models, whether base-version or chat-version, we don't use additional prompts (e.g. "User:"), and use n-shot examples in most tasks. We don't use sampling or beam search and only use the greedy decoding method. 

% 

We divided the models into three groups: small-sized models (around 1B parameters), middle-sized models (around 10B parameters), and large-sized models (bigger than 30B parameters), and conducted tests on all these models. The results are presented in Table~. From the table, we can obtain the following research findings:

% overall analysis 

(1) A general trend indicates that larger models tend to exhibit superior ICL performance, observing the LLaMA series from 7B to 65B. However, the data also presents a considerable variance within models of similar sizes (e.g. LLaMA-7B, LLaMA2-7B, and LLaMA3-8B). Notably, some middle-sized models (e.g. such as Mistral-7B-v0.1 and LLaMA3-8B) demonstrate strong ICL abilities comparable to models having 5x-10x size (e.g. LLaMA-65B). This finding underscores that model size is not the sole determinant of ICL efficacy.

(2) For the exact copying ability, larger models don't have obvious advantages. Surprisingly, in the unstructured context scenario, even the small-sized models (e.g. TinyLlama-1.1B) can achieve a score of , while the scores of LLaMA3-8B and LLaMA3-70B are only  and  respectively. We find that the results are mainly influenced by tokenizers, which we have a detailed discussion in Section~ and Figure~. In the structured context scenario, the results are also unrelated to the models' sizes. With similar model sizes, LLaMA-7B gets  but LLaMA3-8B gets , LLaMA2-70B gets  but LLaMA3-70B gets .

(3) For the rule learning ability, larger models usually have better performance than smaller ones. For challenging rules such as statistics and list mapping, the gaps between small-sized, middle-sized, and large-sized models become more obvious. The highest scores of the three groups of models are  in statistics rules and  in list mapping rules. Especially, the largest model in our evaluation (LLaMA3-70B) gets all the highest scores of the rule learning tasks. 

% 

We examine how the ICL ability evolves during the pretraining stage with the pretraining checkpoints of TinyLlama-1.1B, Baichuan2-7B, and Amber-7B. Figure~ and Figure~ show the detailed results of TinyLlama-1.1B and Baichuan2-7B, and the results of Amber-7B are shown in Appendix~. In addition, we calculate the average scores of Baichuan2-7B and test the TriviaQA  and MMLU  datasets with the same checkpoints, as shown in Figure~.

(1) The abilities of ICL exhibit rapid growth in the initial stage, before about 200B tokens. However, after this point, their growth becomes slow and eventually stops. Figure~ illustrates the results of ICLEval, TriviaQA, and MMLU, which represent the model's abilities in terms of ICL, knowledge, and processing complex questions, respectively. We can see that after 220B tokens, the ICL abilities nearly no increase, while knowledge of models continued to increase. The ability to process complex questions demonstrates little improvement before training 220B tokens but experiences a significant boost afterward. This phenomenon indicates that LLMs acquire different abilities in a sequential order, with ICL ability being relatively easy to obtain.

(2) The exact copying ability emerges in the very early stages of pretraining. As shown in Figure~, the result of copying in  scenarios arrives at the highest before 10B tokens and remains stable until the end of pretraining. Besides, the result of copying in  scenarios arrives at the highest score of about  at the 419B tokens. It is unstable and fluctuates multiple times during the subsequent training stages. Compared with the unstructured text scenarios, we design various similar keys as the interference factors of prefix matching in structured text scenarios. We suppose that the distinguishing ability of similar strings is unstable in the pertaining stage.

% (3) The rule learning ability increases very slowly after the initial stage, about 200B training tokens. Combined with Figure~ and Figure~, we can find that the results of learning format rules are unstable in the pertaining stage, while the results of the other three aspects rules are more stable. We speculate that format learning is more easily interference by the LLMs' inherent preferences, and these inherent preferences continuously change during the training stage. Inherent preferences will be discussed in Section~.

In particular, there is a significant increase in statistics for Baichuan-7B but only a slight improvement in TinyLlama-1.1B. We guess that maybe models only learn to predict the next token based on a small number of previous tokens in the early stages of pretraining. While, during the later stage of pretraining, models learn to use more tokens to predict the next token more accurately. Small-sized models may not have enough attention points capacity for later pretraining, which will be discussed in detail in Section~.

We find some interesting phenomenons in our evaluation results and regard these phenomena to four aspects: distinguishing ability, inherent preferences, attention points capacity, and tokenizer. We also analyze these phenomena with some bad cases. Due to the limitation of pages, we put the bad cases on the Appendix~. 

Similar strings can cause interference for humans as well as for LLMs. As depicted in Figure~, in the dictionary search task, the accuracy of the same model is adversely affected when there are numerous similar keys in the dictionary.   More similar keys make the scores drop more, while models with the stronger distinguishing ability (e.g. Baichuan2-7B) drop less. The results suggest that the excessive presence of similar strings makes the model chaotic, causing it to struggle to distinguish between different keys. This phenomenon indicates that models with weak distinguishing ability may make mistakes when extracting information in complex in-context environments.

We looked into the reasons for errors in certain cases and found two types. First, the model sometimes selects the incorrect value from the dictionary. Second, it occasionally generates a random string that is not present in the dictionary. We further analyze the proportion of right results and the two error types in the pretraining stage of Baichuan2-7B, as shown in Figure~. We find the model's choices of the two types are continuously changing. That indicates the distinguishing ability is unstable in the pretraining stage. 

We observe that some models exhibit unusually weak performance in tasks such as format check or format cloning. Figure~ shows the performance of the format check task. This task is a classification task with six different labels: "JSONL", "CSV", "Tuple", "YAML", "Table", and "XML". From the figure, we can find that ChatGLM3-6B-Chat can get a score approaching 0.7, while lots of models are lower than the random scores. To our surprise, LLaMA-65B gets 0.0 in this task. 

We further give a deeper analysis of the bad cases, and we find that most models tend to respond with "JSON" as their prediction, but we even don't have this label. We suppose that such heavy inherent preferences of these models may come from their pretraining data distribution, making the model cannot adhere to the pre-defined formats or rules presented in the in-context examples. 

Furthermore, we find the chat version of some models such as ChatGLM3-6B, InternLM-7B, and Mistral-7B have obvious improvements to their base version in this task. We think this might be due to the instruction learning process can reduce the impact of the models' inherent preferences to some extent. 

% The attention memory capacity of many models can lead to weak performance.  In the count  navigation task, the "1-dim" setting requires models to analyze and count only two elements ("up" and "down"), whereas the "2-dim" setting involves analyzing and counting four elements ("up", "down", "right" and "left"). We have observed that the scores for the "1-dim" setting are significantly higher than those for the "2-dim" setting, as illustrated in Table~. 

We believe this phenomenon occurs because models face difficulties in effectively utilizing a larger number of tokens within the context to predict the next token. In other words, for models to accurately predict results, they need to pay attention to multiple points within the context. And we call this Attention Points Capacity. However, many models tend to rely on only a few tokens for predicting the next token, thereby failing to incorporate all the available information simultaneously. To validate our hypothesis, we conducted a further analysis of the relationship between the number of elements and accuracy, as depicted in Figure~. The figure demonstrates a significant decrease in accuracy as the number of elements increases.

In our evaluation, we observe that the tokenizer has a significant impact on our results and can affect our adjustment of task difficulty. From a human perspective, we can perceive text at the character level, word level, or sentence level, allowing us to easily distinguish individual numbers, letters, words, and sentences. However, language models process inputs at the token level, which presents a different viewpoint compared to ours. 

In Figure~, we illustrate three types of bad cases caused by tokenization that we find can lead to confusion for LLMs. "Token fracture" occurs when a continuous sequence is split but the encoded tokens of the segmented fragments do not match those of the original sequence; "Token insert" refers to the fact that the special symbols that we can easily overlook also may require 1-3 tokens for encoding; "Token replace" indicates that several consecutive characters can be encoded as a single token, resulting in different encoded tokens for the reversed string compared to the original one.