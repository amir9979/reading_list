We employ an interactive zero-shot chain-of-thought (CoT) prompt  , as defined in Section~, which describes the problem setting  along with an externally summarized interaction history  and reasoning instructions, denoted by . We adopt the prompting template that leads to the best performance among all prompt variations explored in a recent study~ for the classic MAB problem. The LLM agents interact with dueling bandit environments in a round-based manner, with the prompt guiding their decision-making process. We conduct experiments with three LLMs: , , and  through the OpenAI API with temperature = 0. The detailed prompt is provided in Appendix~.

 We compare LLMs against eight well-established baseline algorithms to evaluate their efficacy. The baselines include Interleaved Filter ()~, Beat the Mean ()~, Sensitivity Analysis of VAriables for Generic Exploration ()~, Relative Upper Confidence Bound ()~, Relative Confidence Sampling ()~, Relative Minimum Empirical Divergence ()~, Self-Sparring~, and Double Thompson Sampling ()~. Each of these algorithms employs distinct strategies for selecting arms and estimating preferences, with the ultimate goal of efficiently identifying the Condorcet winner. We assess the performance of LLMs and baseline algorithms using strong regret and weak regret metrics defined in Section~. 

 We evaluate the regret performance of LLMs and baselines across two stochastic environments, each characterized by a distinct preference matrix . The preference matrices are constructed using the Bradley-Terry-Luce (BTL) model , with a generalized form known as the Plackett-Luce model . In this model, each arm is associated with a utility parameter , where  represents the rank of the arm (i.e.,  corresponds to the best arm,  corresponds to the second best arm, and so on). For any pair of arms  and , the probability of  being preferred over  is determined by . Setting the number of arms , we randomize the order of the arms to prevent selection bias, resulting in the following arm ordering: . We use two instances:   and , with their respective  parameters given by: % % \quad % % 

Note that the datasets generated in this way satisfy the  Strong Stochastic Transitivity (SST) and Stochastic Triangle Inequality (STI) properties~ (see Appendix~ for more details). The settings of the BTL model used in our experiments also imply the existence of a Condorcet winner.

 The scale of our experiments is chosen to balance computational feasibility while preserving the ability of obtaining meaningful conclusions. We set the time horizon to  rounds, providing the LLMs and baseline algorithms with sufficient opportunity to learn and adapt to the DB environments. Each experiment is replicated  times for the LLMs and  times for the baseline algorithms, enabling an understanding of their average behaviors and reliable performance estimates.

For brevity, we present our initial analysis focused on the  instance (Figure~).   The analysis is qualitatively similar for the  instance (Figure  in Appendix ).  %present a comprehensive comparison of the performance of LLMs and baseline algorithms on both the  and  instances.   We use  for ,  for ,  for Self-Sparring, and  for ,  and . We analyze the results in terms of the strong and weak regret defined in Section~.

 The strong regret results in Figure~ (Left) reveal that among the LLMs,  demonstrates the best performance in both the  and  instances. Comparing  with the state-of-the-art baseline algorithms ( and Self-Sparring), we observe the following: (i) In the first  time steps,  exhibits similar or better performance than these baselines in both instances. (ii) Over the first  time steps,  maintains a performance level comparable to the state-of-the-art baselines in the  instance and slightly underperforms compared to the state-of-the-art baselines in the  instance, yet still outperforms all the other baseline algorithms. (iii) Throughout the entire time horizon, the strong regret of LLMs grows approximately linearly in both instances, indicating a non-converging behavior of the LLM agents. The results for the  instance (Figure~) are qualitatively similar.

 In terms of the weak regret metric, Figure~ (Right) indicates the exceptional performance of . It consistently outperforms all other LLMs and state-of-the-art baseline algorithms throughout the entire time horizon in both instances, exhibiting the lowest mean weak regret with small variance. The results for the  instance (Figure~) are qualitatively similar.

To more thoroughly evaluate the capabilities of LLMs, we delve into the success and failure cases found in our experiments, with more results presented in Appendix~. In the following analyses, we focus on , which is the top-performing LLM as shown in Figures~ and~.

 As evident in Figures~ and~ (Right),  demonstrates strong zero-shot chain-of-thought (CoT) reasoning capabilities in terms of the weak regret. This can be attributed to its ability to quickly identify and consistently include the best arm in its duels. Figure  in Appendix~ illustrates the fraction of duels including the best arm across different time intervals.  outperforms other LLMs and the state-of-the-art baselines throughout the entire timeline in both instances. These findings suggest that  is able to effectively process the feedback obtained from duels and make informed decisions to identify and select the best arm, leading to superior weak regret performance.

 demonstrates low variance compared to other LLMs and baseline algorithms. As shown in Figure  (Left) in Appendix~,  exhibits the lowest average generalized variance of strong and weak regret in the  instance, suggesting that it can make consistent decisions when the preference structure is simple. For the  instance shown in Figure  (Right),  has a variance level comparable to the state-of-the-art baseline algorithms (except  and , which are still in an early stage of exploration), indicating that its performance stability remains competitive even when faced with a more complex preference structure. This low variance result highlights 's ability to adapt to varying levels of difficulty and maintain a stable decision-making process in DB setting.

Figures~ and~ in Appendix~ present the arm selection traces of  and state-of-the-art baselines on the  and  instances to analyze the failure cases. The trace plots suggest that  exhibits a two-stage Explore-Then-Exploit pattern~ with occasional robustness checks. Analysis of the two stages reveals two failure modes of LLMs:

 In the exploration stage, we observe that  tends to quickly narrow down to a small subset of arms (usually containing the Condorcet winner) and repeatedly compare these arms. In contrast, the baselines exhibit more diverse and extended exploration patterns. This behavior suggests that  may overestimate the quality of arms that win their initial comparisons based on limited historical data. Unlike the baselines, which have explicit exploration mechanisms, LLMs rely on their inherent randomness (via sampling from their output distribution) to explore. Based on these observations, we hypothesize that if  happens to sample a sequence of comparisons that favors suboptimal arms early on, it can get stuck comparing these arms indefinitely. To test this hypothesis, we conducted experiments using noisy prompts with biased history. Our results in Figure~ confirm that 's exploration strategy is indeed vulnerable to biased history initialization and can converge to local optima. 

 Despite 's outstanding weak regret performance, it fails to consistently converge to a single best arm to duel against itself, even when the prompt setting explicitly calls for it. This behavior highlights a fundamental limitation of LLMs: they are primarily designed and trained for word token prediction rather than decision-making. Unlike baselines with explicit stopping conditions,  relies on its inherent language modeling capabilities to determine when to stop exploring. Consequently, in the later exploitation stage,  keeps comparing the same top arms without committing to a single winner (see Figure ), reflecting an intrinsic uncertainty in its decision-making process for DB. The patterns learned during pre-training do not lead to converged decisions, even if the prompt includes emphatic commands, exclamation marks, quotation marks, or other forms of emphasis. This suggests that the language modeling objective alone may not be sufficient for LLMs to achieve optimal control in complex decision-making tasks like DB. 

 Table  summarizes the success and failure cases of using  as an in-context decision-maker for the DB problem.  %Our results demonstrate the potential of LLMs but also highlight several challenges that need to be addressed.  can serve as an effective decision-maker in the short term, by quickly identifying and exploiting the best arm with low variance across different instances. However, the long-term performance is hindered by over-estimation bias in exploration stage and lack of convergence criterion in exploitation stage. This highlights the need for more robust and effective strategies to use LLMs.

To further investigate the algorithmic behavior of LLMs in DB and develop more robust and effective strategies, we seek to answer the following questions:

%  Can we distill the decision-making process of LLMs to understand their algorithmic behavior?In this section, we present the design intuitions of . We begin by discussing the limitations of a naive intervention approach and the desirable properties for an effective LLM augmentation framework. Based on these considerations, we identify IF2~ as the ideal candidate algorithm to serve as the foundation for . Finally, we illustrate the components of  and provide a detailed algorithm description.

 A straightforward approach to addressing the limitations of LLMs is to use a simple if-else condition that forces the LLMs to converge when they appear to select two identical arms, which we call the Convergence-Triggered (CT) intervention strategy. However, CT fails to guarantee the selection of the true Condorcet winner and can reinforce local optima (see Figure~ in Appendix~ for a failure example). This suggests that relying on the LLM's internal convergence behavior for such a mechanism is not robust, as the LLM's exploration is largely driven by its inherent sampling noise rather than a structured exploration policy.  Thus, handling this limitation with theoretical guarantees remains challenging. 

 To address , we seek an algorithmic framework with the following properties: (i) A clear, symbolic logical structure that allows for easy integration with LLM suggestions. (ii) A well-defined exploration-exploitation trade-off that leverages the LLMs' exploration behavior while ensuring convergence. (iii) Strong theoretical guarantees to maintain robustness with various prompting scenarios.

 Among classic DB algorithms, IF2 stands out as particularly well-suited for LLM augmentation:

By selecting IF2 as the base algorithm and leveraging its Explore-Then-Exploit pattern, we address . IF2's alignment with the observed behavior of LLMs (see Figure~) allows for a natural integration that takes advantage of the LLMs' exploration capabilities while maintaining theoretical guarantees.

 The procedures of the -enhanced LLM agent are illustrated in Figure~, presented in Algorithm~ (see more details in Appendix~).  maintains a confidence parameter  and a threshold parameter  that control the algorithm's confidence of matches between arms. The key components of  are as follows:

After , the LLM-augmented agent repeats  until  only contains the best arm. Algorithm~ and Figure~ summarize the phases above, with detailed procedures defined in Appendix~.

%    Procedure}: This procedure ensures that the comparisons are sufficient and bounded when leveraging LLM's suggestions. It compares two arms within a bounded length determined by  to identify the superior arm with confidence . If no winner is found within the bounded length, the input arms are returned unchanged.% The LLM and \ifalg Phases alternate until a single best arm remains. By combining the LLM's guided exploration with \ifalg's uniform scheduling, \ouralg leverages the strengths of both approaches while ensuring convergence and bounded regret. In this section, we present two main theoretical results. First, we characterize the vulnerability of using standalone LLM agents for dueling bandits in Assumption~ and Theorem~. Then, we provide the theoretical guarantees of  in Theorem~, demonstrating its robustness and efficacy.

 Inspired by the adversarial corruptions framework introduced in  for the classic MAB problem, we investigate the vulnerability of standalone LLM agents in the DB setting under adversarial prompts. We consider an attacker with a budget  who employs the following strategy: whenever the LLM agent selects the optimal arm  for comparison, the attacker manipulates the input prompt to the LLM to eliminate  from the duel with probability  (where  is a constant), subject to the constraint of performing at most  attacks over  rounds. This adversarial strategy compels the LLM agent to select suboptimal arms, resulting in poor performance, as formalized in the following theorem with Assumption~.

The proof of Theorem~ is provided in Appendix~. The theorem underscores the suboptimality of standalone LLM agents in DB problems, particularly when input prompts are subjected to adversarial attacks. This vulnerability highlights the need for a more robust approach that harnesses the power of LLMs while offering theoretical guarantees under diverse prompting scenarios.

 Following the algorithm design of  introduced in Section~,  inherits the theoretical guarantees of  (see Appendix~), while leveraging the benefits of LLM-enhanced exploration. Specifically,  offers the following theoretical guarantee:

The proof of Theorem~ is provided in Appendix~. The required assumptions are precisely stated in Appendix~. Theorem~ establishes the efficacy and robustness of :

 Both the strong and weak regret for  are guaranteed to satisfy a worst-case upper bound of , which is only worse than the well-known lower bound of  in~ by a factor of . Additionally, as illustrated in Figures~,~, and~,  has the potential to identify the best arm after a short exploration stage. This results in strong and weak regret bounds of  and , respectively, provided the LLM agent suggests a pair of arms that includes the best arm .

 The worst-case upper bounds on the strong and weak regret hold regardless of the specific prompting scenario, ensuring that  maintains its theoretical guarantees even in the presence of noisy or adversarial prompts, as considered in Theorem~. This robustness is particularly important in practical applications, where the prompts provided to the LLM agent may be subject to various uncertainties or manipulations. By leveraging the strengths of LLMs while incorporating a robust algorithmic framework,  offers a reliable and theoretically grounded solution for DB problems in real-world settings, where the prompting quality may not always be optimal. 

%  [Vulnerable standalone LLM Agent]% % For any constant , there exists an attacker  with budget , such that the standalone learner 's corrupted strong regret  and corrupted weak regret  after the attack are both at least , i.e.,  is neither -strong-robust nor -weak-robust.% % % %     \item According to Lemma , the learner  has expected strong regret and weak regret bounded by  on the original problem.%     \item We introduce an attacker , who can corrupt the learner's internal state at each round , causing the learner to select a suboptimal pair of arms  instead of the intended pair . The attacker's budget is , i.e., the attacker can perform at most  attacks during the  rounds.% % % Consider a Dueling Bandit instance that the Preference Matrix  be such that for any , , where  is a constant. We design an attack strategy  as follows: At each round , when the learner  selects a pair of arms , if at least one of them is the Condorcet winner , the attacker replaces  with a suboptimal arm  with probability ; otherwise, no attack is performed. Here,  is a parameter to be determined later.% Let  be the number of rounds in which the learner  selects at least one optimal arm , and  be the number of rounds the attacker performs an attack. According to the attack strategy, the expected number of rounds in which the attacker performs an attack is:% % This inequality provides an upper bound on the expected number of rounds in which the attacker performs an attack, which helps proof the attacker's behavior in the subsequent proof.% According to the problem setting, we have , where  represents any suboptimal arm, and  is a constant representing the advantage of the optimal arm  over the suboptimal arms. Therefore, whenever the learner selects a suboptimal arm, its expected regret is .% Thus, the total expected regret of the learner for selecting suboptimal arms in  rounds is . By the assumption of strong regret (Bounded by ), we have:% % Since , we can simplify the above inequality to:% % Combining this with , we obtain:% % providing a lower bound on the expected number of rounds  in which the learner selects the optimal arm.% Now, let's reconsider the learner's corrupted strong regret under the attack. In each round, the learner's regret comes from two parts:% 1. When the learner selects two suboptimal arms  and , its regret is , as each suboptimal arm causes its corresponding regret.% 2. When the attacker performs an attack, it replaces the learner's chosen optimal arm with the suboptimal arm , causing an additional regret of .% Therefore, we can express the learner's corrupted strong regret as:% % where  is an indicator function that takes the value 1 when the learner selects two suboptimal arms in round , and 0 otherwise.% Next, we use the definition of  to simplify the above inequality:% % Since , we can rewrite the above inequality as:% % Next, we use the previously obtained lower bound for :% % Substituting this lower bound into the above inequality, we get:% % Since  is a constant, we can omit it, obtaining:% % We use Markov's inequality to estimate the probability that the number of attack rounds  is greater than a certain threshold . For any , we have% % Choosing , where  (As time  increases,  decreases, so the probability of the event  increases. This reflects the growing influence of the attacker on the learner over time), yields% % Let us define , we have:% % % Substituting the lower bound of  into the lower bound of , we obtain:% % Choosing , where , we have:% % Substituting these values into the lower bound of , we get:% % Since  and , we have:% % % Compare the growth rates of  and . Since  (as per the theorem's conditions), we have . This means that  grows faster than  as . Since , we can ignore the  term for sufficiently large . This simplifies the inequality to:% % Since , we have . Substituting this into the inequality:% % % Because:% % We have:% % % This proves that the learner 's corrupted strong regret  after the attack is at least . Similarly, we can prove that the learner 's corrupted weak regret  after the attack is also at least .% Finally, we need to prove that the learner  is not -strong-robust. According to the definition of -strong-robustness, we need to prove% % Since , we have .% When , we have . In this case,% % When , we have . In this case, we cannot directly compare the growth rates of  and . However, we can utilize the condition  to obtain the following derivation:% % The last step holds because when ,  is a constant, while  is a sublinear function of .% Therefore, we have proven that for any constant , the learner  is not -strong-robust. Similarly, we can prove that the learner  is not -weak-robust. This completes the proof of the theorem. % % [Robustness of \ouralg]% % Under the same assumptions as Theorem~, for any constant , the algorithm \ouralg is -strong-robust and -weak-robust with .% % % To prove that \ouralg is -strong-robust, we need to show that its expected corrupted strong regret  is bounded by the robust regret rate  for . % Suppose after  number of steps, the arms suggested by an LLM agent contain the best arm  if no corruption occurs. Let  denote the corruption budget that the attacker has for contaminating the LLM's suggestions.  % Under the same assumptions of \ifalg, with a hyper-parameter , the expected corrupted strong and weak regret of  \ouralg satisfies that %         %    \left[(\ouralg)\right] \leq \min \left\{\left(T_{}+\Phi+{\epsilon_{1,2}}\right),\left({\epsilon_{1,2}}\log T\right)\right\},%      %     %    \left[(\ouralg)\right] \leq \min \left\{O(T_{}+\Phi),\left({\epsilon_{1,2}}\log T\right)\right\},%      % where  hides poly-logarithmic factors of .% For corrupted strong regret, let's analyze the two terms in the  function separately:%  .% Substituting , we get:% % \left(T_{}+\Phi+{\epsilon_{1,2}}\right) &= \left(T_{}+T^\beta+{\epsilon_{1,2}}\right) \\% &= (\max\{T_{}, T^\beta, {\epsilon_{1,2}}\}) \\% &= (\max\{T^\beta, {\epsilon_{1,2}}\}).% % The last step assumes , which is reasonable since the LLM exploration phase is expected to be much shorter than the total number of rounds.%  .% Now, let's compare these two terms with the robust regret rate :% For Term 1, since , we have:% % (\max\{T^\beta, {\epsilon_{1,2}}\}) &= (\max\{}, {\epsilon_{1,2}}\}) \\% &= (\max\{, {\epsilon_{1,2}}\}) \\% &= () \\% &= ().% % For Term 2, we have:% % \left({\epsilon_{1,2}}\log T\right) &= \left({\epsilon_{1,2}}\right) \\% &= \left({\epsilon_{1,2}^2}\log^2 T}\right) \\% &= \left({\epsilon_{1,2}^2}T}\right) \\% &= () \\% &= ().% % where the last step assumes , which is a reasonable assumption for fixed  and .% We have shown that both terms are bounded by the robust regret rate  for .% Therefore, we can conclude that:% % \left[(\ouralg)\right] &\leq \min \left\{\left(T_{}+\Phi+{\epsilon_{1,2}}\right),\left({\epsilon_{1,2}}\log T\right)\right\} \\% &= ().% % This proves that \ouralg is -strong-robust with  for any constant . Similarly, we can also prove that \ouralg is -weak-robust under such conditions.% % competitive ratio Analysis% [Competitive Ratio]% % Let  be the condition number of the preference gaps between arms. Assume that after , there exists a constant  such that the sequence of optimal arm suggestions given by the LLM agent  satisfies that for  rounds, either  or . Then, the competitive ratio of \ouralg with respect to  satisfies:% % % % We analyze the ratio of the cumulative cost between \ouralg and  in three parts.%  In this part, the cumulative cost of \ouralg is at most , while the cost of  is at least 0 and at most . Therefore, the ratio of the cumulative cost between \ouralg and  in this part is at most .%  We consider the  rounds that satisfy  or . According to the Match Arms procedure (Algorithm 1) of \ouralg, starting from the true optimal arm , it takes at most  comparisons to obtain a high-confidence estimate of , where . Therefore, in these  rounds, the average cost per round of \ouralg is at most  times the cost of . %  We consider the remaining  rounds where the optimal arm pair given by the LLM does not contain . Let  be the preference gap between the true optimal arm and the best arm suggested by LLM in round . Then the cost of \ouralg in round  is at most . This is because with  comparisons between each suboptimal arm and , we can identify  with high probability. Summing over the  rounds, the total cost of \ouralg in Part 3 is at most:% % where the last inequality is due to . On the other hand, the total cost of  in Part 3 is at least . Therefore, the ratio between the cost of \ouralg and  in Part 3 is at most: % % Combining the above analyses, for the entire  rounds, we have:% % (\ouralg)}{()} % &\leq }}{T}+\alpha \cdot O\left({\epsilon_{1,2}}\right)+(1-\alpha)\cdot O\left({(1-\alpha)T-T_{}}\right)\\% &=1+O\left(}}{T}+{\alpha}\log\left({\alpha}\log T\right)\right).% % Therefore,% % Regarding , we design a two-fold evaluation to assess efficacy and robustness. The evaluation is conducted on the  instance, which provides higher distinguishability, allowing us to observe convergence and regret differences within a practical number of steps. First, we compare the strong regret of  against state-of-the-art baseline algorithms to validate its efficacy. Second, we investigate the robustness of  with noisy and adversarial prompts.

 In our implementation of  (see Algorithm~), there are two hyper-parameters: the threshold parameter , which controls the maximum number of comparisons between arms, and the confidence parameter , which determines the confidence level for pruning suboptimal arms. For the threshold parameter , we considered values from the set , and for the confidence parameter , we explored values from . After fine-tuning, we found that setting  and  provided the best performance in terms of cumulative regret. These hyper-parameter values strike a balance between the number of comparisons required to identify the best arm and the confidence level for pruning suboptimal arms, enabling  to efficiently explore and exploit the available arms in the dueling bandits setting.

We evaluate the  cumulative strong and weak regret performance of the proposed   with different confidence parameter settings () and :

 Figure~ (Left) demonstrates that  exhibits competitive performance across different  values. When ,  has slightly higher strong regret compared to the state-of-the-art baselines due to the conservative choice of . As  increases to 0.2 and 0.4, the cumulative expected strong regret of  decreases, eventually outperforming both state-of-the-art baselines when . These results highlight a trade-off between robustness and optimality by selecting different confidence intervals in . A smaller  value demands higher confidence in decision-making, leading to more conservative exploration and potentially higher regret. Conversely, a larger  allows for more aggressive exploration, enabling  to identify the optimal arm sooner, leading to reduced cumulative strong regret.

 Figure~ (Right) showcases the superior weak regret performance of . When  and ,  consistently achieves lower cumulative weak regret, demonstrating its ability to effectively identify and include the optimal arm in its comparisons.  As emphasized in Table~, this can be attributed to 's ability to consistently identify the best arm.

% 

Recent studies  have emphasized the importance of varying prompts to elicit the desired behavior from LLMs in decision-making tasks, highlighting the potential limitations of prompt quality. Results obtained from a single prompt template may lead to unreliable conclusions that cannot generalize to real-world situations where optimal prompts are often unavailable. Thus, we evaluate the robustness of  in DB tasks by employing two types of prompt perturbations (see Figure~) along with the original prompt (see Figure~).

 Under the initial prompt,  leverages the LLM's ability to quickly identify the best arm through exploration. As shown in Figures  and  (Top Row), we observe that: (i) Compared to IF2,  benefits from the LLM's exploration ability by initializing with the best arm as the incumbent when entering the IF2 phase. (ii) Compared to , convergence to the Condorcet winner is guaranteed for  with high probability.

 We inject an incorrect history into the prompt, where each non-optimal arm initially wins against the best arm 10 times, while keeping the underlying preference matrix unchanged. In this scenario, standalone LLM agents are observed to get trapped in local optima for extended periods. In contrast,  overcomes this limitation by employing uniform comparisons in the IF2 phase to escape such suboptimal exploration modes (see Figure  (Middle Row)). This effectively addresses the local optima problem highlighted in the disadvantages of standalone LLMs in Table~.

 When the prompt is adversarially modified from maximizing reward to minimizing, the LLM consistently recommends non-optimal arms after its exploration stage. Nevertheless, even with adversarial prompts,  still maintains a robust behavior and achieves near-optimal cumulative strong regret, as shown in Figure  (Bottom Row).  Since the LLM's exploration capability is only utilized within the bounded length of the  procedure, the impact of the reversed goal on the exploitation phase is eliminated.

Figure  presents the cumulative strong regret results comparing  against standalone LLM agents and the IF2 algorithm across three prompt designs. Notably,  with  (consistent with IF2 to showcase the robust behavior) achieves near-optimal cumulative regret with low variance even with noisy and adversarial prompts, validating the regret bounds stated in Theorem . The experiment in this section is extended from 2000 steps to 3000 steps to illustrate 's non-convergence and growing regret.  and IF2 converge to the best arm within 2000 steps, while 's cumulative expected regret continues to increase, indicating the instability of standalone LLM agents.

% In the biased history setting, despite the impacted exploration ability, \ouralg maintains the regret bounds stated in Theorem . The reversed goal scenario has a minimal effect on our method, in contrast to the significant regret incurred by standalone LLMs. This demonstrates that \ouralg effectively leverages the LLM's exploration capability while mitigating the negative impact of suboptimal exploitation strategies. In Procedure~ below, we describe the  procedure used in  (see Algorithm~ and Figure~).

% %  We reprise the  procedure in~ below to complement the presentation of .

It is worth noting the following features of Algorithm~ in its practical implementation.

We introduce the useful assumptions and lemmas for Dueling Bandits that are necessary for the theoretical analysis of our proposed algorithm.

With the TO property satisfied, we assume the preference matrix  further satisfies the following two standard properties~.

Note that the Bradley-Terry-Luce (BTL) model  used in our experiments~ satisfies Assumption~ and~. We restate the following theoretical guarantees for  that is useful in the proof of Theorem~. Let .

The following expected regret bound achieved by  is tight up to multiplicative constants, as indicated by the lower bound (Theorem 4) in~ such that any algorithm  for DB satisfies .

%加一个section

Suppose at each step , aligning with the design of  in~,  is estimated such that each  is the fraction of number of comparisons when  was the winner out of all previous  comparisons. Define a confidence interval  where . Before proceeding to prove Theorem~, we first state a useful lemma from~ as a result of the  Hoeffding's inequality.

Note that Lemma~ can be directly implied by Lemma 1 in~. Now, under Assumption~  and~ such that the preference matrix  satisfies the SST and STI properties, we prove Theorem~.

In this section, we provide the detailed design of the prompts used in our experiments and provide additional results to support our findings. We begin by presenting the original prompt used in the LLM-Env interaction. We then introduce the perturbed prompts, which include both noisy and adversarial variations to test the robustness of our approach. Finally, we provide a five consecutive time steps example of  using original prompt to showcase its behavior in the dueling bandits setting.

% 

We provide an example of five consecutive time steps using the original prompt (see Figure~) to demonstrate 's decision-making process in the dueling bandits setting. This example highlights 's capability to analyze available information, reason about the relative strengths of the arms, and make informed decisions on which pair of arms to duel next.

% 表述细节,每个图的作用(指向正文的哪部分),讲述图与图之间的关系,可以用table来进行总结%

We present supplementary results to complement our case studies. (i) Figure~ shows the strong and weak regret comparisons for the  instance. (ii) Figure~ introduces the Best Arm Ratio, measuring how often the algorithms select the best arm (Condorcet winner) in a duel for both instances. (iii) Figure~ examines the generalized variance of the strong and weak regret for both instances.

% %  We visualize the duel selection trajectory in representative experiments to better understand the behavior of LLM agents and baseline algorithms.

% Arm selection trajectory The reshuffled arm order is , with arm indices from bottom to top: 5, 4, 3, 2, 1. Each filled black cell represents a selected arm at that time step. For instance, black lines in arms 5 and 3 indicate the selection of the duel between (arm 5, arm 3) at that particular time step.

% hard_trace的引用%Figure for IF2% And aug LLM solution%result_reverse, result_trap, result_original三个引用%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % %%% BEGIN INSTRUCTIONS %%%% % The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page% % limit. % % Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:% % % %     \item You should answer , , or .% %     \item  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.% %     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). % %    % \item {\bf The papers not including the checklist will be desk rejected.}% % % % {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.% % The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "" is generally preferable to "", it is perfectly acceptable to answer "" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "" or "" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer  to a question, in the justification please point to the section(s) where related material for the question can be found.% % IMPORTANT, please:% % % %     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},% %     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}% %     \item {\bf Do not modify the questions and only use the provided macros for your answers}.% %  % %%% END INSTRUCTIONS %%%% % \item {\bf Claims}%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?%     \item[] Answer: %     \item[] Justification: The claims in abstract and introduction provide a clear and accurate representation of the paper's content, and are thoroughly investigated and validated through extensive experiments and theoretical analysis in the main text.%     \item[] Guidelines:%     %         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. %         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. %         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. %     % \item {\bf Limitations}%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?%     \item[] Answer: %     \item[] Justification: In the conclusion section, we provide a discussion of the limitations of this work, including the number of arms, other possible preference structures and dueling bandit variants.%     \item[] Guidelines:%     %         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. %         \item The authors are encouraged to create a separate "Limitations" section in their paper.%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.%     % \item {\bf Theory Assumptions and Proofs}%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?%     \item[] Answer: %     \item[] Justification: The paper provides theoretical proof of the expected regret bounds for \ouralg algorithm in Appendix. %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include theoretical results. %         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. %         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. %     %     \item {\bf Experimental Result Reproducibility}%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?%     \item[] Answer:  % Replace by , , or .%     \item[] Justification: The paper includes a comprehensive description of the experimental setup, covering the prompts, LLM configurations, baselines, environments, and random tests. This detailed information allows for the reproducibility of the main experimental results.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. %         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example%         %             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.%         %     % \item {\bf Open access to data and code}%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?%     \item[] Answer: %     \item[] Justification: We will provide the source code in the following phase.%     \item[] Guidelines:%     %         \item The answer NA means that paper does not include experiments requiring code.%         \item Please see the NeurIPS code and data submission guidelines () for more details.%         \item While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines () for more details.%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.%     % \item {\bf Experimental Setting/Details}%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?%     \item[] Answer: %     \item[] Justification: The paper provides a comprehensive description of the experimental setting, which includes information on the prompts, LLM configurations, baselines, environments, and random tests. This level of detail is necessary to understand and interpret the experimental results and is appropriately presented in the main text of the paper.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.%         \item The full details can be provided either with the code, in appendix, or as supplemental material.%     % \item {\bf Experiment Statistical Significance}%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?%     \item[] Answer: %     \item[] Justification: The paper reports error bars in all the Figures related to random tests for assessing the statistical significance of the main experimental results. %     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)%         \item The assumptions made should be given (e.g., Normally distributed errors).%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.%     % \item {\bf Experiments Compute Resources}%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?%     \item[] Answer: %     \item[] Justification: We use OpenAI API for LLM experiments as mentioned in the main text.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not include experiments.%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. %         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). %     % \item {\bf Code Of Ethics}%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ?%     \item[] Answer: %     \item[] Justification: The research presented in the paper conforms to the NeurIPS Code of Ethics. %     \item[] Guidelines:%     %         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).%     % \item {\bf Broader Impacts}%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?%     \item[] Answer: %     \item[] Justification: The paper has explicitly discussed the potential positive or negative societal impacts of the work performed. %     \item[] Guidelines:%     %         \item The answer NA means that there is no societal impact of the work performed.%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).%     % \item {\bf Safeguards}%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?%     \item[] Answer: %     \item[] Justification: The paper does not involve the release of data or models that have a high risk for misuse.%     \item[] Guidelines:%     %         \item The answer NA means that the paper poses no such risks.%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. %         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.%     % \item {\bf Licenses for existing assets}%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?%     \item[] Answer: Final answer:%     \item[] Justification: The paper does not use existing assets, such as code, data, or models, that would require proper crediting or licensing information.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not use existing assets.%         \item The authors should cite the original paper that produced the code package or dataset.%         \item The authors should state which version of the asset is used and, if possible, include a URL.%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,  has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.%     % \item {\bf New Assets}%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?%     \item[] Answer: .%     \item[] Justification: The paper does not release new assets.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not release new assets.%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. %         \item The paper should discuss whether and how consent was obtained from people whose asset is used.%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.%     % \item {\bf Crowdsourcing and Research with Human Subjects}%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? %     \item[] Answer: .%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. %         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. %     % \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?%     \item[] Answer: .%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.%     \item[] Guidelines:%     %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. %         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. %         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.%     % -6mm

In-context decision-making is an important capability of artificial general intelligence, which Large Language Models (LLMs) have effectively demonstrated in various scenarios. However, LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper investigates the performance of LLMs as decision-makers in the context of Dueling Bandits (DB). We first evaluate the performance of LLMs by comparing , , and  against established DB algorithms. Our results reveal that LLMs, particularly , quickly identify the Condorcet winner, thus outperforming existing state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle to converge even when explicitly prompted to do so, and are sensitive to prompt variations. To overcome these issues, we introduce an LLM-augmented algorithm, , which takes advantage of both in-context decision-making capabilities of LLMs and theoretical guarantees inherited from classic DB algorithms. The design of such an algorithm sheds light on how to enhance trustworthiness for LLMs used in decision-making tasks where performance robustness matters. We show that  has theoretical guarantees on both weak and strong regret. Our experimental results validate that  is robust even with noisy and adversarial prompts. 

% This paper investigates the performance of LLMs as decision-makers in the context of dueling bandits (DB), an important variant of the MAB problem. We conduct a comprehensive experimental study comparing state-of-the-art LLMs, including \gpta, \gptb, and \gptc, against well-established baseline algorithms in DB environments. Our results reveal that while LLMs, particularly \gptc, exhibit competitive performance in terms of weak regret, they struggle to converge to the optimal arm in the long run, as evidenced by their linearly growing strong regret. We identify and analyze the successes and failures of LLMs in the DB setting, highlighting their ability to quickly identify the best arm in the early stages of exploration but also their susceptibility to suboptimal decision-making over extended time horizons. To address these limitations, we propose an algorithm-enhanced LLM approach that combines the strengths of both LLMs and classical DB algorithms. Our findings shed light on the capabilities and challenges of LLMs as in-context decision-makers and pave the way for the development of more effective hybrid approaches in complex decision-making tasks.-1mmIntroductionpark2024llm,krishnamurthy2024canpark2024llmkrishnamurthy2024canyue2012kyue2012k,zoghi2014relativelattimore2020banditpreference feedbackfig:systemyue2009interactivelysui2017multizoghi2014relativeyue2012kAre LLMs effective in-context agents for solving the problem of dueling bandits?-7mmailon2014reducing, saha2022versatilestiennon2020learningwang2024rlhfperlis2024clinicaljiang2023legalxie2023wallEvaluation of LLMs' capabilities for preference feedback.sec:llm_mainkrishnamurthy2024cantab:llm_summaryEffective and Robust LLM-augmented DB algorithm.sec:llm_enhanced_alg-3mmPreliminaries-1mmsec:preapp:preDueling Bandits.fig:systemdistinguishabilityyue2012kLLM Agents for Dueling Bandits.sec:prompt_notationfig:original_prompt-3mmContext : a natural language description of the -armed dueling bandit problem, including the number of arms , the time horizon , and the task objective.

  -0.5mmHistory : an externally summarized interaction history up to round , which includes a sequence of pairwise dueling results and the empirical probabilities. 

  -0.5mmReasoning : the zero-shot chain-of-thought (CoT) reasoning  that encourages the LLM agent to reason about the problem in a structured manner. kojima2022large

\left(_1(t), _2(t)\right) = \pi_{}\left((C, H_t, R)\right). eq:strong_regreteq:weak_regretStrong and Weak Regret.sui2017multi,wu2016double,zoghi2014relative,yue2012kstrong regret

   () \coloneq \sum_{t=1}^T \Big( \epsilon\left(b^*, _1(t)\right) + \epsilon\left(b^*, _2(t)\right) \Big).  T weak regret          ()  \coloneq \sum_{t=1}^T \min\Big( \epsilon\left(b^*, _1(t)\right) , \epsilon\left(b^*, _2(t)\right) \Big). sec:resultsapp:pre-5ptRelated Workssec:related_workDueling Bandits.yue2012kzoghi2014relativeIFyue2012kBTMyue2011beatRUCBzoghi2014rucbsui2017multiyue2012ktucker2020preferenceouyang2022trainingLLM Agents for  Multi-Armed Bandits.baheri2023llmskrishnamurthy2024cankrishnamurthy2024canIn-Context LLMs for Decision-Making.wei2022chain, wang2022self, zhou2022least, yao2024treehuang2022inner, hao2023reasoningbrohan2023canma2023eurekawang2023voyagerpark2024llmliu2024largeLLMs as Standalone In-Context Decision-Makerssec:llm_mainfig:easy_mainfig:hard_maineq:strong_regreteq:weak_regretImplementation Details of Experimentssec:experimentsPrompts and Configurations of LLMs.sec:prompt_notationkrishnamurthy2024cansec:promptBaselines.IF2yue2012kBTMyue2011beatSAVAGEurvoy2013genericRUCBzoghi2014rucbRCSzoghi2014relativeRMEDkomiyama2015regretsui2017multiDTSwu2016doublesec:preEnvironments.bradley1952rank,yue2012kplackett1975analysisEasyHard instance: , . %     Easy instance: , . Hardyue2012kapp:preRandom Tests.-5ptExperimental resultssec:resultsEasyfig:easy_mainHardfig:hard_mainapp:supp_expBTMRMEDRUCBRCSDTSsec:preStrong Regret.fig:easy_mainEasyHardDTSHardEasyHardfig:hard_mainWeak Regret.fig:easy_mainHardfig:hard_mainapp:supp_expfig:easy_mainfig:hard_main-1mmAdvantages of LLM Agentssec:advantagesEfficient Best Arm Identification.fig:easy_mainfig:hard_mainfig:fractionapp:supp_expLow Variance of Regret.fig:varianceapp:supp_expEasyHardfig:varianceBTMSAVAGE-1mmDisadvantages of LLM Agentssec:disadvantagesfig:compare_tracefig:localapp:traceEasyHardzoghi2014relativeExploration Vulnerability.fig:localConvergence Instability.fig:compare_trace1pt-0.5ptSummary of Experimental Results.tab:llm_summaryCan we develop an LLM-augmented approach for DB with a theoretical guarantee?How does it perform compared to standalone LLM agents and classic DB algorithms?-5ptAlgorithm-Enhanced LLMs for Dueling Banditssec:our_alg-5ptyue2012ktab:llm_summary-10pt-1mmAlgorithm Design of \ouralgsec:llm_enhanced_alg-1mmyue2012kLimitations of Naive Intervention.fig:llm_ctapp:supp_expDesirable Properties for LLM Augmentation.IF2 as an Ideal Candidate.-3mmIts Explore-Then-Exploit~ structure naturally aligns with the LLMs' tendency to keep exploring without converging (see Figure~), allowing for effective leveraging the LLMs' exploration behavior while mitigating their exploration vulnerability and convergence instability (see Table~).

zoghi2014relativefig:compare_tracetab:llm_summary-1mmIts symbolic representation of the algorithm's logic enables clear integration of LLM suggestions at specific points without disrupting the overall structure and theoretical guarantees. In contrast, algorithms like Self-Sparring~ are less symbolic, making them less suitable for direct LLM augmentation.

sui2017multi-1mmIts strong theoretical guarantees, with an expected regret bound of  matching the DB problem's lower bound of  up to constants (see Appendix~), and empirical performance (see Figures~ and~) provide a robust foundation, ensuring convergence and bounded regret. app:prefig:easy_mainfig:hard_main-4mmfig:compare_traceAlgorithm Description.fig:system_if2alg:llm_algorithmapp:llm_augmented_alg-3mm : The algorithm maintains a set of candidate arms . Given two arms suggested by an LLM agent, the algorithm begins with finding a winner between them, denoted by . The winning arm  is then matched with each remaining arm , and  is removed from  if beaten by  with confidence . This phase continues until  is defeated or all arms in  have been matched. The variable  is used to control the execution of the LLM phase, and it is set to False when  is defeated by another arm, indicating that the LLM's suggestions are no longer trusted.

Phase 1 (LLM Phase):Utilization of LLM recommended armsTrustLLM-1mm   : If  is defeated, the algorithm switches to implementing one round of  with an incumbent arm  selected based on an estimated preference matrix .  Phase 2 (\ifalg Phase):Roll back to \ifalg-2mmPhase 2Phase 1alg:llm_algorithmfig:system_if2app:llm_augmented_algLLM-augmented Agent:~\ouralgalg:llm_algorithmInputInitialize      \hfill 

    \While 

     \hfill \tcc*

    Run \textsc (Procedure~\ref) \hfill      % \colorbox[HTML]{B0B0B0}{\tcc*[r]{\ifalg Phase in Figure~ (Lines 11 to 12)}} OutputOutputEstimated best arm , Total number of comparisons -3mmTheoretical Guarantees for \ouralgsec:our_alg_theory-3mmassump:1thm:vulnerable_llmthm:alg Under the original prompt (see Figure~), the worst-case behavior of an LLM agent in the dueling bandit setting is equivalent to a randomizer that selects actions uniformly at random. assump:1fig:original_prompt-1mmVulnerability of Standalone LLM Agents.hajiesmaili2020adversarialassump:1 For the dueling bandits problem with  arms and time horizon , there exists a preference structure and an attacker strategy with budget , such that any standalone LLM agent, whose policy is represented by Eq.() and whose worst-case behavior under the original prompt satisfying Assumption~, will suffer an expected regret of . thm:vulnerable_llmeq:policyassump:1thm:vulnerable_llmapp:theoryExpected Regret Bounds of \ouralg.sec:llm_enhanced_algapp:pre Suppose after  number of steps, the arms suggested by an LLM agent contain the best arm .  Under the same assumptions of , with a hyper-parameter , the expected strong regret of   satisfies that 

and the expected weak regret of   is bounded from above by

where  hides poly-logarithmic factors of . thm:alg    \left[(\ouralg)\right] \leq \min \left\{\left(T_{}+{\epsilon_{1,2}}\right),\left({\epsilon_{1,2}}\log T\right)\right\},

   \left[(\ouralg)\right] \leq \min \left\{O(T_{}),\left({\epsilon_{1,2}}\log T\right)\right\},     thm:algapp:theoryapp:prethm:algEfficacy:yue2012kfig:easy_mainfig:hard_mainfig:compare_traceRobustness:thm:vulnerable_llmEmpirical Evaluation of \ouralgsec:our_alg_empiricalEasyEfficacy Evaluation: Strong Regret and Weak RegretHyper-parameters.alg:llm_algorithmStrong Regret.fig:our_regretWeak Regret.fig:our_regrettab:llm_summary-2mmRobustness Evaluation: Noisy and Adversarial Prompts% 'r' for right, and 0.5\textwidth for half the width-1mmwidth=0.48\textwidthimages/llm_aug_perturb.pdf-15ptRobustness evaluation under prompt perturbations. The specific noisy (biased history) and adversarial (reversed goal) prompts are shown in Figure~\ref.fig:perturb-5ptkrishnamurthy2024can, loya2023exploringfig:new_promptfig:original_promptOriginal Prompt.fig:result_IF2fig:our_resultBiased History.fig:our_resulttab:llm_summaryReversed Goal.fig:our_resultMatch Armsfig:perturbthm:alg-3ptConclusion-3ptLimitations and Future Works.thm:algzoghi2014rucbRUCBzoghi2014rucbBroader Impacts.unsrt\bibliographyAppendix-2mm\setlength \begin      \item \textbf     \begin         \item Appendix~\ref describes the \ouralg algorithm stated in Section~\ref, detailing its key features and implementation remarks.         \item Appendix~\ref presents the necessary assumptions and lemmas for the theoretical analysis of \ouralg in Section~\ref.         \item Appendix~\ref proves Theorem~\ref and~\ref, establishing \ouralg's regret bounds.     \end

    \item \textbf      \begin         \item Appendix~\ref illustrates the prompt design and prompt perturbations logic.         \item Appendix~\ref provides a five consecutive time steps example of \gptc to showcase its behavior.         \item Appendix~\ref presents supplementary experimental results, providing further insights into the performance and behavior of the algorithms in Sections~\ref and \ref.     \end \end -15ptAlgorithm Design and Analysis of \ouralg-5ptthm:vulnerable_llmthm:algapp:pre-5ptDetailed Procedure Descriptionapp:llm_augmented_algprocedure:1Match Armsalg:llm_algorithmfig:system_if2algorithm0\textsc\textbf: Two arms ,  confidence parameter , and threshold  and       \While\Return , ,  procedure:1yue2012k-5pt: Incumbent arm , candidate arms , confidence parameter 

\textscprocedure:if2Input

\While\Return   such that  and 

\lWhile procedure:2alg:llm_algorithm     The LLM Phase allows for flexible exploration design within the bounded length of the  procedure, not limiting the number of prompts and comparisons performed by the LLM to identify an empirically best arm. Match Arms     The bound length in the  procedure can be adjusted based on empirical requirements. Modifying the confidence parameter  and the threshold  will affect the regret bound and the algorithm's performance. These parameters can be tuned to balance exploration and exploitation, depending on the specific application and desired level of confidence. Match ArmsTheoretical Analysisapp:proofUseful Assumptions and Lemmas for Dueling Banditsapp:pre The preference matrix  satisfies the Total Ordering (TO) property such that for all ,  implies .  yue2009interactively,yue2011beat,yue2012k The preference matrix  satisfies the Strong Stochastic Transitivity (SST) such that for any arms  such that  under the total order , we have . ass:sst The preference matrix  satisfies the Stochastic Triangle Inequality (STI) such that for any arms ,  we have .  ass:stibradley1952ranksec:experimentsass:sstass:stithm:alg Assuming the preference matrix  satisfies the SST and STI, then  has its expected regret (both weak and strong) bounded from above by lemma:if2 

    [(\ifalg)] \leq O\left({\epsilon_{}}\log T\right). yue2012kAlgTheoretical Guarantees of \ouralgapp:theoryPart I: Vulnerability of Standalone LLM Agents

Consider the following Dueling Bandit instance with  arms  and preference matrix :

where  is a small constant. In this instance, arm  is the unique Condorcet winner, and all other arms are suboptimal with a gap of  to .

Now, consider an attacker strategy with budget : Whenever the LLM agent selects a duel containing the optimal arm , the attacker manipulates the input prompt to the LLM agent (as described in Eq.~()) to eliminate  from the comparison with probability  (where  is a constant), subject to the constraint that the attacker can perform at most  attacks over the  rounds.

Let  be the number of rounds in which the LLM agent selects a duel containing the optimal arm  up to round . Due to the attacker's manipulation of the input prompt, in each of these  rounds,  is eliminated from the comparison with probability . However, because of the attacker's budget constraint, the actual number of attacked rounds is at most .

In the rounds where  is eliminated from the comparison, the LLM agent can only select from the suboptimal arms . Let  denote the suboptimality gap of arm  with respect to . Then, the expected regret incurred in each round where  is eliminated from the comparison is at least .

Thus, the expected cumulative regret of the LLM agent after  rounds is at least:

where the first inequality follows from the regret incurred in rounds where  is eliminated from the duel, and the second inequality holds due to Jensen's inequality and the linearity of expectation.

According to the Assumption~, in the worst case, the LLM agent's behavior is equivalent to randomly selecting a duel in each round. For  arms, there are  possible duel combinations. Therefore, the probability of selecting a duel containing  in each round is , which yields . The regret bound becomes:

Therefore, any standalone LLM agent whose policy is represented by Eq.~() under the worst-case assumption will suffer an expected regret of . This lower bound demonstrates the vulnerability of solely relying on LLM agents for DB in adversarial environments when the attacker can manipulate the input prompts.

P_{i,j} =  0.5 + \epsilon, &  b_i = b^*  b_j \neq b^*, \\ 0.5 - \epsilon, &  b_i \neq b^*  b_j = b^*, \\ 0.5, & 

eq:policy-1mm [(T)] \geq p \cdot [\min\{N(T), \Phi(T)\}] \cdot \epsilon \geq p \cdot \min\{[N(T)], \Phi(T)\} \cdot \epsilon, assump:1 [(T)] \geq p \cdot \min\left\{{K}, \Phi(T)\right\} \cdot \epsilon = \Omega\left(\min\left\{{K}, \Phi(T)\right\}\right). eq:policyPart II: Expected Regret Bounds of \ouralgyue2012kthm:algyue2012k     With a confidence parameter  with , a winner between two arms  and  is identified with probability at least , using at most   number of comparisons. lemma:match-1mmlemma:matchyue2012kass:sstass:stithm:alg-1mm

Suppose the arms suggested by LLM agent includes the best arm  after exploring  steps. We prove the two bounds shown in Theorem~ one-by-one.

The first  steps induce accumulated weak/strong regret of at most . Applying Lemma~, with   (by setting a hyper-parameter ) number of comparisons between two arms, since the best arm  is always included in each comparison, the best arm  is correctly identified with probability at least . This procedure leads to no weak regret since  suggested by the LLM agent is always included as the incumbent arm in future comparisons. Regarding the strong regret, the union bound implies that with probability ,  will win all the  matches, thus the expected  strong regret induced by the steps after time  can be bounded by

since there are at most  matches.

Furthermore, consider the adversarial selection of arms from the LLM agent. According to Lemma~, the  procedure with  is implemented at most  times with probability , provided with .  Finally, each implementation of Procedure~ costs regret (both strong and weak) of at most  since there are at most  comparisons of pairs. Applying the expected regret bound in Lemma~ completes the proof.

thm:algFirst Bound.lemma:match-1.5mm     &[(\ouralg T_{})] \\     \leq & \left(1-{K\log T}\right)O\left({\epsilon_{1,2}}\right) + {K\log T}O\left({\epsilon_{1,2}}{\log T}\right)\\     = \ & \left({\epsilon_{1,2}}\right) Second Bound.lemma:matchprocedure:if2lemma:if2-7mmPrompt Design and Supplementary ResultsLLM Experiments-3mmDesign of Promptssec:prompt-30mm-40mm-10mmExemplars of \gptcsec:examplefig:original_prompt \\      \\     Since no duels have been conducted yet, all empirical probabilities between arms are at their initial value of 0.50, indicating no preference or historical data to suggest one arm is better than another. To begin identifying the Condorcet Winner, we need to start collecting data by conducting duels between different arms. A balanced approach would be to start dueling pairs that have not yet competed, but since all pairs are at the same starting point, we can choose any pair. \\

     \\     (1, 2) Answer at Time Step 0:1. Analysis:2. Next Pair: \\      \\     From the results provided, the only conclusive duel so far is between Arm 1 and Arm 2, where Arm 1 won, giving P(1 > 2) = 1.00. This suggests that Arm 1 might be stronger than Arm 2, but more data is needed to confirm its strength relative to other arms.     All other pairwise comparisons have not yet resulted in any duels, so their probabilities remain at the initial estimate of 0.50, indicating no preference between the arms based on the data.     To better understand the relative strengths of the arms and potentially identify the Condorcet Winner, it is crucial to increase the number of duels among arms with less information.

     \\     Given that Arm 1 has shown some potential by winning against Arm 2, it would be informative to test Arm 1 against another arm. Since Arm 1 has not yet been tested against Arm 3, Arm 4, or Arm 5 in a conclusive duel, any of these could be a good choice.     However, to balance the exploration among other arms and gather more data on their performance, let's choose a pair that has not been tested yet. Arm 3 and Arm 4 have not dueled conclusively, and understanding their relative strength could provide insights into the overall hierarchy of the arms. \\

    Choose: (Arm 3, Arm 4) Answer at Time Step 1:1. Analysis:2. Next Pair: \\      \\     - Arm 1 vs Arm 2: Arm 1 has a clear advantage over Arm 2 with a win probability of 1.00. \\     - Arm 3 vs Arm 4: Arm 3 has a clear advantage over Arm 4 with a win probability of 1.00. \\     - Other Duels: All other duels have resulted in a 50\% win probability for each arm, indicating no clear advantage based on the limited data available. \\

    Given these results, Arm 1 and Arm 3 appear to be stronger contenders in their respective duels. However, more data is needed to confidently identify the Condorcet Winner. To gather more information, it would be strategic to pit arms against each other where the win probabilities are still uncertain (i.e., 0.50), especially focusing on those arms that have shown potential strength. \\

     \\     (Arm 1, Arm 3) Answer at Time Step 2:1. Analysis:2. Next Pair: \\      \\     - Arm 1 has shown strong performance against Arm 2 and Arm 3, winning all duels against them. However, it has not yet won against Arm 4 and Arm 5, with the results still uncertain (P=0.50). \\     - Arm 3 has won against Arm 4, suggesting it might be stronger than Arm 4. \\     - Arms 4 and 5 have not won any duels yet, and their probabilities against other arms are still at 0.50, indicating uncertainty and lack of data. \\     - Arms 2, 4, and 5 have not won any duels, and their probabilities against other arms are mostly at 0.50, indicating a need for more data to determine their strengths. \\

    Given this analysis, it seems prudent to focus on gathering more information about the arms that have uncertain outcomes and have not shown strong winning probabilities. Specifically, more data is needed for Arm 1 against Arms 4 and 5, as well as between Arms 4 and 5 themselves, to better understand their relative strengths. \\

     \\     (Arm 1, Arm 4) Answer at Time Step 3:1. Analysis:2. Next Pair: \\      \\     - Arm 1 has shown strong performance against Arms 2, 3, and 4, winning all duels against them. However, it has not yet won against Arm 5, with the probability of winning currently estimated at 0.50. \\     - Arm 3 has also shown some strength by winning against Arm 4. \\     - The other arms (2, 4, 5) have not won any duels yet, and their probabilities of winning against each other are all estimated at 0.50, indicating no clear evidence of superiority among them. \\     - Given the current data, Arm 1 appears to be a strong candidate for the Condorcet Winner, but more evidence is needed, particularly in its performance against Arm 5. \\

    To further clarify the potential of Arm 1 and to explore the capabilities of Arm 5, which has not lost yet but also has not won (except in an undecided duel against Arm 1), it would be strategic to have them duel again. This will help in determining if Arm 1 can consistently outperform Arm 5 or if Arm 5 has potential that hasn't been fully revealed yet. \\

     \\     (Arm 1, Arm 5) Answer at Time Step 4:1. Analysis:2. Next Pair:Supplementary Experimentsapp:supp_expComparisons with Different Metricsfig:hard_mainHardfig:fractionfig:variance-10ptDuel Selection Trajectoryapp:traceDuel Selection Trajectory Explanation:LLM Agents and Baseline Algorithms\ouralg