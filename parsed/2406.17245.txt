Continual learning~ aims to tackle the challenges that arise within the ongoing sequence. Formally, tasks  arrive in sequentially. Each task  contains a separate target dataset with the size of . For any time step , the model is expected to not only adapt itself to the -th task, but also retain its capabilities across all the previous tasks it has been trained on. This study explores two distinct CL settings. In the first setting, where only the MIGU method is employed, the task label is unavailable during the training and testing phases. Secondly, when combined with the three existing types of CL techniques, the model can be exposed to old task data or task information during the training phase.

Our approach employs a two-step process to leverage the inherent differences in magnitude distributions across various tasks for continual learning.: 1) Caching output magnitudes and 2) Updating gradient via a magnitude-based mask. We show the process in Figure~. To illustrate our method, we first consider the fundamental component in LMs, a single linear layer with weight   and only feed an input token  into LMs.

Given the weight matrix , we interpret the columns of  as a set of  vectors, each with dimension :

Given the input vector of the layer , the operation of the layer can be viewed as the dot product between  and each weight vector : 

We then compute the normalized product magnitude  using the L1-norm by : , where  denotes the L1-norm.  Thus, we have the L1-normalized magnitude product distribution vector  for .

After calculating the gradient in the backward phase, we obtain the gradient matrix  for the weight , which presents the optimization direction given the input .  We then define a mask matrix  to partially mask  using the L1-normalized product magnitudes cached during the forward phase.  Formally, we sort the product magnitudes in the descending order and mask the corresponding gradients as follows:

where  is the threshold ratio to mask gradient,   is the actual number t to mask,  is the floor rounding.  The model update rule is then given by:

where  is the learning rate. This formulation ensures that only those weights with L1-normalized magnitudes exceeding the threshold  are updated.

In practice, to apply MIGU, we average the product magnitudes of all tokens on a batch to generate the mask for simple implementation.

 For a Transformer block, we apply our method from Section~ to the Query, Key, Value, and Output linear layer of the multi-head attention (MHA) component, and two~(for T5 and RoBERTa) or three~(for Llama) linear layers in the FFN component.

We also implement MIGU for parameter-efficient finetuning (PEFT) of LMs, particularly we employ Low-Rank Adaptation (LoRA)~. The standard LoRA is mathematically represented as follows:

where  denotes the input representation of the layer,  and  are the low-rank matrices,  is a scaling constant,  is the original weight matrix of the standard linear, and  is the output after applying the LoRA transformation.

To implement MIGU, we apply the same method in Section~ for the matrix . But for the matrix , we use the output of  in Equation~ rather than the output of  in Equation~ to compute the magnitude distribution vector.

We evaluate our approach to continual finetuning on T5-large using the standard CL benchmark and long sequence benchmark.  We follow the setup from  to shuffle the four text classification tasks from the LM dataset ~ into three different orders to form Order 1, 2, 3 for standard CL benchmark. Similarly, we shuffle a mix of 15 tasks (five classification tasks, nine GLUE and SuperGLUE tasks, and the IMDB dataset) to form Orders 4, 5, and 6 for the long sequence benchmark.  For the details on benchmark and sequence, please refer to the appendix~.

We separate the baselines into two categories: without old data or task information  and with old data or task information during training.  For the first category, we include vanilla , which trains all model parameters on a sequence of tasks, and vanilla , in which fixed-size LoRA parameters are trained on a sequence of tasks. For the second category, we have rehearsal-based approaches:  that trains new tasks on LoRA with mixing a 2\% past task,  continuously trains a soft prompt that simultaneously learns to solve the tasks and generate training samples for experience replay; architecture-based approaches:  that incremental learning of new LoRA parameters on a sequential series of tasks, ~, a vanilla MoE with LoRA number equals to the task number, ~ extends IncLoRA by aligning learning process and selection process of LoRA, and ~ continually adds new modules and composes them with existing modules;  parameter-based approaches ~ extends IncLoRA to learn different LoRAs into orthogonal subspaces.

ACC (Accuracy~). The average performance of all tasks after training on the last task, i.e., .

Table~ shows that our proposed approach (+MIGU) improves the performance of all five CL approaches.  Notably, when our method is applied, the vanilla FT and LoRA baselines see substantial improvements.  Some results obtained using our approach are comparable to the SOTA CL methods that leverage task labels or old task data.  Notably, the LoRA+MIGU approach surpasses the vanilla LoRA method by a substantial 15.2\% on the long sequence benchmark, significantly mitigating the drawbacks of LoRA in the CL setting with long sequences.  We choose to combine our method with three LoRA-based techniques to integrate with three CL approaches that leverage old data or additional labels.  The parameter-based IncLoRA+MIGU exhibits the most significant improvement over the original IncLoRA, implying that our magnitude-based approach can effectively mitigate the conflicts among the sequentially learned LoRA parameters in IncLoRA.  The relatively marginal improvement of parameter-based OIncLoRA+MIGU indicates a similar function between our approach and projecting LoRAs into orthogonal subspaces, but our method does not require task labels during the continual training process.  SAPT-LoRA achieves the SoTA performance in long sequence benchmark, but it requires both task labels and past data, which are often infeasible or costly in LMs settings.  We also report an efficiency study in Appendix~ Table~ to show our approach only leads to a minor overhead over the vanilla methods, which is assumed to be more efficient than other CL methods. We provide a full experiment in Appendix~ Table .  We also draw the Violin Plot to show the statistical significance of our approach over baselines in Appendix .

In contrast to the previous continual finetuning setting,  introduces DAS, a new benchmark for continual pre-training of LMs.  DAS is composed of six unlabeled domain corpora, which contain three review domains and three academic paper domains. It is then evaluated using six corresponding classification datasets. Please refer to the Appendix  for the details. 

 For continual pre-training, we utilize MF1 (Macro-F1) and ACC (Accuracy) following  to evaluate the performance after pre-training on the last domain.

 We choose top baselines ranging from vanilla methods that pre-train RoBERTa on domains sequentially with full parameters  and with PEFT  to rehearsal-based (~), architecture-based (~), and parameter-based ~ and ~. %  We evaluate MIGU in another setting in which, we continually pre-train a RoBERTa model to six domains sequentially (domain-adaptive pre-training). Our experimental results in Table  also show promising results of our approach over or on par with the sophisticated CL methods with task labels or old data. For instance, FT+MIGU achieves 0.37\% improvement in MF1 and 0.42\% in ACC. We also explore the performance of the domains in different orders.  We report the average ACC of the first and last two learned domains in Table . The results indicate that while the DAS model exhibits less forgetting in the earlier learned domains, but it also learns less in the last domains, possibly due to the strong regularization used to constrain its parameter updates during the CL process over a long sequence. In contrast, MIGU demonstrates a more sustainable method, exhibiting robust performance on the earlier and recently learned domains.

We further assess our approach on a more demanding LLM continual instruction tuning setting. We finetune a base Llama2-7B on Magicoder-Evol-Instruct-110K for 32 epochs.  This dataset~ contains 72.97M tokens of programming questions and answers. However, due to computation constraints, we sample 20\% of data and conduct experiments on LoRA.  We follow~ to assess LoRA+MIGU's capabilities on both the base ability (forgetting domain) and the code ability (learning domain).  To evaluate code learning performance, we utilize the Humaneval benchmark~, which contains 164 problems that generate a Python program with a docstring and a function signature. A generation is considered correct if it passes all supplied unit tests.  To quantify how much they have forgotten previous knowledge, we follow~ that utilizes average scores of three benchmarks, HellaSwag~, WinoGrade~ and ARC-challenge~.  The experiments are shown in Figure . Compared to baseline FT, our method learns a similar level of new code knowledge but exhibits significantly less forgetting of previous knowledge. This suggests our approach achieves a better trade-off point on the Pareto frontier between learning plasticity and memory stability~.  For example, after 32 training epochs, the average accuracy across the three benchmarks for our method is 59.4, while the baseline model only achieves 58.4.

We plot all five curves of our approach (+MIGU) for gradient mask threshold from 0.0 to 0.9 in Section~.  The optimal threshold value for FT+MIGU, LoRA+MIGU, and IncLoRA+MIGU settings is 0.7 while LoRAReplay+MIGU is 0.4 as shown in Figure~. OIncLoRA+MIGU is only 0.1, which may due to the parameter updating regularized by the OIncLoRA method itself. The optimal value for IncLoRA+MIGU is 0.6, close to FT+MIGU, LoRA+MIGU, and IncLoRA+MIGU settings.  % 

Surprisingly, with only 5\% () or 1\% () parameters updating, LoRA+MIGU still beats LoRA by a wide margin.  This interesting finding may indicate that only a small proportion of proportional weights with large magnitudes is crucial for successful CL settings, which may be worth future investigation.

We further investigate which components within a transformer block should utilize MIGU.  Typically, a transformer block consists of six linear layers: the query, key, and value (QKV) linear layers and the output linear layer (O) in the MHA module, as well as the two linear layers in the FFN. Our analysis in Table~ shows that employing MIGU across all these linear layers achieves the best overall performance, suggesting that the magnitude-based approach is effective for linear layers in different parts of the transformer architecture.

We evaluate task similarity by counting the overlapping ratio of updated parameters (large magnitudes) positions by using 100 samples per task. In Figure , we visualize the task similarity for the first layer of FFN in the last Transformer block of T5-large, comparing FT and FT+MIGU in the Order 6 setting.  The results clearly show that MIGU increases the degree of parameter isolation across tasks, achieving a similar effect by using task information but without relying on such explicit task labels. We further highlight the similarity between the BoolQA, COPA, and Yelp tasks and the notable decrease in similarity among these three tasks.  Analyzing the performance results shown in Table , we find that the significant reduction in overlapping ratio across tasks considerably alleviates the task conflicts, resulting in much more significant performance gains. For example, the accuracy improvement for the COPA dataset is exactly 10\%. We put the full visualization of all linear layers in Appendix~. 

We adapted the code-base from O-LORA. We adapted the code-base from DAS. Our experimental section encompasses datasets including the  and , both of which are utilized for instruction finetuning on the T5-large model; the , which is used for continual pre-training on RoBERTa; the , which pertains to instruction tuning on Llama-2-7B; and the datasets , , and  for evaluating the finetuned Llama-2-7B.

 For continual finetuning, we use MTL5 dataset introduced by~, and follow the setup from LFPT5 and O-LoRA  to pick four text classification datasets (AG News, Amazon reviews, DBpedia and Yahoo Answers) and shuffle the tasks into three different orders.

 extends the Standard CL benchmark by introducing a long sequence benchmark for continual learning benchmark with 15 datasets. This includes five tasks from CL benchmark, four from GLUE benchmark (MNLI, QQP, RTE, SST2)~, five from SuperGLUE benchmark (WiC, CB, COPA, MultiRC, BoolQ)~, and the IMDB movie reviews dataset~. Following~, we select 1000 random samples for training each task and hold out 500 samples per class for validation.

 introduce a new benchmark for continual pre-training of LMs, which is more challenging as the data required to pre-train is much larger and LMs are easier to forget previous knowledge.  DAS is composed of 6 unlabeled domain corpora, which contain 3 reviews: Yelp Restaurant ~, Amazon Phone~, Amazon Camera~; 3 of them are academic papers: ACL Papers~, AI Papers~, and PubMed Papers~. and evaluated by 6 corresponding classification datasets are: Restaurant~, Phone, Camera~, ACL (ACL-ARC in~ ), AI (SCIERC in~), and PubMed (CHEMPORT in~).% This dataset~ contains 72.97M tokens of programming questions and answers. It reproduces the ``Evol-Instruct'' dataset of WizardCoder~: an LLM (GPT-4) is iteratively prompted to increase the difficulty of a set of question-answer pairs (from Code Alpaca~). Due to computation constraints, we pick contain 20\% the samples to instruct tuning the Llama-2-7B model.

For how much they forget the old knowledge, we follow the~ that averages three benchmarks, HellaSwag~, WinoGrade~ and ARC-challenge~. HellaSwag benchmark includes 70K problems, each describing an event with multiple possible continuations. The task is to pick the most plausible continuation, requiring inferences about nuanced everyday situations. WinoGrande benchmark also assesses commonsense reasoning. It includes 44K problems with sentences that require ambiguous pronoun resolution. ARC-Challenge benchmark consists of 7,787 grade-school level, multiple-choice science questions, testing capabilities in complex reasoning and understanding scientific concepts.

The training orders in 3 benchmarks on T5-large and RoBERTa models are shown in table~.

We reuse some baseline descriptions from O-LoRA~.

%  We report more detailed results on the Standard CL benchmark and Long sequence benchmark in table~, including each order results and their corresponding average results.  To more intuitively display our results compared to the baseline, we plotted violin graphs showing the performance with and without our method under the condition of full finetuning as Figure~~~~.

The violin graphs results is shown as Figure~.

% % %  We also conduct an efficiency ablation by comparing FT, FT+MIGU and DAS because continual-pre-training is a relatively computational-intensive setting.  DAS is a typical parameter-based regularization methods.  We record the wall time required for the first three dataset given the same GPU configuration: A100  2.  As shown in the Table~, FT+MIGU only occur an approximately 10\% overhead in wall time, due to the extra masking step in the backward propagation phase while DAS achieves a magnitude larger overhead.

The detailed violin graphs results about ARC-Challenge~, HellaSwag~ and Winogrande~ are seperately shown in Figure~,~,~.

To investigate how our method enhances model performance, we visualized the variation in product magnitudes between an FT model and an FT model augmented with our MIGU technique in Figures~,. We employed heatmaps to depict the similarity in product magnitude distributions across different tasks. Our findings reveal that task similarity in the FT model with MIGU implementation is markedly reduced. This suggests that the models trained with our method exhibit more distinctive weight activations for different tasks, thereby mitigating their conflict. This distinction in activation patterns indicates our method's ability to foster more task-specific representations within the model, contributing to its improved performance across varied learning scenarios.

We plot the L1-normalized magnitude distribution of COPA sample, BoolQA sample, and Yelp sample on the first linear layer of 23-th FFN layer of T5-large model in Figure~.

As detailed in ~, our method encompasses four core processes in cluster-based implementation.  During the data forward phase, the product magnitudes of the weight vectors are computed and tracked.   Subsequently, in the second phase, MIGU caches these magnitudes and employs an L1-norm normalization to derive a gradient mask. This mask is pivotal for modulating the gradients in the subsequent phases.   The third phase involves the standard backpropagation to calculate the gradients of the parameters.   Finally, in the fourth phase, the earlier computed gradient mask is applied to the obtained gradients, ensuring a modulated update of the parameters. This modulation is consistent within each cluster, thereby maintaining the integrity of the expert groupings and enhancing the model's learning efficacy. We also plot a Figure~ to illustrate the differences between Dense, MoE and ours in forward and backward phase.

We explored two distinct clustering strategies:

The outcomes of two distinct clustering approaches, alongside our implementation within LoRA, are illustrated in Figure~. It is evident that, except for the second order, the ``Weight Cluster'' method surpasses the 'No Cluster' approach, which does not employ explicit clustering. However, the 'No Cluster' method demonstrates superior performance across the remaining orders, highlighting its robustness and effectiveness. Nonetheless, the other two explicit clustering techniques still significantly outperform the baseline vanilla continual learning LoRA, indicating their potential for further exploration.

Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL).  Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs.  To address this limitation, we introduce ``MIGU'' (agntude-based radient pdating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers.  MIGU is based on our observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data.  By imposing this simple constraint on the gradient update process, we can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities.  Our experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks.  For example, MIGU brings a 15.2\% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark.  MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. 

MIGUIntroductionmccloskey1989catastrophicliu2019RoBERTa,brown2020language,touvron2023Llamadamonlpsg2023videoLlamashi2024continual,wu2024continualwang2024comprehensiveachiam2023gptshi2024continualscialom2022fine,wang2024insclgururangan2021demix,qin2022elle,zhao2024sapt,wang2024rehearsalzheng2023learn,zhu2024modelwang2023orthogonaltouvron2023LlamaThe term `(L1-)normalized output magnitude distribution' will be referred to interchangeably as `magnitude distribution' for brevity throughout the paper.fig:motivationMIGUliu2019RoBERTaraffel2023exploringtouvron2023LlamaRelated Worksec:relatedContinual Learning for Language Models.mccloskey1989catastrophic,wu2024continualscialom2022fine,wang2024insclgururangan2021demix,qin2022elle,zhao2024sapt,wang2024rehearsalWang2023ACSzheng2023learn,zhu2024modelwang2023orthogonalPartially Updating Parameters in Continual Learning.zheng2023learn,zhu2024modelFinding Important Weights.zhu2023surveyfrankle2019lotteryansell2024scalingzhang2023emergent,song2024turbozhang2022moeficationqiu2024unlockingsun2024simpleMethodmodel comparisonsPreliminary - Continual Learning SetupKe2022ContinualLO, wang2023trace,zhao2024saptMIGU - MagnItude-based Gradient Updating for Continual Learning.subsec:MIGUfig:method_miguFor simplicity, we omit the bias term  here.Feedforward: Caching Output Magnitudes.      = [,\dots, ,\dots }}],   \in ^{d_{}}  =  \cdot  Backward Propagation: Updating gradient via a magnitude-based mask. &= \lfloor T \times d_{} \rfloor \\   &=(, t)

{l} (, t)\\ =  1 &    1-t    \\ 0 &  casesarray _{} \leftarrow   - \eta \cdot  \odot \nabla MIGU in PracticeMIGU in Transformer Block.subsec:MIGUMIGU in LoRA Implementation.hu2022lora     } &=  \cdot  \\     } &= } \cdot  \\      &=  \cdot  + {r} \cdot }, subsec:MIGUeq:xbeq:xoExperimentsliu2019RoBERTaraffel2023exploringtouvron2023Llamaraffel2020exploringqin2021lfpt5,wang2023orthogonalhu2022lorake2023continualtouvron2023LlamaT5_detailed_settingContinual Finetuning on T5-largeCIT_T5Two Benchmarks.qin2021lfpt5,wang2023orthogonalzhang2015characterbaseline_t5_largeBaselines.FTLoRALoRAReplayLFPT5qin2021lfpt5IncLoRAMoELoraluo2024moeloraSAPT-LoRAzhao2024saptMoCLwang2024rehearsalfreeOIncLoRAwang2023orthogonalO-LoRA is original name, we rename it to OIncLoRA to emphasize it is build upon IncLoRA and align with our notation.Metrics.Chaudhry_2018Results on T5.tab:my_label_t5das_detailedtab:efficiencyExperiment_on_T5detained_main_resultsExperiment_on_T5Continual Pre-training on RoBERTaBenchmark.ke2023continualdataset_instructionMetrics.ke2023continualBaselines.FTAdapterDER++buzzega2020darkDEMIXgururangan2021demixHAT-AdapterSerra2018overcomingDASke2023continualResults on RoBERTa.tab:improved_style_with_category_and_multirowtab:verage_two_datasetForgetting Less and Learning the Same: Scaling to Llama2Results on Llama2.wei2024magicoderbiderman2024lorachen2021evaluatingbiderman2024lorazellers2019hellaswagsakaguchi2019winograndeclark2018thinkfig:Llama_code_full1189626,wang2024comprehensiveDiscussionsAblation on Gradient Mask ThresholdCIT_T5fig:thresholdThe ablation on threshold search only reports one run, so it does not align with the results in Section \ref.Ablation on Gradient Mask Componentstab:abl_componentsVisualizationfig:visualization_figure5tab:single_datasetvisualizationConclusionLimitationsanthology,acl_latexExperimental Detailsapp:detailsContinual finetuning on T5T5_detailed_settinghttps://github.com/cmnfriend/O-LoRAFinetuning (FT) and FT with MIGU.The batch size is set to 64.     The optimization is performed using the AdamW algorithm with hyperparameters , , and a weight decay coefficient of .     The initial learning rate is set to , alongside a static learning rate scheduler.     The threshold for mask selection is set at  across orders 1 to 6 in the FT+MIGU configuration. Low-Rank Adaptation (LoRA) and LoRA with MIGU.LoRA configuration: , dropout .     The learning rate is set to , with all other hyperparameters being consistent with the FT+MIGU configuration. Incremental LoRA (IncLoRA) and IncLoRA with MIGU.For each LoRA module: , dropout .     Hyperparameters are identical to those specified in the LoRA and LoRA with MIGU settings. Order-Incremental LoRA (OIncLoRA) and OIncLoRA with MIGU.The threshold for mask selection is set at  across orders 1 to 6 in the FT+MIGU configuration.     All remaining hyperparameters are consistent with the LoRA and LoRA with MIGU settings. LoRA Replay and LoRA Replay with MIGU.The threshold for mask selection is set at  across orders 1 to 6 in the FT+MIGU configuration.     All remaining hyperparameters are consistent with the LoRA and LoRA with MIGU settings. Continual pre-training finetune on RoBERTahttps://github.com/UIC-Liu-Lab/ContinualLMPre-training.The batch size is set to 248.     The optimization is performed using the AdamW algorithm with hyperparameters , , and a weight decay coefficient of .     The initial learning rate is set to , alongside a linear learning rate scheduler.     The threshold for mask selection is set at  on the sequence of tasks. Tuning.The batch size is set to 16.     The optimization is performed using the AdamW algorithm with hyperparameters , , and a weight decay coefficient of .     The initial learning rate is set to , alongside a linear learning rate scheduler. Instruct finetuning on Llama2.The optimization is performed using the AdamW algorithm with hyperparameters , , and a weight decay coefficient of .     The initial learning rate is set to , alongside a cosine learning rate scheduler with warmup  of the total duration.     LoRA configuration: , dropout . Benchmark Instruction  \\ & 5       &  \\ & 6       &  \\  DAS  & 7 & Restaurant  ACL  AI  Phone  PubMed  Camera \\ tabularTask Sequence Orders for Continual Learning Experiments. Orders 1-3 represent the conventional task sequences employed in standard continual learning benchmarks~\cite. Orders 4-6 extend to longer sequences, encompassing 15 tasks each~\cite. Order 7 comprises a sequence of 6 tasks derived from unsupervised pre-training domains, in accordance with~\cite.table:Training_orderDataset Informationdataset_instructionStandard CL benchmarkLong sequence benchmarkDAS benchmarkMagicoder-Evol-Instruct-110KHellaswagWinoGrandeARC-ChallengeStandard CL benchmark.Standardzhang2015characterqin2021lfpt5,wang2023orthogonalLong sequence benchmark.razdaibiedina2023progressivewang2018gluewang2018gluemaas-etal-2011-learningrazdaibiedina2023progressiveDAS Benchmark.ke2023continualDBLP:conf/naacl/XuLSY19DBLP:conf/emnlp/NiLM19DBLP:conf/emnlp/NiLM19DBLP:conf/acl/LoWNKW20DBLP:conf/acl/LoWNKW20\url\urlding2008holistic,hu2004miningding2008holistic,hu2004miningDBLP:journals/tacl/JurgensKHMJ18DBLP:conf/emnlp/LuanHOH18kringelum2016chemprotMagicoder-Evol-Instruct-110K.wei2024magicoderluo2023wizardcodercodealpacaHellaSwag, WinoGrade and ARC-challenge.biderman2024lorazellers2019hellaswagsakaguchi2019winograndeclark2018think\tabcolsep4pt

Summary of the results on two standard CL benchmarks with T5-large model. Averaged accuracy after training on the last task is reported. All results in the last block are averaged over 3 runs. (We reuse the table template and experiment results from O-LoRA~\cite to construct the results of the top two blocks). \textbfdetained_main_resultsTraining orderstable:Training_orderBaselines for all settingsBaselines on Standard CL benchmark and Long sequence benchmarkbaseline_t5_largewang2023orthogonal : train all model parameters on a sequence of tasks (without adding any regularization or replaying samples from the previous tasks).     FTde2019episodic: fixed-size LoRA parameters are trained on a sequence of tasks (without adding any regularization or replaying samples from the previous tasks).     LoRA: incremental learning of new LoRA parameters on a sequential series of tasks (without adding any regularization or replaying samples from the previous tasks).     IncLoRA: finetune the whole model with a memory buffer, and replay samples from old tasks when learning new tasks to avoid forgetting.     Replay : continuously train a soft prompt that simultaneously learns to solve the tasks and generate training samples, which are subsequently used in experience replay.     LFPT5qin2021lfpt5 : learns tasks in different LoRA subspaces that are kept orthogonal to each other and sums all LoRA weights up at testing time.     OIncLoRAwang2023orthogonal : MoCL continually adds new modules to language models and composes them with existing modules.      MoCLwang2024rehearsalfree : In the SAPT method, a Shared Attentive Learning and Selection Module (SALS) is employed to guide training samples through optimal PET blocks for task-specific learning, using a unique instance-level attention mechanism. This process ensures efficient continual learning for large language models.     SAPTzhao2024sapt : MoELoRA considers LoRA as a Mixture of Experts, leveraging the modeling capabilities of multiple experts for complex data domains, as well as utilizing LoRA's parameter-efficient characteristics. MoELORAluo2024moelora (Naive CL) continually DAP-trains the RoBERTa;      NCL continually DAP-trains a set of adapters~     NCL-AdapterHoulsby2019Parameter~ is a replay method based on knowledge distillation. 16.4K tokens are saved for each domain in the replay memory.     DER++buzzega2020dark~ adds a new adapter for each new domain and initializes it with a previous adapter nearest to the new domain;      DEMIXgururangan2021demix~: HAT is an effective  method. HAT is applied to Transformer layers (i.e., self-attention, intermediate and output layers).      HAT-AdapterSerra2018overcoming~: HAT-Adapter uses HAT within adapters.      HAT-Adapterke2021adapting~ DAS proposes a soft-masking method to overcome CF and to encourage KT, and a constrative learning-based method for knowledge integration.  DASke2023continualExperimental ResultsExperiment on T5Experiment_on_T5detained_main_resultsfig:order123fig:avgorder123fig:order456fig:avgorder456Experiment on RoBERTadas_detailedDetailed experiment resultsfig:das_violinARC-Challenge         \includegraphics\label         HellaSwag         \includegraphics\label         Winogrande         \includegraphics\label         Accuracy on ARC-Challenge~\cite, HellaSwag~\cite and Winogrande~\cite, evaluating on Llama-2-7B by MIGU with LoRA and valinna LoRA instruct tuning pre-trained on Magicoder-Evol-Instruct-110k~\cite.fig:visualization_chose1Efficiencytab:efficiencywidth=1\linewidthfigs/method.pdfDifferences between (a)Dense, (b)MoE and (c)MIGUfig:moeExperiment on Llama2clark2018thinkzellers2019hellaswagsakaguchi2019winograndefig:Llama_code_arccfig:Llama_code_hellaswagfig:Llama_code_winograndeVisualizationvisualizationfig:visualization_chose1fig:visualization_chose2Magnitude Distribution.fig:smooth_vector_plotAblation on MIGU+Clusterjiang2024mixtralqiu2023emergentficationzhang2022moeficationImplementationsubsec:MIGUfig:moeWeight Cluster Combination: The weight vectors are clustered into  groups based on their proximity in the weight space.     Co-magnitude Guided Combination: Using a subset of the dataset, we group weight vectors into clusters based on the similarity of their product magnitudes. Result \& Analysisfig:ablation_designFT         \includegraphics         FT + MIGU         \includegraphics         FT         \includegraphics         FT + MIGU         \includegraphics         The product magnitude distribution similarity of different tasks in the FFN of the last transformer block: (a,b) 1-st linear layer; (c,d) 2-nd linear layer.fig:visualization_chose1FT         \includegraphics         FT + MIGU         \includegraphics         FT         \includegraphics         FT + MIGU         \includegraphics         FT         \includegraphics         FT + MIGU         \includegraphics         FT         \includegraphics         FT + MIGU         \includegraphics         The product magnitude distribution similarity of different tasks in the MHA of the last transformer block: (a,b) query linear layer; (c,d) key linear layer; (e,f) value linear layer; (g,h) output linear layer.fig:visualization_chose2width=\textwidthfigs/smooth_vector_plot.pdfThe Magnitudes distribution of COPA sample, BoolQA sample, and Yelp sample on the first linear layer of 23-th FFN layer of T5-large model.fig:smooth_vector_plot