As a first test, we conduct an ablation to assess whether expert pruning can retain the benefits of the larger SMoEs. We use the same set of pretrained models described in Section~ and for downstream task evaluations of the pretrained models, we utilize the list of tasks from SuperGLUE as used in ST-MoE . However, unlike that study, we adopt a two-stage approach for task-specific evaluation as shown in the figure. In the first stage, we perform a one-shot task-dependent expert pruning and then finetune the pruned model on instruction-output pairs (separate from the task in context). This prefinetuning stage helps in evaluation since our models have not been pretrained on enough tokens to adapt easily to downstream instruction-based tasks. In the second stage, we further finetune the pruned and prefinetuned model on the training splits of the tasks individually. For non-pruned SMoE models, the first stage is similar, but without the pruning step.

For the prefinetuning in the first stage, we use the FLAN dataset, introduced in  for the supervised instruction-output pairs. We exclude the tasks from SuperGLUE that we use for evaluation that were also present in FLAN namely, COPA, BOOLQ, RTE, CommitmentBank, RECORD and WSC from the original FLAN mixture since we define our own train and validation splits for these tasks in the second stage for task specific finetuning. Additionally, we add a portion of the pretraining dataset to the FLAN dataset for the first stage to mitigate catastrophic forgetting known to affect multi-stage finetuning.

For the one-shot pruning strategy, we use an expert activation frequency method to prune an  model to an  model, where . For each layer, we calculate the frequency of each expert's selection for the tokens in the training split of the task. We then retain the top  experts in each layer based on these frequencies. Table~ shows the results of the pruned models compared to their equivalent SMoEs trained from scratch. The direct observation is that all pruned models—, , and —fail to recover the performance gains after pruning when compared to the equivalent SMoE models trained from scratch. This occurs despite additional finetuning after pruning, indicating that the loss of token-expert routing information in the pruned experts makes effective model compression through expert pruning challenging. This phenomenon is consistent across all the SuperGLUE tasks we used, demonstrating its generality regardless of the specific task (noting that the pruning in the first stage is task-data dependent).

We consider an SMoE model comprising  experts. Let  represent the  task data that we use for determining the logits from the router of layer . We perform the following process individually for each layer of the SMoE model and at the risk of generalizing, we remove any layer specific identifiers in the notations where possible. The router in layer  parameterized by  generates a logits vector  of dimension , where each element  corresponds to the router's output for expert . To get an SMoE model with reduced expert count of  experts per layer, the goal is to cluster the  experts into  clusters based on expert similarity, as inferred from the logit responses across the  data. The algorithm for  for each MoE layer  is outlined in Algorithm~. We now describe the algorithm briefly.

For a layer with  experts, we construct an expert similarity matrix   (Lines 2-4) where each element  measures the similarity between experts  and , calculated based on their logits across all data points. We apply spectral clustering on   that transforms it into a lower-dimensional space using the eigenvalues and eigenvectors of the Laplacian matrix . We then apply -means clustering to these eigenvectors to partition the experts into  clusters. Inspired by the model merging process used in , we perform weighted averaging of the experts () in each cluster to form one expert per cluster post aligning them per cluster (Lines 12-16). The weights in the averaging are the activation frequencies of the experts on task . When finetuning this cluster-merged model further, we randomly reinitialize the weights of the router accordingly with appropriate dimensions reflecting  experts per layer. It is important to note that more sophisticated weight merging strategies cited in  are not in competition with our method but are complementary; our technique of activation frequency weighted merging could be substituted with these approaches. 

Prior to cluster-merging the experts within a layer, we align expert weight permutations to prevent suboptimal fusion of mismatched neurons following the work done in  and adopted in . In our case, since we consider experts in the same layer, the input and output spaces are similar. Let  and  represent input and output layer weight matrices (two layer feedforward network of the experts), and  be the input vector. Denoting the activation function as , the network mapping  expresses expert operations. To align experts without altering their functionality, we utilize the weight matching optimization technique  from . An optimal permutation matrix  is identified identified for experts  and  with weight matrices  and .  We note that, since we align all the experts in one MoE layer  to the most activated expert  in that layer (Line 11), we do not need to align the rest of the weights of the model, as the weights of  is unchanged and that is our reference weight for that layer. The optimization minimizes  distance between corresponding permuted weights  and , facilitating effective merging. This leads to the following optimization which constitutes a linear assignment problem:   which can be solved using the Hungarian Algorithm . This procedure is captured by () in Line 13 of the algorithm.

The time complexity of the  algorithm is primarily dominated by the similarity computation, which has a time complexity of . Spectral clustering and the linear assignment problem both have a time complexity of . K-means clustering contributes , where  is the number of iterations,  is the number of clusters. For large-scale data where , the similarity computation dominates the overall time complexity.

% [!tbp]% \centering% \small% {!}{% Resize table to fit within the text width% % }% % pert lusterng with ask  Featur}% In this method, we assume that the data we use for clustering come from a set of  distinct tasks denoted as  where . Unlike UNCURL, we explicitly incorporate task specific router activation patterns to guide the clustering in the expert space and then merging. As before, we operate on a layer by layer basis for the expert space reduction of the models. For each task , a batch of tokens is sampled, and the following metrics are computed: : %   For each task , we compute the proportion of tokens assigned to each expert denoted by . Represent this proportion as , forming a  matrix  where each element  indicates the proportion of tokens from task  assigned to expert . :%   We additionally record the routing probabilities for each expert  for each task . We denote this probability as , forming another  matrix  where each element  represents the routing probability of expert  for task .% We ensure that each row in matrices  and  is normalized such that the values for each expert across different tasks sum to unity, reflecting a distribution of engagement and routing probabilities across tasks. We concatenate matrices  and  to form a  feature matrix  for clustering, where each column of  represents an expert with its task-wise token assignment proportions and routing probabilities. We apply a clustering algorithm to matrix , aiming to partition the  experts into  clusters based on their feature representations. Post the clustering, we follow the UNCURL method for expert weight averaging cluster wise and then initializing the weights of the router. We first discuss the results of applying the expert clustering and merging on downstream tasks in this section. Recall that our goal is to understand whether experts in larger SMoE models can be flexibly pruned to a reduced set while retaining benefits over equivalent smaller SMoEs trained from scratch. We first compare the results of the SMoE models as we scale them with more experts per MoE layer in the 354M model. From the results in Table~, we see that for all tasks except Commitment Bank (CB), the performance of the SMoE models improves on the tasks as we scale the models with more experts. This is probably due to the fact that CB has roughly 300 examples for training and even with multi-task finetuning, the larger  model overfits. When comparing the largest SMoE model we trained   with the base dense model of 354M, we observe that we get performance improvements of +7.3\% on the BoolQ task and 6.8\% on RTE which are among the highest improvements we observe throughout.

%  In the following discussion, we explore the principal results of the  algorithm detailed in Section~. Initially, we compare the  model with the  model, both maintaining identical parameter counts. The findings indicate that except for the CB task, the reduced  model consistently outperforms the  model, thus suggesting that performance enhancements can be achieved despite a reduction in the number of experts, compared to smaller SMoE models. Moreover, in the RTE task, the  model slightly surpasses the  model. Interestingly, the  model shows underperformance on most tasks relative to the  model.

When we compare the merged models of  variants, we find that both the  and  perform worse than the  models on all tasks. This suggests that there is an impact of -reduction of the experts and that the pruning ratio plays an important role. A further understanding of a hybrid way to select top experts greedily by expert activation across some layers along with our cluster-merging technique for other layers would be an interesting direction to explore.

When comparing the results with the variants subjected to the greedy one-shot pruning strategy in Table~, we observe that our proposed method can effectively prune the SMoEs by a factor of 2 for most tasks in the  and  variants, and by a factor of 4 for the  variant. This indicates that advanced cluster-merging techniques are essential for task-specific pruning and it allows flexibility of larger models to be later pruned to smaller models, yet be able to retain the performance benefits for tasks.

We next analyze the results of applying the offline expert clustering algorithm  and we show the clustering outputs in the below figure. We apply the t-sne visualization of the eigenvector matrix  for each MoE layer in the  model. Note that the results of the clustering is obtained prior to any further finetuning of the resulting reduced model after expert cluster-merging. 

The top two images in the plot shows the clustering results from the first two MoE layers in the model and the bottom two images show the clustering results on the last two MoE layers.  We observe that there are no visible distinct clusters for the first MoE layer which also suggests that we can skip pruning the first MoE layer for further gains. But what is noticeable is the presence of one or few distinct clusters in the rest of the layers which also emphasizes on the nature of expert specialization and possible redundancies in the expert space. Based on this, we hypothesize that specifically, for the first MoE layer, selecting the top  experts greedily based on activation frequency instead of cluster-merging will yield better results and we leave this as future work.

% Heatmaps of cluetr-merged vs original expert utilization As a simple baseline, we reduce  experts to a target count  per layer in the following manner, where this is applied per MoE layer: we select the top  experts based on frequency of usage.

We then label the rest of the  experts in the same layer to either of these  experts based on the similarity metric described in Lines 2-4 of Algorithm~. Note that there is no clustering per se in this situation. After the labeling, we follow the same procedure as Algorithm~ to align the  experts to their respective matched expert and then perform a weighted average of each group. From the results in Table~, we can observe, the performance degrades, showing the nuances of using frequency based pruning in the top-1 case where merging based on grouping is not superior to our method.

One goal derived from our problem statement is to avoid costly retraining methods like distilling larger SMoE models into dense or smaller models from scratch, and favoring pruning instead to measure the comparisons between pruned larger SMoEs and equivalent smaller SMoEs. This preference is due to the goal of minimizing the significant computational demands of training models from the ground up as would be more common in practical settings. The study most related to our work is the merging technique MC-SMOE presented in . There are two main differences between their algorithm for pruning (Algorithm 1 in  and : (1) their approach first identifies globally dominant experts across layers based on frequency of activation, to which other experts are then merged into. Instead, we focus on local expert clusters within each layer without any notion of activation to identify clusters instead focusing on router logits, and (2) their algorithm does not ``cluster" experts but uses similarity based grouping to reduce the expert space, which contrasts our method.

We have adapted their algorithm to better suit our pipeline. Specifically, we incorporate their global expert identification and permutation alignment strategies without adopting their additional post-merging compression or the use of knowledge distillation auxiliary loss. Note that as mentioned earlier, the method of  does not guarantee the same number of experts per layer.  In their case,   means that we set the number of dominant experts to 64 and their method ensures an average of 64 experts per layer. When we compare their results in Table~, we find that for the  case, their method performs comparably to ours, even surpassing us on RTE. But when the models have more experts, their methods perform substantially worse than our proposed approach. One of the reasons we hypothesize is that globally identifying dominant experts based on activation frequency may be suboptimal in the presence of larger number of experts.

% % Given a matrix \( A \) and \( b \) bits, the linear quantization encodes \( A \) as follows: : , : , where \( s \) is the scaling factor, which can be chosen per channel as shown or for the whole tensor. During inference, the quantized \( Q \) is dequantized back to \( A' \) with the scaling factor \( s \) as . In this context, selectively applying this quantization method to only the expert layers of a mixture of experts (MoE) model helps in reducing the model's memory footprint during loading. By maintaining the precision through dequantization, the model ensures accurate inference while benefiting from the reduced memory usage of the quantized experts.% Quantization does not reduce the inter GPU communication costs with expert parallelism. So while it has the same benefits as our method for memory bound inference, our method implicitly has the latency benefits - can we fit more experts with quantized experts?% % % Note that unlike knowledge distillation (KD) in SMoEs where the sparse models are either distilled into dense models  or into lower depth SMoE models , our cluster-merging technique does not incur any additional cost of extra training to distill SMoEs with larger number of experts top fewer ones. We want to repeat the setting of the goal we set out to achieve: given a pretrained SMoE  with  experts, our goal is to extract an SMoE model  with  experts such that  and the resulting  is specialized to a mixture of data . So the important point to note is that we only have access to  and the task data  and not the pretraining data, which is the usual case in practical downstream settings. In such settings, it is not feasible to train an SMoE model  from scratch even with distillation as that would require access to the pretraining data and large amounts of compute. Instead, in this section, we assess whether the gap between   and the resulting  from UNCURL can be further reduced through knowledge distillation only through finetuning with the task data .% To that end, we follow the same UNCURL technique for obtaining a smaller SMoE  from  as described in Section~, however, we add two auxiliary losses following the work done on KD on BERT . For each MoE layer , let  and  denote the outputs % % Run an ablation to see the cluster overlap with different data