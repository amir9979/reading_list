We evaluate our approach on standard language modeling tasks, using the Pile dataset  containing 360B tokens for training . We assess the performance of LLMs from multiple aspects, including their perplexity, zero-shot accuracy, and instruction-following ability. Perplexity is calculated on the WikiText-103 test set .  We evaluate the zero-shot capabilities of language models on 6 NLP benchmarks, including MMLU , HellaSwag , PIQA , WinoGrande , ARC-E, and ARC-C . For the pre-trained LLMs, we conduct instruction fine-tuning using the Alpaca dataset by GPT4 , and then evaluate their instruction-following abilities on MT-Bench .

 We use the Transformer backbone  and adopt most of the architecture designs from LLaMA . We apply pre-normalization using RMSNorm , use the SwiGLU activation function , and rotary positional embeddings . We also apply FlashAttention-2  to accelerate attention computation. We scale the model demension and obtain 4 different sizes of Transformers: Transformer-370M (hidden=1024, intermediate=2752, hidden=24, attention=16), Transformer-780M (hidden=1536, intermediate=4128, hidden=24, attention=16), Transformer-1.3B (hidden=2048, intermediate=5504, hidden=24, attention=16), Transformer-2.7B (hidden=2560, intermediate=6880, hidden=32, attention=32).

 Unless otherwise specified, the patch size  is 4. The context length for token-level training . For patch-level training, the context length is the patch size . The global batch size is  tokens, and the total number of training steps is . For patch-level training, the number of training steps is , and then the model proceeds with token-level training for  steps. After patch-level training, only the obtained model parameters are used for initialization, and all other states like the optimizer and learning rate scheduler are reset. We use the tokenizer of LLaMA2, whose vocabulary size is . Our models are optimized by the AdamW optimizer  with . The learning rate is  and the cosine learning rate schedule is applied with warmup of  steps. We use a weight decay of  and gradient clipping of , and no dropout is applied during training.

We train a series of LLMs of varying sizes (370M-2.7B parameters) on the Pile dataset. To halve the training costs, we employ patch-level training with the settings of , and compare its performance with the conventional token-level training. For the Transformer-370M, we also explore other choices of  to evaluate its impact. Table  presents the performance comparison of the resulting models. Remarkably, our approach consumes only half of the computational resources and incurs almost no performance loss. It matches the baseline model in terms of perplexity and even demonstrates a consistent gain in zero-shot evaluations, raising the average accuracy by approximately 0.5\%. The model performance is also influenced by the choice of . Within the range of values we set, a smaller  leads to better model performance but also entails larger training costs. A more detailed study on the hyperparameter  will be presented in Section .

We further conduct instruction fine-tuning using the Alpaca dataset by GPT4 to examine the impact of patch-level training on the model's instruction-following ability. We evaluate our models using MT-Bench, a multi-turn question set, and present the experimental results in Figure . As can be seen, our approach maintains a similar instruction-following ability to the original models, with some experiencing a score decrease (Transformer-370M, Transformer-1.3B) and others showing an improvement (Transformer-780M, Transformer-2.7B), which can be viewed as regular variations.

Next, we evaluate our approach in multi-epoch training. We randomly extract a subset of 60B tokens from the Pile dataset and increase the training epochs to 6. The results are shown in Table . To our surprise, patch-level training continues to show superior training efficiency and even outperforms models trained on the full dataset in Table . We speculate that this is because combining patch-level and token-level training on the same data contributes to better model regularization. It also suggests that our approach can be data-efficient by initializing the model with patch-level training for one or multiple epochs, offering a promising direction for boosting model performance.

Our primary motivation for patch-level training is to enhance the model's knowledge acquisition efficiency. Interestingly, experimental results show that this approach can sometimes lead to performance improvements, which is beyond our initial expectation. We conjectured that the longer context length in patch-level training contributes to the improvements. However, when we reduce the context length during patch-level training from  to  for Transformer-370M (), the performance experiences a slight decline (PPL  0.06, zero-shot accuracy  0.2), yet still surpasses the baseline, implying that context length is not the primary factor. We hypothesize that two other factors might explain this phenomenon: first, the patch-level initialization could potentially serve as a form of regularization; second, patch compression reduces the distance between tokens, allowing the model to better learn and capture long-range dependencies.

In the above, we have validated the effectiveness of patch-level training across several model sizes (370M-2.7B), using a training set of 360B tokens. However, state-of-the-art LLMs are generally trained on model sizes and datasets that are at least an order of magnitude larger than our settings. Therefore, it is crucial to know the scaling properties of patch-level training, i.e., how it performs when applied to larger training datasets and models.

In Table , we notice a trend of perplexity related to the model size: the performance advantage of patch-level training appears to decrease as the model size increases. Table  describes this trend more precisely, indicating that the model with patch-level training experiences smaller performance gains from the increase in model size. On the other hand, Table  presents the perplexity changes when maintaining a constant model size and varying the size of the training data. As the data size increases, the performance of patch-level training improves at a faster rate compared to the baseline model.

This phenomenon can be explained from the perspective of knowledge transfer. As the data size increases, more training data is employed to adjust the model from patch-level to token-level, facilitating a smoother knowledge transfer process. However, an increase in model size implies a greater number of model parameters to be transferred to the token-level, which raises the level of transfer difficulty and necessitates more training data. Based on this explanation, patch-level training is better suited for scenarios with abundant training data.

Note that the above conclusions are drawn under the settings of , which may vary with changes in the patch size  and the patch-level data fraction . At present, we have not identified a general scaling law for patch-level training that incorporates  and . Instead, we have made some observations regarding their effects on model performance, which will be discussed in the following.

We investigate the effect of patch size under the settings of 90B training data, 370M model parameters, a batch size of 512K, and . The results are shown in Figure . Across different patch sizes, the loss curves for patch sizes  and  are nearly indistinguishable, while further increasing the patch size to 8 or 16 results in a certain performance decline. Despite this, these models still exhibit significant performance improvements when compared to the model trained from scratch, which does not benefit from the initialization of patch-level training.

Overall, the patch size of  strikes a favorable trade-off between training efficiency and performance. Considering that larger patch sizes can process more data at the same cost, we also experiment with patch-level training using  on 90B tokens, which costs similar computational resources as  on 45B tokens. Following this, both models proceed with token-level training on 45B tokens, and coincidentally, their loss curves are nearly identical. In this context, the advantage of  lies in its data efficiency, as it achieves similar performance while consuming less data.

The hyperparameter  allocates the ratio of training data between patch-level and token-level training. A larger  results in more tokens being compressed into patches, leading to a higher acceleration rate, but it may also leave insufficient data to adjust the model to the token-level. In this section, we investigate the effect of  under the settings of 370M model parameters, a batch size of 512K, and a patch size of . We consider two scenarios:

Figure  shows that the model performance initially rises and later falls as  increases, with a turning point near . The performance improvements when  can be attributed to the inherent benefits of patch-level training, as analyzed in Section . When  exceeds , further increasing  leaves insufficient data to adjust the model to the token-level, leading to a rapid decline in performance. Figure , on the other hand, shows that when computational resources are limited, the optimal value for  is around . Note that these conclusions are specific to the current settings and should be used as a reference only. The optimal  may vary depending on factors such as data size and patch size. To determine the optimal value of  in any scenario, it is essential to establish the scaling law for patch-level training.

In this section, we quantitatively explain why patch-level training leads to better learning efficiency from the perspective of neuron activation. The training of LLMs is essentially a process of embedding knowledge from the training set into the model's parameters. During this process, the model employs all of its parameters to encode every token and updates the relevant parameters based on the gradient feedback. We argue that this is an inefficient process for large models, as the knowledge encapsulated in each token is only associated with a small subset of model parameters, resulting in a limited number of effectively activated and updated parameters.

We substantiate this by measuring the percentage of activated neurons for models of different patch sizes, as depicted in Figure . In the token-level model (K=1), only a small proportion of neurons are activated, particularly in the lower layers. By grouping multiple tokens into a patch, the information density processed at each step is increased, which is manifested as increased neuron activation rates. Therefore, patch-level training exhibits a better learning efficiency compared to token-level training.