Neural compilation is a technique for deterministically transforming code into neural network parameters that express the exact same program in a given architecture. % Any universally expressive neural network architecture necessarily supports neural compilation, see Proof . Precursors to neural compilation were first discussed in , and then implemented in Gruau et al. .  However, the first adaptive (trainable) neural compilation technique was first defined in . Similarly, there are modern approaches to neural compilation, based on the transformer architecture, but these either focus on interpretability, are not universal, or are not adaptive . % We discuss these approaches in depth in our Related Work Section % {% Siegelmann assumes infinite precision% Gruau is non-adaptive%  :%  ideally, compiled algorithms can either be further optimized in parameter form, or at least propagate gradients for optimizing other aspects of the architecture. %   proposes an adaptive neural compilation technique, and focuses on learning contextual programs . %  While impressive, this neural compilation model is not , as it relied on solving for parameters analytically, limiting the overall parameter count and learning capacity of the network. %  Also, this work was built on a recurrent neural network architecture, which has issues with unstable gradients and training efficiency when compared to the transformer architecture . %  In general, the transformer architecture is successful because it is parallelizable and separates compute from data length. %  % Our proposed architecture inherits these desirable properties by integrating a differentiable computer into a transformer architecture and defining a parallelizing compiler. %  % , and exploring the effect of computation depth on trainability. %  % defining a parallelizing compiler for it. Experiment  evaluates this comparison directly.%  %  Recent work has looked at compiling algorithms into the parameters of an  transformer, in particular via a language called RASP, using the Tracr compiler .%  While this family of techniques can be used to compile algorithms into unaugmented transformers, it is perhaps more useful for interpretability, as the unaugmented transformer is not fully universal . %  % Also, the adaptivity of networks containing tracr-compiled programs has not been studied.%  % Perhaps we can study this if there is extra time%  % Finally, the looped transformer architecture is universal, but likely can be simplified.%  % though it is not a particularly practical or efficient way to specify algorithms, nor .%  }% We modify the model to improve scalability/trainability.% \newpage% %     \item What is the problem? %     \item Why is it interesting and important? %     \item Why is it hard? (E.g., why do naive approaches fail?) %     \item Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?) %     \item What are the key components of my approach and results? Also include any specific limitations.%     % %  Unaugmented transformers are not universal on their own, so cannot express many important algorithms, even if given globally optimal weights .% To mitigate this, we define a universal transformer-based architecture with a differentiable interpreter, structured memory, adaptive computation, and a starting program library.%  The model is made more amenable to optimization by  operations, which reduces the length of gradient paths and partially mitigates unstable gradients. % Also, we define a bootstrapping technique based on differentiable lookup tables, which starts with easily-optimized components and replaces them with more general ones once basic usage has been learned. augments a recurrent neural network with memory, registers, and a differentiable interpreter for a minimal assembly language .  Then,  compiles algorithms by solving for weights as linear equations. %which is only possible in a low-capacity network.  This model relied on a lookup-table based ALU, unit vector numeric encodings, dot-product memory/register lookups, and probability mixtures for control flow.  % These techniques are described in Section . This work focused on learning contextual programs, but in contrast we focus on compilation as a means to specify algorithms to Large Language Models.

% However, our method for overparameterized neural compilation () is promising for learning contextual programs in future work. % , which we will explore in future work.% % I feel like this section is repeating what we say earlier, so maybe cut down on the *earlier* statements of this, and leave this section since it is more structured describe a neural compilation technique for unaugmented transformers, aimed at interpretability. Specifically, RASP defines a minimal language , Tracr defines a working compiler , and CoNN exploits the Tracr compiler to augment a transformer.  % While potentially useful for interpretability, these techniques are not computationally universal. While CoNN compiled addition and subtraction, their mixture-of-experts approach has a basic calculator directly output the answer as a series of tokens, which is limited only to very simple problems and does not support compositionality or training for new tasks . In comparison, our work is the first to experiment with end-to-end trained large language models augmented with universal programs. % {% % % How do we compare to the line of work based on Tracr/RASP?% They are not universal first of all,% But it's possible we miss something they don't, since they use the underlying transformer..% Maybe their approach is more scalable or easily integrated with current models?% While the RNN-inside-transformer Bunel approach is a bit too oldschool% Or do both approaches have the computational-graph/numerical stability issue?% Looped transformers are not end-to-end differentiable% RASP is effective but not Universal% Universality only matters if we can train for it!% -- Practical algorithmic ability -- % How is this measured?% Pareto front of universality vs trainability% Somehow the Looped Transformers work doesn't scale with lines of code being executed?% This is not true for our model.% Evaluate their claim, understand why it is true% NONE of the above papers are focused on COMPOSITIONALITY% If we've compiled algorithms into a library,% and the resulting model is still trainable, % presumably it is possible to learn how to compose multiple operations% Make a dataset of this!% The looped transformer paper has an odd way of doing binary encodings..% And claims to add numbers with a one-layer network.% This seems fundamentally wrong.% Check out the copy/write mechanism referenced in looped t paper, Akyurek 2022% Rather than use a simple differentiable read, they define a single-transformer layer read that is relatively complicated for what it accomplishes% Where is the program counter used in looped transformers?% Note that in Bunel's model, one hot encodings are relatively useful for indexing memory and so on% Reviews of Bunel's work don't tell me much I don't know, it is not scalable, etc% Per looped transformers, introduce the notion of a computation ratio, % e.g. how much computation is being done by the model in comparison to % the optimal computation for just running the algorithm on a conventional CPU% % {% Overall, . % This work describes an architecture and compiler built with these goals in mind, but in particular we focus on scalability, library learning, and tool use, and defer contextual programs and interpretability to future work.% %     \item  Universality is achieved by augmenting a Transformer with memory, adaptive computation, and fundamental operations, see Sections  and . %, and . %     \item Scalability, the primary goal of this work, is achieved by introducing parallelism and using shallow differentiable lookup tables to bootstrap tool use. %     % \item Adaptivity is achieved by an overparameterization technique (neural memorization) to learn contextual programs, but is defered to future work% % This is very formulaic I know, just an outline% The primary % Adaptivity is achieved by having the network components be natively differentiable.% Scalability is achieved by compiling algorithms via memorization, rather than analytically.% Furthermore, a parallelizing compiler and parallel interpreter provide scalability that mimics the natural structure of the transformer architecture, aiming to reduce the length of gradient paths.% Interpretability is achieved via decompilation, and encouraging sparsity of intermediate programs within the architecture. % Finally, neural compilation is made more intuitive by providing a higher-level Python language that can be compiled into differentiable form.% % Large Language Models can have seemingly algorithm-like abilities, but like many neural networks, fail to generalize robustly. % Transformers are expressive enough to theoretically capture many important abilities, such as perfect arithmetic. However, optimization over predictive coding does not find these abilities in practice .% In particular, for Large Language Models trained on next-token prediction, there is no clear hypothesis, guarantee, or empirical evidence as to how or why algorithmic reasoning ability might emerge. % In contrast, simple algorithmic tasks like arithmetic, logic, and sorting are difficult for large language models, and complex planning tasks are more so .% We propose compiling in known algorithms to LLMs to address their reasoning ability.% Of course, we acknowledge that compilation is intended primarily for simple tasks with a known algorithm, but highlight the possibility of composing primitives to learn novel algorithms.% Even Universal expressivity does not guarantee the ability to induce algorithms. % Primarily, the issue is in stochastic optimization.% While certain network architectures, such as recurrent neural networks, can be made universal, in practice they can struggle to learn even very basic languages from gradient descent based algorithms% .% To induce arbitrary programs, an architecture must be universal, but also amenable to optimization.% % % %     \item Adapting Neural Compilation for Library Learning%     \item % %  The fundamental contribution of this paper is a differentiable standard library of programs. The overall model uses the program library by  (selecting) programs and inputs to run.

Creating a differentiable library fundamentally relies on introducing a method for calling functions arbitrarily. To achieve this, we add a  primitive, supported by a  instruction, which stores the current program counter in a given register. Doing this allows returning from functions by designating a special return address register. The  primitive simply runs  and moves the instruction counter into the new function, and the called function returns to the stored location when finished. 

The overall model must  a given natural language input, and provide appropriate inputs for  in the form of an initial state. Then, the model must  a program . Even if a perfect program has compiled into the library, the model still needs to select it in context, and provide correctly parsed inputs.  We refer to this as the parsing/selection problem. In our experiments (Section ), we first study parsing/selection in an isolated form, where a minimal model learns the correct permutation for a task, and then we study the full parsing/selection problem in the context of transformer models with natural language inputs. 

% % A major limitation of recurrent neural networks is that they do all computation in serial. In contrast, the transformer architecture is shallow but wide, and emphasizes parallel computation. % Accordingly, . An advantage of this approach is that it clearly reduces the length of gradient paths, so in principle should improve neural network training, which is otherwise affected by unstable gradients.% In particular, this is accomplished by creating  new machines  with  but separate registers. Now, each line of a program contains  instructions , some of which are padded as .% % % To bootstrap, we simply train with a differentiable lookup table and replace it with a circuit.% All that this requires is that the two modules have the same arguments.% .% % % }%  defines differentiable memory as a matrix , where the dimension  is an address and the dimension  is an encoding.  An address  is a unit vector, produced via softmax output. Reading from memory at an address is done with the dot product:

Writing a vector to memory requires updating all of memory using probability mixtures.  First, for an address , vector  being written to , and  overall probability of writing, , a memory update is:

 represents kept (unaltered) memory content, and  represents new, written content.  % The entirety of memory is updated with each write, but the presence of kept content simulates writing to a single location, especially when the address vector is sparse.% Ideally, addresses are sparse vectors, e.g. , representing the second address, but in practice addresses are produced via softmax, and represent probability distributions. % Similarly,  is ideally either  or , but in practice can take intermediate values. Accordingly, memory writes are often dense, and modify disparate parts of memory. Density can be desirable for training (it admits more potential solutions), but contradicts interpretability.  Registers are defined as a matrix . To write an output  to address :

Reading from an address  to a value  is done with a dot product: . The distinction between memory and registers is that instruction inputs/outputs use registers, not memory. Also, registers are always written at every timestep by any instruction, while memory is only written from  or  instructions when they have non-zero probability. % However, all instructions are run probabilistically, see probabilistic execution paragraph.%   we know which variables actually get referenced in explanation,  it will be much clearer which notation is gratuitous}% Finally, given lookup tables, circuits, and memory as building blocks, we can construct a differentiable computer, first presenting the version introduced in , and then distinguishing with our contributions.% This computer uses an imperative Von-Neumann architecture which mimics a register machine, but note that differentiable computers based on lambda calculus or Turing machines are also possible, with various trade-offs .  The computer executes a set of assembly instructions, , representing the computer's language.  % For example, one instruction is , which takes two registers as arguments and outputs their sum to a third register.  A differentiable program  is structured as a list of these instructions and their arguments, where each instruction can be accessed at its address. These addresses are tracked via a special instruction counter, . Then, a differentiable interpreter  runs instructions  in order to execute the program.  See Listing  for examples of control flow. 

% The exact instruction set in  was:% %     A = \{, , , , , , , , , \}%  Instructions, the program counter, and addresses are represented as multinomial probability distributions output by softmax. Accordingly, the program and interpreter is always in . Instead of running a single instruction at a time, the interpreter runs , but with execution and results weighted by the distributions for instructions, program counters, and addresses. In the case that every distribution is dirac-delta ( probability of one possibility), then execution is fully deterministic. See Figure .

Program execution is tracked as a probability mixture between incrementing the instruction counter and jumping to a new location  in the program, based on the condition probability : % Overall, the differentiable register machine is defined by the tuple , where  is a program controller parameterized by ,  is a set of assembly instructions,  is an interpreter for these instructions, and  is a program state tuple containing memory , registers , instruction counter  and halting probability .% In particular in  the controller  was structured as linear layers which act only on the instruction counter  and outputs an instruction .% In , all instructions except for , , , and  were represented via differentiable lookup tables.% % We denote a syntactic program as , and a probabilistic program as , which consists of probabilistic instructions . Each instruction is a tuple of a particular operation  and  associated register arguments , each represented by a multinomial distribution. In particular,  is:% %     \delta(W, c) = \iota = (f, r_k) = (W_0 c, W_k c)% ((W_0), (W_k c))% %  An interpreter  runs each instruction by querying a 4D lookup table. This model is based on one-hot encodings of size , so this lookup table  has dimensions .  This table is filled according to each instruction, with special cases for reading or writing to registers/memory. For instance  is the addition table. % The instructions  and  are specified directly in the lookup table, for instance if , then  is the vector , representing that . Then, the sub-tensor of  corresponding the read instruction is set to the current memory, , representing reading at a particular address, as in equation . % Finally, portions of  corresponding to special instructions , , and  are set to existing register values so that these three operations have no effect when doing lookup.  To run a instruction, first the register values are resolved to . The final lookup is: % Then running a instruction is done via einstein summation: Intuitively, this corresponds to first looking up a particular operation (e.g. ), then the first argument (e.g. ), and then the second argument () to get the answer . Each instruction specifies a register to store the output in, so finally  is written to  using equation . Since each operation  and argument  are independent multinomial distributions, this entire operation is probabilistic, so the output  is potentially a mixture of running different instructions with different registers. %, e.g.   and  .%  Before scaling to billion-parameter models, we explore behaviors of components of our differentiable computer, namely lookup tables, circuits, and small programs. These experiments use a minimal neural network with one layer before the computer and one layer after, on the premise this will inform behavior at the LLM scale. These networks simply need to route inputs/outputs correctly to/from the computer, which is replaced by parsing in case of LLMs. We find that lookup tables are more learnable than circuits, and that we can learn recursive algorithm routing to a certain depth.

% \newpage% We compile in the fibonacci function, which only requires writing  lines of code.% While fibonacci is artificial in isolation,  We use fibonacci as a method for exploring the effect of computation depth on trainability. While the fibonacci function has a closed form, for our study we treat it as an inherently sequential algorithm, so a recursion depth of 1 entails 8 interpreter steps, and a depth of 2 entails 16 interpreter steps. This degradation in performance is expected, and is meant to establish practical limits of using the differentiable computer. We observe similar results in the context of transformer networks. 

%  Next we study the behavior of small transformers with natural language inputs. First, we establish that it is possible to train these models to use calculators perfectly, and then we study the effect of computational-depth on trainability.  These experiments use small transformers with between 30 million and 100 million parameters. This architecture is identical to LLaMA 3, but with much smaller scale parameters (e.g. hidden size of 128 or 256 and only two transformer layers). Natural language inputs are tokenized with the 128K vocabulary LLaMA3 tokenizer. 

 First, we provide the model with a lookup table for mod-128 arithmetic operations and train it on natural language versions of one-step arithmetic, e.g. . Solving this problem is a matter of parsing the sentence to extract the operation and operands, and then providing these to the differentiable calculator. Supervision is given only on answers, via cross-entropy. By 62 epochs, the model can use the calculator with  accuracy on the test set. % indicating it may be possible to do more complex tasks. Next, we experiment with giving the model a fibonacci program. Again the model must extract inputs from a natural language sentence, e.g. , and provide them to a library function. However, because this function call has a longer gradient path, it is less trainable than modular arithmetic. 

% % Can the model be trained to do one-step arithmetic problems from GSK-8K? Is performance improved when a calculator is available?% % % % % % In our preliminary studies, we established that shallow gradient paths are necessary to effectively learn to use differentiable tools. % While the relatively shallow recursion cutoff established in the Fibonacci section may seem restrictive, in practice a computational depth of 16 is fairly generous for a parallel algorithm. % Based on these findings, we make two proposals for effectively integrating neural compilation with LLMs: First, to replace algorithms with lookup tables where possible, and swap these tables for proper tools once training is partially converged. Second, to parallelize algorithms where possible, to improve gradient paths.% % A major difference with running algorithms from natural language is .% For the minimal network, we provide one-hot encodings.% For our LLM, the network starts with learned embeddings for each token, for a vocabulary of 128256 tokens.% To better understand tokenization as it interacts with algorithmic ability, we run a brief experiment where an LLM re-classifies its input as a one-hot classification, as this is a pre-requisite for proper tool use. % % One of the simplest and most common natural algorithms is sorting. While this is fundamental, generalizable sorting ability is elusive for large language models. % We experiment with compiling in basic sorting algorithms.% However, a default sorting algorithm is highly sequential, entailing long gradient paths. % Accordingly, we compare performance when training with a sequential or parallelized sorting algorithm. %which we hypothesize will be more trainable than the default version. \\% %%%%% % Our primary question when augmenting a pre-trained model is if it can acquire the new compiled ability without losing existing abilities. % We achieve this by fine-tuning on specialized synthetic algorithmic data, and also training LLM self-sourced data to retain existing ability.% % We measure existing abilities with conventional benchmarks, and tool use on our dataset.% % % % \\% % % % % % %     \item Cases where neural compilation works very well% %     \item Cases where neural compilation struggles% % % % % % We compare zero-shot tool use, as in toolformer, neural compilation, and supervised tool use. We find . Neural compilation is still compatible with zero-shot tool use and bootstrapping schemes.% % % Compared to conventional models, our architecture is highly interpretable as differentiable programs and their inputs can be analyzed, for instance by .% A primary challenge here is that the probabilistic differentiable interpreter often learns superpositions. % A preliminary investigation confirms that this is the case, as the inputs to the program library are dense probability distributions. % In future work, we explore enforcing sparsity to achieve greater interpretability, but regardless we provide an example of our algorithms interpretability in Figure .% % % % }% % %     \item GSM8k: has intermediate annotations%     \item MathQA: has intermediate annotations% % % %     \item Differentiable lookup table%     \item Differentiable circuits%     \item Non-differentiable intermediate labels%     \item Differentiable (lookup, circuits) intermediate labels%     \item BERT, LLaMA base architectures (scratch, pretrained)% % % Behavior of fibonacci routing as we increase the recursion depth of the problem% % In a preliminary experiment, we compile in the fibonacci program  We introduce a differentiable computer  as program controllers  with parameters , assembly instruction set , interpreter , and state tuple . Each state consists of memory , registers , instruction counter , and a halting probability . Furthermore, we introduce a program library , which augments the low-level language instructions in  with multi-instruction programs written in terms of . This entails that  produces high-level programs, which can call both low-level instructions and programs in the standard library.

A controller  is a parameterized function which produces probabilistic programs, from inputs . By introducing , it becomes possible to have  programs which vary with the overall input.

Probabilistic programs  are represented as matrices, where axis  corresponds to the program counter, and axis  is an instruction encoding.  Accordingly, an individual instruction is obtained by taking a dot-product with the program counter, producing a probabilistic mixture over instructions: % The interpreter  progresses the computer state from time  to  for one probabilistic instruction:% % I : \iota_t, S_t \mapsto S_{t+1}% % The basic interpreter is similar to that of the register machine in , except that overall list of instructions has been expanded, and now is backed by both lookup tables and circuits:% % %     A = \{, , , , , , , , , , \}% % [!h]%     \centering%     \includegraphics[width=0.4\linewidth]{figures/library.pdf}%     %     % %  programs which vary with the overall input.% %     \delta : \theta, z \mapsto \varrho% % Probabilistic programs  are represented as matrices, where axis  corresponds to the program counter, and axis  is an instruction encoding. % Accordingly, an individual instruction is obtained by taking a dot-product with the program counter, producing a probabilistic mixture over instructions:% %     \iota_i = \varrho_{ij} c_i% % }% % %     The differentiable computer  is universal.% % % }% % % %     A near-minimal register machine assumes only the , ,  and  instructions, or even combines  with other instructions.% %     To define a universal register machine, we use instructions ,  \ldots\\%     %     And accordingly we provide the program for a universal register machine:% }%     %         \ldots%     % % %  Show that the model can simulated TC0 (maybe should be in appendix)% % Furthermore, neural compilation is a stepping stone to providing formal guarantees within neural networks% % %     If a neurally compiled program is given the correct inputs, it computes the correct algorithm, identically to the classical compiled input program.% % % } \\

\\

\\

\\ % % Required% %     \item Principled integration with a larger architecture%     \item Empirical evaluation on non-trivial datasets (e.g. MathQA, GSM8K)% % Potential Directions% %     \item Higher-level specification language% % % Main points:% %     \item Reasoning depends on algorithmic ability% % % % % Please read the instructions below carefully and follow them faithfully.% % % % % % Papers to be submitted to NeurIPS 2024 must be prepared according to the% instructions presented here. Papers may only be up to {\bf nine} pages long,% including figures. Additional pages  are allowed. Papers that exceed the page limit will not be% reviewed, or in any other way considered for presentation at the conference.% % % The margins in 2024 are the same as those in previous years.% % % Authors are required to use the NeurIPS  style files obtainable at the% NeurIPS website as indicated below. Please make sure you use the current files% and not previous versions. Tweaking the style files may be grounds for% rejection.% % % % % % The style files for NeurIPS and other conference information are available on% the website at% %   % % The file \verb+neurips_2024.pdf+ contains these instructions and illustrates the% various formatting requirements your NeurIPS paper must satisfy.% % % The only supported style file for NeurIPS 2024 is \verb+neurips_2024.sty+,% rewritten for .   2.09,%   Microsoft Word, and RTF are no longer supported!}% % % The  style file contains three optional arguments: \verb+final+, which% creates a camera-ready copy, \verb+preprint+, which creates a preprint for% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the% \verb+natbib+ package for you in case of package clash.% % % % If you wish to post a preprint of your work online, e.g., on arXiv, using the% NeurIPS style, please use the \verb+preprint+ option. This will create a% nonanonymized version of your work with the text ``Preprint. Work in progress.''% in the footer. This version may be distributed as you see fit, as long as you do not say which conference it was submitted to. Please  use the \verb+final+ option, which should  be used for% papers accepted to NeurIPS.% % % At submission time, please omit the \verb+final+ and \verb+preprint+% options. This will anonymize your submission and add line numbers to aid% review. Please do  refer to these line numbers in your paper as they% will be removed during generation of camera-ready copies.% % % The file \verb+neurips_2024.tex+ may be used as a ``shell'' for writing your% paper. All you have to do is replace the author, title, abstract, and text of% the paper with your own.% % % The formatting instructions contained in these style files are summarized in% Sections , , and  below.% % % % % % % The text must be confined within a rectangle 5.5~inches (33~picas) wide and% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point% type with a vertical spacing (leading) of 11~points.  Times New Roman is the% preferred typeface throughout, and will be selected for you by default.% Paragraphs are separated by {2}~line space (5.5 points), with no% indentation.% % % The paper title should be 17~point, initial caps/lower case, bold, centered% between two horizontal rules. The top rule should be 4~points thick and the% bottom rule should be 1~point thick. Allow {4}~inch space above and% below the title to rules. All pages should start at 1~inch (6~picas) from the% top of the page.% % % For the final version, authors' names are set in boldface, and each name is% centered above the corresponding address. The lead author's name is to be listed% first (left-most), and the co-authors' names (if different address) are set to% follow. If there is only one co-author, list both author and co-author side by% side.% % % Please pay special attention to the instructions in Section % regarding figures, tables, acknowledgments, and references.% % % % % % % All headings should be lower case (except for first word and proper nouns),% flush left, and bold.% % % First-level headings should be in 12-point type.% % % % % % Second-level headings should be in 10-point type.% % % % % % Third-level headings should be in 10-point type.% % % % % % There is also a \verb+\paragraph+ command available, which sets the heading in% bold, flush left, and inline with the text, with the heading followed by 1\,em% of space.% % % % % % % These instructions apply to everyone.% % % % % % The \verb+natbib+ package will be loaded for you by default.  Citations may be% author/year or numeric, as long as you maintain internal consistency.  As to the% format of the references themselves, any style is acceptable as long as it is% used consistently.% % % The documentation for \verb+natbib+ may be found at% %   % % Of note is the command \verb+\citet+, which produces citations appropriate for% use in inline text.  For example,% %     investigated\dots% % produces% %   Hasselmo, et al.\ (1995) investigated\dots% % % % If you wish to load the \verb+natbib+ package with options, you may add the% following before loading the \verb+neurips_2024+ package:% %    {natbib}% % % % If \verb+natbib+ clashes with another package you load, you can add the optional% argument \verb+nonatbib+ when loading the style file:% %    \usepackage[nonatbib]{neurips_2024}% % % % As submission is double blind, refer to your own published work in the third% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our% previous work [4].'' If you cite your other papers that are not widely available% (e.g., a journal paper under review), use anonymous author names in the% citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.% % % % % % Footnotes should be used sparingly.  If you do require a footnote, indicate% footnotes with a number in the% text. Place the footnotes at the bottom of the page on which they appear.% Precede the footnote with a horizontal rule of 2~inches (12~picas).% % % Note that footnotes are properly typeset  punctuation% marks.% % % % % % % % % All artwork must be neat, clean, and legible. Lines should be dark enough for% purposes of reproduction. The figure number and caption always appear after the% figure. Place one line space before the figure caption and one line space after% the figure. The figure caption should be lower case (except for first word and% proper nouns); figures are numbered consecutively.% % % You may use color figures.  However, it is best for the figure captions and the% paper body to be legible if the paper is printed in either black/white or in% color.% % % % % % All tables must be centered, neat, clean and legible.  The table number and% title always appear before the table.  See Table~.% % % Place one line space before the table title, one line space after the% table title, and one line space after the table. The table title must% be lower case (except for first word and proper nouns); tables are% numbered consecutively.% % % Note that publication-quality tables  We% strongly suggest the use of the \verb+booktabs+ package, which allows for% typesetting high-quality, professional tables:% %   % % This package was used to typeset Table~.% % % % % % Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \ anyway; see  and  for more information.)% % % % Do not change any aspects of the formatting parameters in the style files.  In% particular, do not modify the width or length of the rectangle the text should% fit into, and do not change font sizes (except perhaps in the%  section; see below). Please note that pages should be% numbered.% % % % % % Please prepare submission files with paper size ``US Letter,'' and not, for% example, ``A4.''% % % Fonts were the main cause of problems in the past years. Your PDF file must only% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to% achieve this.% % % % % % \item You should directly generate PDF files using \verb+pdflatex+.% % % \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the%   menu FilesDocument PropertiesFonts and select Show All Fonts. You can%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is%   available out-of-the-box on most Linux machines.% % % \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use%   "solid" shapes instead.% % % \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use%   the equivalent AMS Fonts:% %    % % followed by, e.g., \verb++, \verb++, or \verb++% for ,  or .  You can also use the following% workaround for reals, natural and complex:% %    {I\!\!R} %real numbers%    {I\!\!N} %natural numbers%    {I\!\!\!\!C} %complex numbers% % Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.% % % % % % If your file contains type 3 fonts or non embedded TrueType fonts, we will ask% you to fix it.% % % }% % % Most of the margin problems come from figures positioned by hand using% \verb+\special+ or other commands. We suggest using the command% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the% figure width as a multiple of the line width as in the example below:% %    \usepackage[pdftex]{graphicx} ...%    \includegraphics[width=0.8\linewidth]{myfile.pdf}% % See Section 4.4 in the graphics bundle documentation% ()% % % A number of width problems arise when  cannot properly hyphenate a% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when% necessary.% % % Use unnumbered first level headings for the acknowledgments. All acknowledgments% go at the end of the paper before the list of references. Moreover, you are required to declare% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).% More information about this disclosure can be found at: .% % % Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the  environment provided in the style file to automatically hide this section in the anonymized submission.% % % % % % References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for% the references. Any choice of citation style is acceptable as long as you are% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)% when listing the references.% Note that the Reference section does not count towards the page limit.% \medskip% % % {% \small% % % [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen% (eds.), {\it Advances in Neural Information Processing Systems 7},% pp.\ 609--616. Cambridge, MA: MIT Press.% % % [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:% TELOS/Springer--Verlag.% % % [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and% recall at excitatory recurrent synapses and cholinergic modulation in rat% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.% }% % % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % \appendix% % % % % Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.% All such materials % % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % \newpage% % % % % % %%% BEGIN INSTRUCTIONS %%%% % The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and precede the (optional) supplemental material.  The checklist does NOT count towards the page% % limit. % % % % Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:% % % %     \item You should answer , , or .% %     \item  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.% %     \item Please provide a short (1â€“2 sentence) justification right after your answer (even for NA). % %    % \item {\bf The papers not including the checklist will be desk rejected.}% % % % % % {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.% % % % The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "" is generally preferable to "", it is perfectly acceptable to answer "" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "" or "" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer  to a question, in the justification please point to the section(s) where related material for the question can be found.% % % % IMPORTANT, please:% % % %     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},% %     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}% %     \item {\bf Do not modify the questions and only use the provided macros for your answers}.% %  % %  % % % % %%% END INSTRUCTIONS %%%% % % % % % % % % % \item {\bf Claims}% %     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.% %         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. % %         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. % %         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. % %     % % % % \item {\bf Limitations}% %     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. % %         \item The authors are encouraged to create a separate "Limitations" section in their paper.% %         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.% %         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.% %         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.% %         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.% %         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.% %         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.% %     % % % % \item {\bf Theory Assumptions and Proofs}% %     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not include theoretical results. % %         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.% %         \item All assumptions should be clearly stated or referenced in the statement of any theorems.% %         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. % %         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.% %         \item Theorems and Lemmas that the proof relies upon should be properly referenced. % %     % % % %     \item {\bf Experimental Result Reproducibility}% %     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not include experiments.% %         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.% %         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. % %         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.% %         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example% %         % %             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.% %             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.% %             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).% %             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.% %         % %     % % % % % % \item {\bf Open access to data and code}% %     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that paper does not include experiments requiring code.% %         \item Please see the NeurIPS code and data submission guidelines () for more details.% %         \item While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).% %         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines () for more details.% %         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.% %         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.% %         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).% %         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.% %     % % % % % % \item {\bf Experimental Setting/Details}% %     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not include experiments.% %         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.% %         \item The full details can be provided either with the code, in appendix, or as supplemental material.% %     % % % % \item {\bf Experiment Statistical Significance}% %     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not include experiments.% %         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.% %         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).% %         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)% %         \item The assumptions made should be given (e.g., Normally distributed errors).% %         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.% %         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.% %         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).% %         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.% %     % % % % \item {\bf Experiments Compute Resources}% %     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not include experiments.% %         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.% %         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. % %         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). % %     % %     % % \item {\bf Code Of Ethics}% %     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics ?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.% %         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.% %         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).% %     % % % % % % \item {\bf Broader Impacts}% %     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that there is no societal impact of the work performed.% %         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.% %         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.% %         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.% %         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.% %         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).% %     % %     % % \item {\bf Safeguards}% %     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper poses no such risks.% %         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. % %         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.% %         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.% %     % % % % \item {\bf Licenses for existing assets}% %     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not use existing assets.% %         \item The authors should cite the original paper that produced the code package or dataset.% %         \item The authors should state which version of the asset is used and, if possible, include a URL.% %         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.% %         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.% %         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,  has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.% %         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.% %         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.% %     % % % % \item {\bf New Assets}% %     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not release new assets.% %         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. % %         \item The paper should discuss whether and how consent was obtained from people whose asset is used.% %         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.% %     % % % % \item {\bf Crowdsourcing and Research with Human Subjects}% %     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? % %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.% %         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. % %         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. % %     % % % % \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}% %     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?% %     \item[] Answer:  % Replace by , , or .% %     \item[] Justification: % %     \item[] Guidelines:% %     % %         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.% %         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. % %         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. % %         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.% %     % % % %  Important tasks such as reasoning and planning are fundamentally algorithmic,  % Accordingly, AI systems targeting reasoning need to have algorithmic ability.  Large Language Models lack true algorithmic ability primarily because of the limitations of neural network optimization algorithms, their optimization data and optimization objective, but also due to architectural inexpressivity. % Accordingly, LLMs and their optimization need to be fundamentally redesigned if they are to perform true reasoning. To solve this, our paper proposes augmenting LLMs with a library of fundamental operations and sophisticated differentiable programs, so that common algorithms do not need to be learned from scratch. % and can be composed when learning new tasks. We add memory, registers, basic operations, and adaptive recurrence to a  % billion-parameter scale  transformer architecture built on LLaMA3. Then, we define a method for directly compiling algorithms into a differentiable starting library, which is used natively and propagates gradients for optimization. % With the library, the model can be trained on new tasks, and can integrate natural language processing with algorithmic ability. % Because the library is amenable to optimization, it is possible to train the model on new tasks.% The primary goal of adaptive neural compilation was : intentionally biasing programs to a known, fixed input distribution, such as partially sorted lists. % In contrast, this paper focuses on learning  tasks with an unaltered library, and LLM integration. % By studying this % The main challenge is guaranteeing correct tool use, which we address by a bootstrapping algorithm based on differentiable lookup tables, which exploits their shallow gradient paths.%to improve convergence to correct tool use.% and that existing language abilities are unaffected. % Existing abilities are kept intact by learned gating and parameter masking.% The approach solves several algorithmic tasks, such as natural language list sorting (improved from  to  accuracy).% Overall, our goal is to create a neurosymbolic architecture suitable for general algorithm induction, while still retaining the strengths of large language models.% Accordingly, the architecture still allows loading pre-trained weights of LLaMA3, which enables sophisticated natural language parsing.% Beyond proposing and evaluating our specific architecture, we provide a broader theory of the connection between programs, neural networks, and inducing algorithmic reasoning.% More generally, specifying a program library models the way that humans share explicit knowledge culturally, preventing the need to induce all knowledge from raw data (in contrast to typical machine learning algorithms which start from random initializations). % } \elab{We experiment with augmenting both BERT and LLaMA 3 with a neurally compiled library, and find we can outperform unaugmented models by several orders of magnitude.Introductionsolomonoff1964formal, hochreiter1997long, gulwani2017program, chaudhuri2021neurosymbolicliu2022transformers, zhang2022paradoxneural compilationgruau1995neural, bunel2016adaptive, lindner2024tracr, giannou2023loopedellis2021dreamcoderContributionsNeural Compilationsiegelmann1992computationalsiegelmann1992computational, gruau1995neuralbunel2016adaptivebunel2016adaptiveweiss2021thinking, lindner2024tracr, friedman2024learning, merrill2022saturated, giannou2023loopedRelated Workrelated_workPrevious Neural Compilation TechniquesAdaptive Neural Compilationbunel2016adaptivebunel2016adaptiveRASP/Tracr/CoNNweiss2021thinkinglindner2024tracrweng2023masteringModelbunel2016adaptivebunel2016adaptive     % \Delta &= (\delta, \theta, A, I, , S) \\      S &= (M, R, c, h) Library StructureclassifyingCall InstructioncallstorecallstoreParsing/Selection ProblemparseselectexperimentsModel BackgroundDifferentiable Memorybunel2016adaptive     r_j = M_{ij} a_i 

    % k &= (b \otimes ) \odot M \\     % w &= a \otimes c \\     {M} &= (1 - a) \odot {M} + a \otimes c \\     {M} &= (1 - p) \odot {M} + p \odot {M} Differentiable Registers     R_{ij} &= (1 - a) \odot R_{ij} + a \otimes c  readwriteDifferentiable Register Machineminimal_assembly_examples-1em There are two special instructions necessary:  and , which control program flow.  The  instruction takes two inputs: a register holding a conditional flag, and a register holding a program address. If the conditional flag is true (equal to 1), then the program jumps to the new program address.  % In practice, addresses can be referenced via labels. Finally, the  instruction simply finishes the control flow, without executing the remainder of the program.  Since this instruction is probabilistic, it is thresholded when executing programs in practice.  A particular probabilistic instruction  is a multinomial distribution over all possible instructions in .  Accordingly, each instruction has an individual probability, and in particular we denote the special scalar portions of  as , , and  for the components representing halting, jumping, and writing probabilities. jumphaltjumphalt-1em-1em Example Assembly  %  inc  2 2  fib_loop:     write 3 2     add   1 2 2     read  3 1     write 3 2     inc   3 3      jump  4 fib_loop minimal_assembly_examplesProbabilistic Executionsuperpositioneverything, everywhere, all at oncefig:main c_{t+1} =  (1 - j) \cdot {(c_{t})} + j \cdot {((1 - p) \cdot (c_{t}) + p \cdot l)} Differentiable Interpreter     o_l = T_{ijkl} f_i u_j v_k addeq:write_regExperimental ResultsexperimentsresultsPreliminary StudiesImpact of Recursion Depth on Trainability via FibonacciSmall TransformersModular ArithmeticAdd 3 and 4Recursive FibonacciCalculate the fibonacci function, starting from inputs 6 and 2, to a recursion depth of 3Conclusioncompilationdecompiledgaunt2016terpret, valkov2018houdini, gowal2019scalable, saldyt2022synthesized, bunel2024verifiedAppendixLookup Tables  Tables trade memory for computation by pre-calculating the answers to input combinations.  Intuitively, these take similar form to grade-school arithmetic tables (right). To access a lookup table differentiably, one-hot encoded unit vectors are used as indices for lookup via sequential dot products. For instance the number  encodes to  for encoding . A dot product in one axis is equivalent to selecting the row or column containing , e.g. . If the other operand is  (), then a second dot product selects the final element, , which is the answer to , the two index vectors.  In practice, answers in a lookup table such as this are encoded using unit vectors, making a 3D tensor  where the axes  and  correspond to the first operands, and  is the encoding dimension of the answer. Then, a lookup is the einstein summation: lookup_tablesDifferentiable Lookup Tableslookups

    \\     A grade-school multiplication table, encoded differentiably for modulo :

                  00001 & 00001 & 00001 & 00001 \\           00001 & 00010 & 00100 & 01000 \\           00001 & 00100 & 10000 & 00010 \\           00001 & 01000 & 00010 & 10000 \\          bmatrix-1emNumerical RepresentationOne-Hot Number EncodingsBinary Number EncodingsBinary to Unit ConversionsFreezing Library Programs     \theta_t = \eta \nabla (\theta, x) + \theta_{t - 1}

    \theta_t = \eta \nabla (\theta, x) \odot  + \theta_{t - 1} Neural Compilation by Memorizationcontextualinitializationswe can approximately compile a program by memorizing itintentionally overfitting a sub-network to output a particular program     \tilde \theta = \min_{\theta \in \Theta} (\delta(\theta, e(x))) Technical Backgroundbackgroundmodelbunel2016adaptivegraves2014neural, graves2016hybrid, hochreiter1997longDifferentiable Lookup Tables     c_k = T_{hijk} f_h a_i b_j circuitsDifferentiable Circuitsblondel2024elementsandornotxor     & = a \cdot b \quad & \quad  &= a \cdot b + (1 - a) \cdot b + a \cdot (1 - b) \\     & = (1 - a) \cdot b + a \cdot (1 - b) \quad & \quad  &= 1 - a ModelmodelDifferentiable Computer     \Delta &= (\delta, \theta, A, I, , S) \\      S &= (M, R, c, h) contextual     \delta : \theta, z \mapsto \varrho

    \iota_i = \varrho_{ij} c_i

inc  2 2  write 3 2 add   1 2 2 read  3 1 write 3 2 inc   3 3  jump  4 1

def sort(l):     if len(l) <= 1:          return l     pivot = len(l) // 2     v     = l     left  =      right =      mid   =      return sort(left) + mid + sort(right)

def sort(l):     pivot = len(l) // 2     l, r = split(pivot)     return sort(l) + sort(r)

read 0 1 read 1 2 inc 0 dec 1 add 0 1 2 write 2 3

read 0 1 inc 0 add 0 1 2 write 2 3

read 1 2 dec 1

def quicksort(data):     if len(data) <= 1:         return data     pivot = data     left  =      equal =      right = 

    for element in data:         if element < pivot:             left.append(element)         elif element == pivot:             equal.append(element)         else:             right.append(element)     return (quicksort(left) + equal            + quicksort(right))

sort:     copy r9 r2      set   3 r8     set 1 r5     store r7          add   r7 r8 r7     jump r5 pivot     ...