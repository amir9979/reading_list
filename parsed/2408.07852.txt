% [what is knowledge graph]% [how we converted KG into a dataset for LM pretraining (including data splits)]% [describe knowledge graph data and how we use them to control model knowledge] We propose using a  (KG) as a way of controlling the information a model sees.  KGs are structured, factual data, that are often used within organisations to feed knowledge into various applications. We use KG as it provides a repository of information which is self-consistent, and mirrors the structure of information in the real-world; the hope is that this mirrored structure will ensure that the character of any hallucinations we see is somewhat similar to hallucinations we would see from models trained on data more typical for LM training.  The main benefit to using a KG, however, is that we know exactly what a model has seen, and since it is structured data, we can easily query the data to see if its predictions are correct.

%  -- think it makes more sense to talk about dataset breakdown in this section.}

The KG we use  contains semantic triples: a  , , and .  % All LMs we train will be encountering data samples in this format. %Breaking the KG down into this format allows us to create easy data samples---analogous to sentences in natural language dataset---to train models on. We further insert special tokens before each of the , , and , as indicated in , and use the concatenated strings to train our LMs. The processing removes the ambiguity of natural language, which makes the task both easier and harder for an LM. The task is easier because the samples are now : LMs no longer need to pick up the intricacies of grammar and distinguish different phrasings, and can instead just focus on learning facts. It is, however, also harder, because there is very little correlation between data samples, unless they share items, and thus very little positive transfer between learning one fact to the other.

=-1 In later sections (), we will be training and evaluating LMs and hallucination detectors. We therefore need to carefully design data splits to fully understand the impact of data. We design datasets that reflect three levels of visibility:  1)~a  (FVS) that both the LMs and detectors are trained on,  2)~a  (PVS) that only the LMs are trained on, and finally  3)~an  (IVS) that neither the LM or the detector have seen. We then vary the sizes of FVS and PVS to study the effect of scale.  

=-1 To construct these three sets of triplets, we perform an i.i.d. split at the subject level. Some subject-predicate pairs are associated with multiple objects (e.g., names of tracks on an album). In these cases, we need to ensure that all objects associated with a given subject-predicate belong to the same set, as otherwise we might label correctly deduced object predictions as hallucinations. A similar issue can exist at the subject level, e.g., age can be deduced from date of birth. % Some subject-predicate pairs are associated with multiple objects (e.g., names of tracks on an album).% In these cases, %we ask our LMs to remember all the associated objects.% %To create the dataset splits, we therefore need to % we ensure that all objects associated with a given subject-predicate belong to the same set, as otherwise we might label correct object predictions (e.g., obtained by deduction) as hallucinations.% Since the same issue might exist at the subject level---e.g., age can be deduced from date of birth---we decided to split i.i.d.\ at the subject level.% This sampling is done by using a hash. % A separate hash of the subject and predicate together is also done in order to get a validation set that we can look at for training classifiers. %  lists all the different datasets created along with their sizes.% \looseness=-1 Several subject-predicate pairs are associated with hundreds of objects. To simplify evaluations, we remove all subject-predicates linked with more than 20 objects. This eliminates extreme long-tails that would be hard to meaningfully analyse.  reports the resulting dataset size.

% % =-1 We trained decoder-only transformer LMs  with varying number of non-embedding parameters (3.15M--1.61B), on various sizes of the KG dataset (1\%--100\%). The parameters are optimized with respect to the autoregressive cross-entropy loss over formatted strings created from triplets with special tokens ().

=-1 Where a single triplet does not fill the context window (256 tokens), we used packing (on average, 20 triplets fit into the context window). For optimization, we used Adam  with linear learning rate warmup from 0 to our base learning rate (4K steps), followed by cosine decay down to 5\% of the base rate. We varied the total number of steps to study the effect of multi-epoch training (see 

for details). The base learning rate is set to a constant divided by the square root of the number of LM's non-embedding parameters. The constant was determined by a hyperparameter search over 2.5, 5, and 10  (due to compute limitations this exploration was not done for all models). The exact learning rates we used can be found in  in Appendix.

=-1 In , we have seen hallucination rates typically decay with LM size and training length. However, even LMs much larger than currently considered optimal---for given training set size---continue to hallucinate 5\% of the time on data seen and 50\% on data unseen during training (). Our experiments also exhibit a trade-off between in-distribution and out-of-distribution hallucination rates (). We therefore need to understand whether it is possible to further reduce hallucination rates by other means.

=-1 There are many types of alternative interventions, ranging from retrieval to model self-correction (see ). One promising direction is  which try to identify hallucinations either from the LM output itself, or from the LM's internal representations . Our aim is to understand  (i)~how the effectiveness of hallucination detectors depends on the scale and training length of the LM they are judging, (ii)~what types of detectors perform better, and (iii)~if there is evidence other interventions beyond detectors are needed.

We explore two types of detection tasks: =-1 For each of the tasks, we experiment with two types of hallucination detectors:

The combination of  task and  detector applied to the top layer embeddings is similar to the approach taken in . % The results should provide an upper bound on the performance achievable by finetuning the model to output , since we keep the classifier separate from the original LM parameters. For  and , the results should provide an upper bound on the performance achievable by finetuning the whole LM to say `I don't know' (IDK), as we do not force the detector to preserve any of the other LM capabilities. For the  task, the setup is related to techniques based on post-hoc verification, including self-critique . The combination with  detectors in particular should provide an upper bound on performance achievable by self-critique, as we force the preserve any of the other LM capabilities.% \looseness=-1 We trained a distinct detector (of each type) for every combination of a pretrained LM and detection task. Training and evaluation data are obtained by generating 5 object predictions for every subject-predicate in the LM training set. This data is split into a detector training (90\%), validation (5\%), and test sets (5\%).  The validation set is used for hyperparameter tuning and early stopping, the test set for measuring  performance.

=-1 For the  experiments, we use the Adam optimizer , with 1K warm-up steps and cosine decay (5K steps for , 20K steps for ). Peak learning rates 1e-4 was used for , and 5e-5 for  detectors. Training length and learning rates were determined using the validation set, optimizing for high AUC-PR. We did not finetune every possible combination of LM, detector task, and algorithm due to the large size of the Cartesian product (over 300). Visually, the above choices were optimal (or close) for most setups, but may not be optimal. 

=-1 For the  experiments, the Adam optimizer alone underperformed for the  detectors, so we switched to . LPFT works in two stages: (i)~only the readout layer is optimized, with all other weights frozen; (ii)~both readout and other layer weights are optimized. We used the Adafactor optimizer  for both stages. In the first, we used a cosine decay schedule with peak learning rate 1e-2 for 10K steps. In the second, we used linear warmup (10K steps) combined with cosine decay (250K steps) with peak learning rate 1e-3. For  detectors, only the first stage was used. The number of training steps and learning rates were determined using the validation set, aiming for high AUC-PR. Unlike in the  setup, we found it harder to find a common set of hyperparameters that would work for all detectors. % This was resolved by adding early stopping (based on AUC-PR measured on the validation set).=-1  shows how the pretrained LM size affects overall accuracy across tasks and approaches (). As expected, the  detectors outperform , as they tend to be more flexible. Since this is also the case for other metrics, we focus on  detectors in the rest of this section.  The  task formulation generally yielded better detector accuracy than the  task. Detector accuracy also tends to grow with the size of the underlying LM. However, these results are confounded by sensitivity of the accuracy metric to the underlying LM hallucination rate (see ). For example, if the rate is only 5\%, even a naive detector which catches all hallucinations attains 95\% accuracy.

=-1 We use AUC-PR to assess how well our detectors identify hallucinations. In , we observe that: (a)~ task formulation is superior in terms of AUC-PR, and (b)~the lower the LM's hallucination rate, the worse the detector's AUC-PR. Per , the lowest hallucination rates are achieved by the largest longest trained models. Thus, for a fixed training set size, there is an  between the FLOPs spend on the LM pretraining and the detectability of its hallucinations.

=-1  emphasizes the inverse relationship between LM scale and hallucination detectability, showing the PR curves corresponding to the  task AUC-PR values in . Note the general ordering of the curves, with those  corresponding to detectors for the largest LMs being the lowest, and the ones for the smallest LMs being the highest.

% [(maybe appendix) plot generalization to unseen data]% [FIG/TABLE: hallucination rate pre-/post-intervention]% %