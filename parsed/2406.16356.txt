In the story-ending generation task guided by instructions, the goal is to generate a story-ending that aligns with the given story context and reflects the provided instruction. More formally, let  be the story context where  denotes the  sentence of the story context, and  represents the free-from instruction. Given  and  as inputs, the objective is to generate the following story-ending , which can be formulated as 

We used the Possible Stories dataset~ for story-ending generation experiments. This dataset is derived from ROCStories~ and proposed for MRC tasks. Possible Stories comprises triples, each containing four sentences of story context, four one-sentence ending candidates, and questions designed to distinguish one ending from the others. 

In our settings~(Figure~), we regarded the story context as the input and the ending as the output.  Additionally, we converted the question~() into a free-form instruction~() as an additional input using the prompt template, because the questions contain information to generate specific endings, such as dangerous outcomes. Each context has multiple questions, leading to making diversity.

It is not easy to assess how well the LLMs follow the instructions on the story-ending generation directly due to its diversity and style. Therefore, we use the MRC model as a proxy metric for instruction-following ability in story-ending generation. We denote this metric as IFSM~(nstruction ollowing core using the RC model).  In this context, the MRC task is formulated as a multiple-choice that selects suitable story-endings aligned with context and questions from the ending options. This can be formulated as Eq.  where  denotes a predicted label from the MRC model and  represents the th ending candidates. As for the MRC model, we train the model using half of the original Possible Stories dataset for each train, valid, test split.  We use the DeBERTa~ for training because it is the best accuracy model on the original paper with Possible Stories.

Given the MRC model, we obtain IFSM in two steps~(Figure~).  : Generate a story-ending based on story context and free-form instructions, which is converted from the question, from instruction-tuned LLMs, then replace each gold answer with the generated ending on the test set of Possible Stories. For example, if  is the gold ending, then we replace it with . : Predict a suitable ending based on the question, which serves as the source of instruction, from the endings that contain the replaced candidate. Then check whether the predicted ending label matches the gold label, and calculate the average scores for the overall test set, as in Eq. .

where  is the gold label,  denotes the th instance,  represents the number of test dataset, and  returns 1 if  is true and 0, if otherwise. Suppose the story generation model can generate a more suitable ending that follows the instructions.  In that case, a predicted label should match the replaced ending's label, as other endings are created as negative examples for instructions.

We used the instruction-tuned LLMs for our comparison, including FLAN-T5(small to xxl)~, Llama2-7B-chat~, Llama3-8B-Instruct~, Mistral-7B~ as open-source LLMs and GPT-3.5 as proprietary LLMs (See Table  in Appendix~ for the detail).

The following prompt is used for a generation:

Here,  and  are from the Possible Stories. We incorporated a length constraint because some LLMs tend to generate longer endings without length constraints, which can affect a fair comparison. Longer endings can contain more easily conditioned information.

For simplicity, we used default parameters to generate endings for both the open-source and proprietary LLMs.

We confirmed whether IFSM accurately measured the instruction-following ability of LLMs by comparing human judgment. To address this, we sampled LLM-generated endings and asked annotators to rate them. We focused on comparing four LLMs~(FLAN-T5-xxl, Mistral-7B, Llama-3-8B, GPT-3.5) and the oracle for simplicity, and sampled 45 instances. To balance the instance distribution, we sampled instances that are predicted as gold labels (Follow) and labels other than gold (Not Follow) by the MRC model. In our human evaluation setting, annotators were asked to rate the 1-5 scale concerning fluency, coherence, and instruction-following, a higher value indicates a better score. Before the evaluation, we verified the validity of the wording and the operability of the interface for the human assessment using an annotator who is a business-level English speaker.  We asked two other internal annotators who are native English speakers to rate these instances.  Appendix ~ describes the evaluation format and interface.

Table~ shows the results of the human evaluation. A score gap~() exists in the instruction-following perspective between the labels predicted by the MRC model, whereas not much difference exists between the other two perspectives. This demonstrated the MRC model's prediction is valid for our use-case. While the score of "Not Follow" in the instruction-following perspective is not low in absolute value, it is crucial to distinguish between "Follow" and "Not Follow" in a relative manner. This finding indicates that the IFSM can evaluate the instruction-following ability in our settings. We also investigated the correlation between evaluations by the two annotators.  The Pearson's correlations for fluency, coherence, and instruction-following are 0.43, 0.19, and 0.36, respectively. Despite the subjectivity of the evaluation, it correlates to some extent.

We confirmed the reliability of IFSM and then applied it to additional data and models for automatic evaluation.  We introduced a controllability metric (Dissimilarity) other than IFSM because LLMs should generate different endings based on different instructions and IFSM can't evaluate it. We used LaBSE~ to measure the semantic dissimilarity between ending pairs (, ) from the same context with different instructions, formulated as 1 - . We computed the average dissimilarity for each ending pair from the same context.

Table~ shows the results for each score of the compared models. Mistral-7B and Llama2-7B perform best on the IFSM, and Llama3-8B achieve high Dissimilarity.  The open-source LLMs are comparable to GPT-3.5 Although the IFSM of recent LLMs are around the oracle endings created by humans, their dissimilarities are a little behind the oracle. Further analysis is required in future work..