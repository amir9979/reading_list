Given an input , the output of a MoE module is the weighted sum of outputs from its  experts networks :

The  is the router network  output for the -th expert assignment. The router design varies for each MoE architecture~.  The dominant algorithm is  selection of largest  softmax logits from a linear layer router network, with a learnable weight matrix :

For fine-grained control of the routing decision, during MoE training there is usually an auxiliary routing loss. For example, during pre-training the MoE is trained with additional load balancing loss is to encourage uniform expert assignment~. In contrast,  proposes a load concentration loss for fine-tuning MoE to obtain a few experts specialized in downstream tasks.

 We select three open source MoE models with different architecture, size, and training recipe as described below. A summary of the specific MoE model configurations is given in Table .

~. Switch Transformers is a Sparse MoE model based on T5~, but replacing the dense Feed-forward layers (FFN) at every other transformer block with a sparse Switch FFN layer. Switch Transformers adopts Top- routing strategy. T5~ FLOP-matched to Switch Transformer models with the same activated parameter size and pre-training data sets are selected as the dense counterpart to Switch Transformers.

~.~ModuleFormer Language Model (MoLM) is a full MoE model. In each MoLM block, the FFN is a MoE layer. Besides, the self-attention layer in a ModuleFormer block is a Mixture of Attention heads layer (MoA), where only top- attention modules are activated for each token. The router design is an MLP where ,  standing for expert embedding matrix and  for input projection matrix. We select Pythia with similar activated parameter size, and training data as the dense counterparts of MoLM~. 

~.~LlamaMoE is also a full MoE model. It is constructed via parameter partitioning and continuous pre-training based on LLaMA-2-7B~. The router design of LlamaMoE is a single feed-forward layer router network. OpenLlama-3b-v2 is chosen as the dense counterpart~.

For safety evaluation, we want to study the model responses to unsafe instructions. We use a collection of safety benchmarks, including three datasets with a single safety aspect from~:  for malicious and harmful instructions,  for hate speech, and  for controversial instructions. We also incorporate the heterogeneous LLM security benchmark ~. See more dataset details in Table . To measure the harmfulness of the model responses, we employ a threefold methodology to assess the safety of model responses.  pre-trained Language Model (PLM)-based , developed by~ assigns a harmfulness score ranging from  to  to each conversation. We calculate the average of these scores across all prompt-response pairs within each dataset.  LLM-based safety predictor  evaluates whether a model response is unsafe. If so, it identifies the most probable category of violation~. We calculate the ratio of unsafe predictions to total response amount for each dataset.  assigns a risk score between  and  across eleven different risk categories for each conversation. We calculate the average of the highest scores of all responses in a dataset. Note that for all safety metrics, higher scores indicate greater harm.

For hallucination evaluation, we test the models on -shot  multi-choice    dataset~ and -shot question answering task of Natural Questions ()~.  is a collection of commonsense questions that are challenging for humans to answer accurately. Each query within this dataset is accompanied by an array of accurate and inaccurate answers. The evaluation of  is a set of multiple-choice-based metrics (MC). For , the correctness of the model response is evaluated by the Exact Match ratio. In hallucination evaluation, higher score indicates superior model performance.

Since safety and hallucination evaluations are all generation tasks, we select two sets of larger and decoder-only MoE models,  and . To better study the in-situ trustworthiness of MoE, we test all models after instruction tuning, a technique to train LLMs to follow instructions in studying the behaviors of LLMs to harmful questions and producing hallucination~. Specifically, we train them on general-purpose instruction dataset Alpaca~, with k instruction-answer pairs, where safety-related samples are removed according to~. We employ standard Alpaca prompt and finetune all models for a single epoch. By default, We update all model parameters with AdamW optimizer~, and adopt the batch size of  and learning rate of  in all cases.

The safety and hallucination evaluation results of  and  families are shown in Figure  and Table , respectively. For safety evaluation results we present two sets of models, see Appendix~ for the complete results. The observations are:

In responding to harmful questions, MoE performance is competitive to that of the similar-sized dense models. The superiority of MoE is most distinctly in the smallest model pair ( and ). Such findings substantiate that MoE is effective for not only scaling model size but also improving reliability, under greater constraints of computational resources. 

 Concerning the degree of output hallucination, MoE exhibits variability across different task types. On , all MoE models outperform dense models with distinct edges. It may be attributed to the scaling of parameter sizes, whereby larger models acquire a broader knowledge base. Conversely, on  multiple choice task, dense models outperform all  variants and . Furthermore, within MoE models, larger models tend to underperform smaller ones, as exemplified by the  and . This finding aligns with a feature of  on dense LLM, named inverse scaling, where larger models are less likely to generate correct answers~. The inverse scaling phenomenon on MoE is reasonable as its expert and router design, allow for a broader parameter search space. The expanded parameter space not only enhances generative capabilities but also potentially intensifies the formation of false beliefs during training.

 In comparing  and  model families, the latter demonstrates greater stability in safety and truthfulness across varying model sizes. For exmaple, the average safety score gap between the best and worst performing models on all safety dataset is  for , as opposed to  for . The factors contributing to this outcome are multifaceted. First,  benefits from a larger number of activated parameters. Additionally, the architecture of  is founded upon a pre-trained dense model, whereas  is trained from scratch and dependent on initial model scale.

To assess adversarial robustness, we employ a combination of standard and adversarial datasets. Standard Natural Language Inference ()~ is the standard dataset, without any adversarial tactics. The adversarial datasets include Adversarial NLI ()~ and ~.  is produced through an iterative, adversarial process involving both humans and model-in-the-loop, spanning three rounds. In each round, humans annotate examples that fully trained, powerful LLMs failed to label correctly and add them to the next round. This process underlines the weakness of LLMs, making  sufficiently difficult for evaluating adversarial robustness. ~ is a more challenging version of  test set~, by eliminating possible superficial cues. In evaluation, we measure the classification accuracy of both MoE and dense models on adversarial and standard test sets. 

Our adversarial evaluations include standard and adversarial training, each has a standard testset and an adversarial testset. For the Standard-trained model (. ), models are trained with  training set, and evaluated on  for standard accuracy (),  for adversarial robust accuracy (). While adversarial models (. ) are trained with the mixture of  and  training sets, following the method in . Then they are evaluated on  for  and  for . Specifically,  task training is split into three rounds (R1-R3) of training and testing, following the setting of . The experiments are conducted on three pairs of models:  and , both are encoder-decoder models;  decoder-only  and ;  larger decoder-only model  and . All three sets of comparative models share a common feature: the activated parameter of the MoE is almost less than or equal to that of the dense model.

The results on standard and adversarial datasets are presented in Table . Several observations can be made from here:

 From the classification accuracy, it is evident that MoE models surpasses the dense models with noteworthy difference. For encoder-decoder model,  outperform  by an average of  in .  and  in . . For decoder-only  and , despite the fact that fewer parameters are activated per token, MoE trumps the dense model by an average of  in .  and  in . . For  and , same with the fact that fewer parameters are activated per token, MoE model either performs poorer() or slightly better() than the dense model on standard test sets. However, it significantly outperforms the dense model on adversarial datasets by an average of  in .  and  in . . This observation validate the superior robustness of MoE against formidable adversarial examples across architecture.

There may be a case for skepticism that the increased classification accuracy on adversarial datasets is a consequence of larger model size, as scaling laws~ suggested. The overall parameters in MoE far exceed that of the dense model because of sparsity, despite the same or fewer parameters activated for each token. Thus, we evaluate models on standard datasets to compare the performance increase in standard and adversarial datasets. The result shows that the advantage of MoE is more significant in adversarial . , which is ,  and , compared with that of of ,  and  in standard dataset. This phenomenon is also observed in the .  dataset. Overall, The performance enhancement of MoE on adversarial datasets exceeds that on standard datasets. This may indicate that the adversarial robustness of MoE does not stem exclusively from larger total parameters. 

To assess out-of-distribution () robustness, we incorporate benchmark  in our study, with of several style transformations~  formulated by . For this benchmark,  is selected as the in-distribution () dataset. We synthesize OOD data from  in two levels:  word-level transformations include both generic text augmentations and substitutions with Shakespearean style words, and  sentence-level style alterations draw on paraphrasing methodologies from , culminating in a total of  OOD datasets. 

In all the OOD benchmarks, MoE and dense models are fine-tuned on the In-domain dataset and evaluated utilizing both the test sets of the In-domain and OOD datasets. To draw a balanced comparison of the OOD robustness between models, we compare the average performance across all OOD datasets with that of In-domain datasets. Similar to adversarial robustness evaluation, we experiment with  and ,  and  and  and .

The results on the  datasets are presented in Table . From the results, we can observe that MoE consistently outperforms the dense model in adversarial and OOD robustness. Some findings can be concluded here.

:  In the evaluation results of  and , we observe a substantial  increase in accuracy of the MoE over the dense model on OOD datasets, compared to a  improvement in that of the In-domain datasets.  MoE models outperform larger dense models in adversarial and OOD benchmarks, even when less as good as dense in standard and In-domain tests. For example. Compared to  is 0.67\% behind in In-domain data, but 0.34\% better in OOD. This also applies to  to and . All these findings again proves the robust characteristics of MoE.  This question echoes the same inquiry brought up in the section of adversarial robustness evaluation. We compare model improvements on OOD datasets with those on In-domain datasets, mirroring the comparison made between adversarial and standard datasets with consistent results. The  () outperforms the  () by  in SST-2 but doubles that improvement on OOD datasets of . The same trend is observed with the  () and  () comparison, with roughly 1.7 times greater improvements noted on OOD datasets than on In-domain datasets, even though fewer parameters of  are activated for each token than . Furthermore, the  () outperforms larger dense model  () in OOD benchmark, even when less as good as it in In-domain tests. As such, we can conclude that the OOD robustness of MoE is not a consequence of its larger total parameter count alone.

To better support our analysis that MoE routing enhances model robustness, we append a case study here. We trace the change of router output of the MoE model  on standard SST test set, and all style-transformed versions in . Specifically, for each OOD dataset and the original version, we calculate the L1 distance in routing decision ( number of different-routed tokens) to all experts at each layer. We select a few layers results from all dataset average results in Table~, and the average results on word and sentence level OOD datasets are shown in Figure~ (see detailed results in Figure~). These results indicate that routing difference widely exists across OOD datasets and model layers, meaning routing decision shifts between the same sample in In-domain and OOD situations. Especially, the routing changes concentrate in the middle layers (especially the 8th layer). Many studies prove the core information is encoded in LLM bottom and top few layers.

In our case, the semantics between the original and OOD share a high similarity. Thus, the flexibility of MoE layer-wise routing design enables keeping the core information extraction and decoding in the bottom and top layers, while diverse parameters are activated in the middle layers to handle distribution shifts. However, in the dense model, all parameters will be unconditionally activated.In particular, as the degree of style transformation increases (from p=0 to p=0.6), route differences grow larger, which means that routing can adapt to stronger OOD inputs with more different paths for tokens.

Data quality is an important factor for model performance. Previous application of LLM safety alignment  suggests fine-tuning Llama on the blend of Alpaca and safety data (, pairs of harmful instructions and refusal examples) can improve the model safety. We explore this approach by mixing  pairs of randomly sampled safety data as suggested by~ with original Alpaca dataset. Then, we train and evaluate all models on the updated dataset as described in~. Figure~ demonstrate the harmful scores and their decrease compared to training without safety samples. It shows that MoE is more prone to adapt to safety data, as all model families exhibit greater improvement across datasets and metrics. In particular, the harmful scores of  decrease the most.

Many training strategies tailored for MoE have been proposed, among which the most popular approach involves  direct fine-tuning on all layers, and  freezing the router then fine-tuning backbone of the MoE model . As outlined in , fine-tuning with fixed routers slightly improve the performance on downstream tasks.  proposes a novel training framework for CNN-based MoE, highlighting the robustness of MoE by iteratively training routers and backbone, encouraging the routers and experts to collaboratively elevate the overall robustness. Inspired by it, we add a similar  bi-level training methods, where the router and the backbone of the models are trained iteratively. Further, we extend original -step bi-level training to K-step bi-level training methods, where the interval for switching iterative training is set to K. When the size of K larger than half of total training steps, this training method falls into a fix-and-free training method. In this approach, the routers join the training process after the backbones are fully fine-tuned on downstream task.

Our experiments are conducted on the NLI dataset collections in . Results presented at Table . we find a slight improvements on first types of training ( train with routers free) than the second type ( train with routers frozen), with a considerable large expert dropout rate. Regrettably, we observe minimal improvement or even negative results with the third type of training strategy (, bi-level based methods). This may stem from the fact that LLM MoE is considerably more sparse than CNN-MoE, and the relationship between routers and the backbone is far more intricate. Therefore, vanilla bi-level training methods require further optimization before being applied to LLMs.

Training MoE can be challenging due to the additional gating layer and sparsely activated expert layers, which also create more optimization space for better performance. We explore the MoE-specific hyperparameters here, including the  and the weight of the . Based on the study of , a higher  is shown to be effective in fine-tuning downstream tasks. And the non-zero weight of  can have positive effects when models are pre-trained with . We further investigate these two hyperparameters and explore their impact on the model's generalization ability (, performance on OOD datasets out of context).  The benchmark employed is all classification task from the OOD dataset suite : Natural Language Inference (), Sentiment Analysis, and Toxic Detection (), each containing  In-domain dataset and  OOD datasets.The results are presented in Tables  and . From our analysis, we identify two key findings:  A larger  increases the model's accuracy on training tasks and improves its generalization to unseen domains, whether routers are frozen or not. This finding suggests that experts of MoE may benefit from a higher dropout rate because they are sparsely activated.  Setting the weight of  for MoE to non-zero will significantly improve its generalization ability. This is because non-zero  encourages models to route tokens evenly to each expert, making each expert capable of certain tasks, thus enhancing the generalization ability of MoE. These two findings highlight the untapped potential of MoE models.  In light of these two findings, we proceeded to train MoE models and compare them to fully fine-tuned dense models, the results of which are presented in Table . Our findings indicate that MoE models consistently outperform models that have undergone complete fine-tuning.

Since the result of LLM generation depends on decoding strategies, many studies have investigated factual error mitigation from the perspective of decoding procedures~. Here we take the contrast decoding proposed by~ as an example to examine whether the general LLM hallucination reduction method applies to MoE. To reduce hallucination by contrasting the generation probabilities of different layers of LLMs, as they find that linguistic and factual information is encoded. In our implementation, we take all even numbered layers from the top half of the models as premature layers to contrast layer logits. The results are presented in Table~. From the results, MoE shows a higher increase in metrics with contrasting decoding for the previously underperformed  benchmark, most of the MoE models outperform the dense counterparts with contrast decoding. 

%  Figure~ and~ present the full safety evaluation results of  and  model families. 

 More experiments comparing the out-of-distribution (OOD) robustness of Mixture of Experts (MoE) models and dense models are carried out across all classification tasks of BOSS as indicated in reference , results shown in Table . All MoE models are fine-tuned with specified  and . The OOD performance is an average result from three corresponding OOD datasets. In these tasks, the MoE models continue to outperform the dense models significantly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \icmltitle

\icmlsetsymbol \icmlsetsymbol

\begin \icmlauthor \icmlauthor \icmlauthor \icmlauthor %{sch} %{sch} \end

\icmlaffiliation \icmlaffiliation \icmlaffiliation \icmlaffiliation \icmlaffiliation \icmlaffiliation

\icmlcorrespondingauthor \icmlcorrespondingauthor

\icmlkeywords

\begin \centering   This paper includes examples and model-generated content that may be deemed offensive.   \end

\vskip 0.2in * Equal contribution, \dag Corresponding Authors

Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs).  However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose , the first comprehensive assessment of SMoE reliability from three aspects:  safety and hallucination,  resilience to adversarial attacks, and  out-of-distribution robustness.  Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at MoE-RBench(i)(ii)(iii)https://github.com/UNITES-Lab/MoE-RBenchhttps://github.com/UNITES-Lab/MoE-RBenchIntroductionde factotouvron2023llama,Achiam2023GPT4TRbrown2020language,kaplan2020scalingetcHsieh2023DistillingSO,Lin2023AWQAWOutrageouslyLNswitch,gshard,Zoph2022DesigningESRiquelme2021ScalingVW,Mustafa2022MultimodalCL,Puigcerver2023FromSTswitchkaplan2020scaling,Frantar2023ScalingLFZoph2022DesigningES,Lewis2021BASELSWang2021AdversarialGA,Uppaal2023IsFN,Wei2023JailbrokenHD,Zhu2023PromptBenchTE,Zhang2023SirensSINarang2021DoTM,Artetxe2021EfficientLSMoE-RBenchMoE-RBenchfig:teaser(i)(ii)(iii)e.g.-3mm-2mm We design , which examines whether a MoE model matches with similar dense networks from multiple reliability dimensions, including generating safe and accurate responses, resisting adversarial attacks, and adapting to shifted data distributions. 

    MoE-RBench-2mm Our empirical observations show that the robustness of MoE models to adversarial and out-of-distribution (OOD) samples exceed their dense counterparts with a clear advantage. Moreover,  MoE robustness are sensitive to specific training configurations, and hyperparameter settings.  

    -2mm Our study also reveals that MoE models are on par with dense models and further benefit from existing instruction tuning and inference techniques aimed at enhancing security and truthfulness, even though their initially performance might lag.

    -2mm These insights are derived from extensive experiments on different model architectures (both encoder-decoder and decoder-only), model sizes, and multiple datasets. These results suggest that with optimal training and inference practices, the potential of MoE models can be more effectively harnessed. 

-2mm-4mmRelated Works-2mmSparse Mixture-of-Experts (SMoE).OutrageouslyLN,Zoph2022DesigningESmoesurvey,OutrageouslyLN,gshard,switch,Zoph2022DesigningES,Riquelme2021ScalingVW,Wu2022ResidualMO,Puigcerver2023FromST,You2021Speechmoe2MM,Mustafa2022MultimodalCLswitch,Shen2023ModuleFormerLM,Zoph2022DesigningESZhang2021MoEficationTF,Komatsuzaki2022SparseUT,llama-moeNarang2021DoTM,switch,Zoph2022DesigningESShen2023FlanMoESI,Zadouri2023PushingMONote:MoESMoE-2mmReliability Evaluation of LLMs.-1mmHallucination and Safety. Qi_Zeng_Xie_Chen_Jia_Mittal_Henderson_2023Qi_Zeng_Xie_Chen_Jia_Mittal_Henderson_2023,Mei_Levy_Wang_2023,Bianchi_Suzgun_Attanasio_RÃ¶ttger_Jurafsky_Hashimoto_Zou_2023li2023halueval, ji2023survey,bang2023multitask,zhang2023siren,lin2021truthfulqatouvron2023llama,peng2023check,ouyang2022training,Yao2023EditingLL-1mmRobustness.survey_llm_roustnessBert_attack,Is_bert_robuststyle_transformer,transformer_improve_ooddecoding_trustdecoding_truststyle_transformer-2mmPreliminary-2mmSparse Mixture of Experts-2mm      \sum_{i=0}^{n-1}()_{i}\cdot_i() -2mmgshard,switch,thor-2mm      = (( ))      -2mmgshard, OutrageouslyLN, Zoph2022DesigningESShen2023ModuleFormerLMMoE Model Architecturestab:model_statSwitch Transformersswitcht5t5ModuleFormerShen2023ModuleFormerLMBiderman2023PythiaASLlamaMoEllama-moetouvron2023llamaopenllama-2mm-2mm\texttt: how reliable is the MoE?-2mmMoE-RBenchiiiiiiTakeaways:182183width=\textwidthfigs/safety_eval_4models.pdf-3mm\small The mean harmfulness score of \textit and \textit for each dataset calculated by the \textbf, \textbf, and \textbf. Lower scores indicate less harmful (safer) responses. Different colors for each model family: (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit.fig:safety_eval_4modelsSafety and Hallucination Evaluationinstruction_tuningEvaluation Datasets and MetricsBianchi2023SafetyTunedLLMaliciousInstructionsCoNaControversialDo-not-answerWang2023DoNotAnswerADsafedata(i)Reward ModelBianchi2023SafetyTunedLL\url(ii)Llama GuardInan2023LlamaGL\url(iii)OpenAI Content Moderation API-2mmTruthfulQAlin2021truthfulqaNQKwiatkowski2019NaturalQATruthfulQATruthfulQANQ-3mmImplementation DetailsModuleFormerLlamaMoEBianchi2023SafetyTunedLL,Qi_Zeng_Xie_Chen_Jia_Mittal_Henderson_2023alpacaWang2023HowFCLoshchilov2017FixingWD-4mmEvaluation ResultsMoLMLlamaMoEfig:safety_eval_4modelshallucination_evalmore_safe172Can MoE safely respond to harmful instructions?MoLM-350M-K2pythia-410M173Does MoE answer common sense questions correctly?NQTruthfulQAMoLMLlamaMoE-3.5B-K2MoLM-700M-K2MoLM-350M-K2TruthfulQAMckenzie2023InverseSW174Which MoE is better?MoLMLlamaMoELlamaMoEMoLMLlamaMoELlamaMoEMoLM-6mmAdversarial Robustness Evaluation\small Classification accuracy (\%) of MoE and dense models on \texttt. \texttt and \texttt. \texttt after fine-tuning. The \texttt. \texttt and \texttt. \texttt refer to accuracy of standard-fine-tuned model on \texttt and \texttt. The \texttt. \texttt and \texttt. \texttt mean the accuracy of adversarial-fine-tuned model on \texttt and \texttt.1.7\columnwidth! -2mmadv_eval_a-2mmEvaluation Datasets and MetricsSNLISNLI\urlANLIadv_nli\urlSNLI-hardSNLI_hard\urlANLIANLISNLI-hardSNLI_hardSNLISNLI-2mmImplementation DetailsStdModelSNLISNLISASNLI-hardRAAdvModelSNLIANLIexplanation-improve-advSNLISAANLIRAANLIadv_nli(i)switch-baseT5-base(ii)MoLM-350M-K2pythia-410M(iii)LlamaMoE-3B-K2OpenLlama-3B-3mmEvaluation Resultsadv_eval_a-2mm172Does MoE enhance adversarial robustness?switch-baset5-baseAdvRAStdRAMoE-350M-K2pythia-410MAdvRAStdRAOpenLlama-3BLlamaMoE-3B-K2AdvRAStdRA173Does increased robustness benefit from larger parameter sizes?scaling_lawAdvRAStdRA-2mmOOD Robustness EvaluationEvaluation Datasets and MetricsOODStyle-oodstyle_transformerdecoding_trustSST-2sst2IDSST-2(i)(ii)transformer-method-2mm\small Classification accuracy (\%) of Mixture of Experts (MoE) and dense models on the SST-2 dataset under different out-of-distribution transformations (word-level, sentence-level). The parameter \textit corresponds to the top-p value used in nucleus sampling within paraphrasing methods \citep. A larger \textit value indicates a greater degree of perturbations and aligns more closely with the target style.1.7\columnwidth! -2mmood_eval-3mmImplementation Details(i)switch-baseT5-base(ii)MoLM-350M-K2pythia-410M(iii)OpenLlama-3BLlamaMoE-3B-K2-3mmEvaluation Resultsood_eval_chapterStyle-oodood_eval172MoE models surpass dense counterparts in OOD robustness with distinct advantagesswitch-baseMoLM-350M-K2pythia-1.4B, MoLM-350M-K2LlamaMoE-3B-K2OpenLlama-3B173Is the increased robustness simply due to a larger total parameter count?switch-baseMoEt5-basedenseStyle-oodMoLM-350M-K2MoEpythia-410MdenseMoLM-350M-K2pythia-410MLlamaMoE-3B-K2MoEOpenLlama-3BdenseImpact of MoE Routing on Robustnessood_eval_chapteri.e.router_difffig:router_tracingfig:route_disHow to Train A Superior MoE?Takeaways:182183width=\textwidthfigs/safety_eval_after_4model.pdf\small The mean harmfulness score of \textit and \textit for each dataset mixed with safety samples, calculated by the \textbf, \textbf, and \textbf. Lower scores indicate less harmful (safer) responses. Numbers in front of the bars refer to harmfulness score decrease compared to training without safety samples, larger decrease indicate better improvement. Different colors for each model family: (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit.fig:safety_eval_after_4models-4mmEnhanced data augments MoE safety.Bianchi2023SafetyTunedLLi.e.Bianchi2023SafetyTunedLLinstruction_tuningfig:safety_eval_after_4modelsLlamaMoETraining Strategy(i)(ii)ST-MoE,shen2023mixtureofexpertsshen2023mixtureofexpertsCNN_SMOE(iii)BOSSrouter_traini.e.i.e.i.e.-2mm-4mm-2mmHyperparameter Selectionexpert dropout rateload-balancing-lossswitchexpert-dropout-rateload-balancing-lossload-balancing-lossi.e.BOSSBOSSNLITD\texttt of Toxic Detection is replaced with \texttt due to the former's unavailability.aux_lossexpert_dropout(i)expert-dropout-rate(ii)load-balancing-lossload-balancing-lossboss_result-2mmIntervention in inference decoding alleviates MoE hallucination-2mmLee2022FactualityEL,Shi2023TrustingYE,Chuang2023DoLaDBChuang2023DoLaDBhallu_eval_afterTruthfulQA-3mmConclusionMoE-RBenchMoE-RBenchImpact StatementAcknowledgementSMoEevalicml2024AppendixAdditional Implementation DetailsAdditional Experiment ResultsMore evaluations of safetymore_safefig:safety_evalfig:safety_eval_afterMoLMLlamaMoEwidth=\textwidthfigs/safety_eval.pdf-3mm\small The mean harmfulness score of \textit and \textit model families for each dataset mixed with safety samples, calculated by the \textbf, \textbf, and \textbf. Lower scores indicate less harmful (safer) responses. Different colors for each model family: (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit.fig:safety_evalwidth=\textwidthfigs/safety_eval_after.pdf-3mm\small The mean harmfulness score of \textit and \textit model families for each dataset mixed with safety samples, calculated by the \textbf, \textbf, and \textbf. Numbers in front of the bars refer to harmfulness score decrease compared to training without safety samples, larger decrease indicate better improvement. Different colors for each model family: (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit (\sqbox1) \textit.fig:safety_eval_afterOOD evaluation on \texttt benchmarkBOSSboss_resultexpert-dropout-rateload-balance-losswidth=0.6\textwidthfigs/molm_router_tracing2_all.pdf-3mm\small The detailed routing difference of on all OOD benchmarks of \textit. We compute the L1 distance between routers of the same model when receiving in-domain and OOD samples. Lighter colors indicate larger routing differences.fig:route_dis