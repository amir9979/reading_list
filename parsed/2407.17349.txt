For lack of a Socratic teaching-based mathematics dataset, we collect and annotate a diverse dataset, , to promote the research of this domain. We construct the dataset with three phases: data collection, pre-annotation and human annotation.

The questions are mainly derived from the real primary school exams in China. To guarantee diversity, these problems cover the main maths knowledge points at the primary school level, ensuring that all the questions are manually labeled with solutions using markdown format. The questions consist of multiple-choice, fill-in-the-blank, and answer questions.  % We convert all the questions into markdown format with solutions and knowledge points. To reduce the cost of human annotation, we pre-annotate the conversations for all 8935 questions using GPT-4.  We design an in-context prompt using a manually authored high-quality example to enhance the quality of the generated conversation.  % We give the solution and knowledge points of questions to reduce hallucination problems. Additionally, we let GPT-4 act as a Socratic-style teacher with various student personality requirements (such as naughtiness, self-confidence, and carelessness) to ensure the richness of the generated dialogue. 

% More information is shown in the accompanying pages(XXXXXXXX). Though GPT-4 generates the conversation with a Socratic style for each math question, their quality is limited. First, LLMs are not good at math reasoning and their answers can be tainted with factual errors . Second, the LLMs always give solutions to answer students' questions while lacking the teaching skills with inspiration and guidance.  Thus, we clean and re-annotate the conversation to improve the data quality. Particularly, we first eliminate the data with an abnormal number of dialogue rounds.  Then, we manually perform the annotation work on the data to optimize the logic and coherence of the conversation. Each sample is labeled by three experts, who are good at teaching and math.  Due to the complexity of the conversation, the three experts label the conversation one by one to revise the errors iteratively.  Particularly, we delete more than 23\% dialogues and modify more than 18\% dialogues, where over 5\% utterances are revised.

We present the statistical information of our  dataset in Table .  Our dataset contains 513 knowledge points, almost all the knowledge points of math at the primary school level.  To better guide the student to solve the math problem, the average number of turns for each conversation is about 5.  The average length of utterances is about 86 words to provide detailed information patiently.

To show the advantages of , we compare it with existing typical mathematics datasets (Table ).  Most datasets only provide the equation or textual steps directly to solve the math problem. MATHDIAL dataset contains the semi-annotated conversation where the students' questions are generated by LLMs.  Furthermore, MATHDIAL mainly focuses on answering students' questions without the Socratic method, which requires teaching skills to inspire, guide, and inquire actively step by step.  We also provide extensive attribution information, such as related knowledge points and difficulty levels, where  and /Conv are the total number of knowledge points and the average number of knowledge points for each conversation. 

% % [t!]% \small% \centering% % % % % % % [t!]% \small% \centering% % % % % %   We adopt several typical automatic metrics for generation tasks, including BLEU  (marked as B-1/2/3/4), ROUGE  (marked as R-1/2/L), METEOR  and BARTScore , to evaluate the effectiveness of  turn by turn. We also conduct human evaluation and GPT-4 evaluation .

 We compare  with several typical and strong seq-to-seq models.  %  fine-tunes the generative base model BART  on large-scale dialogue datasets.  trains a text-to-text transformer model on multilingual datasets via multi-task learning.  and  are strong LLMs fine-tuned on our dataset.  and  act as a mathematics tutor with the Socratic method, one of the SOTA conversation models.

 We select Qwen1.5-7B as the base LLMs and train it on A800 GPU with 80G. We set the rank of LoRA as 64. The learning rate is 3e-4 and the batch size is 64.

We report the main results of  and the selected baselines using automatic, human and GPT-4 evaluation (Table ).

From the results, we observe that  achieves better results by comparing with the strong baselines over automatic metrics in most cases, indicating our model's effectiveness.  Note that GPT-4 outperforms  in terms of METEOR without training because SocraticMATH is modified based on the dataset generated by GPT-4.  Furthermore, these automatic metrics can not measure the quality of conversation in Socratic mathematical teaching.  They mainly calculate the semantic information between the generated responses and the reference while ignoring the logic and fact errors in the output text.

Single reference-based automatic metrics are not always reliable to reflect the real quality of the generated responses . Therefore, we also conduct human and GPT-4 evaluations by crowd-sourcing and GPT-4.  Particularly, we ask the three experts and GPT-4 to label 150 samples randomly selected from the test set with guidelines.  Based on a pre-determined scoring rubric, they annotate the generated response from reliability and Socratic strategy with scores 1-10.  Reliability judges whether the model corrects the students' errors precisely and Socratic represents the guide and heuristic abilities of the model.  % We use Fleiss' kappa to calculate the inter-annotator agreement, which indicates the high quality of the annotation.  We report the average scores here. From the results, we observe that  obtains the best results in both human and GPT-4 evaluations, showing that our model can reduce the hallucination with the Socratic method.  Moreover, in the human evaluation, we find that LLMs like ChatGPT tend to believe the users' responses without a doubt. It is interesting to explore in further work.

 We also conduct an ablation test  to explore the effectiveness of the main parts consisting of   by removing Socratic-style prompt (- Prompt), extra knowledge (- Knowledge) and all of them (Qwen1.5-7B), respectively.  We observe that both the Socratic-style prompt and extra knowledge are useful for . The Socratic-style prompt enhances the model to learn the teaching skills based on structured conversation.  Then, incorporating the extra knowledge into  reduces the hallucination problem by correcting the fact errors.