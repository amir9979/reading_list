A significant part of the state-of-the-art works employs standard Supervised Fine-Tuning (SFT) performed on annotations produced by a single LLM (Large Language Model) as a mechanism to improve SLMs. In our contribution, we take a step further and use Instruction-tuning, which is a task-oriented specialization of SFT (Supervised Fine-Tuning), in coordination with a teacher-student alignment approach (detailed in Appendix ). In this phase, the SLM (student) is fine-tuned on a dataset produced by LLM (teacher) comprising a set of tuples in the form of , where  represents a specific instruction,  is the input question (e.g., math-word problem), and  is the expected output and CoT answers generated from the teacher in response to the instruction and input. This setup is intended to transfer to the student models foundational problem-solving abilities, emphasizing the generation of outputs that conform to the provided instructions. The CoT answer  is articulated as:

with  indicating the sequence length. At each timestep , the action  is derived from the policy , where  can be any token from the models vocabulary, and the state  encapsulates the concatenation of all previously generated tokens and the optional input  if provided. The state transition is defined as:

The Instruction-tuning loss function explicitly integrates the instruction , aligning the models' learning process with the instructional context. This loss function is formulated as:

Here,  is conditioned on both the state , the input , and the instruction , ensuring that the model prioritizes instruction compliance in its output generation. This methodological shift from SFT to Instruction-tuning underlines the principle of enhancing the models' ability to accurately interpret and execute complex instructions.

In the second phase, the instructed SLMs (students) that have improved CoT properties via Instruction-tuning (Section ) self-refine these properties with the support of Direct Preference Optimization (DPO) . This refinement can be conducted in an SFT style, relying exclusively on labeled preference data. The policy model, defined as , learns by repeatedly sampling the answers generated by teachers and students.

In the standard DPO approach , a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs  and . However, we propose an optimization step via Self-generated annotation by the students , which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.

For each Demonstration , we prompt the students using the input  ( or ) (blue block in Figure ). Hence, for each instance within the Demonstrations we collect the  () that are the answers generated by the student given the input , and the  () are the answers that deliver CoT generated by the student elicited via CoT mechanism . 

In particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with  and responses when prompted with  just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation :

where  is the sigmoid function, and

where  is a hyperparameter.

We propose the  that uses as optimization technique  (described in details in Appendix  in Equation . In particular, in  the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response. 

In this paper, we selected different tasks that focus on reasoning tasks: 

 We adopt two benchmarks to evaluate commonsense reasoning: CommonSenseQA  (CSQA) and OpenBookQA  (OBQA) are two multi-choice commonsense question-answering tasks. 

 We adopt two benchmarks to evaluate reasoning in the context of everyday situations, aiming to establish the most reasonable solution: Interaction Question Answering (PIQA)  and Social Interaction Question Answering (SIQA) , which emphasizes people's actions and social implications.

 We use two math word problem benchmarks to evaluate the models of mathematical reasoning. MultiArith  covers a set of multi-step arithmetic reasoning tasks, while GSM8k  covers a set of primary school-level mathematical problems. 

 Finally, to evaluate the adaptability of our proposal, we conduct further analysis on two additional evaluation benchmarks: MATH , and MMLU .

Since the test split is not prescribed for all the benchmarks, we adopt the following strategy: for SIQA, PIQA, CSQA, and OBQA, we use 4000 examples with equally distributed target classes as training data and the validation versions found on huggingface as test data, while for GSM8K and MultiArith we use the full huggingface datasets. In Table , we report the descriptive statistics and splitting ratios, while in Table , we report one example for each benchmark. The supporting datasets are publicly accessible as described in Table .

The Self-refine Instruction-tuning comprises the annotation process conducted by the LLMs teachers that are prompted in the zero-shot scenario (as shown in Table ), as explained in Appendix . We selected Llama-2-70 , Mixtral7x8  and GPT-3.5  as LLMs (teachers) and Llama2-7, -13  and Mistral-7  SMLs (students) models. 

Hence, the students models are tuned, as proposed in  and evaluated with probing pipelines (detailed in Section ). The students are instructed via Demonstrations that contain the answers generated by the teachers, as explained in Section .  Downstream of the teacher-student CoT transference process, the optimization technique (proposed in Section  and detailed in Appendix ) is employed to improve alignment and self-refine the quality of the generation. 

We conduct the Self-refined Instruction-tuning in two different phases. Firstly, we start with Instruction-tuning phase using QLoRA . This approach allows Instruction-tuning to be performed while reducing memory usage. In particular,  propose several techniques for tuning models with many parameters on GPUs with limited resources while preserving 16-bit tuning performance. We follow the training approach proposed in , setting four training epochs using a learning rate of 2e-5 with a 1e-4 weight decay. We use the cosine learning rate scheduler with a warm-up ratio of 0.03.  Furthermore, we conduct the Self-refine phase following the approach proposed in . In particular, we use the huggingface  to support its reproducibility. We follow the parameters proposed in . Hence, for the DPO policy, our work employs a learning rate of 1e-6,  set at , and a warm-up step count of 100. The batch size is configured to 128. The optimization process is capped at a maximum of 1000 steps, where we save the checkpoint corresponding to the lowest loss on the validation set. The experiments were conducted on a workstation equipped with four Nvidia RTX A6000 with 48GB of VRAM.

The most commonly used evaluation methods for question-answering tasks are language-model probing, in which the option with the highest probability is selected , and multiple-choice probing, in which the models are asked to commit to an answer. The evaluation in the first case is performed with a function taking the  and, in the second case, with a direct string matching. The second method is more widely used in recent evaluations as it can be inclusive to the larger GPT family models, where probability values are not readily accessible. In the experiments, we chose the latter to have a comparable and scalable pipeline (Details provided in Appendix ). Finally, string matching is performed between the generated outputs and the target choice to evaluate the percentages of the correct answers. 

Instruction-tuning led by Larger Language Models (teachers models), which are able to deliver multi-step reasoned answers, induces this property within Smaller Language Models (students models). This can be seen in the experiments in Figure , Figure  and additional evaluations in Appendix . The student models behind instruction-tuning on demonstrations produced by teacher models outperformed the baselines of the proposed benchmarks. 

While one can observe consistent improvements in performance across the board, there are moderate variations across models and tasks.  The teacher models that generate Demonstrations stem from different families and perform differently, as shown in Table . The consequence of this phenomenon can be seen in Figure  and Figure  (horizontal lines that are the reported performance of the teachers and bars 'Instruction-tuning' that are the performance of the students). Therefore, the teacher-student alignment is not complete as there is a gap between the performances of the teachers and the students tuned via Instruction-tuning (only phase presented in Section ). In addition, it is possible to differentiate between in-family and out-family alignment. In the in-family, where students are instructed with Demonstrations delivered by the teachers of the same family, performances vary from 6.3 points on average in question-answering (QA) tasks and 8.2 points on average in math word problems (MWP) tasks. Meanwhile, in the out-family alignment, the performances vary by 8.5 on the QA and 8.7 on the MWP.

Hence, to improve the alignment both in-family and consistently out-family, we have proposed an optimization technique based on a self-refinement approach (introduced in Section ), the results of which we discuss in Section .

The Self-refine process enables complete in-family student-teacher alignment by consistently increasing performance in out-family settings and improving the qualities of generated answers. The results obtained in Figure  show that the students (SLMs instructed with Self-refine Instruction-tuning) outperform the non-self-refined students and perform comparably to their teachers. The same behaviour can be observed from the out-family setting shown in Figure . In particular, the teacher  showed a more robust baseline performance (Table ). Although Instruction-tuning alone transfers some of the abilities to the student models, they were significantly lower when compared to the out-family teacher models. In contrast, the teacher-student performances significantly converged after the self-refine phase, leading to the alignment completion. Finally, a positive impact can also be observed on the quality of students' generations, as shown in the additional experiment discussed in Appendix .

The performances appear completely aligned, but the students were tested only for in-domain tasks. The proposed approach could cause students to over-specialize in in-domain tasks, running the risk of losing the ability to solve out-domain tasks. For this reason, we performed a set of assessments evaluating students on in-domain and out-domain tasks and discussed the results in Section .

The Self-refine Instruction-tuning approach complements student-teacher alignment and improves students' generalization abilities in out-domain tasks. These results can be observed in Table  with  as students and  as teachers (in Appendix Table  with Llama2-13 Table  with Mistral-7). In particular, behind the evaluations performed on in-domain and out-domain tasks, the students Self-refine Instruction-tuned outperform the baselines and the Instruction-tuned models. Furthermore, to observe the impact of the optimization phase (introduced in Section ) on the downstream performance, we conducted a further experiment by fixing the Instruction-tuning phase and switching the Self-refine ones across different evaluation tasks (e.g., we instructed a student on OBQA and then optimized via self-refine approach on CSQA). As shown in lines  of Table , students warmed up on tasks other than those they are optimized, outperformed the others, and obtained similar performances to those obtained from in-domain models. This shows that optimization positively impacts the alignment of generalization abilities in out-domain tasks. Finally, following evaluations in out-domain tasks and across scenarios, we evaluate the performance of the proposed approach by reducing the number of demonstrations available for alignment in Section .

Self-refine Instruction-tuning achieves sustainable performances in low-resource settings. In fact, in Figure , it is possible to observe that the performance achieved by the self-refined students consistently outperforms that of the non-self-refined students (where only phase 1 described in Section  was performed) (technical details on the breakdown can be found in Appendix ).  Although it emerges that only the optimization process via DPO is more performant than the instruction-tuning process alone, the combination of the two phases achieves the best results in both in-family and out-family alignment in each proposed splitting that are described in Appendix .

Previous works focus on Chain-of-Thought (CoT) prompting techniques, studying the impact of prompting design and engineering, proposing specialized interventions to improve CoT generalization and fine-grained multi-step reasoning properties .

On the prompting design side,  proposed using Python programs as a CoT prompt, demonstrating more accurate reasoning steps and significant improvements behind CoT prompting .  introduced a code generation approach to verify the intermediate reasoning step . 

In parallel, there have been improvements in the accessibility of lower-parameter versions of Large Language Models (LLMs), which we define as Small Language Models (SLMs), on which previous CoT improvements cannot be fully observed . Therefore, several works are emerging at this gap, aiming to transfer LLM reasoning properties to SLMs. Pioneering proposals in this direction proposed teacher-student alignment methods through a series of approaches geared towards the distillation of the knowledge generated by the teacher for the fine-tuning of the student . Later,  proposed specialized Instruction-tuning using Alpaca-like style demonstrations  specialized for mathematical tasks, while  proposed supervised fine-tuning reinforced with rewarding algorithms.

A significant component that promotes the generative reasoning delivering CoT is provided by refinement via RL methods. Recent work that applies Proximal Policy Optimization (PPO)  for aligning human preferences . Several methods have been proposed to improve the efficiency of alignment , including Direct Preference Optimization (DPO) . 

In this work, we adopt RL to refine performance over conventional SFT. For mathematical problem solving,  trained an outcome- or process-based reward model to perform re-ranking , achieving better performance than SFT and majority voting .  adopted reinforcement learning as an extension of traditional supervised tuning. We adopt DPO and automate the reward process in a teacher-student context. We focus on the transfer of CoT-style, step-wise reasoning and propose a refinement technique applied to models downstream of the instruction-tuning phase. 

Complementing and enhancing foundational approaches ,   several papers have been published simultaneously  (Table  summarises the main features). These works prove the effect of supervised fine-tuning to transfer the ability to produce multi-step reasoned answers from larger to smaller models, as described in Section .  Our work goes beyond the state-of-the-art by:

    In order to observe the impact of the Demonstrations, we produced a series of experiments by systematically decreasing the Instruction-tuning data.     In particular, we chose three sub-sets with 75\%, 50\%, and 25\% from the total number of demonstrations.     In detail, the Self-refine Instruction phases on the number of equal Demonstrations are performed by taking about 3000 examples in splitting 100\%, 2250 in splitting 50\%, 1500 in splitting 50\%, and 750 in splitting 25\%. We chose the value 3000 because it has the smallest CoT Demonstrations available. For the total Demonstrations, we selected random samples. Using these splitting, we performed the evaluations incrementally as the demonstrations used to do Instruction-tuning, to do Self-refine, and to do Self-refine Instruction-tuning.

    The annotation phase that the Teachers performed was done on the training set. The evaluation phase of both the basic models and the Students and the Teachers was done on the test splitting. The evaluation, described in Section , was done with question probing and string matching of the generated answers. More specifically: 

    We performed the annotation phase for each benchmark by delivering to ,  and   the prompts structured as shown in Table  and Table  (customized for each benchmark). We set the temperatures to 0.7 for  and 0.1 for  as recommended in technical reports. Moreover, we kept all the other parameters as default. All parameters are shown in our code .

    We evaluated the performance of the Small Language Models (, , ) by prompting them with the same format used for the Teachers. For both the baselines and the instructed models, we set the temperature to 0.1 and kept all the other parameters as default. The evaluation pipelines and generation parameters are available in our code.

Experimental DetailsData Splittingsec:experimental_details_splittingParameterssec:parameterssec:EvaluationTeachersGPT-3.5-turboMixtral7x8Llama-2-70-chattab:es_prompttab:es_prompt_CoTGPT-3.5-turboLlama-2-70-chatBaseline \& StudentsLlama-2-7-chatLlama-2-13-chatMistral-7b