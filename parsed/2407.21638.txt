The proposed framework for auditing radiology report generation is shown in Fig.~. The backbone of the framework is the report generation model. The generalised vision language modeling problem of generating a textual sequence T given an input image I can be framed as estimating the conditional distribution , as a product of the conditional probabilities:

where  is modelled as a sequence of word tokens , with  being the set of tokens preceding , where  is the number of tokens in the sequence, and  the model parameters .

An overview of our report generation model, , is shown in Fig.~. The model is trained to maximise the probability of generating the target sequence from the image,  from eq~. Image features , with dimensionality , are extracted by image encoder . A linear layer, , projects  to the space of token embeddings that will be input to the language generator, giving , a set of  image token embeddings with dimensionality , similarly to LLaVA~.  A tokeniser maps text to a set of  token vectors , where  is the vocabulary size. Each  is embedded by a linear layer to . The concatenated embeddings of image and text tokens  are passed to a Transformer decoder (Fig.~b) which predicts a probability distribution over  vocabulary tokens, expressing which is more probable in the sequence. The decoder has  with Attention blocks that each has  parallel heads . The model is trained using the common paradigm of diagonal attention masking . A report  is generated with auto-regressive inference, starting only with image token embeddings  as input, then predicting the first and each following word token as the one with highest predicted probability. %alternative phrasing below%In autoregressive text generation, each word or token is generated sequentially, with the model predicting the next word based on the image and previously generated words, starting with . Each subsequent token in the sequence is selected based on the highest predicted probability, until the complete report is generated.

While some types of mistakes in AI-generated radiology reports, such as grammatical errors, are merely inconvenient, semantic mistakes related to disease diagnosis can be detrimentalor patient care.  %We here introduce a framework for auditing AI-generated reports for correctness of semantics of interest.%,  based on modular, auxiliary auditing components (ACs). %The framework consists of three parts (Fig.~ b,c,d). Consider a semantic concept of interest, denoted . We focus on the case where  represents a specific disease, with the class label  indicating the presence or absence of that disease. %, though the framework could be applied for other cases.  To audit whether a generated report  is semantically correct about ,  we need to extract the value  described in text  via a mapping function . This can be any text-based classifier, or prompt-based LLM that can answer questions in the form "Does this text suggest evidence or not for disease ?", which have become widely available. Herein we use a pretrained CheXbert text-classifier  as , obtaining  (Fig~b).

We then introduce to the framework an independent model , which infers value  for concept of interest  from image  (Fig.~c). We term such models auxiliary auditing components (AC). In the examined case when  is existence of specific disease,  is a classifier predicting  if image  shows the disease or  otherwise, with model confidence . If multiple concepts  are of interest (e.g. different diseases), an independent AC model can be used per , for ease of modular development, or a multi-label AC model.

Auditing is performed by comparing values  and  about the concept  of interest (e.g. disease) extracted from the report or the image respectively (Fig~d). To consider the contents of the report reliable with respect to , agreement between the report and AC is required, i.e. . We further extend this by taking into consideration the confidence  of the AC. We only consider the auditing successful if it satisfies the additional requirement that the value  has been inferred by the AC with confidence  over a pre-determined threshold . Formally, for a successfully completed auditing we require: 

where  is the logical  operator.  %Eq.~ means we enforce that both AC and report generator are consistent with each other and the AC is confident that class  is correct with confidence margin  above 0.5. The set of generated reports that pass the audit for a particular concept  is denoted by . The remainder, , can be deferred to the human user for review as potentially inaccurate for that disease.

The framework is designed based on ACs that are modular, independent models. The design draws inspiration from N-Version programming  and the principle of redundancy for reliability in software engineering. Assuming independence of components (, , ) with label error rates  and , the error rate of the auditing framework %and the reports  that were deemed reliable  is , which is lower than the individual error rates of  and , as failure of auditing requires both components to fail simultaneously to incorrectly satisfy Eq.~. Modular ACs enable independent component training and validation prior to integration into the audit pipeline. %Furthermore, although here we demonstrate our framework for ACs that are disease classifiers due to their semantic importance for diagnosis, our framework can be naturally extended with ACs that audit for other semantics of interest, such as disease location.% <- saves space Experiments are performed using MIMIC-CXR , containing chest X-rays (CXR), associated radiology reports, and disease labels for 228,000 studies of over 65,000 patients. As common practice, we consider only the frontal-view images, and only the ``findings'' section of the reports, which describes evidence of pathologies and support devices.  The labels %, extracted by a rule based labeler, CheXpert,  include 14 classes (12 diseases, 1 support devices, 1 no findings) and can be positive, uncertain, negative, or not mentioned.  We report 5 out of 14 classes separately (Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion) to facilitate comparison with literature . We divide data into splits for training (226,261), validating hyper-parameters (1,864), and testing performance (3,595).  To train and evaluate the report generation, we further exclude studies that do not include a ``findings'' section, resulting in 155,322 training, 1,231 validation and 2,607 testing samples. Finally, to help address class imbalance and noisy labels, following past works , we consider uncertain disease labels as positive and class no-mention as negative.

 We here show that our report generation model, , is of representative quality in comparison to previous work to serve as effective backbone for the study of our auditing framework.  To build GenX we use ResNet-50 as the image encoder , which we pre-train as a multi-label classifier of the 14 classes described above. Its penultimate layer of dimension  is then projected to  image token embeddings with . These are concatenated with up to  text embeddings, resulting from a GPT-2 tokeniser () , followed by a projection, . The Transformer has , , and dimension  of its feed-forward layer. GenX is then trained end-to-end (Fig.~), learning parameters of the randomly initialised projections and Transformer, and fine-tuning the  encoder.

%We assess the quality of our radiology report generation model to establish whether it can serve as a representative backbone in our full pipeline to demonstrate the capabilities of our auditing framework. Table~ reports test performance by our report generation model, along with previous works. %{Report Evaluation NLP metrics BLEU (BL)~, METEOR (M)~ and  (R-L)~ assess generated language quality. %Perhaps m More important for this study is the assessment of diagnosis-related semantics in generated reports, using disease-classification metrics such as F1 score, by comparing agreement of class labels, , extracted from generated and reference reports via CheXbert . Our report generator, GenX, achieves NLP metrics similar to  Ratchet , which has the most comparable architecture. Importantly, GenX achieves a high F1 score in comparison to Ratchet, R2Gen , even RGRG  that needs additional labelled bounding boxes for training. It approaches F1 score by  Trans  that trains using a more complex framework and reinforcement, and is surpassed by TERG which uses extra labels for training. We conclude that GenX produces reports of sufficient quality to serve as effective backbone for the following study of our auditing method. Two examples of generated reports are shown in Table . The generated reports use realistic language, though they can occasionally miss a diagnosis. Thus a framework that can extract and audit report semantics is crucial.

We analyse whether including auxiliary auditing components, ACs, enable us to quality control the generated reports and filter out those that are more likely to contain errors. We evaluate on the 5 diseases most commonly reported in the literature (Sec.~), from the CheXpert competition  . For this, we first train 5 independent disease-specific AC image classifiers (), where each is a ResNet-18.  We then apply the framework as described in Sec.~ to the test-split. For each image, the 5  models predict 5 disease labels , one per disease . GenX generates a report for each test image and we extract 5 report-based class labels  via Chexbert, for each disease. For a given scan, per disease , we compare image and report based labels,  and , to check if they fulfil Eq.~. We perform the check in two settings: First, without considering a confidence threshold for , satisfying only the first condition in Eq.~; Second, with a confidence threshold  where Eq.~ is fulfilled if , to investigate whether stricter auditing criteria results in more accurate reports  each disease, the images that fulfil the above criteria are considered to have successfully passed the audit. For these images, we calculate average F1 score by comparing their report-based labels  with the reference report labels, to determine whether the average score is higher than that of all unfiltered reports generated by GenX. , we repeat these experiments using a bigger ResNet-50 AC, trained as a multi-label image classifier of all 14 classes (). This is to evaluate whether it's beneficial to integrate a larger AC that learns from all 14 labels available per scan, or independent, disease-specific ACs. 

 Table~ shows the results. Class labels  extracted from reports generated by GenX achieve an average F1 score of  over the 5 diseases. Reports that agree with , fulfilling the auditing without the extra requirement for confidence , achieve average F1 score . These benefits are realised over a significant percentage of the reports (72-80\% across the diseases), while the rest would be deferred for user inspection and error correction in an actual workflow. Reports that pass the stricter auditing and  additionally fulfil the confidence requirement  present even less errors, with average F1 score . This restricts the number of reports that pass the audit (30-65\%), in return for higher reliability. In comparison to the unfiltered reports of GenX, the reports that fulfil the audit show increased F1-scores for 4 out of 5 studied diseases. The exception is Consolidation, the class where both GenX and AC image classifiers have very low performance. Interestingly, for 3 out of 5 diseases, the reports that successfully pass the audit show F1-scores that are even higher than the auditing  classifiers. These results show that measuring consistency between semantics of reports and auditing classifiers is a promising framework.

Finally, we find that a single, multi-label classifier, trained for all 14-classes as , achieves significantly lower F1 scores for image classification, which also leads to lower effectiveness of the auditing. Although  is a ResNet-50, larger than ResNet-18s used for the 5  classifiers, %and  leverages more training labels per image,  it is difficult to develop it at the level of disease-specific classifiers. This demonstrates the benefits of modularity, with independently developed AC for each concept of interest.

%This allows us to have more confidence in the generated reports where the extracted diseases match the image classifier. Reports that are observed to be inconsistent across components are more likely to contain a diagnosis error and can be referred to the user for additional checks on a per disease basis.