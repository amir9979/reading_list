Our dataset comprises almost all currently accessible large-scale LLM pretraining datasets with permissive licenses. These include: The Pile , SlimPajama , RefinedWeb , C4 , PeS2o , , and StarCoder . The Pile, SlimPajama, and RefinedWeb are general language modelling datasets which focus on text data and are primarily derived from Common Crawl and a few other sources. PeS2o is a dataset focused on scientific writing and is primarily comprised of arXiv and journal papers. StarCoder is a dataset focused on code and is comprised of code scraped and filtered from github.

Prior to deduplication, following common practice , we performed heuristic syntactic filtering for quality and to remove low-quality data, such as meaningless strings, large quantity of random numbers, as well as pornographic or otherwise objectionable content. Our filtering pipeline consisted of two stages: (1) substring replacement and (2) document-level filtering and removal.

For the first stage, we deployed regexes that replace certain substrings with more sanitized versions. This was primarily to fix common formatting issues that we noticed in our original analyses of the datasets. Examples included excessively long sequences of dashes, full-stops, ` r' characters, and other punctuation characters which presumably arose from quirks of formatting or the processing pipelines that lead to these documents. Such strings appeared with reasonable frequency and often were found amid otherwise unobjectionable documents, so we did not wish to simply remove documents in which they appeared. Typically, we replaced large numbers of repeated characters with a single or just a few characters.  For instance, we replaced large numbers of linebreaks ` n' and carriage returns ` r' with just a single linebreak or carriage return. Similarly, for long sequences of dashes, we replaced them with a a single dash. For the second stage, we performed document-level filtering based on a set of syntactic heuristics which can be cheaply computed from the raw text of a document. If the threshold of the filter was exceeded, the whole document was removed from the dataset. These filters broadly fell into three categories: (1) removing syntactically broken or otherwise gibberish documents, (2) removing semantically meaningless documents, and (3) removing meaningful but objectionable content. Examples of the first kind of filter include methods like filtering based on the proportion of alphanumeric characters in a document, which at high proportions uniformly corresponds to documents comprised of entirely gibberish strings generated either through broken preprocessing or through unknown processes on the internet. Semantically meaningless documents include documents full of seemingly random numbers, cryptographic strings, and lists of seemingly unrelated URLs. Objectionable content included primarily pornographic and offensive content, which we removed with specialized word lists.

Our general philosophy in filtering was to not filter excessively and keep false positives (i.e. good documents removed) relatively low. We manually tested and tuned each of the filters presented here on the Pile dataset. We tuned the filter thresholds so that we obtained a false-positive threshold of about 20\% â€“ that is, 20\% of the filtered documents were seemingly unobjectionable, while 80\% were obviously harmful and it was correct to remove them. This provides a reasonable trade-off between excessive filtering while still removing the majority of content the filter was aimed at reducing. For every filter, we spent significant time manually tuning the threshold and other parameters, then looking at the filtered outputs and attempting to identify true vs false positives. We primarily performed this tuning on the Pile dataset and performed only sanity checking of the filter outputs on the other datasets. However, we believe that many of the categories of low-quality text likely have similar distributions between datasets, since so many of them are derived from the same source. A full list and description of all filters can be found in Appendix .  All datasets were filtered using the same rules except for StarCoder, which we exempted since it consists entirely of code which has a significantly different distribution than the primarily text data in the other datasets. Additionally, the StarCoder authors performed a thorough code-specific filtering of the dataset before release. Table  shows the proportion and number of rows removed from each dataset by our filtering process. We observe that the primary source of data removed by our filters is the Pile and arXiv. 

To identify duplicates we used Locality Sensitive Hashing (LSH) based on MinHash signatures~.  This technique allows fast approximate identification of duplicate candidates based on Jaccard similarity of sets of -grams  in documents. For example, using  and  for  hash functions, we can measure the resemblance of documents  and  using:

In the rest of the paper, we refer to LSH-\% as the LSH with at least \% resemblance, as expressed by Equation . We deduplicated each dataset both against itself and against the other datasets in our full dataset. We built the LSH index by inserting the components in the following order: first Pile-uncopyrighted, then C4-en, peS2o, , RefinedWeb, SlimPajama, and finally StarCoder.

For our deduplication pipeline, we used 13-grams based on words to form our n-gram subsets and a minhash signature size of 128. Before generating 13-grams, we performed NFC normalization, conversion to lower case, and removal of punctuation and consecutive spaces, newlines, tabs in the middle and in the beginning and end of the strings. We performed deduplication at two Jaccard similarity thresholds: 40\% and 80\% (documents are considered duplicates if their similarity measure is equal or greater than the threshold). The parameters of LSH index were optimized to minimize the rates of false positives (FP) and false negatives (FN): for the 40\% threshold, minhash indices were split into 32 bands each with a range of 4, while for the 80\% threshold, we used 9 bands with a range of 13. From these parameters we can derive the following false-positive and false-negative rates: for 40\% threshold, the false-negative rate is  and false-positive rate is , for 80\% threshold, the false-negative rate is  and false-positive rate is . Table  summarizes how many tokens (using gpt-neox tokenizer) were removed at different thresholds. Our raw dataset consists of 2T tokens, and we end up with 1.5T for 80\% threshold and 1.3T for 40\% threshold.

After identifying duplicate pairs using the LSH minhash technique, we clustered documents into a graph of connected components with the nodes being documents and the edges connecting duplicate pairs. We then kept only one document from the cluster while removing the rest. To determine which document to keep, we sorted the documents in the clusters by their dataset of origin and kept the highest-ranking one according to the following order: 1) StarCoder; 2) RefinedWeb; 3) peS2o; 4) arXiv; 5) C4; 6) Pile-uncopyrighted; 7) SlimPajama. We chose StarCoder as the highest-ranking dataset because it was specifically designed for code, so we hypothesized that any duplicate code snippets are likely to be of the highest quality from this source. We chose the rest of the ranking based on heuristic assessments of quality.

We then performed random sampling of duplicates in the clusters to manually explore examples. The largest clusters usually contained either short low-quality documents or documents with widely distributed texts, such as license agreements, advertisements, etc. We did notice that at 40\% threshold LSH minhash was performing a qualitatively different kind of deduplication than at 80\%: while at 80\% most duplicates looked very similar, at 40\% we started seeing duplicates across formats, especially between peS2o and arXiv components of our dataset, where we observed, for instance, the same paper but formatted in two different ways.

Since the LSH minhash algorithm only performs approximate deduplication, we also sampled 4.8 million duplicate pairs to estimate the actual false-positive rate based on Jaccard and edit similarities. We define the  between two documents as their edit distance divided by maximum length of two documents. We found good agreements between theoretical and estimated false-positive rate based on Jaccard similarities, while the estimated false-positive rate based on edit similarity for the 40\% threshold was even lower than the theoretical estimate at . Figures  and  show the distribution of edit and Jaccard similarities for the 40\% threshold version of our dataset: As expected, the vast majority of identified duplicates are above the threshold (marked as red dash line), and for the edit similarity metric, the distributions is skewed toward higher values (which is expected, since it corresponds to a lower false-positive rate). To compare the 40\% and the 80\% versions of Zyda, we trained 1.4B transformers on 50B tokens sampled from each version of Zyda. We found that 40\% Zyda performed slightly better (Fig. ). Due to this, we chose to release the 40\% version as our primary Zyda dataset.

In this Appendix, we describe the filters that were implemented in our filtering pipeline as well as additional filters that we experimented with which we did not find useful. We also present a number of qualitative insights we found while exploring Zyda in depth during our manual analysis and tuning of the filters.

The filters that were used in the final pipeline were as follows:

There was also a number of filters that we tried and found ineffective, including a number of recommendations from prior works. These include:

% \FloatBarrier

We computed the distribution of sources of duplicates across the components (see Figure ). On Figure 7 we plot the proportion of duplicates that are coming from different datasets: e.g., for Pile we identified roughly 90 million documents duplicated in the Pile itself and other datasets, and the figure tells us that 35\% of those are coming from itself, 24\% from StarCoder, 17\% from SlimPajama, and 23\% from the rest of the components. We observe extremely high number of duplicates in arXiv are coming from peS2o since they have the same provenance. We also observe that our approach correctly identified a lot of documents in SlimPajama are present in C4, since C4 was included in RedPajama (the parent dataset of SlimPajama). Big proportion of duplicates in RefinedWeb, C4 overlap with each other probably due the fact that they are derivative of Common Crawl.

%

To estimate the true false positive rate beyond the theoretical calculation, we sampled 4.8 million duplicates pairs (see Figure  for the dustribution of lengths), and computed their empirical edit and Jaccard similarities, as well as manually inspected a number of examples. On Figure  we plot a dependence on document length of what we call a cumulative FP rate: given length , cumulative FP rate is the FP rate amoung duplicates for which the length is at most . We found that the length of the document had a strong effect on the proportion of false positives with short documents being more likely to be categorized as false positives than longer ones. This is likely due to our use of 13-grams for similarity matching as well as the fact that noise fluctuations in similarity are of large scale relative to the threshold at small lengths.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%As large language models have been scaled up orders of magnitude over the last few years, their compute and data needs have grown enormously. State of the art language models trained today, even small ones, are typically trained for at least a trillion tokens and often significantly more. This rate of progress has outpaced the development of open-source freely accessible datasets for large-scale LLM pretraining. In this paper, we present Zyda (Zyphra Dataset), a permissively licensed dataset of 1.3 trillion tokens, created by combining all major existing well-respected open-source datasets into a high-quality dataset. We perform extensive and thorough filtering and deduplication (both intra- and inter-dataset) to ensure that the quality of the dataset is preserved and enhanced from its component datasets. We demonstrate that Zyda performs strongly against other open datasets such as Dolma and RefinedWeb, while significantly outperforming matched models of the Pythia suite and, due to our data processing pipeline, significantly outperforms all of its subsets alone.

The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.

Introductionvaswani2017attention,radford2019language,brown2020language,team2023gemini,achiam2023gpt,sevilla2022computehestness2017deep,kaplan2020scaling,hoffmann2022traininghoffmann2022trainingtouvron2023llama,jiang2023mistralmosaic2024dbrxbrown2020languagerae2021scaling,elazar2023whats,raffel2020exploringlee2021deduplicatingabbas2023semdedup,tirumala2024d4xie2023data,ilyas2022datamodelsxie2024doremitouvron2023llama,team2024gemma,jiang2023mistralraffel2020exploring,slimpajama,kudugunta2024madlad,penedo2023refinedwebslimpajama\urlfig:zyda-vs-external-datasetshttps://huggingface.co/datasets/Zyphra/Zydahttps://github.com/Zyphra/Zyda_processingDataset Composition and processingCompositiongao2020pileslimpajamapenedo2023refinedwebraffel2020exploringpeS2oarxiv\_s2orc\_parsedarxiv-s2orc-parsedli2023starcoderFiltering%\small Number of document and percentage of all documents removed from each subcomponent dataset by our filtering process.tab:filtering_removedgao2020pile,rae2021scalingFiltering_detailstab:filtering_removedDeduplication-4ex\small Initial and final number of tokens before and after filtering and deduplication with two thresholds (80\% and 40\%)0.2cmtab:deduplication_token_countsbroder1997minhash

{k} \sum_{i=1}^k [\min h_i(S_n(A)) = \min h_i(S_n(B))]. eq:resemblancearxiv\_s2orc\_parsedWe chose 13-gram based on what \citep use, which is a common choice of -gram. Other choices of  can be successfully applied to deduplicate text. For example, \citep successfully use 5-grams for deduplicationtab:deduplication_token_countsfig:distr_esfig:distr_jsfig:dedup-ablations