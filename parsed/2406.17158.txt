The benchmark was created by Venktesh Viswanathan, Deepali Prabhu, and Avishek Anand. The licensing information of each of the underlying datasets included in the benchmark is provided in Table.. For dexter toolkit, we release the software with Apache 2.0 license. The benchmark is aimed to be used by researchers wanting to test various components of the RAG pipeline for the task of complex question answering. Currently, we have evaluated a range of retrievers and language models, as shown in the Results Section in the main paper. Our benchmark is also open to evaluation, and it is easy to add new retrieval and generative models. We have made the models and code used in our main experiments available on our GitHub repository. This allows others to reproduce our empirical results, develop their own models or datasets, extend our framework, and conduct meaningful evaluations. Users can easily access the available datasets by downloading the corresponding data files from our repository  and utilizing the data loaders in DEXTER. Retrievers can be loaded using our retriever classes and the publicly available checkpoints. For LLM engines, users can either use their own API keys or the publicly available checkpoints. A summary of publicly used checkpoints for models is provided in Table.. The users of DEXTER can easily extend any of its components to load custom datasets, retrievers or LLMs. We aim to ensure that the benchmark is not limited to our components or datasets alone, allowing for flexible and broad usage.

For benchmarking purposes, we have chosen to convert each of the base datasets into two types of files. The first file includes questions, their answers, context mappings, and metadata. The second file contains the  documents that contain the relevant contexts with their metadata. The projection to an open domain setting is further explained in Section 3.5 of the main paper. We use the train, test, and validation split configuration from the base dataset. The first file containing question-answer pairs has a free structure, with most of them following the original structure of the dataset. Listing. shows a single sample formed by the data loaders in DEXTER. The file with the corpus has a fixed structure as shown in Listing.. All the code to convert raw files from the base dataset to the standard format used in DEXTER is made on our GitHub repository.

% % Each of the datasets included in DEXTER inherits its statistics from the original repository. We do not exclude any samples or extend any of our base datasets, only transform them. The dataset statistics are presented in Table.{table:}.% % % Include extra information in the appendix. This section will often be part of the supplemental material. Please see the call on the NeurIPS website for links to additional guides on dataset publication.% % \item Submission introducing new datasets must include the following in the supplementary materials:% %   \item Dataset documentation and intended uses. Recommended documentation frameworks include datasheets for datasets, dataset nutrition labels, data statements for NLP, and accountability frameworks.%   \item URL to website/platform where the dataset/benchmark can be viewed and downloaded by the reviewers.%   \item URL to Croissant metadata record documenting the dataset/benchmark available for viewing and downloading by the reviewers. You can create your Croissant metadata using e.g. the Python library available here: https://github.com/mlcommons/croissant%   \item Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license.%   \item Hosting, licensing, and maintenance plan. The choice of hosting platform is yours, as long as you ensure access to the data (possibly through a curated interface) and will provide the necessary maintenance.% % \item To ensure accessibility, the supplementary materials for datasets must include the following:% %   \item Links to access the dataset and its metadata. This can be hidden upon submission if the dataset is not yet publicly available but must be added in the camera-ready version. In select cases, e.g when the data can only be released at a later date, this can be added afterward. Simulation environments should link to (open source) code repositories.%   \item The dataset itself should ideally use an open and widely used data format. Provide a detailed explanation on how the dataset can be read. For simulation environments, use existing frameworks or explain how they can be used.%   \item Long-term preservation: It must be clear that the dataset will be available for a long time, either by uploading to a data repository or by explaining how the authors themselves will ensure this.%   \item Explicit license: Authors must choose a license, ideally a CC license for datasets, or an open source license for code (e.g. RL environments).%   \item Add structured metadata to a dataset's meta-data page using Web standards (like schema.org and DCAT): This allows it to be discovered and organized by anyone. If you use an existing data repository, this is often done automatically.%   \item Highly recommended: a persistent dereferenceable identifier (e.g. a DOI minted by a data repository or a prefix on identifiers.org) for datasets, or a code repository (e.g. GitHub, GitLab,...) for code. If this is not possible or useful, please explain why.% % \item For benchmarks, the supplementary materials must ensure that all results are easily reproducible. Where possible, use a reproducibility framework such as the ML reproducibility checklist, or otherwise guarantee that all results can be easily reproduced, i.e. all necessary datasets, code, and evaluation procedures must be accessible and documented.% \item For papers introducing best practices in creating or curating datasets and benchmarks, the above supplementary materials are not required.%  We evaluate diverse retrieval models with publicly available checkpoints, shown in Table . Due to the length limit of transformer based models, we restrict the document length to first 512 word pieces, as done in prior work . We select diverse retrieval models based on their unique characteristics. BM25 and SPLADE serve as strong lexical and sparse retrievers respectively. DPR is a well known off-the-shelf retrieval model employed for opne-domain QA. Tas-b is chosen due to the strong training objective that uses dual supervision. ANCE employs better sampling of negatives during training and MPNET is chosen due to different pre-training objective compared to BERT based language models. ColBERTv2 is a well known late-interaction model that mitigates the cost of cross-encoder by employing late interaction based attention mechanism while providing superior performance to bi-encoder based dense retrieval models. 

We employ temperature of 0.3 to reduce randomness in generated outputs. We employ 5 few-shot samples to account for context length limitations in LLMs and to accommodate retrieved documents in RAG setup. The frequency and brevity penalty are set to 0.8 and 0.6 respectively. For all datasets except FinQA we evaluate on dev set due to lack of publicly available test sets with gold labels. For FinQA we evaluate on test set. While the original MusiqueQA dataset contains a large number of question in validations et filtering out unanswerable questions yields a validation set of 1252 questions . For 2WikiMultiHopQA we perform retrieval and LLM inference for the first 1200 2-hop questions to mitigate cost of LLM inference as done in prior work .