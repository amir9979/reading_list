%帮我将以下学术写作内容翻译成英文:LLMs在许多下游任务上展现出了杰出的零样本推理能力的同时,依然会生成不符合逻辑或是人类偏好的结果。一些研究工作已经发现通过将自然语言形式的规则prompt给LLMs,能够一定程度上约束LLMs的行为,促使其生成符合规则逻辑的输出。 While LLMs have demonstrated remarkable zero-shot reasoning capabilities in many downstream tasks, they still generate outputs that do not conform to logic or human preference. Some research studies have found that compared with the reasoning enhancement methods based on LLMs themselves like Chain-of-Thought , Self-reflection , and Self-refinement , providing LLMs with relevant rules with Retrieval-Augmented Generation (RAG) paradigm do better in helping them conduct reasoning in the downstream tasks .  %These methods usually adopt an external memory to store task-specific rules and adopt a retriever for case-based rule retrieval.  However, the inferential rule-following capability of LLMs is far from satisfactory. Few works have comprehensively evaluated whether LLMs can benefit from the provided rules under different scenarios and how LLMs can follow the rules better. To make up for this gap, this paper conducted a series of experiments to evaluate the inferential rule-following capabilities of several State-of-The-Art LLMs and provide some insights into how LLMs can follow rules better.

Instruction-following has been generally considered an important capability of LLMs , and some previous works have been done to evaluate the instruction-following capability of LLMs . However, only a few works have cast their attention to the question of inferential rule-following. Recent works focused on the rule-following capability of LLMs  confined the rule-following to instruction-following. Instead, this paper proposes the scenario of inferential rule-following and sets up useful baselines for future works.

%yi, phi-3, claude

For open-source LLMs, we adopt Llama-2-7b-chat , Meta-Llama-3-8B , Mistral-7B-Instruct-v0.2 , Yi, and Phi-3.  For closed-source LLMs, we adopt gpt-3.5-turbo, gpt-4-turbo , and gpt-4o from OpenAI. The comprehensive performance comparison of them is shown in Figure~ and the explanation and analysis is in .

%规则数量 To evaluate whether inferential rules are helpful for the reasoning of LLMs, we adopt the following settings to test the LLMs.

All these rule settings are tested in a zero-shot manner. As shown in Figure~, in most cases, LLMs enjoy great performance improvements while being prompted with one golden inferential rule ().  Nevertheless, as the number of irrelevant rules increases, LLMs will find it hard to trigger and leverage the golden rule and thus have a performance drop ().

Besides, we find that by following inferential rules, LLMs have better performance improvements on tasks that require complex reasoning, such as CLUTRR and CAIL2018. On the commonsense reasoning tasks, as the LLMs have parametric knowledge, the performance improvements brought by following inferential rules are relatively slim. Moreover, we find that all LLMs fail to follow the inferential rules in the task of TheoremQA, which illustrates the defect of current LLMs that can not follow complex mathematical or physical rules.

%规则形式 Formal language is widely used in early Artificial Intelligence, which is able to conduct efficient and generalized reasoning. However, LLMs have shown competitive or even superior reasoning performance over traditional formal language rule-based engines, i.e. Knowledge Graphs .  In contrast to formal language rule-based reasoning, reasoning with LLMs is more flexible and robust to various data and tasks. Therefore, we would like to know if we can combine these two paradigms, i.e. whether LLMs can follow formal language rules.

To evaluate whether LLMs can follow formal language rules, we transform the natural language rules of each benchmark into the form of First-Order Logic (FOL) by executing deterministic functions or prompting ChatGPT (Appendix~). Then we compare the reasoning performances of LLMs which are prompted by different forms of inferential rules in both zero-shot  and  settings. 

As shown in Figure~, in most cases, LLMs conduct reasoning better with natural language rules than formal language rules. This aligns with our intuition that LLMs are mostly pre-trained with natural language and thus the inferential rules expressed with natural language are closer to the pre-trained distributions of LLMs than the inferential rules expressed with formal language. Nevertheless, in most cases, LLMs can follow the formal language rules. This reveals the possibility of learning formal language rules from a symbolic reasoning engine and then using LLMs for neural inference.

%规则思维链 Chain-of-Thought  has been widely verified as a useful prompting technique to help LLMs conduct multi-hop reasoning. To evaluate whether LLMs can use CoT to apply inferential rules in the inferential rule-following scenario, we choose the few-shot  and  settings. We created two demonstrations with CoT and two demonstrations without CoT under such settings for LLMs to conduct In-context Learning.

However, as shown in Figure~, LLMs with CoT have not exhibited stronger inferential rule-following performances in most cases. This may be attributed to the lack of  of CoT. CoT conducts straightforward reasoning from the question to the answer with multiple reasoning hops. However, when applying the inferential rules, it involves trying to apply each rule to the current question and thinking about whether to execute this rule. Therefore, plain CoT is inadequate for LLMs to apply the inferential rules. Prompting techniques (e.g. Tree of Thought, ) or decoding algorithms (e.g. KCTS, ) that involve planning steps are needed for helping LLMs to apply the inferential rules.

%反事实场景 Although we have verified the effectiveness of the inferential rules, it is still unclear whether LLMs completely follow the given inferential rules or merely use their parametric knowledge. Therefore, we designed the scenario of .

To evaluate whether LLMs can follow counterfactual rules, we construct corresponding counterfactual benchmarks and rule sets of CLUTRR, SALAD, ULogic, and CAIL2018. Specifically, we replace the ground truth of each question and the conclusion of the corresponding rule with a random incorrect answer. So in this counterfactual setting, the LLMs are supposed to generate the ``incorrect answer'' based on the given counterfactual rules. %For example, for the question: , its ground truth: , and its corresponding rule: , we replace the word  in both ground truth and the rule with another random kinship to construct the counterfactual data sample.

As shown in Figure~, in most cases of both  and  settings, LLMs have significant performance drops when following counterfactual rules, compared with following factual rules. These results indicate that the performance improvements brought by following rules are partly attributed to the parametric knowledge of LLMs, besides following inferential rules. 

%行为分析 To understand why LLMs fail to follow the given inferential rules in the reasoning process, we made a behavioral analysis of LLMs in the failure cases of LLMs inferential rule-following. Specifically, we adopt the few-shot  settings for LLMs to follow the rule-applying demonstrations to apply the given inferential rules to the current question. We ordered the LLMs first to choose an inferential rule to follow and then reason with it. By parsing the output of LLMs we can classify the failure cases of LLMs inferential rule-following into two categories:  and .  indicates that the LLMs choose an irrelevant rule for the current case and therefore lead to an incorrect reasoning result.  indicates that although LLMs have chosen the correct rule for the current case, they fail to draw the correct conclusion of . To faithfully describe the inferential rule-following behavior of LLMs instead of being affected by the parametric knowledge of LLMs, we run the analysis under the counterfactual settings of the selected benchmarks.

%帮我将以下学术写作内容翻译成英文:从这些结果中可以看出,LLMs rule-following 场景中很大一部分错误原因来源于触发错误。因此,在RAG推理增强范式中,检索器事实上扮演着十分关键的角色。然而,已有的工作往往用BM25等简单的稀疏检索器,这将极大的损害LLMs rule-following性能。 From the results shown in Figure~, we can tell that when tackling different tasks, LLMs exhibit different behaviors in following rules. While rules have a heavy head for triggering (e.g. in CLUTRR and CAIL2018, the rule head will be a series of relational hops among characters), the LLMs are likely to make . While the rule head is easy and commonsensical (e.g. in SALAD and ULogic), but the conclusion of the rule body is ambiguous or confused (the counterfactual scenario), the LLMs are likely to make .

To avoid  in the scenario of rule-enhanced reasoning with RAG paradigm (), the  plays a crucial role. The  can be eliminated if the  only retrieved the golden rules. However, existing works often employ simple sparse retrievers such as BM25 , which greatly compromises the inferential rule-following performance of LLMs.

To avoid  in following rules, the LLMs need to faithfully execute the rule body and avoid generating conclusions of illusions. Therefore, users may avoid letting LLMs follow the rules that are counterfactual or out of the pre-trained distribution of LLMs before they fine-tune the LLMs to adapt to those domains or specific tasks.

To make a comprehensive evaluation of the inferential rule-following capability of the LLMs, we categorize the experimental results in the previous sections into 5 dimensions: , , , , and .  The details of these dimensions are shown in Appendix~.

% [itemsep=1pt,topsep=1pt,parsep=0pt,leftmargin=*]%     \item . We average the results in all  settings to obtain the capability of  of LLMs. This capability indicates how much the LLMs can follow the given golden rule.%     \item . We average the results in all  settings to obtain the capability of  of LLMs. This capability indicates how much the LLMs can resist the interruption of irrelevant rules and find the golden rule.%     \item . We average all the results with formal language rules to obtain the capability of  of LLMs. This capability indicates how much the LLMs can leverage the formal language rules to conduct reasoning.%     \item . We average all the results where LLMs apply rules with CoT to obtain the capability of  of LLMs. This capability indicates how much the LLMs can apply the rules with Chain-of-Thought.%     \item . We average all the results with counterfactual rules to obtain the capability of  of LLMs. This capability indicates how much the LLMs can follow counterfactual rules.% 

As shown in Figure~, while the closed-source LLMs show dominant performances in the scenario of inferential rule-following, some open-source LLMs, like Llama-3-8B, exhibit competitive performances and have balanced capabilities in all dimensions. Among the closed-source LLMs, gpt-4-turbo is more capable of following formal language rules while gpt-3.5-turbo shows a stronger capability of following counterfactual rules.

Generally, LLMs are not very good at inferential rule-following. This may be attributed to the lack of training in inferential rule-following in the current LLMs. As  (IFT) has been a standard step in the pipeline of training LLMs and thus ensures their strong instruction-following capability, in the next section, we propose a fine-tuning method to effectively further improve the inferential rule-following capabilities of LLMs. %we think that a  (RFFT) steps could fundamentally enhance the rule-following capability of LLMs.% 能否抽象出能力? 还是按数据集分,画雷达图% 1. follow规则能力,2. 抵抗不相关rule,3.形式语言,4.Apply规则链的能力,5.应用反事实规则能力%  To further improve the inferential rule-following capabilities of LLMs, we propose  (IRFT). Compared with IFT, IRFT involves inferential rules as a part of the prompt. The inferential rules can be only the golden rule or the golden rule with a few randomly sampled noise rules. This orders the LLMs to learn to infer the answer not only by the parametric knowledge but also by triggering and executing the golden rule. The tuning objective can be formalized as:

Where the  stands for the question, the golden rule, and the answer from the training set, respectively.  stands for randomly sampling  rules from the entire rule sets as the noise rules. Based on the training data in RuleBench, we constructed training data in the settings of No Rule for IFT, and Few Rule ()  Golden Rule () for IRFT. 

As shown in Table~, our proposed IRFT further significantly improves the performances of LLMs in the inferential rule-following scenarios and greatly outperforms IFT. This indicates that IRFT can effectively teach the LLMs the capabilities of inferential rule-following. %Besides, we found that IRFT also helps LLMs generalize to counterfactual scenarios.

Although IRFT has shown remarkable performances on RuleBench, beyond using IRFT on specific downstream tasks, we are looking forward to extending IRFT to the pre-training stage of LLMs (like IFT), such that it is possible to enable LLMs to master more basic and generalized inferential rule-following capabilities.