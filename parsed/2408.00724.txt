We explore the following question:  We are the first to formulate this problem and study the inference time scaling law, setting our work apart from previous scaling law studies (Fig. ).

To address this, we represent the problem-solving error rate  as a function of the number of model parameters , the number of generated tokens  and the inference strategy . The computational budget  is a deterministic function , based on  and . Our goal is to minimize  under the test-time compute constraint :

where  and  denote the optimal allocation of a computational budget .

Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently ranking them using a reward model. We primarily consider sampling and tree-search approaches with reranking or Majority Voting as the means to consume more tokens, including Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on tree search methods.

We consider the following sampling-based inference strategies which are popularly used in practice:

Before diving into more sophisticated inference algorithms (e.g., tree search), we present theoretical results on the asymptotic behavior of voting-based strategies given infinite compute in Theorems . Informally, we show that the accuracy of standard/Weighted Majority Voting converges with infinite samples, and the limit only depends on the distribution modeled by the language model (and the reward model).  This theoretical finding is also aligned with our empirical findings shown in Sec. . The proofs are presented in Appendix .

 Let  be a  vocabulary and  its Kleene closure, i.e., the set of all strings. Given a problem , we say a language model answers  to this problem if the model outputs  where  can be any ``reasoning path'' and  denotes a special token that marks the end of reasoning. We further assume that the answer string is always shorter than  tokens, i.e.,  for some fixed  where  denotes the length of . For a language model , denote by  the probability of generating  given input (prompt) . For a reward model , denote by  the score it assigns to the string .  We use  to denote the indicator function.

 Theorems  state the convergence of the accuracy with increasing number of samples, indicating that the performance gains of using more samples will saturate for any fixed models. The limit is determined by the likelihood of generating the correct answers through all possible reasoning paths (and the likelihood should be viewed as a weighted sum for Weighted Majority Voting). This motivates us to consider inference algorithms that search for ``good'' reasoning paths, such as the tree-search-based variants detailed in Sec. .

Theorem  also present insights to compare standard Majority Voting with Weighted Majority Voting. Informally, as long as the reward model is ``better than random'', i.e., assigning higher rewards to correct solutions on average, the accuracy limit of Weighted Majority Voting is higher than that of Majority Voting. In our experiments, we consistently find that Weighted Majority Voting dominates Majority Voting. Thus, we focus on Best-of-N and Weighted Majority Voting in the main paper and defer Majority Voting results to Appendix .

Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required~. Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process~. In this context, MCTS is often paired with a value model to score and guide the exploration steps. For additional background, we provide a review of MCTS in Appendix .

Recent work in MCTS or its variants (e.g., Tree of Thoughts~) mainly focus on improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of MCTS with conventional methods like Best-of-N and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or  indicating inference-time issues.  For example, MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.  Specifically, a significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions. However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.

This highlights the need for developing a new tree search method that can  achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like Weighted Majority Voting and best-of-N. This need motivates the development of our new method named Reward Balanced SEarch (REBASE), as is introduced next. 

The  tree search method, illustrated in Fig. , inherits the exploitation and pruning properties of tree search, while using the reward model alone to estimate the nodes' qualities without additional computation for estimating values by sampling children. The efficiency is achieved by constraining the total expansion width of the tree at a certain depth.  balances the expansion width among the nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details are provided below:

%  We view the fine-tuned LLM as a policy  which generates the solution step by step. Given a question  and the first  steps of a solution , the -th step is sampled from .   effectively generates a solution tree during inference, in which the root node the question  and other nodes corresponds to solution steps. When generating solution trees, we generate children of  by sampling from .  Here we slightly abuse notations and use the corresponding question/solution step to denote a node. The reward of a node  is generated by the PRM: .

 Given the question , balance temperature , and sampling number of solutions , we sample  instances of the first step for the question, yielding all the nodes of depth 1 in the search tree. We let the sampling budget of depth 0, , to  at initialization.

 In the -th iteration, the PRM assigns the rewards to all the nodes at depth . After that, the algorithm examines whether the solutions up to depth  are complete. Supposing there are  completed solutions, we update the sampling budget using . If , the process ends, and we obtain  solutions.

 For all of the nodes  with reward  in the depth  of the tree, we calculate the expansion width of the  as:

Then we sample  children for  for all the nodes in depth , and start the next iteration.

We conduct experiments on two mathematical problem-solving datasets to investigate the scaling effects of computation and our REBASE method for both challenging and simpler problems. Specifically, MATH  and GSM8K  are datasets containing high school mathematics competition-level problems and grade-school level mathematical reasoning problems, respectively. Following , we use the MATH500 subset as our test set.

 To study the inference compute scaling effect, we choose Pythia  as our base models since various model sizes are available in the Pythia family. For tree search, we use math-specialized Llemma models . We further finetune these models on the MeatMath dataset  using full parameter supervised fine-tuning (Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the Mistral-7B  to expand our findings across different models and architectures.

 All of the experiments use the same Llemma-34B reward model, which we finetuned on the synthetic process reward modeling dataset, Math-Shepherd~. We added a reward head to the model, enabling it to output a scalar reward at the end of each step.

We use sampling and tree search methods to generate multiple samples and select the answer through Best-of-N, Majority Voting, or Weighted Voting. Each configuration has been run multiple times to calculate the mean and variance, thereby mitigating the randomness and ensuring the reliability of our conclusions.

In order to compare the compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the standard formula from .

The scaling effect of inference computation budget across different model sizes is shown in Fig. . We note that the error rate first decreases steadily and then starts to saturate. Where smaller model first takes advantage since it can generate more samples given limited budget, larger models are preferable with more FLOPs due to the saturation of small model performances. As highlighted in the right panel of Fig. , the optimal model size can be different under various computation budgets.  We performed a regression analysis on inference FLOPs  and corresponding model sizes  to establish a relationship between a given computational budget and its optimal model size. The resulting regression equation, , enables us to estimate the optimal inference model size for a specified computational constraint.

 Fig.  show the curves of error rates versus total number  of inference FLOPs per question. Inference methods with different model sizes are plotted in the same diagram. We found that Llemma-7B costs approximately  less total FLOPs than Llemma-34B under the same method (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy. This result suggests that, with the same training dataset and model family, training and inference with a smaller model could be more favorable in terms of compute budget if multiple sampling or search methods are employed.

While MCTS undeperforms Sampling (Fig. ), from Fig.~, , , and , we found that  consistently outperforms the Sampling method in all settings, when fixing the model and the evaluation task. Table~ shows that  can achieve competitive accuracy with even a lower compute budget than the sampling method. This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling~. Table  shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using  yields higher accuray than sampling.

 For example, our proposed ~leads to , , and  performance gains on MATH for Mistral-7B, Llemma-7B, Llemma-34B, respectively. The order of accuracy increase is inversely related to the model's corresponding greedy search on those datasets. This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like .

 From Fig.~ and Fig.~, we observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH, which we attribute to the difference of the difficulty level. This can be explained through the LLM may assign high probability to the true answer in easy problems than those of harder problems, as suggested by Theorems  with their proofs . On MATH (Fig.~), we see that REBASE finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE samples the truth answer with higher probability than sampling. And as demonstrated by Theorems , the upper bound becomes higher.