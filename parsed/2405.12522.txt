Autoregressive, decoder-only transformers rely on self-attention to weigh the importance of different parts of the input sequence , with a goal to predict the next token. In the multi-head attention mechanism, each attention head operates on a unique set of query, key, and value matrices, allowing the model to capture diverse relationships  elements of the input. The output of the -th attention head can be formally described as:

where  is the input sequence,  are the query, key, and value matrices for the -th head, and  is the dimensionality of the key vectors. The outputs of the individual attention heads are then concatenated and linearly transformed to produce the overall output of the multi-head attention layer:

where  is the projection matrix. Each attention head output  resides in the residual stream space , contributing independently to total attention at that layer .

The  refers to the sequence of token embeddings, with each layer's output being added back into this stream. Attention heads read from the residual stream by extracting information from specific tokens and write back their outputs, modifying the embeddings for subsequent layers. This additive form allows us to analyse each head's contribution to the model's behaviour by examining them independently. By tracing the flow of information across layers, we can identify computational circuits composed of multiple attention heads.

Sparse autoencoders provide a promising approach to learn useful representations of attention head outputs that are amenable to circuit analysis. Given a set of attention head outputs , where , we train an autoencoder with a single hidden layer and tied weights for the encoder  and decoder . The autoencoder learns a dictionary of basis vectors  such that each  can be approximated as a sparse linear combination of the dictionary elements:  where  are the sparse activations and  is the dimensionality of the bottleneck layer. The autoencoder is trained to minimise a loss function that includes a reconstruction term and a sparsity penalty, controlled by the hyperparameter :

The dimensionality of the bottleneck layer  can be either larger (projecting up) or smaller (projecting down) than the input dimensionality . While projecting up allows for an overcomplete representation and can capture more nuanced features, projecting down can also be effective in learning a compressed representation that captures the most essential aspects of the attention head outputs . We propose a subtle but significant shift in perspective by treating sparse autoencoding as a compression problem rather than a problem of learning higher-dimensional sparse bases in the context of transformers. We hypothesise that compression is likely a key mechanism in identifying features which represent circuit-related computation, and in contrast which computation is shared between positive and negative examples.

To further simplify the representation and facilitate the identification of distinct behaviours within the attention heads, we discretise the sparse activations obtained from the autoencoder using an argmax operation over the feature dimension, , where  is the discrete code assigned to the -th attention head output. This yields a discrete bottleneck representation analogous to vector quantization . We will next discuss how to leverage the resulting discrete representations to identify important task-specific circuits in the transformer.

We compile datasets of 250 ``positive'' and 250 ``negative'' examples for each task. Positive examples are text sequences where the model must use the circuit of interest to correctly predict the next token. In contrast, negative examples are semantically similar to positive examples but corrupted such that there is no correct ``next token.'' This dataset design ensures that the learned representations are common between positive and negative examples for attention heads processing semantic similarities but different for heads involved in circuit-specific computations. Table  shows task examples, and Appendix  contains details of each dataset.

The  (IOI) task involves sentences such as ``When Elon and Sam finished their meeting, Elon gave the model to'' with the aim being to predict ``Sam'', the indirect object . Negative examples introduce a third name, eliminating any bias towards completing either of the two original names.

The  task involves sentences of the form ``The  lasted from  to '', where the aim is give all non-zero probability to years > . Negative examples consist of impossible completions, with the ending year preceding the starting century.

The  task assesses the model's ability to predict argument names in Python docstrings based on the function's argument list . Docstrings follow a format with  followed by an argument name. The model predicts the next argument name after the  tag. Negative examples employ random argument names.

Our methodology consists of two stages: training the sparse autoencoder to conduct dictionary learning on the cached model activations, and using these learned representations to identify model components involved in the circuit (Figure ).

We first take all positive and negative input prompts for a dataset and tokenize them. Since each prompt is curated to have the same number of tokens for all positive and negative examples across all datasets, we concatenate the prompts into a single tensor .  (see Figure ). The remaining examples are used as an evaluation set.

 circuit discovery predicts model components (i.e., attention heads) as part of the circuit based on individual outputs in isolation. In contrast,  circuit discovery predicts whether the information flow (i.e., the edge) is important by considering how certain components act together, specifically the frequency of co-activation of specific codes in different heads. For a full discussion of the details and semantics of node-level and edge-level discovery, see Appendix .

After training the SAE, we perform a forward pass of all examples  through the encoder  to obtain the learned activations . We then apply an argmax operation across the feature (bottleneck) dimension of , which yields a matrix of discrete codes , where each code represents the most activated feature for a particular attention head.

 Let  be a matrix of one-hot vectors indicating which codes are activated for each head in the positive examples, and let  be the corresponding matrix for the negative examples.

We next compute a vector , where each element  represents the number of unique codes that appear only in the positive examples, optionally normalised by the total number of codes across all examples, for the -th attention head: .  Intuitively, a high value of  indicates that the -th head activates a large proportion of codes that are unique to positive examples. We then apply a softmax function to  and select a threshold  to determine if a head is part of the ground-truth circuit (Figure ). Whilst we vary  to construct analyses such as ROC curves, in practice a single  should be selected to predict a circuit.

 Let us construct co-occurrence matrices  and  for the positive and negative examples, respectively. Each entry  represents the frequency of co-occurrence between codes  and  in heads  and :	

We then compute a matrix , where each entry  represents the number of co-occurrences that appear in the positive examples but not in the negative examples for the head pair :  where . Once the head pairs  are sorted in descending order of their corresponding values in , we introduce a hyperparameter  to determine the number of top-ranked head pairs to include in the predicted circuit. We set  to be half the total number of head pairs for all analyses, and show that this is a robust choice in Appendix .

The next step is to initialise  as a zero vector. For each of the top  head pairs , the corresponding entries in  are incremented:  and . We then apply softmax across  and choose a threshold  to predict whether a particular head is part of the circuit (Figure ).

Our circuit-identification method outperforms ACDC, HISP, and SP in terms of ROC AUC across all datasets, regardless of ablation type used for these methods (Figure ), with the exception of ACDC, which achieves a higher ROC AUC on edge-level circuit identification on the docstring task. We find a strong correlation between the number of unique positive codes per head and the presence of that head in the ground-truth circuit (Figure , Appendix ). ROC curves are constructed by sweeping over thresholds (Figure ): for our method, we sweep over the threshold  required for a softmaxed head's number of unique positive codes to be included in the circuit, while for ACDC, we sweep over the threshold , which determines the difference in the chosen metric between an ablated and clean model required to remove a node from the circuit. Notably, we identify negative name-mover heads in the IOI circuit  (heads that calibrate probability by downweighting the logit of the IO name), which other algorithms struggled to do  (see Appendix ). 

% 

To demonstrate the robustness of our method to its hyperparameters, we consider two distinct groups: (1) those controlling the capacity and expressiveness of the sparse autoencoder (SAE), namely the size of the bottleneck and the sparsity penalty , and (2) the threshold  for selecting a head after softmax. We trained 100 autoencoders with varying numbers of features in the hidden layer and different values of . We observed no significant drop-off in ROC AUC for IOI and Docstring tasks, and a slight drop-off for Greater-than, as we increase both hyperparameters (Figure ). Finally, we examine the robustness of the value of  on the pointwise F1 score (node-level) for both the IOI and GT datasets (Figure ). The optimal threshold is approximately the same for both tasks, suggesting we may be able to set this threshold for any arbitrary circuit. For edge-level discovery, we also find that performance is robust to  (Appendix ).

We note that a more abstract hyperparameter is the construction of negative examples. We present an examination of this in Figure  in Appendix , and find that our choice of semantically similar yet corrupted examples yields the best performance.

We evaluate the effectiveness of our circuit relative to the full GPT-2 model, a fully corrupted counterpart, and a random complement circuit of equivalent size across two distinct tasks. The corrupted activations are created by caching activations on corrupted prompts, similar to our negative examples (see Appendix ). To measure a given circuit, we replace the activations of all attention heads not in the circuit with their corrupted activation. We use metrics specifically designed for each task, and our circuit is chosen by using the maximum F1 score across thresholds.

For the IOI task, the primary metric is , calculated as the difference in logits between the indirect object's name and the subject's name. Our circuit achieves a logit difference of 3.62, surpassing the full GPT-2 model's average of 3.55, indicating that the correct name is approximately  times more likely than the incorrect name. However, our circuit performs slightly worse than the ground-truth circuit identified by ; full results are in Table  (see Appendix  for details).

For the Greater-than task, we focus on  (PD) and  (CS), as defined by . These metrics evaluate the model's effectiveness in distinguishing years greater than the start year and the sharpness of the transition between valid and invalid years (see Appendix  for formal details). Despite having fewer attention heads, our circuit achieves a PD of 76.54\% and a CS of 5.76\%, slightly outperforming the ground-truth circuit and significantly surpassing the clean GPT-2 model. The corrupted model and random complements exhibit negative PDs and negligible CS values; see Table .

In the absence of a ground-truth circuit, evaluating whether our learned circuit reflects the true circuit used by the model is challenging. To this end, we employ the concept of  introduced by . Faithfulness represents the proportion of the model's performance that our identified circuit explains, relative to the baseline performance when no specific input information is provided. We measure faithfulness by selecting a threshold  to determine which heads to include in the circuit and ablating all other heads by replacing them with corrupted activations. Faithfulness is computed as , where , , and  are the average performance metrics over the dataset  for the identified circuit, all heads ablated, and the full model, respectively. By sweeping over all , we track performance improvement as we add circuit components. For comparison, we randomly select  heads to use clean ablations for at each , repeat this sampling 10 times, and average the metrics.

%to produce the "Random" curve in Figure .% We measure faithfulness by selecting a threshold  to determine which heads to include in the circuit and ablating all other attention heads outside the circuit by replacing them with their corrupted activations. We then compute the faithfulness as , where , , and  represent the average performance metric  over the dataset  when running the model with only the identified circuit components active, all attention heads ablated, and the full model, respectively. By sweeping over all , we show how quickly performance improves as we add components from our circuit prediction. To compare our results to a randomly predicted circuit of the same size, we randomly select  heads to use clean ablations for at each threshold  that gives us  attention heads, repeat this sampling process 10 times for each threshold, and average the metric at each threshold to give the ``Random'' curve in Figure . Our results are shown in Figure . We also show the same faithfulness and metric curves applied to  (EAP) . As we add attention heads from our circuit in order of threshold, performance quickly approaches that of the full model across all metrics and, in some cases, even outperforms the full model with considerably fewer heads. Importantly, our predicted circuit performs better or equal in all metrics than EAP.

Sparse and discrete representations of transformer activations have gained attention for their potential to enhance model interpretability.  and  are generally considered the first groups to explore sparse dictionary learning to untangle features conflated by superposition, where multiple features are distributed across fewer neurons in transformer MLPs. Their work highlighted the utility of sparse representations but does not fully address the identification of computational circuits.  were the first to show that SAEs also learn useful representations when applied to attention heads rather than MLPs, and scaled this to GPT-2 .

 integrated a vector-quantized codebook into the transformer architecture. This technique demonstrates that incorporating discrete, interpretable features incurs only modest performance degradation and facilitates causal interventions. However, it necessitates architectural modifications, rendering it redundant for interpreting existing large-scale language models.  used recursive analysis to trace the activation lineage of target dictionary features across layers. While this offers insights into layer-wise contributions, it falls short of mapping these activations to specific model components or elucidating their role within the residual stream. 

Most closely related to our work and conducted in parallel is that of , who employed a large SAE trained on diverse components, defining a framework for explicitly finding circuits. Their method relies on attribution patching (see below), which introduces practical difficulties at scale and again relies on a choice of metric. Additionally, their approach requires an SAE trained on millions of activations with significant upward projection to the dictionary, making it impractical for identifying specific circuits. Similarly,  used SAE-learned features to map attention head contributions to identified circuits. However, their approach still uses a form of attribution patching and suggests a tendency for identified features to be polysemantic.

Ablation-based methods are fundamental in identifying critical components within models.  introduced the ACDC algorithm, which automatically determines a component's importance by looking at the model's performance on a chosen metric with and without that component ablated. ACDC explores different ablation methods, such as replacing activations with zeros , using mean activations across a dataset , or activations from another data point . Despite its effectiveness, ACDC is computationally intensive and sensitive to the choice of metric and type of intervention. The method often fails to identify certain critical model components even when minimising KL divergence between the subgraph and full model.

Subnetwork Probing (SP) and Head Importance Score for Pruning (HISP) are similar methods. SP identifies important components by learning a mask over internal components, optimising an objective that balances accuracy and sparsity . HISP ranks attention heads based on the expected sensitivity of the model to changes in each head's output, using gradient-based importance scores . Both methods, however, are computationally expensive and sensitive to hyperparameters.

Recent advancements have addressed limitations of traditional circuit discovery methods.  introduced Edge Attribution Patching (EAP), using linear approximations to estimate the importance of altering an edge in the computational graph from normal to corrupted states , reducing the need for extensive ablations. However, EAP's reliance on linear approximations can lead to overestimation of edge importance and weak correlation with true causal effects. Additionally, EAP fails when the gradient of the metric is zero, necessitating task-specific metrics for each new circuit.  recently proposed Edge Attribution Patching with Integrated Gradients (EAP-IG) to address these issues, evaluating gradients at multiple points along the path from corrupted to clean activations for more accurate attribution scores. Future work will benchmark our method against EAP and EAP-IG to understand the tradeoffs of each.

% Recent advancements have aimed to address the limitations of traditional circuit discovery methods.  introduced Edge Attribution Patching (EAP), which uses linear approximations to estimate the importance of altering an edge in the computational graph from its normal state to a corrupted state . This method reduces the need for extensive ablations. However, EAP's reliance on linear approximations can lead to overestimation of edge importance and a weak correlation with true causal effects. Additionally, EAP does not work when the gradient of the metric is zero, meaning we must craft task specific metrics for each new circuit we wish to find. To overcome these limitations,  recently proposed Edge Attribution Patching with Integrated Gradients (EAP-IG). EAP-IG evaluates gradients at multiple points along the path from corrupted to clean activations, providing a more accurate and faithful attribution score for each edge. While EAP and EAP-IG streamline the circuit discovery process, they still depend on gradient-based approximations and exhibit increased computational complexity. Future work will involve benchmarking our method against that of EAP and EAP-IG more rigorously to understand the various tradeoffs of each.

Our method has several limitations that will be addressed in future work. First, although we learn discrete representations of attention head outputs, the interpretability of these learned codes may still be limited. Further work is needed to map these codes to human-interpretable concepts and behaviours. Second, we require the generation of a dataset of positive and negative examples for a circuit. This means we cannot do unsupervised circuit discovery and must carefully craft negative examples that are semantically similar to the positive ones, but are still corrupted enough to switch off the target circuit. To address this limitation, we plan to apply techniques such as quanta discovery from gradients  to automatically curate our positive and negative token inputs.

In addition to these method-specific limitations, any circuit discovery method faces the fundamental limitation of relying on human-annotated ground truth. The circuits found by previous researchers through manual inspection may be incomplete  or include edges that are correlated with model behaviour but not causally active . Further, SAEs have been shown to make pathological errors ; until these are resolved, we may need to include these errors in the circuit discovery process itself (much like ).

One promising direction for future exploration is investigating the compositionality of the identified circuits and how they interact to give rise to complex model behaviours. Developing methods to analyse the hierarchical organisation of circuits and their joint contributions to various tasks could provide a more comprehensive understanding of the inner workings of large language models. A key aspect of this research could involve applying autointerpretability methods  to our learned features in discovered circuits.

Finally, extending our approach to other model components, such as feedforward layers and embeddings, could offer a more complete picture of the computational mechanisms underlying transformer-based models. By combining insights from different levels of abstraction, we can work towards developing a more unified and coherent framework for mechanistic interpretability, thus advancing our understanding of how transformer models process and generate language.

% % In conclusion, our work presents a significant step forward in the quest for efficient and reliable circuit identification in large language models. By leveraging sparse autoencoder-learned features and discrete representations, we demonstrate a simple yet effective approach that outperforms existing state-of-the-art methods while reducing computational complexity. Our findings contribute to the growing body of research on mechanistic interpretability and pave the way for more transparent and understandable AI systems. As we continue to develop and refine these techniques, we move closer to the ultimate goal of building AI systems that are not only powerful but also interpretable, trustworthy, and aligned with human values.% Following , our SAEs are one-layer MLPs with a tied pre-encoder bias. In more detail, our SAEs have parameters% % where the columns of  are constrained to be unit vectors. Given an input activation% % and reconstructions via% % The feature vectors  are the columns of .% Much like , our sparse autoencoders (SAEs) consist of a single hidden layer with tied weights for the encoder  and decoder . The SAE learns a dictionary of basis vectors  such that each attention head output  can be approximated as a sparse linear combination of the dictionary elements:% % where  are the sparse activations and  is the dimensionality of the bottleneck layer. In more detail, our SAEs have parameters:% % where the columns of  are constrained to be unit vectors, representing the dictionary elements . Given an input attention head output , the activations of the bottleneck layer are computed as:% % and the reconstructed output is obtained via:% % Notably, the dimensionality of the bottleneck layer  can be either larger (projecting up) or smaller (projecting down) than the input dimensionality . Whilst we experiment with both, hyperparameter sweeps found that projecting down (using fewer dimensions in the bottleneck layer than the dimension of the input) worked best for circuit identification. 

Much like , our sparse autoencoders (SAEs) consist of a single hidden layer with tied weights for the encoder  and decoder . The SAE learns a dictionary of basis vectors  such that each attention head output  can be approximated as a sparse linear combination of the dictionary elements:

where  are the sparse activations and  is the dimensionality of the bottleneck layer. Our SAEs use the following parameters:

The columns of  are constrained to be unit vectors, representing the dictionary elements . Given an input attention head output , the activations of the bottleneck layer are computed as:

and the reconstructed output is obtained via:

where the tied bias  is subtracted before encoding and added after decoding. The dimensionality of the bottleneck layer  can be either larger (projecting up) or smaller (projecting down) than the input dimensionality . Hyperparameter sweeps found that projecting down (using fewer dimensions in the bottleneck layer than the dimension of the input) worked best for circuit identification. Additionally, we use a custom backward hook to ensure the dictionary vectors maintain unit norm by removing the gradient information parallel to these vectors before applying the gradient step.

We collate 250 and 250 positive examples for each dataset. We randomly sample 10 examples for training the SAE from this collection of examples, unless otherwise specified. We tokenise these examples and stack them into a single tensor of shape , which can easily be passed through the SAE in a single forward pass. We then cache all attention head results in all layers - these are the inputs to our sparse autoencoder.

The SAE is trained to minimise a loss function that includes a reconstruction term and a sparsity penalty, controlled by the hyperparameter :

where  is typically about 0.01, our learning rate is 1e-3, and we train for 500 epochs using the Adam optimiser. We use a single NVIDIA Tesla V100 Tensor Core with 32GB of VRAM for all experiments.

Edge-level circuit identification aims to predict which attention heads are part of a circuit by analysing patterns in how the heads perform similar computation in tandem, rather than in isolation.

The first step is to construct two co-occurrence matrices,  and , which capture how often different token codes co-occur between each pair of heads in the positive and negative examples, respectively. For instance,  counts the number of times code c1 in head h1 occurs together with code  in head  across the positive examples.  does the same for the negative examples.

Next, we compute a matrix  that identifies code co-occurrences that are unique to the positive examples for each head pair. An entry  sums up the number of positive-only co-occurrences between heads  and  - that is, cases where a code pair has a positive count in  but a zero count in  for that head pair.

Intuitively,  captures which head pairs tend to jointly attend to particular token patterns more often in positive examples compared to negative examples. Head pairs with high values in  are stronger candidates for being part of the relevant circuit.

The head pairs are then sorted in descending order by their  values. To build the predicted circuit, we take the top  head pairs from this sorted list, where  is a hyperparameter. For each of these top  pairs , we increment the entries for  and  in a vector . This vector keeps track of how many times each head appears in the top head pairs. The reason for using only the top  pairs is that including all pairs would make each head co-occur with every other head once, leading to a uniform  that would not distinguish between heads. 

Applying softmax to  normalises it into a probability distribution, allowing us to set a threshold  to make the final predictions, with  being the same scale for any arbitrary circuit (i.e. between 0 and 1). Heads that have a value exceeding  are predicted to be part of the circuit. This whole process is outlined in Figure , where we step through an example of the process on a 1-layer transformer with 6 attention heads and using 6 text examples (3 positive, and 3 negative).

A key benefit of our method over existing approaches is its efficiency. Whilst ACDC takes upwards of several hours to run on a V100 or A100 GPU for IOI on GPT2-Small , our method completes in under 3 seconds for GPT2-Small, and under 45 seconds for GPT2-XL. In fact, as we previously showed that one may be able to use only 10 examples when training the SAE, if this trend holds across model scales we can reduce time to less than 10 seconds for GPT2-XL.

We show the specific model specifications in Table . If the number of text examples for both the SAE and counting positive codes remains constant, the main contribution to increased time for our method is  and , as each example is in . Since the counting of unique positive codes involves elementary set operations over only a few hundred arrays of integer codes, it is only training the SAE that takes perceptibly longer as we increase the size of the underlying language model.

We compare the performance of our circuit to the full model, a fully corrupted model, and a random complement circuit of the same size. The metric is logit difference: the difference in logit between the indirect object's name and the subject's name. The full model's average logit difference is 3.55, meaning the correct name is  times more likely than the incorrect name.

To create a corrupted cache of activations, we run the model on the same prompts with the subject's name swapped. Replacing all attention heads' activations with these corrupted activations gives an average logit difference of -3.55. When testing our circuit, we replace activations for heads not in the circuit with their corrupted activations.

Our circuit has a higher logit difference (3.62) than the full GPT-2 model. The ground-truth circuit from  has a logit difference of 4.11. We compare this to the average logit difference (-1.97) of 100 randomly sampled complement circuits with the same number of heads as our circuit. These results are shown in Table .

We also provide the normalised logit difference: the logit difference minus the corrupted logit difference, divided by the signed difference between clean and corrupted logit differences. A value of 0 indicates no change from corrupted, 1 matches clean performance, less than 0 means the circuit performs worse than corrupted, and greater than 1 means the circuit improves on clean performance.

What seems to be a key advantage of our method over ACDC is our ability to detect both negative name mover heads and one of the previous token heads.  found that there exist attention heads in GPT-2 that actually write to the residual stream in the  direction of the heads that output the remaining name in the sentence, called . These likely ``hedge'' a model's predictions in order to avoid high cross-entropy loss when the sentence has a different structure, like a new person being introduced or a pronoun being used instead of the repeated name .  heads copy information from the second name to the word after and have been found to have a minor role in the circuit.

 found that they were unable to identify either of these types of heads as part of the circuit unless using a very low threshold , which led to many extraneous heads being included in their prediction. This is despite the fact that negative name mover heads in particular are highly important in calibrating the confidence of name prediction in the circuit . The fact that we find both negative name mover heads (L10H7 and L11H10) and one of the previous token heads (L2H2) is highly promising evidence that our the distribution of our SAE activations provide a robust representation of the on-off nature of any given head in a circuit. Being able to identify negative components (that actively decrease confidence in predictions) in circuits is really important, because many circuits involve this general behaviour, known as .

The Greater-than task focuses on a simple mathematical operation in the form it appears in text i.e. a sentence of the form ``The  lasted from the year  to the year '', where the aim is to give all non-zero probabilities to completions  . We use the same setup as . We use nouns which imply some form of duration, for example war, found used FrameNet . The century  is sampled from  and the start year  from . Because of GPT-2's byte-pair encoding , more frequent years are tokenised as single tokens (e.g. ``'' instead of ``'') and so these are removed from the pool. Years ending in ``01'' and ``99'' are removed so as to ensure that there is at least one correct and incorrect valid tokenised answer. Figure  shows our predicted circuit and the canonical ground-truth circuit from . We have a high similarity, although we predict several heads from layers 10 and 11 that  attribute to MLP layers instead. It is possible that by only looking at attention head outputs and not MLP layers that these later layer heads appear to be doing the role that in actual fact is largely done by MLP layers. An interesting follow-up is examining why these later layer heads appear in our predicted circuit if they're not actually doing any useful computation for the task, or whether they might actually be doing some sort of relevant manipulation of the residual stream.

We again examine the performance of the predicted circuit in the context of the clean model and the ground-truth circuit from . We produce a dataset of 100 examples according to the same process outlined above. The corrupted examples are the same prompts but the  is replaced with ``01''. We define two metrics measuring the performance of the model, introduced by .

Let  be the start year of the sentence, and  be the probability assigned by the model to a two-digit output year . The first metric, probability difference (), measures the extent to which the model assigns higher probability to years greater than the start year. It is calculated as:

Probability difference ranges from -1 to 1, with higher values indicating better performance in reflecting the greater-than operation. A positive value of  indicates that the model assigns higher probabilities to years greater than the start year, while a negative value suggests the opposite.

The second metric, cutoff sharpness (), quantifies the model's behaviour of producing a sharp cutoff between valid and invalid years. It is calculated as:

where  is the probability assigned to the year immediately following the start year, and  is the probability assigned to the year immediately preceding the start year. Cutoff sharpness also ranges from -1 to 1, with larger values indicating a sharper cutoff. Although not directly related to the greater-than operation, this metric ensures that the model's output depends on the start year and does not produce constant but valid output. A high value of  suggests that the model exhibits a sharp transition in probabilities between the years adjacent to the start year.

Table  presents the performance of our predicted circuit in comparison to the clean GPT-2 model, the corrupted GPT-2 model, the ground-truth circuit from , and random complement circuits. The performance is measured using probability difference (PD) and cutoff sharpness (CS). Our predicted circuit, consisting of 29 attention heads, achieves a PD of 76.54\% and a CS of 5.76\%, slightly outperforming the ground-truth circuit (PD: 71.30\%, CS: 5.50\%), albeit having more heads. Notably, our circuit also surpasses the performance of the clean GPT-2 model (144 heads) in CS, and is essentially the same in PD. In contrast, the corrupted GPT-2 model and the average of 100 random complement circuits of the same size as our predicted circuit show negative PD and near-zero CS, indicating poor performance in capturing the greater-than operation and producing a sharp cutoff between valid and invalid years. These results demonstrate that our predicted circuit effectively captures the relevant information for the task while being significantly smaller than the full GPT-2 model.

We also did some exploratory analysis of whether the learned activations from the encoder, and their corresponding codes, were associated at all with the starting year of the completion. For instance, were there codes that only activated for high year numbers, such as  and above? If we only use 100 codes, do these codes roughly get distributed to particular years, so there is a soft bijection between codes and two-digit years?

To answer some of these questions, we created a new dataset consisting of prompts of the form ``The war lasted from the year  to '', and trained a sparse autoencoder on the attention head outputs of GPT2-Small on these prompts, with 100 learned features.

We initially examined the t-SNE dimensionality reduction  of embeddings for all examples across all heads, shown in Figure . We colour the points by the year in the example (e.g. the 14 in ``The war lasted from 19 to 19''). Interestingly, we notice two distinct clusters of activations. The first, on the upper right in Figure , seems to have a fairly well-defined transition between examples with low year numbers to examples with high year numbers. However, the other cluster (the lower left in both plots) appears to have no discernible order. This suggests that the SAE may be learning degenerate latent representations for examples that differ only in the century used. 

We then show the same plot, except colouring each example by the century of the example (e.g. the 19 in ``The war lasted from 14 to 19''). Incredibly, there is almost perfect linear separation between the classes (where the classes pertain to centuries). If we instead produce the same plot with the background being the century of the 10 nearest neighbouring points, some structure with regards to year of the example begins to emerge (Figure ). There seems to be a stronger gradient within groups, with the year number increasing linearly in a certain direction. However, there is still a significant amount of noise, and future research should examine why the SAE learns representations that focus more on the century than the year, when the year is evidently more important for successful completion of the Greater-than task.

We also examined the top 2 principal components of the encoder activations on individual attention heads across examples to determine if they had some relationship to the year number in the example. This is shown in Figure . These four individual heads are selected to show a variety of behaviours. For some, like  and , the principal components seem to directly correspond to ``low'' years and ``high'' years, with many examples in the approximately the same decade being mapped to almost exactly the same PCA-reduced point. Other heads, such as  and , have significantly more variability, but seem to follow some gradient of transitioning from lower years to higher years as we move across the space.

Our predicted Docstring circuit is shown in Figure . Interestingly, our circuit identification method does not predict L0H2 and L0H4 as being part of the circuit, whereas  does. However, after running the ACDC algorithm (as well as HISP and HP, and manual interpretation) on the docstring circuit,  concluded that these two heads are not relevant under the docstring distribution. The agreement between ACDC and our method with regard to these heads that are manually confirmed to not be part of the circuit is promising for the reliability of our approach.

ACDC operates primarily on edges rather than nodes, even though the procedure is agnostic to whether we corrupt nodes or edges in the computational graph. The reason for this is that operating on edges allows ACDC to capture the compositional nature of reasoning in transformer models, particularly in how attention heads in subsequent layers build upon the computations of previous layers.

By replacing the activation of the connection between two nodes (e.g., Layer 0 and Layer 1) while maintaining the original activations between other nodes (e.g., Layer 1 and Layer 2), ACDC can distinguish the effect of model components in different layers independently. This is crucial for understanding the role of each component in the compositionality of computation between attention heads in subsequent layers .

Although ACDC can split the computational graph into query, key, and value calculations for each attention head, the authors focus primarily on attention heads and MLP layers to complete their circuit identification within a reasonable amount of time. This is similar to the approach taken in our method, where we also focus on attention heads, as the canonical circuits for each task are largely defined in terms of this level of granularity.

% % To better understand the differences between node-level and edge-level operations in ACDC, let's consider the Indirect Object Identification (IOI) task as an example. This task involves identifying the indirect object in a sentence, and the authors include a predefined ``ground truth'' circuit for this task in a dictionary.% Node-level operations in the IOI task:% % \item The nodes in the computational graph for the IOI task represent specific components of the GPT-2 Small model, such as attention heads, query/key/value projections, and MLP layers.% \item The dictionary defines groups of attention heads that are believed to be responsible for specific functions in the IOI task, such as ``name mover'', ``backup name mover'', ``s2 inhibition'', etc.% \item A function creates a list of nodes to mask based on the attention heads not present in the dictionary. This is used to create an initial correspondence object using an iterative correspondence function.% % Edge-level operations in the IOI task:% % \item A function modifies the edges in the correspondence object based on the predefined connections in the dictionary:% % \item It removes input -> head connections for all heads in the circuit.% \item It removes connections between heads in the circuit that are not in consecutive layers.% \item It removes connections from heads in the circuit to the output node.% % \item A set defines additional edges between groups of nodes in the dictionary, such as connections from the input to ``previous token'' heads, ``previous token'' heads to ``induction'' heads, etc.% \item The function iterates over these special connections and sets the corresponding edges in the correspondence object to be present.% % Differences between node-level and edge-level operations:% % \item Node-level operations focus on identifying the relevant components (attention heads, MLPs) that are believed to be involved in the IOI task based on the predefined dictionary.% \item Edge-level operations focus on modifying the connections between these nodes based on the predefined special connections and removing connections that are not part of the IOI circuit.% % In the context of the ACDC algorithm, the function serves as a way to define the ``ground truth'' subgraph for the IOI task. The ACDC algorithm would then iterate over the nodes in the computational graph, removing edges that don't significantly affect the performance on the IOI task, as measured by a validation metric (e.g., KL divergence, logit difference, or fraction correct). To find the final components in the circuit, they traverse the graph backwards (from output to input) following the remaining edges. Any node that can be reached from the output node through a path of remaining edges is considered part of the final predicted circuit. Nodes that are not reachable from the output node (i.e., nodes that have become disconnected due to the pruning of edges) are not included in the final predicted circuit.

The final output of an ACDC circuit prediction is a subgraph of the original computational graph, which contains the critical nodes and edges for the given task. The nodes in this subgraph represent the components specified in the original computational graph, such as attention heads, query/key/value projections, and MLP layers. The edges represent the connections between these components that are essential for the model's performance on the task.

For most of the circuits examined in the ACDC paper, including the IOI task, the authors focus on attention heads, as these have canonical ground-truths from previous works. This allows for a direct comparison between the ACDC-discovered circuits and the manually identified circuits, providing a way to validate the effectiveness of the ACDC algorithm in recovering known circuits. . Regardless of the approach in finding the circuit components, the final output of both methods is a predicted circuit of attention heads we can compare to the ground-truth for the relevant task.

So why do we believe it makes sense to compare our node-level and edge-level circuit discovery with ACDC's node- and edge-level discovery, when the methods of determining the importance of a node or an edge are fundamentally difference in either case? For instance, we determine the importance of an ``edge'' between two heads by examining the number of unique co-occurrence code-pairs for that pair of heads. ACDC instead ablates the activation of the connection between these two heads. However, we note that the result of both of these methods (that is, a binary classification of a head as being in the circuit or not being in the circuit) is the same. We simply group the edge-level and node-level methods together for comparison because edge-level focuses on the information moving  nodes (via the residual stream), whereas node-level looks at the output of an individual head (to the residual stream) .

Subnetwork probing  and head importance score for pruning  are both predecessors of ACDC used to examine which transformer components are important for certain inputs, and thus which components might be part of the circuit for a specific type of task. Whilst they are not the focal comparison of our results, we include the methodology used here largely as a supplement to Figure . We follow the exact same setup as , and direct the reader to the ACDC repository for implementation details .

% % To compare our circuit discovery approach with Subnetwork Probing (SP) , we follow the same setup as ACDC . SP learns a mask over the internal model components (such as attention heads and MLPs) using an objective that combines accuracy and sparsity, with a regularisation parameter  (we refrain from referring to this parameter in the main text to avoid confusion with our sparsity penalty  for training the SAE). The original SP method trains a linear probe after learning a mask for every component, but we omit this step to align with ACDC's approach.% We make three key modifications to the original SP method:% % \item We change the objective to match ACDC's, using either KL divergence or a task-specific metric, instead of the negative log probability loss used in .% \item We generalize the masking technique to replace activations with both zero activations and corrupted activations, as corrupted activations are more commonly used in mechanistic interpretability . This is done by linearly interpolating between a clean activation (when the mask weight is 1) and a corrupted activation (when the mask weight is 0) by editing activations rather than model weights.% \item We use a constant learning rate instead of the learning rate scheduling used in .% % The number of edges for subgraphs found with SP is computed by counting the edges between pairs of unmasked nodes. For further details on the implementation, please refer to the ACDC repository .

To compare our circuit discovery approach with Subnetwork Probing (SP) , we adopt a similar setup to ACDC . SP learns a mask over the internal model components, such as attention heads and MLP layers, using an objective function that balances accuracy and sparsity. This function includes a regularisation parameter , which we do not refer to in the main text to avoid confusion with the sparsity penalty used in training our sparse autoencoder (SAE). Unlike the original SP method, which trains a linear probe after learning a mask for every component, we omit this step to maintain alignment with ACDC's methodology.

We made three key modifications to the original SP method. First, we adjusted the objective function to match ACDC's, using either KL divergence or a task-specific metric instead of the negative log probability loss originally used by . Second, we generalised the masking technique to replace activations with both zero and corrupted activations. This change reflects the more common use of corrupted activations in mechanistic interpretability and is achieved by linearly interpolating between a clean activation (when the mask weight is 1) and a corrupted activation (when the mask weight is 0), editing activations rather than model weights. Third, we employed a constant learning rate instead of the learning rate scheduling used in the original SP method.

To determine the number of edges in subgraphs identified by SP, we count the edges between pairs of unmasked nodes. For further implementation details, please refer to the ACDC repository .

To compare our approach with Head Importance Score for Pruning (HISP) , we adopt the same setup as ACDC . HISP ranks attention heads based on an importance score and retains only the top  heads to predict the circuit, with  being a hyperparameter used to generate the ROC curve. We made two modifications to the original HISP setup. First, instead of using the derivative of a loss function, we use the derivative of a metric . Second, we account for corrupted activations as well as zero activations by generalizing the interpolation factor  between the clean head output (when ) and the corrupted head output (when ).

The importance scores for components are computed as follows:

where  is the output of an internal component  of the transformer. For zero activations, the equation is adjusted to exclude the  term. All scores are normalized across different layers as described by . The number of edges in subgraphs identified by HISP is determined by counting the edges between pairs of unmasked nodes, similar to the approach used in Subnetwork Probing. For more details on the implementation, please refer to the ACDC repository .

% % SP learns a mask over the internal model% components (such as attention heads and MLPs), using an objective that combines accuracy and sparsity, with a regularisation parameter . At the end of training, we round the mask to 0 or 1 for each entry, so the masked computation corresponds exactly to a subnetwork of a transformer. SP aims to retain enough information that a linear probe can still extract linguistic information from the model's hidden states. In order to use it to automate circuit discovery, we make three modifications. We i) remove the linear probe, ii) change the training metric to KL divergence, and iii) use the mask to interpolate between corrupted activations and clean activations rather than zero activations and clean activations.% Here we provide the technical detail and motivation for these modifications:% % \item We do not train a probe. ACDC does not use a probe. Cao, Sanh, and Rush (2021) train a linear probe after learning a mask for every component. The component mask can be optimized without the probe, so we just omit the linear probing step.% \item We change the objective of the SP process to match ACDC's. ACDC uses a task-specific metric, or the KL divergence to the model's outputs. In order to compare the techniques in equivalent settings we use the same metric (be it KL divergence or taskspecific) in SP.  use negative log probability loss.% \item We generalise the masking technique so we can replace activations with both zero activations and corrupted activations. Replacing activations with zero activations is useful for pruning (as they improve the efficiency of networks) but are not as commonly used in mechanistic interpretability , so we adapt SP to use corrupted activations. SP learns a mask  and then sets the weights  of the neural network equal to  (elementwise multiplication), and locks the attention mask to be binary at the end of optimization . This means that outputs from attention heads and MLPs in models are scaled closer to 0 as mask weights are decreased. To allow comparison with ACDC, we can linearly interpolate between a clean activation when the mask weight is 1 and a corrupted activation (i.e a component's output on the datapoint , in the notation of Section 3) when the mask weight is 0 . We do this by editing activations rather than weights of the model.% % Additionally, we used a constant learning rate rather than the learning rate scheduling used in .% The number of edges for subgraphs found with Subnetwork Probing are computed by counting the number of edges between pairs of unmasked nodes.% % The key mechanism of HISP is to define an importance by which we can rank heads, and then keep only the top  heads to predict the circuit.  is then the hyperparameter swept over to obtain the ROC. % The authors use masking parameters  for all heads, i.e scaling the output of each attention head by , similar to the approach in Subnetwork Probing (Appendix D.1), so that each head Att  's output is  on an input . The authors keep MLPs present in all networks that they prune. We generalise their method so that it can be used for any component inside a network for which we can take gradients with respect to its output.% The authors define head importance as% % where the equivalence of expressions is the result of the chain rule. We make three changes to this setup to allow more direct comparison to ACDC: i) we use a metric rather than loss, ii) we consider corrupted activations rather than just zero activations and iii) we use the 'head importance' metric for more internal components than merely attention head outputs.% Since our work uses in general uses a metric  rather than loss Appendix , we instead use the derivative of  rather than the derivative of the loss. The HISP authors only consider interpolation between clean and zero activations, so in order to compare with corrupted activations, we can generalize  to be the interpolation factor between the clean head output  (when  ) and the corrupted head output  (when  ). Finally, this same approach works for any internal differentiable component of the neural network. Therefore we study the HISP applied to the query, key and value vectors of the model and the MLPs outputs.% In practice, this means that we compute component importance scores% % Where  is the output of an internal component  of the transformer, which is equivalent to ``attribution patching''  up to the absolute value sign.% To compute the importance for zero activations, we adjust the above equation so it just has a  term, without the  term. We also normalise all scores for different layers as in . The identical setup to the equation then works for outputs of the query, key and value calculations for a given head, as well as the MLP output of a layer. We use query, key and value components for each head within the network, as well as the output of all MLPs.% The number of edges for subgraphs found with HISP is also computed by counting the number of edges between pairs of unmasked nodes, like Subnetwork Probing.

Edge Attribution Patching (EAP) is designed to efficiently identify relevant model components for solving specific tasks by estimating the importance of each edge in the computational graph using a linear approximation . Implemented in PyTorch, EAP computes attribution scores for all edges using only two forward passes and one backward pass. This method leverages a Taylor series expansion to approximate the change in a task-specific metric, such as logit difference or probability difference, after corrupting an edge. For the IOI and Greater-than tasks, EAP used edge-based attribution patching with absolute value attribution. Both tasks employed the negative absolute metric for computing attributions. EAP pruned nodes using a single iteration, with the pruning mode set to ``edge''. This approach avoids issues with zero gradients in KL divergence by using task-specific metrics, making it a robust and scalable solution for mechanistic interpretability. We adapted the code from 's original paper, available .

We noted above that EAP is limited in the metrics we can apply for discovery because the gradient of the metric cannot be zero; we elaborate here. For instance, this means we cannot use the KL divergence metric to find importance components. The KL divergence is equal to 0 when comparing a clean model to a clean model (i.e. without ablations) and is non-negative, so the zero point is a global minimum and all gradients are zero here.

The effectiveness of using SAE-learned features for identifying circuit components raises an important question: why is it necessary to project raw head activations into the SAE latent space to distinguish between positive and negative circuit computations? To investigate whether this projection aids in reducing noise or interference, we analysed the mean per-head activation averaged across positive and negative examples and computed the difference. We then calculated the norm of this difference for each head, applied a softmax function over all heads, and evaluated the ROC AUC against the ground-truth circuit. This analysis was conducted using the same number of examples (10) that the SAE was trained on.

As shown in Figure , head activations do contain some signal regarding the heads involved in circuit-specific computation. However, they are not as effective as our method in distinguishing these computations. This may be due to the variation in particular dimensions within the residual stream across all heads (corresponding to the vertical stripes in Figure ), which likely requires non-linear computation to disentangle positive and negative examples, a task the SAE likely performs effectively.

We observed that performance for the IOI and Greater-than tasks improved as the number of activations over which we computed the mean difference increased. Specifically, both tasks reached a ROC AUC of approximately 0.80-0.85 around 500 examples. However, for the docstring task, performance actually worsened with an increased number of examples. This suggests that while the mean difference method can serve as an initial sanity check, it lacks robustness for reliable circuit identification.

The choice of negative examples is a crucial factor in the performance of the circuit identification method. In this study, we selected negative examples that were semantically similar to the positive examples but corrupted enough to prevent the model from using the current circuit to generate a correct answer.

To investigate the sensitivity of our method to the choice of negative examples, we conducted experiments with five different types of negative examples for the greater-than task:

The results of these experiments are presented in Figure . The findings clearly demonstrate that our heuristic of selecting semantically similar examples that switch off the circuit is an effective approach to maximising performance. This is evident from the drop in performance when using  types compared to . When using  types, the circuit likely remains active when detecting the need to find a two-digit completion greater than ``01". In contrast, the  type makes the negative examples nonsensical by setting the completion century in the past, which likely switches off the circuit.

Interestingly,  negative examples lead to a considerable drop in performance, which we explore further below.

Various studies suggest that hard negative samples, which have different labels from the anchor samples (in this case, our positive examples) but with very similar embedding features, allow contrastive-loss trained autoencoders to learn better representations to distinguish between the two . However, in our case, our negative samples are specifically designed to all be hard negatives.

Currently, there is no reason to believe that the most important codes for differentiating between positive and negative examples should capture all the codes in the IOI task. This is because the IOI negative examples are actually  positive examples. For instance, we would expect previous token heads to be exactly the same in both the negative and positive examples (since both involve two names at least). So, we actually need to give the model data such that some of the codes are forced to be assigned to non-IOI related behaviour. This will hopefully make the remaining codes more relevant for finding the right attention heads in the right layer. This suggests that we should include some non-IOI related data, such as samples from the Pile dataset, in the training data.

We experimented with whether inclusion of ``easy negatives'', defined as random pieces of text sampled from the Pile , would allow the autoencoder to produce representations that were better for us to pick out the important model components for implementing the task. For example, if the positive samples and hard negative samples shared heads for the IOI task, such as detecting names, we would not identify those heads as important because importance is defined by whether the discrete representation helps distinguish a positive sample from a negative one. Thus, including easy negatives could make those particular heads important.

However, as seen in Figure , inclusion of easy negatives actually leads to a decrease in performance on the IOI task. It's possible that the model is forced to assign codes to expressing concepts and behaviours unrelated to the IOI task, and thus cannot as meaningfully distinguish between the semantically-similar positive and negative examples.

A key design choice is whether to take the softmax across the vector of individual head counts or whether to take it across individual layers; that is, first reshape the vector into a matrix of shape . A valid concern is that taking the softmax across layers will make unimportant heads seem important. For instance, if there is a layer with a head that has 1 unique positive code, and all other heads in that layer have 0, this head will have a value of 1.0 and thus be selected no matter what the threshold is. However, it possible that the law of large numbers will cause the number of unique positive codes to be approximately uniform in unimportant layers, so this may not be an issue.

We show the effects of taking the softmax across layer and across individual heads on the node-level ROC AUC in Figure . Softmax across heads performs best in all three tasks, with significant improvement compared with softmax across layers in the IOI and Docstring tasks.

We also hypothesised that we may be able to improve performance by normalising by the overall number of unique codes per head. The reasoning is as follows: if a head has a large number of unique positive codes, our current method is likely to include it in the circuit. However, if the head also has a large number of unique negative codes, then clearly it has a large range of outputs and the autoencoder deemed it necessary to assign many codes to this head, . In essence, the head is important regardless of if we're in the IOI task or not; including the head in our circuit prediction may be erroneous. Normalising by the overall number of unique codes should correct this.

As shown in Figure , normalising seems to have a relatively minor effect. It decreases node-level ROC AUC in the Docstring and IOI tasks, and slightly increases performance in the Greater-than task.

Finally, we show in Figure  how node-level performance varies with the number of positive and negative examples used to calculate the head importance score (i.e. the softmaxed number of unique positive codes per head). We find that we can use as little as 10 examples for the IOI and Docstring tasks, but require the full 250 positive and 250 negative for the Greater-than task. We suspect this is to do with the numerical nature of the Greater-than task and the fact that there are 100 two digit numbers specifying appropriate completions, and so a larger sample size may be required to represent what each of these different attention head outputs look like.

Additionally, we note that the number of examples the SAE requires during training to learn robust representations is actually only about 5-10. Figure  shows that node-level performance actually  for IOI and Greater-than (both GPT-2 tasks) as we increase the number of training examples for the sparse autoencoder. Whilst the Docstring task does not see a decrease as we increase the training example set, it still achieves near-maximal performance around 10 examples. For Docstring and IOI, we can also achieve near-maximal performance with just 10 examples for both steps (training the SAE and counting unique positive codes). However, Greater-than requires a significant number of examples for the latter step.

We compile  transformers for four different tasks: reversing a list (), counting the fraction of previous tokens in a position equal to a certain token (), sorting a list (), and sorting a list by the frequency of the tokens in the list (). All code required to compile these models is available in the . Note that all of these transformers output a vector of tokens rather than a single token, and each of the compiled transformers has a maximum sequence length, which we set to 6 for all examples.

We then simulate our circuit-discovery methodology as follows. For each vocabulary of inputs to each task, we generate 250 permutations with replacement for our positive examples. Because  transformers have compiled weights that only implement a single task, there is no way to ``turn off'' a circuit with negative examples. Due to this, we corrupt the residual stream directly to create our negative examples. To do this for each example, we add Gaussian noise to attention head vectors  or  if the respective actual weight matrices ,  or  in the transformer contain all zeros; simultaneously, we zero out the attention head vectors  and  if the respective weight matrices  or  contain a non-zero element. We define an attention head component (,  or ) as being in the ground-truth circuit if it makes a non-trivial write the to residual stream (i.e. the output to the residual stream is non-zero). Finally, we train our SAE on 10 randomly sampled positive and negative examples and use the full 500 examples to calculate the number of unique positive codes.

Our method achieves a perfect ROC AUC of 1.0 on all -tasks (Figure ). Whilst we should expect good performance on what amounts to a relatively simple task (since the residual streams are corrupted directly), this does provide further evidence of the mechanism that makes our approach successful. Clearly, circuit identification really does boil down to heads that are active on positive examples but inactive on negative. It appears that the SAE then . Active heads are then obviously more likely to be involved in the circuit. We are currently investigating how to collect further evidence in support of this hypothesis.

% % The original Tracr paper explored inducing artificial superposition  by learning a compression matrix  that would read and write to the residual stream of Tracr models before and after each attention and MLP layer , as shown in Figure . We attempted to compress the above Tracr models by learning this matrix with SGD, and then applying our sparse autoencoder to the outputs of attention heads after the compression matrix had been applied.% % % However, we found that Tracr performance substantially decreased on each of the tasks as the learned matrix introduced meaningful compression. Furthermore, compressing the residual stream in this way induces superposition  variables in a given circuit, rather than superposition between circuits themselves . This is likely not a natural state for a model to be in, and thus unlikely to be a useful avenue for testing the abilities of SAEs.

We provide two visualisations of the correlation between the number of unique positive codes and the presence of a head in the circuit. First, we show how the number of unique positive codes per head alongside the ground-truth circuit, arranged by layer and head, in Figure . Clearly, heads with a high number of positive layers are much more likely to be in the circuit. 

We show the high correlation between number of unique positive codes per head and its presence in the circuit in Figure . This corresponds to the node-level circuit identification, where (after softmax) we predict a head's presence in the circuit if it exceeds the pre-defined threshold .

Similarly, we show matrices of the co-occurrence of codes in Figure , alongside the ground-truth circuit. However, because we have an array of size , this time we colour the ground-truth circuit as follows: a (head, head) array entry is dark blue if both heads are in the circuit, light blue if only one is, and white if neither are. Again, we see a very strong similarity between the number of co-occurrences of codes in (head, head) pairs and the presence of one or both heads in the circuit.

We also visualise the  between the number of unique positive codes and unique negative codes per head (Figure ). Again, we see a strong pattern where heads with a high difference (many more unique positive codes than unique negative) are very likely to be in the ground-truth circuit.

We also examine the sparsity of our learned representations, and whether there is any different between positive and negative representations. We plot the histogram of average non-zero activations across all heads for the positive and negative examples in Figure . Whilst there doesn't appear to be any significant difference, the average non-zero activations were slightly higher per positive example (0.56) compared to negative examples (0.42).

Finally, we investigate the relationship between the most common positive codes and their activations in the ground truth heads. Figure  presents activation histograms for the most frequently occurring positive code in each of the three identified heads (141, 127, and 93) for the IOI task. By comparing the activation distributions of these codes for positive and negative examples, we observe a consistent pattern of separated activations between positive and negative examples. The relatively clear separation between the positive and negative activation distributions further highlights the importance of these codes in the functioning of the identified circuit. This analysis provides valuable insights into the fine-grained workings of the learned representations and their role in capturing task-specific patterns.

The best hyperparameters for each task are given in Table . Importantly, all optimal hyperparameters are approximately the same between the Greater-than and IOI tasks, with some differences to Docstring. We hypothesise that this makes sense because Docstring has a smaller number of heads in the model and thus may need higher regularisation i.e. a higher sparsity penalty .

Another hyperparameter introduced by edge-level detection is the number of (head, head) co-occurrence pairs to add to keep from the sorted list of positive co-occurrence counts before taking the set and only keeping heads existing in the remaining tuples in that set. Figure  illustrates that ROC AUC is relatively robust to the selection of ; most of the values of  lead to essentially the same performance, except for at the very start and very end. In fact, it seems that selecting  to be half of the number of overall co-occurrence pairs of codes in heads is a robust heuristic for optimising performance across datasets.

We show contour plots from the Optuna optimisation in Figure . It seems that a lower  across datasets is beneficial. However, the number of learned features does not seem to be that important for Docstring or IOI, whilst a low number of learned features leads to lower AUC for the Greater-than task.

% % % % % One final question we wish to answer is whether our method may be used to find causally influential circuits in other types of neural networks. To test this, we aim to find a ``dog'' circuit in a ResNet9 residual convolutional neural network trained on CIFAR10, using our conceptual approach adapted to the specifics of the model and its activations.% To find the ``circuit'':% % \item We train a ResNet9 on CIFAR10 for 10 epochs, achieving a final test classification accuracy of 86\%. % \item We collate 250 examples of the ResNet's activations on dog images and 250 on cat images (as cats are semantically similar to dogs but hopefully enough to ``switch off'' the circuit). We collect activations for the four convolutional layers, and the two residual connections.% \item For each batch of activations of shape  (where  is the number of images,  is the number of channels, and  and  are width and height respectively), we flatten the last two dimensions and get a tensor of shape , where  is equivalent to our model dimension from above. % \item We then analogously train our SAE on these concatenated positive (dog) and negative (cat) examples,  (because all components are different dimensions). That is, we have a separate SAE for each model component. We then determine the most important channels in each component (i.e. most important channels per SAE) exactly the same as above: finding the number of unique positive codes per channel, softmaxing, and choosing a threshold .% % This gives us a circuit consisting of specific channels in specific layers in the ResNet that is supposedly used for identifying dogs. However, we don't have a groundtruth for this circuit. How do we evaluate whether this is actually the dog circuit or not?% % \item We begin by ablating all channels in all layers that  in the circuit by setting them to zero. We then chop off the final linear projection layer of the ResNet (the head), freeze all other weights, and training a linear project layer that just predicts `dog' or `not dog'. Again, this model has all channels not in the circuit ablated to zero. We measure classification accuracy across the CIFAR10 dataset on `dog' versus `not dog'.% \item We use gradient-optimisation to create images that maximally and minimally activate the channels in our circuit as qualitative evidence that our channels are responsible for dog-like features. Similarly, we collect the max-activating images of our circuit (summed) across the CIFAR10 dataset, and the min-activating features. We also measure the correlation between an image's activation of a circuit component and the image's activation of the SAE code representation of that circuit component.% \item We examine class activation maps of our ablated ResNet on images of dogs and images of other things, hopefully showing activations only on dog features.% \item Finally, we show how the ResNet can be tricked by causally intervening with our circuit. We show that by inserting the decoded representation of our SAE feature back into the specific channels in the circuit, we can significantly increase the probability that the ResNet predicts `dog' on any arbitrary image.% % % % % Now compare these with the max and min activating images from a selected circuit component (channel) in the fourth convolutional layer, where our method only provides as much signal as a random set of channels:%