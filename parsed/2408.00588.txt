First, we fine-tuned PRIMERA, LongT5, and Llama-2 using the LoRA method (Figure~b). We observed that fine-tuning considerably improved the performance of most models (, Figure~). Specifically, LongT5 models benefited the most from fine-tuning, which led to an increase from 14.72 to 24.61, 15.06 to 28.27 in METEOR, 15.15 to 38.81 in CHRF, and 36.05 to 51.43 in PICO-F1 (Supplementary Table~). In contrast, PRIMERA demonstrated a relatively moderate improvement with a ROUGE-L increase from 18.90 to 20.48, METEOR increase from 25.15 to 26.50, and PICO-F1 increase from 43.22 to 49.47. However, there was a slight decrease in the CHRF from 39.25 to 37.84. Overall, the fine-tuned LLMs improved the ROUGE-L score by an absolute of 9.89\% (95\% confidence interval of improvement: 8.94-10.81), the METEOR score by 13.21 (95\% confidence interval of improvement: 12.05-14.37), and the CHRF score by 15.82 (95\% confidence interval of improvement: 13.89-16.44). These recently released models all outperformed the fine-tuned variant of BART, the previous SoTA. The fine-tuned BART achieved 17.74, 27.49, and 40.54 in ROUGE-L, METEOR, and CHRF, respectively. We compared the fine-tuned models with GPT-3.5-turbo, one of the most widely known and cutting-edge closed-source LLMs. Zero-shot GPT-3.5-turbo achieved 23.15 in ROUGE-L, 28.83 in METEOR, and 39.74 in CHRF scores. The performance gaps between the open-source models and GPT-3.5-turbo were reduced after fine-tuning. The fine-tuned LongT5 achieved similar results as GPT-3.5-turbo (Supplement Table ). We also conducted a pilot study using GPT-4. The summaries generated by GPT-3.5-turbo and GPT-4 are not significantly different; as such, we only use GPT-3.5-turbo for comparison.

Furthermore, while few-shot learning helped reduce the performance gap, LLMs fine-tuned with the full training data still demonstrated better performance. We constructed two sets of few-shot learning baselines, one based on few-shot prompting and the other based on few-shot fine-tuning (see details in Methods). Under 1-, 2-, and 5-shot prompting, Mixtral-8x7B achieved 24.87/24.53/24.99 in Rouge-L, 27.82/25.78/27.59 in METEOR, 37.63/35.61/37.42 in CHRF respectively. After fine-tuning using 100 randomly selected samples, PRIMERA achieved 19.11 in ROUGE-L, 24.98 in METEOR, and 39.01 in CHRF; LongT5 was moderately improved with few-shot fine-tuning, resulting in 15.06 in ROUGE-L, 16.17 in METEOR, and 24.81 in CHRF.

We also calculated Pearson correlation coefficients among different automatic metrics using the entire test set (Supplementary Figure~). Most metrics are strongly positively correlated with each other; with a few exceptions, most coefficients are larger than 0.7. The only exceptional metric is the PICO coverage, which is only weakly positively correlated to the others, with coefficients ranging between 0.33 and 0.46. As discussed above, zero-shot models tend to verbatim replicate the input, which is already abundant in PICO concepts, since we selected only the objective and main results section of the review abstracts.

Next, we investigated whether fine-tuned smaller models have the capability to outperform zero-shot larger models. To this end, we fine-tuned the LongT5-base model, which has 10\% fewer parameters than LongT5-xl. Figure~ shows that fine-tuned LongT5-base outperforms zero-shot LongT5-xl. We also found that this observed trend holds across different LLM architectures. For instance, the performance of fine-tuned PRIMERA and LongT5 exceeded that of zero-shot Llama-2 (Supplementary Table~), even though the latter model comprises at least 20 times more parameters.

Finally, we conducted a comprehensive human evaluation and a GPT-4 simulated evaluation of machine-generated summaries. Our baseline, zero-shot Llama-2, is one of the latest and largest open-sourced LLMs. In both evaluations, we requested clinical experts or GPT-4 to select the better summary from a pair of candidates - one generated using the baseline and the other generated using our fine-tuned models. The win-rate is the ratio of machine-generated summaries evaluated as better than the baseline. The baseline win-rate is 50\%, given that (1) zero-shot Llama-2 is being compared to itself, (2) all pairs of summaries to be compared are identical, and (3) ties are broken randomly.

According to the human evaluation, fine-tuned Llama-2 was preferred to zero-shot, with the win-rate increased from 50\% to 59.20\% (Figure~a, Supplementary Table~). Fine-tuned PRIMERA and LongT5 models also achieved 54.47\% and 59.68\% win-rate against the zero-shot Llama-2.

We further asked the evaluators to share the rationale behind their preference for the chosen summaries. The fine-tuned models were compared with zero-shot Llama-2 on multiple dimensions that have been established as desired properties of summaries. Figure~ shows the number of cases where zero-shot LLama-2 generated better summaries (left/red), in contrast to the cases where the fine-tuned models generated better summaries (right/blue). With few exceptions, LLMs were improved in all aspects after fine-tuning (Supplementary Table~). By manually comparing the summaries generated by zero-shot and fine-tuned models, we found that zero-shot models tend to present a detailed background of the summarized studies but do not provide any findings or conclusions, i.e., they present a high resemblance of leading sentences in the paragraphs. Recall that word embeddings are combined with positional embeddings to represent each token in a document in transformer architecture. In general summarization tasks, the key information is typically presented in leading or concluding sentences. The ordering of key information and other redundant information can impact the positional embeddings during pre-training. The zero-shot open-source summarization models mostly extract the leading sentences instead of key information. This indicates that positional embeddings significantly impacted the summary more than word embeddings in zero-shot open-source models. On the other hand, fine-tuned models align more closely with ground-truth summaries, which can provide supportive evidence or identify the lack of sufficient evidence for intervention outcomes.

The GPT-4 simulated evaluation also indicates a significant improvement in all models after fine-tuning (Figure~b). In addition, 257 out of 378 simulated evaluation results concord with the judgment of human experts (68\% accuracy).

We also evaluated all models on two distinct test sets of review articles (Supplementary Table~). One group, denoted as ``after cutoff,'' was published after the latest knowledge cutoff date of every model, i.e., no articles in this group were used in pre-training the foundation models. The other group, denoted as ``before cutoff'', was published before the knowledge cutoff date; the articles in this group may be used for pre-training purposes. On both the ``after cutoff'' and the ``before cutoff'' test sets, fine-tuned models demonstrated improved performances. Recap that all articles used for fine-tuning were published ``before cutoff''. This demonstrates the generalizability of fine-tuned models ``before cutoff'' data on ``after cutoff'' test data.

We collected 8,161 abstracts of systematic reviews from the Cochrane Library. Unlike abstracts of the biomedical literature, which are highly condensed summaries, systematic review abstracts provide a structured overview that enables readers to quickly determine the validity and applicability of the review. These abstracts typically follow a common structure, detailing preferred reporting items. The Cochrane review abstracts present , , , , , , and  Within such a self-contained structure, the authors' conclusion presents a narrative summary of the most salient details of the included clinical studies. This section is one of the first to consult when healthcare providers seek answers to clinical questions. Given the meta-analysis results as input, we aim to automatically reproduce this narrative summary. The collected reviews cover a wide range of topics, including but not limited to neurology, gastroenterology, rheumatology, nephrology, and radiology. The publication dates of the reviews range from April 1996 to June 2023.

We split the dataset into distinct training (91.56\%), validation (4.83\%), and test (3.61\%) sets, ensuring that all of the samples appear in one set (Supplementary Table~). All LLMs were prepared with a large extent of public textual data collected until a certain moment, known as the cutoff. To maintain a legitimate comparison between LLMs, we put all articles published after September 2022 as test data (because most LLMs studied in this study used the data up to September 2022). Articles published prior to this are primarily used for training and validation. The division of training and validation was stratified according to the time of publication.

Few-shot learning has been proven an effective and sample-efficient strategy for optimizing task-specific LLMs. We construct two few-shot learning baselines, one based on prompting and the other on fine-tuning.

For few-shot prompting, we use Mixtral-7x8B for the few-shot prompting foundation model. Due to the limit on token numbers, other open-source models cannot fit demonstrations of long document summarization in the context windows. We randomly selected 1, 2, and 5 samples from the training set as demonstrations. For few-shot fine-tuning, we followed the findings of the PRIMERA report that LLMs can be reasonably adapted to domain-specific tasks with a limited number of labeled samples. We used the same setup as the few-shot experiments in the PRIMERA, where we randomly selected 100 samples from the training set and fine-tuned LLMs.

We investigated several LLM architectures that have recently surfaced for tasks related to summarization tasks or as foundation models. In this study, we only consider models that satisfy the following two conditions. First, models need to be publicly accessible and open-sourced to ensure the transparency and accountability of the models. Second, context windows need to be long enough to digest input without requiring condensation or truncation. Bearing all these factors in mind, we included PRIMERA, LongT5, and Llama-2 in our studies. PRIMERA deploys a pretraining strategy named Entity Pyramid to select and aggregate salient information focusing on document summarization. LongT5 is an extension of T5 architecture that adopts a summarization pretraining strategy to scale up the input length. Llama-2 is one of the recently released open-source, scalable foundation models with 7B, 13B, and 70B parameters. Since Mixtral-8x7B demonstrated similar benchmark performance as Llama-2, we did not fine-tune Mixtral-8x7B. Instead, we report the few-shot prompting performance of Mixtral-8x7B.

Following a previous work, we selected the objective and main results sections of a review abstract as input and used the authors' conclusion as a reference to fine-tune models. We applied the LoRA method, which keeps the original parameters frozen and adjusts only a relatively small number of extra parameters via matrix decomposition. The exact number of parameters depends on the rank hyperparameter in the LoRA method. We refer the readers to the original LoRA paper for technical details.

Our implementation uses the following libraries, transformers, torch, and PEFT. Most fine-tuning jobs were completed on AWS and our local lab servers. Llama-2 models were fine-tuned on SageMaker platform. All models were fine-tuned for 1 epoch, within which the validation loss had already stopped decreasing. We set the learning rates to 3e-5, 1e-4, and 1e-3 for PRIMERA, LLama-2, and LongT5 models, respectively. We set the rank hyperparameter of LoRA to 8.

We first use the natural language generation (NLG) metrics to evaluate the quality of the generated summary. These metrics include ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation) and METEOR (Metric for Evaluation of Translation with Explicit Ordering) scores. We also include the CHRF (CHaRacter-level F-score), which was reported to correlate highly with the readability of generated text. Their values range from 0.0 to 100.0, with a score of 100.0 indicating that the generated summaries are identical to the reference summary. The model performance on our test is approximately normally distributed and the performance of each model is independent of others. As such, we calculated the p-value using a paired t-test to determine the statistical significance of the difference between the two models.

NLG metrics are known to be inadequate for evaluating factual completeness and consistency. We therefore propose to use a PICO (Participants, Interventions, Comparison, and Outcomes) extraction system to evaluate the accuracy of the generated summaries. More specifically, we fine-tuned a BERT-based model to extract PICO concepts and then score a generated summary by comparing the values of these PICO elements obtained from the reference. We consider a PICO element to be a true positive, if it satisfies two conditions: (1) the text in the reference overlaps with the text in the generation, and (2) the two entity types should have the same PICO category. The micro averages for precision, recall, and F1 scores are all computed over the PICO components.

We conducted a review of the summary quality via human evaluation. The quality is measured from four aspects: consistency, comprehensiveness, specificity, and readability, which were established as essential factors for measuring machine summary quality. Consistency indicates whether the summary contradicts the input source. Comprehensiveness measures coverage of key information of input. Specificity measures the preciseness and conciseness of the summary. Readability indicates a machine summary is fluent and free of grammatical errors that hinder understanding.

To evaluate the machine summaries, we invited 7 clinical experts, each specializing in one or two of the following specialties, including Gastroenterology, General Surgery, Internal Medicine, Nephrology, Neurology, Radiology, and Rheumatology. All experts have obtained MD training and currently provide direct patient care. Following a recent LLM study, we request the experts to compare the quality of different machine summaries (interface shown in Supplementary Figure~). Specifically, according to their domain knowledge, each expert was assigned review abstracts along with three summaries: (1) the Authors' conclusion section, (2) zero-shot Llama-2, and (3) one of the fine-tuned models. From the zero-shot baseline and the fine-tuned models, the experts will select which one generates a better summary. To reduce the potential order-related bias, the order of summaries generated by zero-shot Llama-2 and fine-tuned summaries was randomized. We further asked the experts to choose the reasons for their choices.

Like many other annotation scenarios, collecting experts' feedback is not scalable with respect to the samples to be labeled. In addition to our manual review, we explored the use of GPT-4 as a simulated expert to answer the same questions as those assigned to human experts. Instead of selecting a sample of test data as in the manual review, we used the model summaries for all test articles for GPT-4 evaluation. We also analyzed the percentage of questions that human judgments agree with the GPT-4 evaluation.

The data can be accessed at .

The code can be accessed at .

This project was sponsored by the National Library of Medicine grant R01LM009886, R01LM014344, National Human Genome Research Institute grant R01HG012655, and the National Center for Advancing Clinical and Translational Science awards UL1TR001873 and UL1TR002384. Q.J. and Z.L. are supported by the NIH Intramural Research Program, National Library of Medicine. We also want to express our gratitude to Amazon Web Services (AWS) for providing the computational resources used in our research. The funder had no role in the design and conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the manuscript; and decision to submit the manuscript for publication.

Study concepts/study design, ; manuscript drafting or manuscript revision for important intellectual content, ; approval of final version of the submitted manuscript, ; agrees to ensure any questions related to the work are appropriately resolved, ; literature research, ; experimental studies, ; human evaluation, ; data interpretation and statistical analysis, ; and manuscript editing, .

The authors declare no competing interests.

Not applicable.

% Large language models (LLMs) hold great promise in summarizing medical evidence. Most recent studies focus on the application of proprietary LLMs. Using proprietary LLMs introduces multiple risk factors, including a lack of transparency and vendor dependency. While open-source LLMs allow better transparency and customization, their performance falls short compared to proprietary ones. In this study, we investigated to what extent fine-tuning open-source LLMs can further improve their performance in summarizing medical evidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs of systematic reviews and summaries, we fine-tuned three broadly-used, open-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned LLMs obtained an increase of 9.89 in ROUGE-L (95\% confidence interval: 8.94-10.81), 13.21 in METEOR score (95\% confidence interval: 12.05-14.37), and 15.82 in CHRF score (95\% confidence interval: 13.89-16.44). The performance of fine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore, smaller fine-tuned models sometimes even demonstrated superior performance compared to larger zero-shot models. The above trends of improvement were also manifested in both human and GPT4-simulated evaluations. Our results can be applied to guide model selection for tasks demanding particular domain knowledge, such as medical evidence summarization. IntroductionintroductionPeng2023-zh, Concato2000-npBorah2017-ahNLM2024-qwPage2021-nlWallace2021-zh, Tang2023-oaTang2023-oaBarzilay2002-nv, Pivovarov2015-km, Zweigenbaum2007-jj, Li2010-rc, Demner-Fushman2007-tkGu2021-ni, Guo2021-qv, Xiao2021-et, Zhang2020-fq, Lewis2019-qk, Devlin2018-ct, Mrabet2020-xjPeng2023-zh, Singhal2023-jcZack2023-jd, Jin2023-pkTang2023-oa, Jiang2024-jw, Ouyang2022-ww, Touvron2023-we, OpenAI2023-cnNosek2015-yy, Zhang2024-buGutierrez2022-xuTang2023-oaXiao2021-etGuo2021-qvTouvron2023-weTadros2022-ulHu2021-eyCochrane-wgtab:datasetTang2023-oafig:overviewResultsresultsComparison of different LLMs in automatic evaluationscomparison-of-different-llms-in-automatic-evaluationsfig:overviewfig:automatictab:automatictab:automaticfig:pearsonComparison between zero-shot LongT5-xl and fine-tuned LongT5-basecomparison-between-zero-shot-longt5-xl-and-fine-tuned-longt5-basefig:comparisontab:automaticQualitative evaluationqualitative-evaluationOpenAI2023-cnOuyang2022-wwfig:humantab:humanTang2023-oa, Fabbri2021-gmfig:numbertab:numberfig:humantab:simulatedDiscussiondiscussionTang2023-oafig:comparisonfig:numbertab:automaticfig:humanMethodsmethodsData CollectionCochrane-wgPage2021-nlbackgroundobjectivessearch methodsselection criteriadata collection and analysismain resultsauthors' conclusions.Tang2023-oatab:datasetFew-shot BaselinesFine-tuning LLMsfine-tuning-llmsXiao2021-etGuo2021-qvTouvron2023-weJiang2024-jwTang2023-oaHu2021-eyHu2021-eyWolf2020-bsPaszke2017-jnMangrulkar2022-svXiao2021-etTouvron2023-weGuo2021-qvHu2021-eyEvaluation Metricsevaluation-metricsFabbri2021-gmPICO metricspico-metricsFabbri2021-gmGu2021-niHuman evaluationhuman-evaluationOuyang2022-wwfig:user interfaceGPT-4 evaluationgpt-4-evaluationOpenAI2023-cnData availabilityhttps://github.com/ebmlab/MedReviewCode Availabilityhttps://github.com/ebmlab/MedReviewAcknowledgementsacknowledgementsAuthor Contributions:G.Z., C.W., Y.P.G.Z., Q.J., Y.Z., S.W., B.R.I., Y.L., E.P., J.G.N., M.E.S., A.S., T.C., Z.L., C.W., Y.P.G.Z., Q.J., Y.Z., S.W., B.R.I., Y.L., E.P., J.G.N., M.E.S., A.S., T.C., Z.L., C.W., Y.P.G.Z., Q.J., Y.Z., S.W., B.R.I., Y.L., E.P., J.G.N., M.E.S., A.S., T.C., Z.L., C.W., Y.P.G.Z., Y.P.G.Z., Q.J., Y.Z., S.W.Q.J., B.R.I., Y.L., E.P., J.G.N., M.E.S., A.S.G.Z., Y.P.G.Z., Q.J., Y.Z., S.W., B.R.I., Y.L., E.P., J.G.N., M.E.S., A.S., T.C., Z.L., C.W., Y.P.Competing Interestscompeting-interestsEthics declarationsethics-declarationsmedlinereftable0figure0Supplementary FigureSupplementary Table