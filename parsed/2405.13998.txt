A large class of operator learning architectures can be described as encoder-decoder architectures according to the following construction.  To learn an operator , an input function  are first mapped to a finite dimensional latent representation through the encoding map .  This latent representation is then mapped to a callable function through a decoding map .  The composition of these two maps gives the architecture of the operator approximation scheme, .  The class of encoder-decoder architectures includes the DeepONet , the PCA-based architecture of , and NoMaD .

These encoder-decoder architectures can be seen as neural fields with a global conditioning mechanism by viewing the role of the decoder as an operator which performs the conditioning of a base field, with a latent code  derived from a parameterized encoder, .  

For example, in a DeepONet, the base field is a neural network  (trunk network) appended with a last linear layer, and the decoder map conditions the weights of this last layer via the output of the encoding map  (branch network); see Figure (a) for a visual illustration.  In other words, the decoder map is a weighted linear combination of  basis elements , where the weights are modulated by a global encoding of the input function , yielding outputs of the form . The PCA-based architecture of  can be viewed analogously using as basis elements the leading  eigenfunctions of the empirical covariance operator computed over the output functions in the training set.

Seidman  demonstrated how encoder-decoder architectures can employ nonlinear decoders to learn low-dimensional manifolds in an ambient function space. These so-called  NoMaD architectures typically perform their conditioning by concatenating the input function encoding with the query input, i.e., ; see Figure (b) for an illustration.

Integral Kernel Neural Operators form another general class of neural operators that learn mappings between function spaces by approximating the integral kernel of an operator using neural networks. They were introduced as a generalization of the Neural Operator framework proposed by Li . The key idea behind here is to represent the operator  as an integral kernel, , where  is a kernel function. In practice, the kernel function  is parameterized by a neural network, and the integral is then approximated using a quadrature rule, such as Monte Carlo integration, Gaussian quadrature, or via spectral convolution in the frequency domain.

A single layer of the GNO  is given by

where  is a trainable weight matrix, and  is a local message passing kernel parameterized by  . As illustrated in Figure (c), the GNO layer is a conditioned neural field with local and global conditioning on the input function . The base field uses a fixed positional encoding  that maps query coordinates  to input function values at neighboring nodes , i.e., , followed by a linear layer. The encoding width equals the maximum number of neighboring nodes, with zeros filling tailing entries for queries with fewer neighbors. The GNO layer is conditioned globally via the parameterized kernel  modulating the linear layer weights, and locally via a position-depended bias term  added as a skip connection. The layer outputs  are finally obtained by summing the linear layer outputs and the skip connection, then applying a nonlinear activation function .

The FNO introduced a computationally efficient approach for performing global message passing in GNO layers assuming a stationary kernel  . Leveraging the Fourier transform, a single layer of FNO  is given by

where  and  are trainable weight matrices,  is the Fourier transform truncated to the first  modes, and  is the left inverse of this operator.  We  show that the FNO layer can be viewed as a conditioned neural field employing both local and global conditioning on the input function . Without loss of generality, we demonstrate this for a one-dimensional query domain . To this end, note that the convolutional part of the FNO layer () can be re-written as (see Appendix  for a derivation)

where  denotes the truncated Fourier transform of , and  and  denote the real and imaginary parts of a complex vector .  We may interpret this transformation of  as a linear transformation (which depends on ) acting on the positional encoding, %%%  %     \gamma(y) =  \cos(2\pi \langle k_1, y\rangle ) \\ \vdots \\ \cos(2\pi \langle k_n, y\rangle \\ \sin(2\pi \langle k_1, y\rangle) \\ \vdots \\ \sin(2\pi \langle k_n, y \rangle).%     .%  The resulting expression () is then acted on with a position dependant bias term , followed by a pointwise nonlinearity .  In this manner, we see that an FNO layer is a neural field with a fixed positional encoding (first  Fourier features), a global conditioning of the weights by the Fourier transform of , and a local conditioning acting as a bias term by ; see Figure (d) for an illustration. This neural field interpretation also suggests an alternative way for implementing FNO layers by explicitly using  to compute the inverse Fourier transform, thereby allowing them to be evaluated at arbitrary query points instead of a fixed regular grid.

In practice, GNO and FNO layers are stacked to form deeper architectures with a compositional structure. These can also be interpreted as conditioned neural fields, where the base field corresponds to the last GNO or FNO layer, while all previous layers are absorbed into the definition of the conditioning mechanism. Analogously, one can examine a broader collection of models that fall under the class of Integral Kernel Neural Operators, such as .

Here we provide our main results across three challenged benchmarks. The full details on the underlying equations, dataset generation and problem setup for each case are provided in the Appendix; see Section ,  and , respectively.

 The first benchmark involves predicting the transport of  discontinuous waveforms governed by a linear advection equation with  periodic boundary conditions. We make use of the datasets and problem setup established by Hoop .   This benchmark evaluates our model's capability in handling discontinuous solutions and shocks in comparison to popular neural operator models.

% the discussions on advertion is missing Figure  illustrates the test sample corresponding to the worst-case prediction of each model we compared. We observe that CViT is able to better capture the discontinuous targets and yield the most sharp predictions across all baselines. This is also reflected in both the relative L2 and Total Variation error metrics; see Appendix Table  for a more complete summary, including details on model configurations, as well as average, median and worst case errors across the entire test dataset.

 The second  benchmark involves a fluid flow system governed by the 2D shallow-water equations. This describes the evolution of a 2D incompressible flow on the surface of the sphere and it is commonly used as a benchmark in climate modeling. Here we adhere to the dataset and problem setup established in PDEArena , allowing us to perform fair comparisons with several state-of-the-art deep learning  surrogates.

Table   presents the results of CViT against several competitive and heavily optimized baselines.  Our proposed method achieves the lowest relative  error. % for both one-step and 5-step roll-out predictions (see Appedix  for more details).  It is also worth noting that the reported performance gains are achieved using CViT configurations with a significantly smaller parameter count compared to the other baselines. This highlights the merits of our neural fields approach that enables the design of parameter-efficient models whose outputs can also be continuously queried at any resolution. Additional visualizations of our models are shown in Figure  and .

In addition, here we perform extensive ablation studies on the hyper-parameters of CViT. The results are summarized in Figure . We observe that a smaller patch size typically leads to better accuracy, but at higher computational cost. Moreover, model performance is significantly influenced by the type of coordinate embeddings used in the CViT base field. In order to ablate this choice, we consider two additional approaches to constructing these embeddings: (a) a small MLP, and (b) random Fourier features . Our experiments reveal that the proposed grid-based embedding achieves the best accuracy, outperforming other methods by up to an order of magnitude. We also investigate the impact of the resolution of the associated grid. Our findings suggest that best results are obtained when the grid resolution matches the highest resolution at which the model is evaluated. Additionally, the CViT model shows sensitivity to the hyper-parameter  used for computing interpolated features; too small  values can degrade predictive accuracy. Finally, while we observe minor improvements by increasing the latent dimension of grid features, or the number of base field attention heads, the overall model performance remains robust to variations in these hyper-parameters; see Appendix Figure .

Our last benchmark corresponds to predicting the evolution of a 2D buoyancy-driven flow, depicting a smoke volume rising in a bounded square domain. The underlying governing equations are the incompressible Navier-Stokes equations coupled with a passive scalar transport equation. For a fair comparison with various neural operators and other state-of-the-art deep learning models, we use the dataset generated by PDEArena , and follow the problem setup reported in Hao .

Our main results are summarized in Table , indicating that CViT outperforms all baselines we have considered. Notably, the previous best result (DPOT-L (Fine-tuned)) is achieved with a much larger transformer-based architecture, which involves pretraining on massive diverse PDE datasets followed by fine-tuning on the target dataset for 500 epochs. Furthermore, CViT exhibits remarkable parameter efficiency. CViT-small, with only 13M parameters, achieves accuracy comparable to the largest pretrained model (1.03B). Additionally, CViT shows good scalability; as the number of parameters increases, performance improves. These findings strongly support the effectiveness our approach in learning evolution operators for complex physical systems. Additional results and sample visualizations are provided in Figure , as well as Figure ,  and  in the Appendix.

% % Experiments on super resolution, resolution invarience% rope embedding, VITAR