We study abstention in the scenario of LLMs as AI assistants, exemplified by chatbots such as % for humans such as  ChatGPT~, Claude~, LLaMA~, and others~. % shown in Fig.~.  We propose an idealized abstention-aware workflow for these systems in Fig.~. Given an LLM  that supports arbitrary generative modeling tasks and the users' input ,  generates an output . We analyze the decision to abstain from three distinct but interconnected perspectives:

 For examples of queries and outputs meeting conditions for abstention, please see Appendix Tab.~.

Consider an LLM . When given a prompt ,  generates a response .  We model refusal to answer (abstention) as a function  where  indicates the system will abstain from answering and  indicates the system will return the output .

We define  as the conjunction of three functions, to be defined by a system designer, to assess query , % with respect to the LLM ,  the  of the LLM's response to the query, and the  of the query and response.  %If  any of these three aspects fail to meet thresholds, the system will abstain. We define these three functions as:

 The refusal function  determines whether the LLM should provide output  in response to input  as: % if the following conditions are met:%  where , , and  are score thresholds for each of the three functions.

Pretraining methods to promote abstention are rare. One notable exception is~, who perform data augmentation in pretraining to encourage LLMs to predict unanswerable when presented with an empty or randomly sampled document.

 \\ %  Due to the associated costs, there is little work that explores pretraining methods for encouraging abstention. We propose two possible directions.  By incorporating examples of abstention in pretraining data, the model may learn to balance the trade-offs between providing answers and abstaining on unanswerable or unsafe questions. We could augment existing datasets or manually curate new datasets that contain ambiguous or incomplete queries, contradictory information, or misaligned queries and inject abstention responses where appropriate. Additionally, LLMs could generate synthetic examples where abstention would be an appropriate response.  Since data cleaning has been shown to be effective for improving LLM helpfulness and harmlessness \todocite, we propose abstention-based data filters to remove inappropriate abstention data from pretraining corpora, in order to encourage LLMs to abstain properly.  To improve abstention capabilities,

construct an honesty alignment dataset by  substituting LLM's wrong or uncertain responses with ``I don't know'' and finetuning on the resulting data.  introduce -tuning, constructing and finetuning on a refusal-aware dataset and showing improved abstention capabilities; they argue that refusal-aware answering is task-independent and could benefit from multi-task training and joint inference. In recent work,  demonstrate that finetuning with LoRA~ significantly improves abstention while maintaining general task performance. Furthermore,  show that various LLMs exhibit differing levels of abstention improvement following fine-tuning with QLoRA~. However,  present contradictory findings that abstention instruction tuning struggles to generalize across domains and LLMs.   find that instruction-tuning makes models more conservative, leading to incorrect refusals.

Towards alignment with human values,  show that adding a small number of safety instructions to instruction tuning data reduces harmful responses without diminishing general capabilities, whereas an excessive number of safety instructions makes LLMs overly defensive.  construct responses for unsafe prompts by combining fixed refusal responses with Llama-2-generated safe responses, and see similar results.  finetune LLMs to follow hierarchical prompts, enhancing the fine-grained abstention ability of LLMs.  also fine-tune LLMs with distinct goal priority instructions and instruct LLMs to prioritize safety over helpfulness during inference. 

However, safety tuning is not always helpful for abstention on unseen data. For example,~ note that benign finetuning can increase unsafe behaviors in aligned LLMs . To address this,  finetune LLMs to evaluate their own outputs for harm and append a ``harmful'' or ``harmless'' tag to its responses instead of directly tuning LLMs to abstain.

 \\  Tuning LLMs on abstention-aware data may lead to overly conservative behavior, causing erroneous refusals of queries.  address this through Direct Preference Optimization~, encouraging the model to answer questions it knows and refuse questions it does not know. Similarly,  train a reward model using Proximal Policy Optimization~ to learn abstention preferences, leveraging hallucination scores to determine the LLM's knowledge boundary.  Factuality alignment methods~ employ a factuality-aware reward model.

Safety alignment methods  use explicit or implicit preference models to reduce harmfulness, which though not explicitly focused on abstention, will encourage abstention on unsafe prompts. E.g.,  apply an iterative self-refinement process, where rejected answers are those labeled as harmful by the reward model, and chosen answers (usually abstention) are obtained through self-feedback. Other studies have explored multi-objective alignment approaches~ to encourage safe and helpful model behavior.

 \\  

We categorize inference stage methods as input-processing, in-processing, or output-processing approaches based on when they are applied. 

From the query perspective, LLMs can choose to abstain based on the query answerability. In this context,  try to first predict the ambiguity of questions derived from the AmbigQA dataset  to disentangle the ambiguity from query to model.

Other methods aim to identify queries that are misaligned with human values. For example,  and  detect malicious queries needing abstention by removing suspect words from the query and analyzing the resulting drop in perplexity. Some more exhaustive approach  further investigate changes in backward probability following word deletion, word replacement, sentence paraphrasing, perplexity filtering and retokenization. The BDDR framework~ employs a pre-trained language model classifier to discern discriminative results after altering suspicious words. Similarly,  measure changes in representation between original and paraphrased queries using a set of distributional anchors to identify harmful queries.  suggest a self-adversarial training pipeline as an attack classifier; when a malicious query is detected, the LLM has the option to either abstain from responding or modify the input accordingly.

 \\ % white-box methodsRecent studies, such as those by  and , focus on training calibrators based on LLM internal representation to predict the accuracy of the model's responses, enabling abstention when the likelihood of error is high. Further probing into the internal representations of LLMs to discern between answerable and unanswerable queries has been conducted by , , and . Additionally,  introduce the EigenScore, a novel metric derived from LLM's internal states, which can facilitate abstention by quantifying the reliability of the model's knowledge state.

In terms of leveraging the LLMs' internal states for safety judgments,  extract safety-related vectors (SRVs) from safety-aligned LLMs; which are then used as an abstention gate to steer unaligned LLMs towards safer task performance. Furthermore,  demonstrate that integrating a safety vector into the weights of a finetuned LLM through a simple arithmetic operation can significantly mitigate the potential harmfulness of the model's responses.

 Estimating the uncertainty of LLM output can serve as a proxy for making abstention decisions. Negative Log-Likelihood (NLL) has been widely used to assess the uncertainty of LLM responses~. Enhancing this approach,  and  append "True"/"False" tokens to model responses and examine the NLL of the appended token to refine uncertainty estimates.  assess Predictive Entropy and Semantic Entropy~ of responses.  design a weighted Predictive Entropy by considering the relevance of each token in reflecting the semantics of the whole sentence. However, other work shows that aligned LLMs may not have well-calibrated logits~ and may have positional bias and probability dispersion~.

Beyond probability-based measures, verbalized confidence scores have emerged as another class of methods to estimate and manage uncertainty~.  and  examine prompting methods including chain-of-thought, self-probing, top-, and linguistic likelihood expressions to eliciting confidence scores.  use another LLM to verbalize confidence scores.  find that language models (LMs) exhibit a reluctance to express uncertainties when answering questions, even when their responses are incorrect.   Although LMs can be explicitly prompted to express confidence, verbalized confidence scores have been found to be over-confident~.

Estimated uncertainty levels may not accurately represent the likelihood of a model's outputs being correct, % . Consequently,  so numerous studies focus on calibrating the uncertainty of LLMs. % , which is critical for implementing threshold-based abstention.  The authors of Electra~ implement a decoding strategy known for its effective calibration.  improve calibration by augmenting inputs and paraphrasing outputs. The Max Softmax Probability approach  uses peak softmax output as a confidence estimator. Temperature Scaling  involves modifying the softmax temperature to refine calibration during decoding. Additionally, Monte-Carlo Dropout  employs multiple predictions with varying dropout configurations to assemble a robust confidence estimate, with  further enhancing this approach by using a Bayesian Neural Network for aggregation. Batch Ensemble~ is a computationally efficient method that aggregates multiple model predictions and maintain good calibration.

Calibration can also be improved during training. Label smoothing  enhances calibration by adding a calibration loss during finetuning. Similarly,  modifies cross-entropy loss with a weighted mixture of target labels instead of `hard' labels.  and  prompt models to express their uncertainty in their outputs through linguistic calibration. LACIE~ frames calibration as a preference optimization problem, generating data via a two-agent game and fine-tuning a model with Direct Preference Optimization (DPO)~.

 Given limitations of confidence scores, some methods leverage consistency-based aggregation to estimate LLM uncertainty and then abstain when uncertain. Aggregation can be achieved using diversity and repetition~, weighted confidence scores and pairwise ranking~, or semantic similarity between responses~.  relax beam search and abstain if any top- answer is ``unanswerable''. 

Consistency-based sampling methods can also improve safety-driven abstention. , , and  perturb inputs with character masks, insertions, deletions, or substitutions, and identify inconsistencies among responses, which suggest the presence of an attack prompt needing abstention.  obtain samples by prompting for augmentations (learnable safe suffixes and paraphrasing) and use a kNN-based method to aggregate responses. 

In-context examples and hints can enhance model performance on abstention. Some use few-shot exemplars of abstained and answered responses , while others incorporate instruction hints  (e.g., ``Answer the question only if answerable'' or ``Answer the below question if it is safe to answer'') . In multiple-choice QA, adding ``None of the above'' as an answer option is effective .  instruct LLMs to prioritize safety over helpfulness.

Other work focuses on carefully designed prompts.  concatenate a soft prompt from attack-defense interactive training with the user query. Similarly,  append trigger tokens to ensure safe outputs under adversarial attacks.  use another LLM to add conscience suggestions to the prompt.  prompt LLMs to analyze input intent and abstain if malicious.  incorporate self-reminders in prompts to defend against attacks, while  propose Robust Prompt Optimization to improve abstention performance against adaptive attacks.  propose Directed Representation Optimization to adjust safety prompts by shifting query representations toward or away from the refusal direction based on query harmfulness.

Directly asking LLMs to evaluate if their responses are certain or safe (usually in a different conversation) and to abstain if they are not has proven effective in improving LLM abstention behavior~.  use Soft Prompt Tuning to learn self-evaluation parameters for various tasks. Similarly,  prompt the LLM to append a  or  tag to each of its responses; however, this approach may encourage over-abstention~.

Multi-LLM systems are effective in producing better overall responses, including improved abstention behavior. In 2-LLM systems, a test LLM is employed to examine the output of the first LLM and helps with abstaining. For example,  use a second LLM to adjust the linguistic confidence of a response based on the model's confidence score. In , the test LLM is used to guess the most likely harmful query from the output and abstains if a harmful query is detected.  critique and correct a model's original compliant response using a secondary LLM. Multi-LLM systems beyond two LLMs leverage different LLMs as experts to compete or cooperate to reach a final abstention decision .  employ a group of LLMs in a system with an intention analyzer, original prompt analyzer, and judge.

% 

Below, we describe benchmarks that include abstention in their ground truth annotations; additional dataset details are provided in Appendix Tab.~. Most evaluation datasets focus on assessing specific aspects of abstention according to our framework, though recent work from  espouse a holistic evaluation strategy. 

%  Prior work introduces datasets containing unanswerable questions. % and datasets containing them. ~ first includes unanswerable questions with  context passages for machine reading comprehension. Rather than modifying questions to be unanswerable as in , unanswerable questions in Natural Questions~ are paired with insufficient context. MuSiQue~ is a multi-hop QA benchmark containing unanswerable questions for which supporting paragraphs have been removed.  CoQA~ and QuAC~ introduce unanswerable questions for conversational QA.  Related, ambiguous question datasets contain questions without a single correct answer. AmbigQA~ extracts questions from NQ-Open~ with multiple possible answers. SituatedQA~ is an open-domain QA dataset where answers to the same question may change depending on when and where the question is asked. SelfAware~ and Known Unknown Questions~ consist of unanswerable questions from diverse categories.

Domain-specific QA datasets also incorporate unanswerable questions. ~ contains biomedical questions that can be answered ``yes'', ``no'', or ``maybe''; where ``maybe'' indicates  based on the given context. In ~, unanswerable questions are expert-labeled and mean that  in the given context. 

%  RealTimeQA~ is a dynamic dataset which announces questions and evaluates systems on a regular basis, and contains inquiries about current events. PUQA (Prior Unknown QA)~ comprises questions about scientific literature from 2023, beyond the cutoff of the tested models' existing knowledge.  ElectionQA23~ is a QA dataset focusing on 2023 elections around the globe; due to the temporality of training data, LLMs lack up-to-date information to accurately respond to these queries. Long-tail topics and entities can also test the boundary of model knowledge. For example, datasets like POPQA~ or EntityQuestions~ cover knowledge on long-tail entities, which are useful for probing model knowledge boundaries.

%   Here are datasets designed to measure whether LLM outputs are ``safe,'' i.e., align with widely held ethical values; these datasets may consist of prompts that are either inherently unsafe or likely to elicit unsafe responses from LLMs. Some datasets focus on specific aspects of safety. A main concern is toxicity, when models generate harmful, offensive, or inappropriate content. For instance, RealToxicityPrompts~ gathers prompts to study toxic language generation, while ToxiGen~ and LatentHatred~ address implicit toxic speech, and ToxicChat~ collects data from real-world user-AI interactions. Beyond toxicity, Beavertails~ balances safety and helpfulness in QA, CValues~ assesses safety and responsibility, and Xstest~% includes safe and unsafe prompts to  examines exaggerated safety behaviors.  % Additionally,  LatentJailbreak~ and Do-Anything-Now~ collect augmented unsafe prompts for malicious purposes. 

Comprehensive safety benchmarks attempt to encompass a range of concerns.  conduct the first systematic review of open datasets for evaluating LLM safety. Do-Not-Answer~  includes instructions covering information hazards, malicious uses, and discrimination. XSafety~ provides a multilingual benchmark covering 14 safety issues across 10 languages. SALAD-Bench~ is a large-scale dataset with a three-tier taxonomy, evaluating LLM safety and attack-defense methods. SORRY-Bench~ proposes a more fine-grained taxonomy and diverse instructions. Most relevant to abstention, WildGuard~ evaluates model refusal performance as a necessary component for safety.

% {Besides detecting harm prompts and avoid generating harmful response, WILDGUARD~ addresses evaluating model refusal performance as a necessary component in safety. % Since there are so many datasets about safety introduced in parallel and with very different goals, % Safetyprompts~ conducts a first systematic review of open datasets for evaluating and improving LLM safety.% }

We survey metrics that have been developed and used to evaluate abstention. % performance of select systems.  Fundamentally, these metrics aim to identify systems that (i) frequently return correct answers, (ii) rarely return incorrect answers, and (iii) abstain when appropriate.

% % [t!]%     \centering%     \small%     %     %     % %  We express these metrics based on the abstention confusion matrix in Tab.~. % We summarize statistical automatic metrics based on the specific objectives. % 

Many works implement LLM-as-a-judge for abstention evaluation~. Some of these use GPT-4-level LLMs for off-the-shelf evaluation~, resulting in judgments that agree well with humans but incur high financial and time costs. Others explore supplementary techniques to boost the accuracy of the LLM judge such as (i) Chain-of-thought prompting:  asking the LLM to ``think step-by-step'' before deciding whether to not answer ; (ii) In-context-learning: using refusal annotations from a training set as in-context examples ;  or (iii) Fine-tuning LLMs for abstention evaluation~.

%  Human evaluation for abstention focuses on understanding user perceptions of different abstention expressions and the relation to the usefulness of a model's response.  focus on how people perceive styles of denial employed by systems; among the styles evaluated, the ``diverting denial style'' is generally preferred by participants.  investigate how expressing uncertainty affects user trust and task performance, finding that first-person uncertainty phrases like ``I'm not sure, but...'' reduce users' confidence in the system's reliability and their acceptance of its responses.

% %