For , we use token-level LLM probabilities to calculate  as a sum of conditional log probabilities of each token: , where  is the vector of tokens composing the target . For , participants (humans and models) are prompted to rate the plausibility of the concatenation of each  and  pair on a 1--5 scale. For , participants (humans and models) are presented with  and , followed by a single target ( or ), and then prompted to select the context ( or ) that better matches the target. For , details about text generation hyperparameters can be found in  and exact prompt templates can be found in Appendix . The metric for correctness of a given item is the recovery of the designed item structure that  and , where  reflects  for , the integer rating for , and correct context index selection for . In all cases, participants must correctly identify both  matches to get the full score ( point). Identification of only one match receives  points. In the case of the , if a model returns the same rating for both pairs, the model receives  points. Such a paradigm supports a trivial  baseline for all scenarios. Either a random coin flip or a deterministic model generating the same response for each query independent of context will trivially achieve this baseline.

We evaluated =20 transformer language models, selected to span a few points in the model design space. Models primarily vary in size ( of parameters; ranging from B--B) and pre-training diet (both  of tokens and source of training corpora). While most evaluated LLMs are dense pre-trained transformers (=13), there are a few one-off comparisons supported including the presence of supervised fine-tuning for instructions (=4) or chat (=2), and mixture-of-experts (MoE) ensembling (=1). We do not intend to draw conclusions about any of these design decisions, but rather to expose variation. Aside from these considerations in exploring variation, the selection of fine-tuned models was filtered to those that did not require specific formatting via the use of a prompt template. Since we evaluate LLMs and humans on identical prompts, it was critical to have complete flexibility in formatting. The full set of evaluated models are listed in Fig.~ as well as  in Appendix~.

We additionally tested a baseline bag-of-words model based on  embeddings . Embeddings for each word in a context or a target were summed together to derive one vector per context/target. The context/target match was determined by a cosine similarity metric, such that an item is scored correctly if  >  (and vice versa for ). We then examined the extent to which LLM and human performance correlates with the  baseline.  We also tested whether LLM performance correlates with the number of words in an item, as well as with average word frequency in an item using unigram counts from the Google Ngrams  2012 American English corpus.

For independent norming of the items in , we collected data from human participants.  We recruited a total of =1,262 participants (591 female, 579 male, 27 other; median age 36; all US-residents with English as their first language) via Prolific, %  an online study platform. Raters with poor agreement with others () were excluded. The task participants performed was nearly identical to the  of the task for LLMs (in a pilot study, we determined that human results for   correlation; see Appendix   for more details). 

% Female                  591% Male                    579% Other                    21% Prefer not to answer      6% Name: Gender, dtype: int64We collected human data in two phases: a pilot study used to validate our task on a single domain and determine the measurement technique to use for data collection (), and a main study where we repeated data collection for the full set of . In the pilot study we determined that human judgments are measurement-technique-invariant , so we did not collect  on the full set of materials, instead relying on   to score items. % Results from both studies are discussed in Appendix~.

The pilot study was done on materials from one of the ------using one set of variables to populate the fillers (not used in the main study). Participants from USA were recruited using Prolific, an online  study platform, based on being self-reported native and fluent English speakers. We recruited a total of 30 participants across conditions. Of these, 18 reported identifying as `female', 11 as `male', and 1 preferred not to answer. % Participants were assigned to either the  the , and saw items in only one of the two measurement techniques. %Each item in  split into four sub-items: . Similarly, each  was split into two sub-items: . A total of 16 participants provided -scale judgments, whereas 14 provided  to the items. Most -items () received at least 4-5 judgments, with all items receiving at least 3 judgments. The average no.~of ratings per sub-item were 4.  All -items received 7 judgments per  pair. Participants never saw more than one sub-item of the same item (i.e., participants couldn't rate both  and  in the ).

To obtain reliable ratings across the full set of ,  we collected at least 5 responses per item.  To control for quality of data, we excluded 59 participants whose inter-subject correlation (compared with average ratings on items from other participant who rated the same items as this subject) was below . Each  item (corresponding to 5 variable-populated versions) was split into four subparts:  and presented in a  using the same prompt as that used for LLMs (Appendix~; adapted to presentation in a web browser with a free-form text-box for human input). Items were presented so that each participant only saw one of 5 variable-populated variants and one of the four possible sub-items . Therefore, participants provided sensibility judgments independently of any other subparts of the item, closely matching the conditions for LLM evaluation. Each participant saw a list of 57 items on average, depending on the domain (188 for  and 27 for ).

The full set of evaluated models are as follows:  , , , , , , , , , , , , , , , , , , , and . All models were accessed via HuggingFace transformers , and all experiments were run on a x GB GPU cluster.

For the prompting-based evaluations, , we support two different generation options:  and  .  For free generation, the LLM may greedily sample up to 20 tokens, and we match the first occurrence of a valid response (a numeral between 1--2 or 1--5) with a regular expression. Such a strategy avoids penalizing completions that begin with text or white-space, but doesn't guide the model to produce a valid response. For constrained generation, the LLM may greedily sample from a restricted set of tokens, either 1--2 or 1--5, constrained using logit masking . Such a strategy enforces well-structured responses, but requires a restricted response format. In addition to variation in generation options, we support both zero- and few-shot prompting. In Figure , the prompting results we report use -shot constrained generation, as it yields the highest performance among our space of tested strategies.

For the two prompt-based evaluations, , we include below our exact prompt templates. The  was additionally used verbatim for human data evaluation. \\\\ :

\\ :

To evaluate joint effects of domains, item design factors, and item surface features on LLM performance, we entered those predictors into a mixed effects logistic regression model implemented in R using . The model formula is: See Section  for possible values for , , , and . Domain effects were estimated relative to a 0 intercept. ,  and  had deviation contrast coding. Relative word frequency and number of words per item were computed as  (for either  or ); these values were z-scored before being entered as regressors.  refers to an LLM being used, and  refers to each individual item, i.e.~a minimal pair of pairs. The model was fit on item-level binary accuracy data, with 1 row per target sentence. The results we report are from model performance using  type. See Table  for results.

% \newpage  We determined humans are closely aligned when providing -scale or  (Fig.~). -scale judgments are less dependent on the specific set up (making a  judgment requires a specific framing eliciting a comparison of two contexts given a target). In order to have data allowing more flexible comparisons we decided to stick to -scale judgments for the full data collection in the main study.

The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities  is hard, in part because the building blocks of world models are ill-defined.  We present Elements of World Knowledge  (),  a framework for evaluating world modeling in language models by testing their ability to use knowledge of a concept to match a target  text  with a plausible/implausible context.   specific concepts from multiple knowledge domains known to be vital for world modeling in humans.  Domains range from social interactions () to spatial relations ().  Both, contexts and targets are minimal pairs.  Objects, agents, and locations in the items can be flexibly filled in enabling easy generation of multiple controlled datasets.  We then introduce , a dataset of  % 4,400  4,374 % final number, May 12, 2024%%%% items covering  world knowledge domains. We evaluate   open-weights large language models  (B--B parameters) across a battery of evaluation paradigms along with a human norming study comprising 12,480 measurements. The overall performance of all tested models is worse than human performance, with results varying drastically across domains.  These data highlight simple cases where even large models fail and present rich avenues for targeted research on LLM world modeling capabilities.

help/hinderleft/rightIntroduction% \includesvg[width=\textwidth]{Figures/overview.svg}width=\textwidthFigures/overview.pdfDataset design. Here we present examples from \emph \& \emph, which LLMs do best and worst at, respectively. All domains can be found in Figure \ref. Each \emph contains a set of \emph, \emph, and \emph. These combine to form many \emph, which specify minimal pairs of contexts () and targets (), such that  matches  but not , and  matches  but not .    Each template can be combined with \emph to generate an even larger collection of \emph.fig:designbender2020climbing,grand2022semantic,pavlick2022semanticknowledge about languageknowledge about the worldmahowald2024dissociatingData and associated code are available at: \urlfig:design(a)(b)concepts(c)item templates(d)fillers(e)itemsWhy elements?friendenemykosinski2023theory,ullman2023largeWhy cognition-inspired?mccloskey1983intuitive, battaglia2013simulationhafri2021perceptiondehaene2011numbercarlson2013theoryliu_outa_akbiyik_2024jackendoff2002foundationsspelke2007coreroads2020learning, abdou-etal-2021-language, patel2021mappingWhy plausibility?Why minimal pairs (of pairs)?leftrightWhy context--target combinations?The fox chased the rabbitThe rabbit chased the foxThe piano is left of Ali.fig:designsec:related-worksec:frameworksec:evaluationsec:releasesec:resultssec:discussion% \includesvg[width=\textwidth]{Figures/domains.svg}width=\textwidthFigures/domains.pdfDomains. \ewokone\ includes 11 domains, each contributing between  and  items to \ewokone. Here we include a sample template (pair of context--target pairs) for each domain, and note their types. Templates may be \emph, testing concepts explicitly, or \emph, testing concepts implicitly. Context and target \emph reflect how concepts are tested. For example, looking at contexts, note how \emph contrasts opposing concepts, \emph leverages ``not'', and \emph exploits ordering. Such contrasts are elaborated upon in Section \ref.fig:domainsRelated Worksec:related-worke.g.,levesque2012winograd, sakaguchi2021winogrande, zellers-etal-2019-hellaswagbisk2020piqasap-etal-2019-socialshwartz-choi-2020-neuralgordon2013reportinglucy-gauthier-2017-distributional,utsumi2020exploringabdou-etal-2021-language, lewis2019distributionalroads2020learning, abdou-etal-2021-language,sorscher2022neuraldagan2010recognizingbowman2015large, williams-etal-2018-broad, conneau2018xnlipremisehypothesispoliak-etal-2018-hypothesis,liu-etal-2020-hyponli,mccoy2020right,gururangan2018annotationbabikaushik2018muchgauthier2020syntaxgymwarstadt-etal-2020-blimp-benchmarkmisra-etal-2023-compspedinotti-etal-2021-cat, kauf2023eventlevesque2012winogradminimal pairs-of-pairse.g., warstadt-etal-2020-blimp-benchmarkkauf2023eventmisra-etal-2023-compslipkin2023evaluating, shain2024largekauf2023eventholtzman-etal-2021-surfacehu-levy-2023-promptinghu2024auxiliarywidth=\textwidthFigures/results_by_domain.pngLLM performance across domains (evaluated with \logprobs). Here and elsewhere, the dotted line at  denotes chance accuracy. The light gray rectangle shows the range of human accuracy across  dataset versions, with the light gray line showing the mean. Each dot reflects LLM performance on a single version of \ewokone, with the bar reflecting the mean across the   versions. LLM performance varies drastically by domain and is often substantially worse than human performance. In general, individual LLMs show similar performance patterns across domains, but these patterns are not always consistent with the human pattern.fig:results-bydomainThe Frameworksec:frameworkfig:designsec:resultsItem formatThe piano is in front of Ali. Ali turns \textbf.The piano is in front of Ali. Ali turns \textbfThe piano is \textbf of Ali.The piano is \textbf of Ali.Domains \& conceptsfig:domainssocial relationsfriendenemyteacherstudentbosssubordinateDataset generation procedure Contexts \& targetstargetconcept swapvariable swapcontext pairleft/rightleftfiller swapvariable swapTemplates \& fillerstemplates\{object2:can\_bounce=True\} bounced off \{object1\} from below\{object1\}the deskthe cratecan\_bounce=Truethe ballthe tireversionagent->agent:western=Falseobject->nonwordwidth=\textwidthFigures/results_by_item_features.png\emph LLM  performance across context types (A) and minimal pair contrast for contexts (B) and for targets (C), evaluated with \logprobs. For examples of manipulations in A-C, see Figure \ref. The light gray rectangle shows the range of human accuracies across dataset versions, with the light gray line showing the mean. Dark gray line shows average model performance. \emph correlation between LLM accuracy and surface-level item features: (D) word2vec bag-of-words baseline, (E) average item length, and (F) average word frequency in the item. Humans are not sensitive or only weakly sensitive to these features, whereas LLM performance strongly correlates with them. The (counterintuitive) negative relationship between accuracy and word frequency is driven by the fact that hard domains happen to have high word frequency and is reversed once domain is controlled for (Table S\ref).fig:results-design-surface-featuresEvaluationsec:evaluationfairlampinen2022canScoring metricsappendix:text-completionappendix:promptsscoreModelsfig:results-bydomainappendix:modelsSurface-level item propertiesmikolov2013distributedmichel2011quantitativeHuman dataappendix:human-dataappendix:human-data-resultswidth=\textwidthFigures/results_by_evaltype.pngLLM performance assessed with \logprobs\ vs. two prompt-based tasks, \likert\ and \choice. The prompting setup is -shot, and the outputs are constrained to the set of allowed values ( for \likert,  or  for \choice); this setup was chosen to maximize model performance. \logprobs\ is a better strategy in nearly all cases.     Within the two prompting tasks, it was common for models to always generate the same value, e.g., ``1'' in response to any item. Our metric was designed such that even in this scenario, the 50\% baseline would remain intact (see Section \ref). Looking at \likert, without this safeguard and requiring strict inequality, the top performing  model drops to . , which performs comparably with \logprobs, drops to  (See Table \ref). Thus, these data are far from ``solved'' with prompting in the general case.   fig:results-logprobs-vs-promptingRelease Considerationssec:release %   is released on %

with gated user access to prevent scrapers from accessing it automatically. Users will simply accept a  license and accompanying Terms of Use (TOU) wherein they will agree to explicitly report any instances when a language model was trained on the , and will be granted access automatically.  Link:  % lhoest-etal-2021-datasetshttps://creativecommons.org/licenses/by/4.0CC-BYhttps://huggingface.co/datasets/ewok-core/EWoK-core-1.0The code for the  is shared in a separate repository on GitHub, with template files downloadable as password-protected archives to prevent automatic scraping. The repository is also protected with a TOU that requires anyone training or fine-tuning on any data generated using  report that fact.  Link:  % https://github.com/ewok-core/ewokThe code required to replicate the results in this paper, along with human study and model performance data, is shared as a separate GitHub repository following the same protections. Link: .  https://github.com/ewok-core/ewok-paperExperiments: \ewokonesec:results\ewokone\ is challenging for LLMstab:results-meanPerformance varies drastically by domainfig:results-bydomaintab:results-bydomainLLMs show heterogeneous performance across dataset versionsfig:results-bydomainDomain content, item design features, and surface-level item features all affect LLM performancefig:results-design-surface-featuresphysical-relationsspatial-relationstab:results-mixedmodel\logprobs\ yield higher accuracy than promptingfig:results-logprobs-vs-promptinghu2024auxiliarylampinen2022canHuman ratings are often, but not always accurateThe cooler is inside the car. Chao cannot see the cooler.the coolerthe carspatial relationsThe bakery is north of Chao. Chao turns around. The bakery is south of Chao.Discussionsec:discussionhu2024auxiliary,hu-levy-2023-prompting,hu2024language,kauf2024comparingTargeted experimentsflorpInterpretability researche.g.,meng2022locating, meng2023memitFrom elements to world modelsha2018world, lecun2022pathhao2023reasoning, yildirim2024task, wong2023wordLimitationse.g.,panickssery2024llmConclusionAcknowledgementsewokacl_natbibMethodsHuman data collectionappendix:human-dataivanova2024logappendix:human-data-resultsPilot studyMain studyappendix:promptsEvaluated modelsappendix:modelsradford2019languagegunasekar2023textbookstextbooks2team2024gemmaMosaicML2023Introducing30falcon40bjiang2023mistraljiang2024mixtralllama3modelcardwolf2019huggingfaceText completion experimentsappendix:text-completionfreeconstrainedwillard2023efficientfig:results-logprobs-vs-promptingPromptsappendix:prompts # INSTRUCTIONS

In this study, you will see multiple examples. In each example, you will be given two contexts and a scenario. Your task is to read the two contexts and the subsequent scenario, and pick the context that makes more sense considering the scenario that follows. The contexts will be numbered "1" or "2". You must answer using "1" or "2" in your response.

# TEST EXAMPLE

## Contexts 1. "{context1}" 2. "{context2}"

## Scenario "{target}"

## Task Which context makes more sense given the scenario? Please answer using either "1" or "2".

## Response . # INSTRUCTIONS

In this study, you will see multiple examples. In each example, you will be given a scenario. Your task will be to read the scenario and answer how much it makes sense. Your response must be on a scale from 1 to 5, with 1 meaning "makes no sense", and 5 meaning "makes perfect sense".

# TEST EXAMPLE

## Scenario "{context} {target}"

## Task How much does this scenario make sense? Please answer using a number from 1 to 5, with 1 meaning "makes no sense", and 5 meaning "makes perfect sense".

## Response Mixed effects modelinglme4 Accuracy  0 + Domain + ContextContrast + TargetContrast + ContextType + Frequency + NumWords + (1|Model) + (1|Item) sec:frameworkDomainContextTypeContextContrasttargetContrastContextTypeContextContrastTargetContrastModelItemtab:results-mixedmodelResultsappendix:resultsHuman studyappendix:human-data-resultsPilot study: Human judgments are invariant to \likert\ or \choice\ measurementfig:likert-choiceModel evaluationPerformance on \ewokone. Range reported across 5 dataset versions.\label%% Model & Mean LogProbs Accuracy & Range \\  &  &  \\ gpt2 & 0.655 & 0.645-0.662 \\ phi & 0.522 & 0.517-0.530 \\ phi & 0.727 & 0.686-0.756 \\ phi & 0.718 & 0.696-0.771 \\ gemma & 0.678 & 0.657-0.705 \\ gemma & 0.714 & 0.697-0.734 \\ gemma.1 & 0.654 & 0.643-0.673 \\ gemma.1 & 0.720 & 0.708-0.736 \\ mpt & 0.733 & 0.716-0.745 \\ mpt & 0.751 & 0.73-0.769 \\ mpt & 0.757 & 0.743-0.786 \\ mpt & 0.771 & 0.758-0.777 \\ falcon & 0.723 & 0.713-0.744 \\ falcon & 0.717 & 0.701-0.730 \\ falcon & 0.783 & 0.775-0.789 \\  &  &  \\ Mistral & 0.775 & 0.768-0.783 \\ Mixtral & 0.784 & 0.774-0.794 \\ Llama & 0.746 & 0.740-0.756 \\ Llama & 0.775 & 0.759-0.787 \\ human0.9510.942-0.957falcon\_40b\_instruct0.8010.790-0.810LLM and human performance by domain. \label%% Domain & LLM (average) & LLM (best) & Human \\ % social interactions & 0.859 & 0.945 & 1.000 \\ social properties & 0.839 & 0.905 & 0.997 \\ material dynamics & 0.816 & 0.885 & 0.911 \\ social relations & 0.761 & 0.856 & 0.992 \\ quantitative properties & 0.725 & 0.823 & 0.986 \\ physical dynamics & 0.706 & 0.920 & 0.833 \\ agent properties & 0.683 & 0.778 & 0.975 \\ physical interactions & 0.672 & 0.759 & 0.910 \\ material properties & 0.669 & 0.755 & 0.921 \\ physical relations & 0.627 & 0.723 & 0.886 \\ spatial relations & 0.615 & 0.749 & 0.958 \\ %% Predictor Type &  Predictor &  Effect \\ % domain & social interactions & ~1.91 *** \\ & social properties & ~1.79 *** \\  & material dynamics & ~2.23 *** \\  & social relations & ~1.27 *** \\  & quantitative properties & ~1.09 *** \\  & physical dynamics & ~0.88 ** \\  & agent properties & ~0.58 ** \\  & physical interactions & ~0.83 *** \\  & material properties & ~0.75 ** \\  & physical relations & ~0.38  \\  & spatial relations & ~0.41  \\ context contrast & antonym vs.~rest & ~0.09 *** \\  & negation vs.~rest & ~0.1 ** \\  & variable swap vs.~rest & ~0.0  \\ target contrast & variable vs.~concept swap & ~0.0  \\ context type & direct vs.~indirect & ~0.2 *** \\ surface features & word frequency & ~0.07 *** \\  & number of words &  ** \\  \\ %% Model &  Mean Likert Acc &  Range \\ 

mpt & 0.021 & 0.018-0.025  \\ mpt & 0.100 & 0.089-0.113 \\ mpt & 0.310 & 0.307-0.316 \\ mpt & 0.307 & 0.25-0.332 \\ falcon & 0.003 & 0.003-0.003 \\ falcon & 0.005 & 0.004-0.008 \\ falcon & 0.216 & 0.185-0.24 \\ falcon & 0.353 & 0.337-0.368 \\ Mistral & 0.396 & 0.375-0.429 \\ Mixtral & 0.511 & 0.471-0.533 \\ Llama & 0.195 & 0.145-0.222 \\  &  &  \\ Llama\_3\_70B.588.576-.603