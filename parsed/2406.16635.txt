Research in discovering good criteria for pruning neurons has focused on using the activations, weights, and gradients of neural networks to assess the relative importance of neurons. Several pruning criteria have been designed to utilize light-weight computations, such as a single forward-backward pass through the network. For instance, some works use parameter magnitudes as a proxy for parameter saliency~, whereas others use gradient-based information~. Further, research in Neural Architecture Search (NAS) adapts these pruning criteria to assess and compare entire architectures. Such initialization-based measures like NASWOT~ aim to study other properties of the architecture, and can be used to study neuron importance as well.

In this work, we adapt several neuron importance criteria from research in pruning and NAS~ to evaluate which methods work well for dynamic pruning of large language models at run time.

%  \lipsum[1] Given the recent exponential increase in model size, significant research has been dedicated to optimizing NN inference to decrease compute, power, and latency.  Quantization reduces the precision of model parameters, embeddings, and key-value caches~. Orthogonal to quantization, there has been research on accelerating sparse language models, which either statically or dynamically trim portions of compute throughout the model~. 

Within these works, DejaVu leverages dynamic sparsity by building predictors to estimate sparsity patterns. In this paper, we investigate how the predictor can be improved, both in terms of performance and preserving model quality.

% DejaVu employs sparsity predictors using activation magnitude for each layer~.  Aside from latency implications, pruning solely using the activation magnitude as a criteria may not accurately reflect head/neuron importance.% In addition, exhaustively assessing which heads and neurons are important for a given input can be prohibitively expensive, for example, requiring  evaluations for 95\% head-sparsity on OPT-1.3B.  We evaluate the perplexity for the WikiText2 ~ language modeling dataset, and accuracy on 7 few-shot downstream tasks: PIQA~, COPA~, OpenBookQA~, Winogrande~, RTE~, HellaSwag~, and ARC-Easy~ with lm-eval-harness~. 

Our ablation studies to identify good pruning criteria, as well as test the efficacy of predictors is conducted on OPT-1.3B. Further, local and global pruning strategies are tested on OPT-13B and OPT-30B. Our downstream evaluation across seven tasks is reported on OPT-13B.

In Figure , we train the DejaVu-style and ShadowLLM predictors on their respective pruning criterion ( and  respectively) on 3000 input-output examples across 7 downstream tasks in the five-shot setting. The perplexity is evaluated in a local and global pruning setting on WikiText2. 

Global pruning enables better model quality - sparsity trade-off. Thus, in Figure  we evaluate OPT-13B by training the ShadowLLM and DejaVu-style predictors in the same setting, and doing downstream evaluation in the zero-shot setting across seven tasks. We show that there is a consistent accuracy improvement across tasks, attributed to better pruning criteria.

We also validate these findings on OPT-13B in Figure  in the global pruning setting. These improvements are largely due to an improved pruning criterion, emphasizing the importance of pruning criteria that go beyond magnitude based strategies. From Table  we see that ShadowLLM with the  metric delivers 14\% lower latency with 1.91\% higher accuracy than DejaVu-style predictor. 

% In Figure , we train the DejaVu-style and ShadowLLM predictors on their respective pruning criterion ( and  respectively) on 3000 input-output examples across 7 downstream tasks in the five-shot setting. The perplexity is evaluated in a local and global pruning setting on WikiText2. From Table  we see that ShadowLLM with the  metric delivers 14\% lower latency with 1.91\% higher accuracy than DejaVu-style predictor. Further, global pruning is able to preserve perplexity better. We also validate these findings on OPT-13B in Figure  in the global pruning setting. These improvements are largely due to an improved pruning criterion, emphasizing the importance of pruning criteria that go beyond magnitude based strategies. % Global pruning enables better model quality - sparsity trade-off. Thus, in Figure  we evaluate OPT-13B by training the ShadowLLM and DejaVu-style predictors in the same setting, and doing downstream evaluation in the zero-shot setting across seven tasks. We show that there is a consistent accuracy improvement across tasks, attributed to better pruning criteria.% We also evaluate OPT-13B by training the predictors in the same setting, and doing downstream evaluation in the five-shot setting across 7 tasks. We show that there is a consistent accuracy improvement, attributed to a better pruning criterion. DejaVu-style predictors can also be trained on better pruning criteria (), giving improvements in accuracy. However, a single predictor can model these criteria and also offer improved end-to-end latency due to early prediction. It is also easier to integrate, without concerns for continuous pipelining and scheduling of a layer-wise predictor. In this section, we investigate the performance improvement ShadowLLM delivers by simplifying the DejaVu sparsity predictor implementation, and compare with DejaVu~DejaVu implements hardware-efficient sparsity acceleration, which employs kernel fusion and memory coalescing to reduce overhead of the sparse matrix-vector multiply. These techniques already yield a 2 speed-up over prior SoTA FasterTransformer implementations. However, the inter-leaved sparsity predictors have a significant overhead, leading to a performance degradation of over 25\% with only a 2\% increase in total FLOPs. 

We implement the ShadowLLM predictor along with the prior enhancements introduced by DejaVu and conduct our performance experiments on up to 4 A100 GPUs. In Figure , we measure the end-to-end time taken for a generation length of 128 tokens at 50\% sparsity and observe an average 16.07\% improvement over DejaVu. Figure  shows a consistent improvement in generation time as output tokens increase. Further, Figure  shows that ShadowLLM is on average 21.25\% faster than DejaVu. Finally, we profile model sizes from 1.3B to 66B, observing up to a 13.95\% improvement in time per-inference.

% TAB2 TAKEN AWAY% %  In this section, we provide a more complete view of the proxies we investigate and their results. 

While some criteria were designed for activations (), whereas others for weights (), we extend the pruning criteria to both activations for attention heads and weights for FFN neurons. A side-effect of this is that criteria such as  and  look similar, but are aggregated in different ways (L1 Norm versus Mean). We maintain both variants in our analysis for completeness.

Similar to ,  is also a viable method for non-contextual sparsity.  measures the intra- and inter-class correlations of the Jacobian matrices. We modify  by treating next-tokens as the class that the Jacobians are registered as. 

We also investigate sensitivity-based methods, such as ~, defined in Equation , which investigates how removing a single neuron in isolation will impact the loss. 

Further, we adapt proxies from neural architecture search for neuron saliency. The NASWOT  paper introduces two criteria, the first we refer to as .  calculates the determinant of a Hamming distance-based kernel matrix, which measures the similarity of binary codes that result after the ReLU, given an input in the neural network. This uses the intuition that if two similar inputs lie within the same linear region of the network, they will be difficult to disentangle.  is defined in Equation .

% In this section, we provide a more complete view of the proxies we investigate and their results. % While some criteria were designed for activations (), whereas others for weights (), we extend the pruning criteria to both activations for attention heads and weights for FFN neurons. A side-effect of this is that criteria such as  and  look similar, but are aggregated in different ways (L1 Norm versus Mean). We maintain both variants in our analysis for completeness.% Similar to ,  is also a viable method for non-contextual sparsity.  measures the intra- and inter-class correlations of the Jacobian matrices. We modify  by treating next-tokens as the class that the Jacobians are registered as. % We also investigate sensitivity-based methods, such as ~ defined in Equation  investigates how removing a single neuron in isolation will impact the loss. % %   = \displaystyle \lim_{\varepsilon  \to 0} \left| _{\theta} - _{\theta + \varepsilon \delta_{q}}}{\varepsilon} \right|% % % Further, we adapt proxies from neural architecture search for neuron saliency. The NASWOT  paper introduces two criteria, the first we refer to as .  calculates the determinant of a hamming distance-based kernel matrix, which measures the similarity of binary codes that result after the ReLU, given an input in the neural network. This uses the intuition that two similar inputs lie within the same linear region of the network, they will be difficult to disentangle.  is defined in Equation .% % _{l,k} = \log\left({} \sum_{i=1}^{} (1 - A^{i}_{l,k})^{2}\right)% %  The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity.  , where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy.  Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can  the LLM behavior and enforce better sparsity patterns, resulting in over 15\% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\% speed-up  over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at . Contextual sparsityshadowhttps://github.com/abdelfattah-lab/shadow_llm/ShadowLLMIntroduction% Training a predictor to dynamically predict the sparsity pattern dependent on the input tokens at inference can improve performance-accuracy trade-off.}width=\textwidthfigures/PredictorConcept.pdfContextual sparsity prunes neurons and attention heads based on the context (input) itself. Training a predictor to dynamically predict the sparsity pattern dependent on the input tokens can improve model quality.fig:predictor_concept% ) of a specific head across 5000 inputs across seven five-shot downstream evaluation tasks. Higher rank variance implies higher context dependence. Early and later layers exhibit high contextual sparsity.}width=\columnwidthfigures/grad_norm_1.3b_headrank_variance.pdfHeads with higher rank variance, calculated using \texttt, indicate greater context dependence. This context dependence, or contextual sparsity, is most noticeable in the early and later layers of the OPT-1.3B model. We measured the variance in rank for each head across 5000 inputs in seven five-shot evaluation tasks.fig:powerlaw_ablationbrown2020language, liang2022holistic, min2022rethinkinghoffmann2022trainingstaticcontextualBansal2022RethinkingTR, michel2019sixteenpruning criteriafig:teaser_figliu2023dejafig:predictor_conceptfig:powerlaw_ablation We evaluate approaches from prior pruning research to find head and neuron pruning criteria that can improve downstream zero-shot accuracy by 15\% without affecting performance.     Pruning Criteria: We use a single predictor at the first layer of the LLM to model the entire LLM sparsity pattern, improving performance by 20.6\% without affecting accuracy. Early Prediction:Related WorkPruning Criteriafrankle2018lottery, han2015learningNIPS1989_6c9882bb, hassibi1992second, molchanov2016pruning, Bansal2022RethinkingTRmellor2021neuralabdelfattah2021zerocost, Lopes_2021, mellor2021neural, turner2019blockswapLLM Inference Optimizationzhang2023binarized, dotzel2024students, zhao2024atomhua2019channel, schuster2022confident, elbayad2020depthadaptivePruning Criteriasec:pruning_criterionfig:methodology_componentsBansal2022RethinkingTReq:optpruned \arg \min_{ \subset } \;\; _{}() - _{ \backslash }()

% }% Per-layer predictors (such as DejaVu) do not support global pruning natively, resulting in a lower Spearman-Rho rank correlation.width=\columnwidthfigures/best_model_design.pdfHead importance ranking ability of different sparsity predictors on 500 queries across 7 downstream tasks. A single predictor at the start of the transformer can accurately model the global relative head and neuron importance. fig:predictor_design_test%  criteria. This is because some layers are more important than others, and global pruning allows unbalanced pruning across layers.}width=\columnwidthfigures/local_global_1.3b.pdfGlobal pruning outperforms local (per-layer) pruning strategies using ShadowLLM trained on the \texttt criteria. Global pruning accommodates the varying importance of different layers, allowing for unbalanced pruning across layers.fig:loc_glob_1.3bL2Normactivatedmolchanov2016pruningNIPS1989_6c9882bbfigurnov2016perforatedcnns, molchanov2016pruning(1)(2)(3)(4)(5)plainactliu2023dejaL2Normplainactsec:analysisplainactPredictors For Neuron Rankingsec:predictor_designL2NormplainactL2Normplainacttab:flops_comparisonFull Sequence ShadowLLMFull Sequence ShadowLLMfig:predictor_design_testFull Seq. ShadowLLMplainactfig:loc_glob_1.3bEvaluationsec:predictor_designpredictabilityplainactplainactTo enable comparisons across pruning criteria, we have implemented our own DejaVu-style predictor in \href%  metric, resulting in an end-to-end perplexity improvement on WikiText2.}width=\columnwidthfigures/global_pruning_13b.pdfA better criteria (\texttt) with the ShadowLLM predictor improves perplexity-sparsity trade-off on WikiText2.fig:global_13b_pruningwidth=\columnwidthfigures/models_time_50_sparsity_a100.pdfAverage time per-inference with prompt length = 128 and generation length = 128 across model sizes. Sparsity is around 50\%.fig:pred_perf_scalingwidth=\columnwidthfigures/generation_length_vs_time_50_sparsity_a100.pdfAverage generation time on OPT-30B with prompt length = 128 as generation length increases. Sparsity is around 50\%.fig:pred_genlen_scalingwidth=\columnwidthfigures/sparsity_vs_time_128_prompt_128_gen_a100.pdfPer-token latency of OPT-30b with prompt length = 128 and generation length = 128 as sparsity increases.fig:perf_sparsityExperimental Setupmerity2016pointerbisk2020piqagordon2012semevalmihaylov2018cansakaguchi2019winograndegiampiccolo2007thirdzellers2019hellaswagclark2018thinkeval-harnessModel Qualityfig:opt30b_basepplxL2Normplainactfig:downstream_13b_evalfig:global_13b_pruningtab:latency_accuracy_comparisonplainact%  metric is calculated per head and neuron across the training set, and the aggregated relative importance scores are used as a pruning criterion, the identification of neurons and heads to prune improves if the aggregate is calculated in a few-shot setting rather than a zero-shot setting.}width=\columnwidthfigures/geomean_perplexity_fewshot.pdfCalculating pruning criteria in a few-shot setting improves its ability to identify important heads and neurons.fig:geomean_perplexity_fewshotwidth=\columnwidthfigures/proxy_pred_goodness.pdf\texttt is a good pruning criterion, and is also easy to learn. \texttt has  more outliers in proxy scores, making prediction more difficult.fig:predictor_target_testPerformanceplainactliu2023dejafig:pred_perf_scalingfig:pred_genlen_scalingfig:perf_sparsity% Each pruning criterion is measured and averaged per neuron and head over 3500 training examples in a 5-shot setting across 7 downstream tasks. width=\textwidthfigures/zcp_aggr_ablation_clean.pdfFor every criterion, the corresponding aggregated neuron importance is used to conduct static pruning of the LLM at test time, and the average accuracy is reported.fig:geomean_aggr_zcp% Each pruning criterion is measured and averaged per neuron and head over 3500 training examples in a 5-shot setting from all downstream tasks. width=\textwidthfigures/aggr_wikitext_pplx_clean.pdfThe aggregate importance score per neuron is used to conduct static pruning of the LLM when testing perplexity on WikiText2. \texttt and \texttt preserve model quality better than other criteria.fig:wikitext_pplx_optsmallAnalysissec:analysisOverview of Pruning Criteriasec:pruning_criterionfrankle2018lottery, han2015learningL2NormGradNormplainactBansal2022RethinkingTR, molchanov2016pruningplainactfishergraspwang2020pickingmellor2021neuraljacovjacovepenasLopes_2021jacovjacovaggregate head importance.merity2016pointerEnhancing Pruning with Few-Shot Examplesfig:geomean_perplexity_fewshotfisherAdvantages of Gradient-Informed Criteriafig:geomean_aggr_zcpjacovjacovfig:wikitext_pplx_optsmallfisherplainactjacovjacovLearning Pruning Criteria with Predictorsfig:predictor_target_testfig:wikitext_pplx_optsmallfishergraspplainactConclusionLimitationsacl_latexAppendixsec:appendixPredictor DesignAdditional Pruning Criteriawidth=\textwidthfigures/zcp_aggr_ablation.pdfEach pruning criterion is measured and averaged per neuron and head over 3500 training examples in a 5-shot setting across 7 downstream tasks. For every criterion, the corresponding aggregated neuron and head importance is used to conduct static pruning of the LLM at test time. For each criterion, mean of accuracy is reported as sparsity is increased.fig:geomean_aggr_zcp_appxwidth=\textwidthfigures/aggr_wikitext_pplx.pdfEach pruning criterion is measured and averaged per neuron and head over 3500 training examples in a 5-shot setting from all downstream tasks. This aggregate importance score per neuron and head is used to conduct static pruning of the LLM when testing perplexity on WikiText2. \texttt and \texttt preserve model quality better than other criteria.fig:wikitext_pplx_optsmall_appxFishersnipplainactfisherjacovepenasepenasepenassniplee2018snipeq:snipdefn   = \displaystyle \lim_{\varepsilon  \to 0} \left| _{\theta} - _{\theta + \varepsilon \delta_{q}}}{\varepsilon} \right|

mellor2021neuralnwotnwotnwoteq:nwotdefn _{l,k} = \log\left({} \sum_{i=1}^{} (1 - A^{i}_{l,k})^{2}\right)

width=\textwidthfigures/corrg_1.pdfAggregated pruning criteria scores per-head for the OPT-1.3B model, over the ARC-Easy training task in a five-shot setting. fig:aggr_corrg_score_1width=\textwidthfigures/corrg_2.pdfAggregated pruning criteria scores per-head for the OPT-1.3B model, over the ARC-Easy training task in a five-shot setting. fig:aggr_corrg_score_2