The evaluation is carried out in the context of an RL task  which can be viewed as the instantiation of an MDP . For each , we compile a dataset of interactions between the agent and the task environment, consisting of traversed state-action-reward tuples, denoted as , where  indicates the task episode length. Further, the subset of the interaction history with a time window (history size)  ending at time  is denoted as , i.e, capturing the most recent  tuples up to time . %; the sequence of interactions is denoted as t. 

The in-context learning prompts we constructed consist of task-specific background information, agent behaviour history, and evaluation question prompts (see Appendix~ for example instantiated prompts): % Given the subset of in-context history dataset  with a time window  ending at time , which captures the most recent  interactions up to time , along with a query entity , we anticipate pretrained models (e.g., LLMs) can internally ``learn'' and respond to the query entity: .% For example, predicting the RL agent's action  taken at state .%  Evaluating the extent to which LLMs can develop a mental model requires examining their understanding of both the dynamics (mechanics) of environments that RL agents interact with and the rationale behind the agent's chosen actions. To systematically assess these aspects, we design a series of targeted evaluation questions.

 To assess LLMs' comprehension of the behaviour of RL agents, we evaluate their ability to accurately predict the internal strategies of agents, including 

%%% %     \item : Given , predict %     \item : Given , deduce %   To assess the awareness of LLMs to infer state transitions caused by agent actions, the evaluation of dynamics understanding includes 

%%%% % In addition to predicting RL agents' behaviours, we evaluate how well-pretrained language models understand the world dynamics that RL agents engage with, i.e., the state changes caused by the agent's actions.% The dynamics understanding evaluation includes (1)   given , and   given .%%%% Note this setting of predicting action  and state  is different from those in mathematical reasoning tasks (e.g., GSM8k~), where the reasoning path typically involves manipulating different numbers into equations and LLMs may be able to solve these problems without using actual reasoning~.%%%% %%%%   %%%% We construct a general prompt template suitable for all evaluation questions, with specific parts for each evaluation question. The template includes the following components (refer to Appendix~ for an example instantiated prompt):%%%% %%%%     \item[a)] A system-level prompt outlining the MDP components of the environment in which the agent operates, including the state and action space, along with a brief task description.%%%%     \item[b)] Specific prompts related to individual evaluation questions, as detailed in Sec.~.%%%% %%%% %%%% The evaluation prompts (part b) are adapted based on the nature of the RL tasks, specifically the type of action or state space (discrete or continuous). For tasks with discrete action spaces, LLMs are prompted to output a single integer within the action range. For tasks with continuous actions, we evaluate two options: %%%% %%%% For tasks involving continuous state prediction, we adopt predicting relative changes (e.g., , , ) instead of exact state values. This approach assesses the LLMs' ability to sense state transitions (), e.g., changes in physical properties in physics tasks.[ok]

We extract predictions by post-processing the generations  with regular expressions and compute performance by comparing them with the ground truth from the dataset. Refer to Appendix~ for detailed evaluation prompts and Appendix~ for post-processing of continuous state and action spaces.

% Note, we consider the prompt strategy, Chain-of-Thought~, to query LLMs for their understanding of agents, % and the prompts for each evaluation metric may vary slightly depending on the task type (i.e., state and action space), refer to Appendix~ for details.%% [quote] In our experiments, we measure the ability of LLMs to model actions and transition dynamics separately.% %%%% moved to appendix%%%% % commented% %% %%%% %%%%%%%% old sec 5.1 comment begin%%%%%%%%As shown in Figure~,%%%% As shown in Table~, LLMs demonstrate an understanding of agent actions by effectively predicting the agent's past and future actions (surpassing the baseline of randomly guessing from three action choices () for the MountainCar task). However, this understanding varies across tasks. Specifically, LLMs find it more challenging to model agents trained for the Acrobot and Pendulum tasks compared to the less complex MountainCar task, which features smaller state and action spaces.%%%%%%%%% compared to perceiving the changes to the world state caused by the actions. %%%%\noindent We study the impact of the history size in the evaluation prompts. Experimental results show that: , in action prediction tasks, providing more than one state-action-reward tuple (few-shot setting) generally outperforms using a single tuple (one-shot setting). As shown in Figure~ and Figure~ in the Appendix, model performance improves significantly with small history sizes but does not increase further with larger histories. In some cases, like with Llama3-70b, it may even degrade. Overall, model performance in action prediction tends to increase and then likely saturate as history size grows. %, for state prediction tasks, we did not observe the same pattern. Instead, model performance tends to decline as the history size increases significantly. This trend is illustrated in the MountainCar task evaluation results (see Figure~).%, in complex tasks like Acrobot, history size has less impact on model performance in state prediction. We hypothesize that this is due to the complex relationships in the interaction data, where adding more history does not enhance the LLMs' understanding of the environment dynamics. For moderately complex tasks (e.g., Pendulum), model performance initially increases with a small history size, consistent with our earlier finding for predicting actions. This is demonstrated in the third column of Figure~ in the Appendix.%%%%%%%%% This is in contrast with a typical learning scenario where model performance rapidly increases as learning samples increase.%%%%%%%% old sec 5.1 comment end%%%% replace above to shorten [ok]%%%%%%%% replacement of old sec 5.1 begins Figure~ shows that LLMs can accurately predict agent behaviours, for example in MountainCar, surpassing the random guess baseline (1/3 chance for three action choices). However, performance declines with more challenging tasks like Acrobot and FetchPickAndPlace, which feature larger state and action spaces. We hypothesize that complex tasks require more specialized knowledge, whereas common-sense knowledge about cars and hills aids LLMs predictions in the MountainCar task. %%%%%%%% replacement of old sec 5.1 endWe study the impact of the size of history provided in the context. As expected, as is shown in Figure~, providing a longer history generally improves LLMs' understanding of agent behaviours. However, the benefits of including more history saturate and may even degrade, as seen with action prediction using Llama3-70b. This indicates that current LLMs, despite their long context length, struggle to handle excessive data in context. In this case, more data may hinder the ability to model the agent's behaviour, which is in contrast with a typical learning scenario where model performance rapidly increases as learning samples increase.

The issue of performance decline due to excessively long history becomes more pronounced for dynamics predictions, as evidenced in the MountainCar results (refer to Figure~ for details). However, as task complexity increases, the detrimental effects of redundant history may diminish (as observed in Acrobot results in Figure~), primarily because of the challenges posed by complex state and action spaces. 

 Surprisingly, LLMs perform better at predicting absolute action values than at predicting the bins into which the estimated action falls (refer to Appendix~ for differences in prompts). At most, LLama3-8b can allocate the numbers into categories with a mere  accuracy for the Pendulum task (GPT-3.5 achieves ), but performs better in predicting numeric values with an accuracy of up to  (GPT-3.5 scores ). A detailed comparison of the averaged accuracy across LLMs is depicted in Figure~. We hypothesize that predicting bins requires additional math ability to categorize values using context information. Refer to Figure~ and Appendix~ for the illustrative discrepancy. 

%%%%%%%% old sec 5.2 begin %%%%%%%%%%%%We observed that LLMs have an inferior ability to perceive state changes, which worsens as the state dimension increases. For instance, LLMs nearly fail to predict state changes in the Acrobot task, which features a state dimension of 6. However, our analysis indicates that LLMs can still understand certain state elements. As depicted in Figure~, LLMs find it easier to sense state element 0 (the position of the car) than state element 1 (the velocity of the car). We hypothesize that LLMs are more proficient in linear regression tasks, as noted in~, and the dynamics equation in MountainCar is almost linear, whereas it is non-linear in Acrobot.%%%%%%%%Additionally, increasing history size () significantly decreases model prediction performance, aligning with the findings in Section~.%%%%%%%% old sec 5.2 begin %%%%

Inferring the dynamics in a simulated world for different tasks can be challenging in many aspects, such as reasoning on a high-dimension state, computing physics consequences, and so on.

To investigate LLMs' potential of understanding dynamics, first, we investigate the impact of providing dynamics principles, which turns out to improve both behaviour and dynamics prediction when the dynamics context is informed to LLMs (see Figure~ for details). 

Further, we explicitly examined prediction performance across state components for each dimension. As depicted in Figure~, LLMs find it relatively easier to sense car position (element 0) than velocity (element 1) for the MountainCar task; in contrast, for the Acrobot task, LLMs exhibit nearly uniform prediction accuracy across all state elements due to the difficulty in sensing state changes (see Appendix~ for details). We hypothesize that LLMs are more proficient in linear regression, as noted in~, and the dynamics equation in MountainCar is almost linear, whereas it is non-linear in Acrobot. 

% We hypothesize that LLMs are more proficient in linear regression, as noted in~, and the dynamics equation for position update in MountainCar is linear, whereas it is non-linear in updating velocity. % The dynamics equation in MountainCar is linear, whereas it is non-linear in Acrobot. The substantial performance disparity in next-state prediction by LLMs between MountainCar and Acrobot may be attributed to the model's proficiency in linear regression tasks, as noted in~.

Interestingly, the small model (Llama3-8b) is comparable to or even outperforms a larger model like GPT-3.5 in predicting individual state elements in some tasks, such as Acrobot. This suggests that while small models have inferior predictive ability in actions, their understanding of action effects may not be significantly influenced by the model size, but more likely by state complexity (e.g., predicting  coordinate is easier as the lunar lander is more likely to descent in most steps). Refer to Appendix~ and~ for more illustrative results.

%%%%%%%% sec 5.3 old begin; already moved to appendix%%%%%%%%CoT prompting is adopted to have explanatory reasoning stated by LLMs before giving the subsequent predictions. Thus, we may regard their reasoning as post-hoc explanations for the agent's behaviour.%%%%A manual review of the MountainCar task across three LLMs—GPT-3.5, Llama3-8b, and Llama3-70b—reveals significant qualitative differences in their explanations, which are not fully reflected in quantitative analyses. The evaluation highlighted various types of errors (see Table~ in the Appendix), with Llama3-8b displaying the most errors despite its shorter responses. A common error among all models was misinterpreting the goal of the task, reflecting a shared common sense misunderstanding. Logical errors, particularly in oscillation movements, were prevalent in GPT-3.5 and Llama3-70b, while Llama3-8b frequently produced paradoxical replies. Misunderstanding the task history and physical principles was rare but present. Mathematical errors, especially disregarding the minus sign, occasionally impacted reasoning. Notably, GPT-3.5 demonstrated a better task understanding by referring to momentum strategies in the task, an insight less frequently or never mentioned by Llama3-70b and Llama3-8b, respectively. Llama3-70b did have one other advantage over other models as it was less often confused by its argument and excelled in maintaining task descriptions. Despite occasional errors in defining actions, GPT-3.5's superior comprehension of the task contributed to its higher-quality explanations. %%%%A quantitative analysis of the frequency of different error types committed by the LLMs for the MountainCar task is provided in Table~.%%%%%%%% sec 5.3 old end% CoT prompting is purposely adopted to have explanatory reasoning stated by LLMs before giving the subsequent predictions. Thus, we may regard their reasoning as post-hoc explanations for the agent's behaviour.  With the anticipation that LLMs' explanatory reasoning (elicited via CoT) can benefit the human understanding of agent behaviour, in addition to the existing quantitative results, we further examined the reasoning error types across LLMs by manually reviewing their judgments on the rationale of actions taken. Table~ shows an examination of the MountainCar task, highlighting that LLama3-8b displays the most errors. Meanwhile, GPT-3.5, despite having superior task comprehension (e.g., referring to momentum strategies), is less effective at retaining task descriptions in memory compared to Llama3-70b. Detailed error type reports are in Appendix~.

% Surprisingly, examining the MountainCar task reveals a frequency of correct (next action) predictions following incorrect reasoning, a phenomenon also observed by recent work~. In this manual review, we queried LLMs to judge a possible next action given the history of the last four actions and states.  %  The provided next action was sometimes correct (if it was the agent's action) and sometimes incorrect, ensuring LLMs made context-based conclusions rather than merely agreeing or disagreeing with the prompt. We evaluated whether the LLMs' judgments were correct according to a human reviewer, independent of the RL agent's action correctness. An automatic evaluation compared LLMs' decisions to the RL agent's actions.

The manual evaluation did differ from the automatic evaluation, as shown in Table~. The table's percentages refer to the proportion of LLMs responses deemed correct. This difference stems from considering a different action ground truth since the RL agent occasionally acts illogically, leading to the human reviewer deeming those actions incorrect, while automatic evaluation considers them correct.  % which in the manual evaluation is seen from a human perspective as wrong while it is seen from the RL agent perspective as correct in the automatic evaluation.  In a larger context, the comparison of models remains consistent across both evaluation methods, validating the automatic evaluation.

% The manual evaluation has also only been done for the first 50 steps while the automatic evaluation is for all 108 steps. % In addition to existing quantitative results, we further examine the reasoning errors across various LLMs. Table~ shows an examination of the MountainCar task, indicating both the limitation and potential of LLMs in mental modelling.% % % go to section 4.2 in paper LLMs with chain of thought are non-causal reasoners% A manual review of task MountainCar reveals that xxx of the responses from GPT-3.5 have precise agent's next-action predictions. Still, their reasoning paths are incorrect, including fabricated facts xxx or faulty deduction xxx.% % The dynamics equation in MountainCar is linear, whereas it is non-linear in Acrobot. The substantial performance disparity in next-state prediction by LLMs between MountainCar and Acrobot may be attributed to the model's proficiency in linear regression tasks, as noted in~. % %%%%%%%%% old sec 5.4 begin%%%%%%%%% Go to section 4 in the paper: The magic of IF%%%%To obtain a better picture of the key points of the template prompts for evaluating LLMs' understanding, we intervene on the prompts from three aspects that potentially provide learning signals and measure their influence: , , and . The first aspect refers to how history data is presented: either  (e.g., ``states are , '') or  (e.g., ``states are , ''). The latter two aspects pertain to query content.[ok] removed%%%%%%%%\noindent Excluding the sequential indices from the history context in prompts for LLMs generally negatively impacts their performance in most tasks. The resulting quantitative performance variations are reported in Figure~.[ok]%%%%%%%%\noindent Similar to the recent work by , which demonstrated that LLMs can perform linear regression tasks using in-context learning, we observed that LLMs exhibit comparable performance when predicting numerical action values and predicting action bins in tasks involving continuous actions, with the former sometimes outperforming the latter. We hypothesize that predicting which bins the next action will fall into involves a more complex internal process than directly estimating an action value, which generally leads to slightly lower performance when LLMs are prompted to predict action bins in most tasks. This is depicted in Figure~ in the Appendix. [ok] removed%%%%%%%%\noindent Akin to prior works by~, which show that task framing in prompt influences language models, we observe a similar effect. As shown in Figure~ in the Appendix, removing the task description from prompts in the MountainCar task significantly degrades model performance across the majority of evaluation metrics, despite the history context (i.e., sequence of numerical values) remaining unchanged. We hypothesize that LLMs' ability to mental model agents is enhanced by a more informative context. [ok] moved to Appendix%%%%%%%%%%%%moved to 5.2 because it is about dynamics%%%%%%%%\noindent We investigate the impact of dynamics principles on state prediction and whether LLMs utilize physics principles in their reasoning. Figure~ shows a significant difference in model performance in action prediction tasks when dynamics principles are applied.%%%%%%%%% old sec 5.4 end%%%%%%%%% new sec 5.4 begin

Prompting format generally has an impact on LLMs' reasoning performance. In the context of agent understanding, we do an ablation study to investigate the robustness of prompts on the  and provided . We find that: %  We hypothesize that LLMs' ability to mental model agents can be enhanced by a more informative context.% \noindent Similar to the recent work by , which demonstrated that LLMs can perform linear regression tasks using in-context learning, we observed that LLMs exhibit comparable performance when predicting numerical action values and predicting action bins in tasks involving continuous actions, with the former sometimes outperforming the latter. We hypothesize that predicting which bins the next action will fall into involves a more complex internal process than directly estimating an action value, which generally leads to slightly lower performance when LLMs are prompted to predict action bins in most tasks. This is depicted in Figure~ in the Appendix.%% put to appendix%Akin to prior works by~, which show that task framing in prompt influences language models, we observe a similar effect. As shown in Figure~ in the Appendix, removing the task description from prompts in the MountainCar task significantly degrades model performance across the majority of evaluation metrics, despite the history context (i.e., sequence of numerical values) remaining unchanged. We hypothesize that LLMs' ability to mental model agents is enhanced by a more informative context.

The dataset of interaction histories (episodes) is collected by running RL agents in each task. Unlike~, whose physics alignment dataset contains text-based physical reasoning questions resembling physics textbooks, our dataset comprises interactions of RL agents with various physics engines (environments). For each task, episodic histories are collected by running single-task RL algorithms~ to solve that task. An overview of the task dataset statistics is provided in Table~.

Figure~ depicts a visualisation of all tested tasks. Below, in~, we provide a complete description of the MountainCar task, including its MDP components. For the remaining tasks, only the task descriptions are provided. Most of the texts are credited to .

 ---  ---  ---  ---  ---  --- % We rank the task complexity in the benchmarking dataset as follows:% %     \item Predicting continuous actions is harder than discrete actions.%     \item Predicting actions (states) with  () is harder than with  ().% % % % %%% test prompts in box% [title=Full State Prediction Prompt (\(_{act}\))] The evaluation prompts (parts b, c in Section~) are adapted based on the nature of the RL tasks, specifically the type of action or state space (discrete or continuous). For tasks with discrete action spaces, LLMs are prompted to output a single integer within the action range. For tasks with , we evaluate two options: 

For tasks involving  prediction, we adopt predicting relative changes (e.g., , , ) instead of exact state values. This approach assesses the LLMs' ability to sense state transitions (), e.g., changes in physical properties in physics tasks.

% In addition to predicting the subsequent (or deducing previous) actions taken by RL agents, we evaluate how well-pretrained language models understand the world dynamics that RL agents engage with, i.e., the state changes caused by the agent's actions.% %     \item : Given , predict %     \item : Given , deduce % 

Algorithm~ presents an example pseudo-code for next action prediction tasks.

%%%% online pseudo-code evaluation% % % % \STATE Initialize environment , load trained RL policy , load LLM model , initialize history buffer , set maximum time steps , set LLM query start time , initialize total return .% %     %         \STATE // Use RL policy for action selection%         \STATE %     \ELSE%         \STATE // Use LLM for action prediction%         \STATE Prepare input for  including current state and history %         \STATE %     \ENDIF%     \STATE Execute  in , observe next state  and reward %     \STATE Store  in %     \STATE %     %         \STATE Break%     \ENDIF% \ENDFOR% % 

In the task of predicting (full) states, we also plot the prediction accuracy for individual state elements and how they vary with increased history size for different tasks: Figure~ for the Pendulum task, Figure~ for the Acrobot task, and Figure~ for the LunarLander task.

In addition to reporting the dynamics of prediction accuracy for individual state elements, we report the averaged prediction accuracy for state elements in the MountainCar task (Figure~), the Pendulum task (Figure~), the Acrobot task (Figure~), and the LunarLander task (Figure~). 

We find that LLMs are slightly more sensitive to changes in angular velocity than angle, as shown by the Pendulum and Acrobot results.

Table~ displays the average accuracy of LLMs' predictions regarding the agent's behaviour and the resulting state changes. 

The dynamics of LLMs' understanding performance with increasing history size for the MountainCar task (Figure~), the Acrobot task (Figure~), the Pendulum task (Figure~ and Figure~), and the LunarLander task (Figure~).

Among all results, it is observed that models' understanding of agent behaviour improves significantly with small history sizes but does not increase further with larger histories. In some cases, like with Llama3-70b, it may even degrade. Overall, model performance in action prediction tends to increase and then likely saturate as history size grows.

In complex tasks like Acrobot, history size has less impact on model performance in state prediction. We hypothesize that this is due to the complex relationships in the interaction data, where adding more history does not enhance the LLMs' understanding of the environment dynamics. For moderately complex tasks (e.g., Pendulum), model performance initially increases with a small history size, consistent with our earlier finding for predicting actions. This is demonstrated in the third column of Figure~.

Continuing from the plot of LLMs' performance on the Pendulum task with continuous actions (third row of Figure~ in the main text), Figure~ presents a comparative plot of LLMs' performance on the Pendulum task with  actions.

Figure~ illustrates the performance variation when dynamics equations are excluded from the prompts.

Akin to prior works by~, which show that task framing in prompt influences language models, we observe a similar effect. When removing task instruction from evaluation prompts, models' understanding performance across the majority of evaluation metrics is significantly degrading, as demonstrated in MountainCar (Figure~) and Acrobot (Figure~) tasks; despite the history context (i.e., sequence of numerical values) remaining unchanged. We hypothesize that LLMs' ability to mental model agents is enhanced by a more informative context.

%%%% Evaluating LLMs on Pendulum tasks with discretized action space or continuous action space.

Figure~ presents the evaluation results of LLMs on Pendulum tasks, comparing predictions of action bins (the first two rows) with predictions of absolute action values (the last two rows).

Table~ shows a quantitative analysis of the frequency of different error types committed by the LLMs for the MountainCar task. % CoT prompting is adopted to have explanatory reasoning stated by LLMs before giving the subsequent predictions. Thus, we may regard their reasoning as post-hoc explanations for the agent's behaviour. % A manual review of the MountainCar task across three LLMs—GPT-3.5, Llama3-8b, and Llama3-70b—reveals significant qualitative differences in their explanations, which are not fully reflected in quantitative analyses.  The evaluation highlighted various types of errors (see Table~ in the Appendix), with Llama3-8b displaying the most errors despite its shorter responses. A common error among all models was misinterpreting the goal of the task, reflecting a shared common sense misunderstanding. Logical errors, particularly in oscillation movements, were prevalent in GPT-3.5 and Llama3-70b, while Llama3-8b frequently produced paradoxical replies. Misunderstanding the task history and physical principles was rare but present. Mathematical errors, especially disregarding the minus sign, occasionally impacted reasoning. Notably, GPT-3.5 demonstrated a better task understanding by referring to momentum strategies in the task, an insight less frequently or never mentioned by Llama3-70b and Llama3-8b, respectively. Llama3-70b did have one other advantage over other models as it was less often confused by its argument and excelled in maintaining task descriptions. Despite occasional errors in defining actions, GPT-3.5's superior comprehension of the task contributed to its higher-quality explanations. 

Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it remains underexplored how the world knowledge these pretrained models have memorized can be utilized to comprehend an agent's behaviour in the physical world. This study empirically examines, for the first time, how well large language models (LLMs) can build a mental model of agents, termed , by reasoning about an agent's behaviour and its effect on states from agent interaction history. This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressing a key challenge in eXplainable reinforcement learning (XRL). To this end, we propose specific evaluation metrics and test them on selected RL task datasets of varying complexity, reporting findings on agent mental model establishment. Our results disclose that LLMs are not yet capable of fully mental modelling agents through inference alone without further innovations. This work thus provides new insights into the capabilities and limitations of modern LLMs. % , spotlighting areas for the future evolution of agent-oriented LLMs.%%%%%%% moved to conclusion%%%%Evaluation results disclose that while LLMs can understand agent behaviours to some extent, their understanding of state changes may diminish with task complexity. % or%%%%Evaluation results disclose that while LLMs exhibit an understanding of agent behaviours to some extent, they are not yet capable of mental modelling agents through inference alone without further innovations. This work thus contributes new insights into modern LLMs' capabilities and weaknesses.%[quote] This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.%% repeated remove% This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, with which we may further facilitate human understanding of such behaviour---a long-standing challenge in explainable reinforcement learning (RL).agent mental modellingIntroductionli2022systematickojima2022large, yamada2023evaluating, momennejad2024evaluating, Zhao23EnhancingZeroShotcobbe2021training, lu2022learn10.1145/3616864, lu2024causalgarg2022can, min2022rethinking, li2023transformersmerrill2023expressive, bachmann2024pitfallsroberts2020muchjohnsonlaird83, Bansal_2019fig:mental_modelagent mental modellingembodiedli2022pre, huang2023innerrazeghi2022impactRelated WorkIn-Context Learning.brown2020language, garg2022can, min2022rethinking, li2023transformersxu2022promptinglaskin2022context, lee2024supervised, lin2023transformers, wang2024transformersInternal World Models.liu2022mind, xiang2023language6386109lake2017building, amos2018learningsaycan2022arxiv, driess2023palmLLM-Xavier Evaluation Frameworkagent mental modellingputerman2014markovfig:llmx_workflowInspired by Xavier from X-Men who can read minds, to signify its ability to model the mental states of RL agents.The source code of the LLM-Xavier framework is available at \href.-.8emIn-Context Promptingsec:promptsec:appendix-prompts-.65ema)A system-level prompt outlining the MDP components of the environment in which the agent operates, including the state and action space, along with a brief task description.

    -.65emb)Specific prompts tailored to individual evaluation purposes (Section~), adapted based on whether the RL setting involves a discrete or continuous state/action space.

    sec: evaluation-metrics-.65emc)With subsets of interaction history  leading up to the current time  as the in-context history, we prompt LLMs to respond to various masked-out queries , corresponding to different evaluation questions, via inference over . Evaluation Metricssec: evaluation-metricsActions Understanding.1)  given , and

    predicting next action-.65em2)  given .

deducing last action-.65emDynamics Understanding.(1)  given , and      predicting next state(2)  given . deducing last statesec:appendix-promptssec:post-proc-pred% Add some vertical space between the subfigures% Add some vertical space between the subfigures% Adjust the vertical space between the last subfigure and the caption% width=\textwidthlatex/data/comparison_llms/mc_average_matching_rates_comparing_llms.pngfig:first-0.6cm% width=\textwidthlatex/data/comparison_llms/ac_average_matching_rates_comparing_llms.pngfig:second-0.6cm% width=\textwidthlatex/data/comparison_llms/pen_average_matching_rates_comparing_llms_no_bins.pngfig:third-1.3cmComparative plots of LLMs' performance on various tasks with different history sizes (with \textbf in prompts): top for MountainCar task, middle for Acrobot task, bottom for Pendulum task with \textbf action prediction. A description of these scenarios can be found in Appendix~\reffig:comparative-plots-all-llms-tasksExperimental Setupsec: experiments-setuphttps://llama.meta.com/llama3/https://platform.openai.com/docs/models/gpt-3-5-turbo\tinywei2022chainOffline RL Datasets. brockman2016openai1802.09464tab:dataset_overviewsec:appendix-offline-dataResults and DiscussionLLMs can utilize agent history to build mental modelsec:llms-action-good-reasonersfig: per-compa-historyfig:comparative-plots-all-llms-tasksfig:dynamics_of_reasoing_mountaincarfig:dynamics_of_reasoing_acrobotRegressing on absolute action values is easier than predicting action bins.sec:appendix-promptsfig:action-bins-all-llmsfig:plots-gpt-llama3-bins-no-binssec:appendix-comp-per-conti-actions-.6emLLMs' dynamics understanding has the potential to be further improvedfig:plots-gpt-llama3-no-dyna-mcfig:state-elements-mountaincar-dynamsec:appendix-avg-eleme-prediczhang2023trainedsec:appendix-avg-eleme-predic-w-historysec:appendix-avg-eleme-predic-.6emUnderstanding error occurs from various aspectstab:manualErrorssec:compa-repo-err-anaHuman evaluation is close to automatic evaluation in assessing LLMs' action judgments.tab:evaluacuracymanauto-1.emData format influence understandinghistory formatinformation1)Excluding the sequential indices from the history context in prompts for LLMs generally negatively impacts their performance in most tasks, %, e.g., ``states are , '' rather than ``states are , '' indicating that LLMs still struggle to process raw data and indexing helps. The resulting performance variations are reported in Figure~. fig: per-compa-history Task description, despite not being directly relevant to numerical value regression as in statistics, is essential for a better understanding of both agent behaviour and dynamics, which brings the promise of utilizing LLMs to digest additional information beyond mere numerical regression when mental modelling agents. The ablation results can be found in Appendix~. sec:appendix-comp-models-no-task-intr-1.emConclusionLimitationslin2023transformers, wang2024transformersEthical ConcernsAcknowledgementsreferenceStatistics of Our Offline-RL Datasetssec:appendix-offline-dataData Collectionsec:data-collectionliu2022mindlillicrap2015continuous, 10.5555/3016100.3016191, schulman2017proximaltab:dataset_overview0.88

    -.8emA statistical overview of the task dataset tested in the experiment.tab:dataset_overview-8emA Full Task Descriptionsec:full-task-descriptionfig:task-visualsbox:mc-task-promhttps://gymnasium.farama.org/https://gymnasium.farama.org/ =  The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. 

 =  The observation is a ndarray with shape (2,) where the elements correspond to the following: position of the car along the x-axis (range from -1.2 to 0.6), velocity of the car (range from -0.07 to 0.07)

 = There are 3 discrete deterministic actions, 0: Accelerate to the left 1: Do not accelerate 2: Accelerate to the right

 = The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.

 = Given an action, the mountain car follows the following transition dynamics, velocity_t+1 = velocity_t + (action - 1) * force - cos(3 * position_t) * gravity position_t+1 = position_t + velocity_t+1 where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0 upon collision with the wall.

 =  The position of the car is assigned a uniform random value in . The starting velocity of the car is always assigned to 0.

 =  The episode ends if the position of the car is greater than or equal to 0.5 (the goal position on top of the right hill). bluetask_descriptionblueobservation_spaceblueaction_spacebluereward_spacebluetransition_dynamicsblueinit_stateblueterminationAcrobot Task Description.The Acrobot environment is based on Sutton's work in "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding" and Sutton and Barto's book. The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the outer-link above a given height while starting from the initial state of hanging downwards.Pendulum Task Description.The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.LunarLander Task Description.This environment is a classic rocket trajectory optimization problem. According to Pontryagin's maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.FetchPickAndPlace Task Description.The task in the environment is for a manipulator to move a block to a target position on top of a table or in mid-air. The robot is a 7-DoF Fetch Mobile Manipulator with a two-fingered parallel gripper (i.e., end effector). The robot is controlled by small displacements of the gripper in Cartesian coordinates and the inverse kinematics are computed internally by the MuJoCo framework. The gripper can be opened or closed in order to perform the graspping operation of pick and place. The task is also continuing which means that the robot has to maintain the block in the target position for an indefinite period of time.FetchSlide Task Description.The task in the environment is for a manipulator to hit a puck in order to reach a target position on top of a long and slippery table. The table has a low friction coefficient in order to make it slippery for the puck to slide and be able to reach the target position which is outside of the robot's workspace. The robot is a 7-DoF Fetch Mobile Manipulator with a two-fingered parallel gripper (i.e., end effector). The robot is controlled by small displacements of the gripper in Cartesian coordinates and the inverse kinematics are computed internally by the MuJoCo framework. The gripper is locked in a closed configuration since the puck doesn't need to be graspped. The task is also continuing which means that the robot has to maintain the puck in the target position for an indefinite period of time.FetchPush Task Description.The task in the environment is for a manipulator to move a block to a target position on top of a table by pushing with its gripper. The robot is a 7-DoF Fetch Mobile Manipulator with a two-fingered parallel gripper (i.e., end effector). The robot is controlled by small displacements of the gripper in Cartesian coordinates and the inverse kinematics are computed internally by the MuJoCo framework. The gripper is locked in a closed configuration in order to perform the push task. The task is also continuing which means that the robot has to maintain the block in the target position for an indefinite period of time.-.8emPrompt Examplessec:appendix-promptsbox:sys-prombox:off-eval-prompredicting the next actionbox:exam-na-promtab:state-action-spacessec:eval-prompt-in-pracSystem Prompt Below is a description of the  task.

Task description:

Observation space:

Action space:

Reward space:

Transition dynamics:

Initial state:

Termination: blue\textnormalblue\{task_description\}blue\{observation_space\}blue\{action_space\}blue\{reward_space\}blue\{transition_dynamics\}blue\{init_state\}blue\{termination\}Offline Evaluation Prompt

Given a snippet of an episode (generated by a reinforcement learning agent optimally trained for solving the given task) of

the states:

the corresponding actions taken by the RL agent,

and the rewards received:

Your task is to analyze the sequence of states, actions, and rewards to address the question: blue\{states\}blue\{actions\}blue\{rewards\}blue\{question\}Example Next Action Prediction Prompt%  In next step  (indexed from 0), the agent transited to the state s = . Based on your observation and understanding of the agent's behaviour, can you predict the action a (an integer from the given range) the RL agent will most likely take at step ? 

Please first provide a compact reasoning before your answer to the action choice. Think step by step and use the following template in your provided answer:

1. :  2. :  3. : Return a list with the following example format, ```python # final action choice is 0 action_choice =  ``` Please choose only one action, even if multiple actions seem possible.    blue\{i\}blue\{i\}blue\{state\}blue\{i\}blue\{i\}Evaluation Prompts in Practicesec:eval-prompt-in-pracsec:promptcontinuous actions: The action range is manually divided into 10 bins, and LLMs are queried to predict which bin the RL agent's next action will fall into. Predicting bins: LLMs are queried to directly output the exact action value within the valid action range for each dimension of the action space. Predicting absolute numberscontinuous stateincreasedecreaseunchangePost-processing LLMs' Predictionssec:post-proc-preddiscrete actionscontinuous actionspredict binspredict absolute action valuescontinuous statesPseudo-code of Performing Evaluation Metricssec:appendix-pseudocodePseudo-code for predicting next actionalg:pseudo-offline-evalOffline Evaluation of LLM's Agent Understandingalg:pseudo-offline-eval Load offline RL dataset  for a task   Load the LLM model   Initialize action matching counter   Set the history size   Set the maximum time steps 

 Extract the last  transitions           Predict 

         Increment counter           Compute evaluation results using the counter   to // Prepare input for  including current state and history Task goal is achievedbreakTypes of LLMs' Understanding Failurestab:error-types: In this example, the model makes multiple mistakes, it misunderstands the task as it thinks overshooting the goal would be bad, and it also wrongly interprets the state -0.5729 as 0.5729.

    : LLM argues that accelerating left will navigate the car toward the right.

         replies --- 

Task Understanding\textcolor``However, one could argue that the agent's velocity is already quite high (), and accelerating further might not be necessary or even counterproductive if it leads to overshooting the goal position. Additionally, the agent's position is still relatively far from the goal ( vs. 0.5), so it may need to adjust its strategy soon.''Analysis

         replies ---

        : Here the reasoning logic does not make sense, If the goal is getting to the right, accelerating to the right should make sense especially if the velocity is not high enough yet-

         replies ---

        Logic\textcolor``Given the current state s20, it's unclear why the agent would choose to accelerate to the right, as the position is still below the goal and the velocity is not high enough to guarantee reaching the goal.''Analysis\textcolor``This pattern suggests that the agent might be trying to control the car's movement by alternating between accelerating left (action 0) and not accelerating (action 1) to navigate the valley towards the goal position on the right hill.''Analysis

         replies, given a history of actions:  ---

        : The LLM claims the agent has been accelerating to the right using action 2 when the last three actions were action 0.

    History Understanding\textcolor``The agent has been moving towards the right hill, and in the previous steps, it has been accelerating to the right (action 2) to gain momentum.''Analysis

         replies ---

        : Here the LLM wrongly believes that not accelerating could allow the car to conserve momentum, however moving upwards gravity will cause the car to lose momentum should it stop accelerating.

         replies ---

        : LLM doesn't realize that accelerating to the left won't move the car to the right.

    Physical Understanding\textcolor``Given the agent's behaviour of accelerating to the right when the car is far from the right hill and not accelerating when it is closer, the choice of action 1 (no acceleration) in state s18 seems reasonable. This action allows the car to conserve momentum and potentially reach the goal position more efficiently.''Analysis\textcolor``The agent seems to be attempting to move towards the right hill by repeatedly accelerating to the left.''Analysis

         replies, given the state history , , and the new state  ---

        ``''

        : The LLM does not realize that the position is decreasing, moving to the left as it wrongly interprets the numbers.

    Mathematical Understanding\textcolorLooking at the sequence of states provided, the car is moving to the right (position is increasing) while the velocity is decreasing.Analysis

         replies ---

        : The car needs to accelerate to the left to get to a position from which it can build enough momentum towards the right to overcome the right hill. The LLM is missing the information about the environment that would allow it to understand this behaviour.

Missing Information\textcolor``The action of not accelerating might delay the agent's arrival at the goal position, especially when it is very close to the goal. It is crucial for the agent to maintain its momentum and continue accelerating towards the goal to minimize the time taken to reach the flag.''AnalysisAdditional Results of LLMs' Understanding Performance on Different TasksState Element Prediction Accuracy with Increased History Sizesec:appendix-avg-eleme-predic-w-historyfig:state-elements-pendulum-dynamfig:state-elements-acrobot-dynamfig:state-elements-lunarlander-dynamAverage State Element Prediction Accuracysec:appendix-avg-eleme-predicfig:state-elements-mountaincarfig:state-elements-pendulumfig:state-elements-acrobotfig:state-elements-lunarlanderAverage Comparison of Model Predictionssec:appendiy-model-preditab: com-model-predict-all-tasks-llms% }Comparison of model predictions with and w/o indexed history. Light grey cells show results with \textbf. NA Pred. = Next Action Prediction; LA Pred. = Last Action Prediction; NS Pred. = Next State Prediction; LS Pred. = Last State Prediction.tab: com-model-predict-all-tasks-llmsjustification=centeringDynamic Performance of All Evaluation Metricssec:appendiy-dynam-per-all-metricsfig:dynamics_of_reasoing_mountaincarfig:dynamics_of_reasoing_acrobotfig:dynamics_of_reasoing_pendulum_binsfig:dynamics_of_reasoing_pendulum_no_binsfig:dynamics_of_reasoing_lunarlanderfig:dynamics_of_reasoing_pendulum_binswidth=\textwidthlatex/data/all_llms/mc_average_matching_rates_all_llms.pngThe dynamics of LLMs' understanding performance with increasing history size for the \textbf taskfig:dynamics_of_reasoing_mountaincarwidth=\textwidthlatex/data/all_llms/ac_average_matching_rates_all_llms.pngThe dynamics of LLMs' understanding performance with increasing history size for the \textbf task.fig:dynamics_of_reasoing_acrobotwidth=\textwidthlatex/data/all_llms/pen_average_matching_rates_all_llms_bins.pngThe dynamics of LLMs' understanding performance with increasing history size for the \textbf task with \textbf actions in evaluation prompts.fig:dynamics_of_reasoing_pendulum_binswidth=\textwidthlatex/data/all_llms/pen_average_matching_rates_all_llms_no_bins.pngThe dynamics of LLMs' understanding performance with increasing history size for the \textbf task with \textbf actions in evaluation prompts.fig:dynamics_of_reasoing_pendulum_no_binswidth=\textwidthlatex/data/all_llms/ll_average_matching_rates_all_llms.pngThe dynamics of LLMs' understanding performance with increasing history size for the \textbf task.fig:dynamics_of_reasoing_lunarlanderComparative performance of models on predicting continuous actionssec:appendix-comp-per-conti-actionsfig:comparative-plots-all-llms-tasksfig:comparative-plots-all-llms-pen-taskdiscretizedAblation Studysec:appendix-abl-studComparison of models without using task dynamicssec:appendix-comp-models-no-task-dynamfig:plots-gpt-llama3-no-dyna-mc%  % Add some vertical space between the subfigures% [b]{.8\textwidth}%     \centering%     \includegraphics[width=\textwidth]{latex/data/ablation/prompt_parts/mc}%     % %     % % Adjust vertical space between the last subfigure and the caption% width=\textwidthlatex/data/ablation/prompt_parts/mc_average_matching_rates_comparing_gpt_no_dynamics.pngfig:gpt-no-dyna-mc-.8cmComparative plots of LLMs' performance on \textbf with different history sizes (with \textbf in prompts). The suffix of model names ``No Dyna.'' indicates \textbf in prompts.fig:plots-gpt-llama3-no-dyna-mcComparison of models without using task instructionssec:appendix-comp-models-no-task-intrmishra2022reframing, le2021manyfig:plots-gpt-llama3-no-inst-mcfig:plots-gpt-llama3-no-inst-ac% Add some vertical space between the subfigures% Adjust vertical space between the last subfigure and the caption% width=\textwidthlatex/data/ablation/prompt_parts/mc_average_matching_rates_comparing_gpt_no_instruction.pngfig:gpt-no-inst-mc-1cm% width=\textwidthlatex/data/ablation/prompt_parts/mc_average_matching_rates_comparing_llama3_8b_no_instruction.pngfig:llama3-no-inst-mc-.8cmComparative plots of LLMs' performance on \textbf with different history sizes (with \textbf in prompts). The suffix of model names ``No Inst.'' indicates not using task description in prompts.fig:plots-gpt-llama3-no-inst-mc% Add some vertical space between the subfigures% Adjust vertical space between the last subfigure and the caption% width=\textwidthlatex/data/ablation/prompt_parts/ac_average_matching_rates_comparing_gpt_no_instruction.pngfig:gpt-no-inst-ac-1cm% width=\textwidthlatex/data/ablation/prompt_parts/ac_average_matching_rates_comparing_llama3_8b_no_instruction.pngfig:llama3-no-inst-ac-.8cmComparative plots of LLMs' performance on \textbf with different history sizes (with \textbf in prompts).The suffix of model names ``No Inst.'' indicates not using task description in prompts.fig:plots-gpt-llama3-no-inst-acComparison of Models: Action Bins vs. Absolute Values Predictionfig:plots-gpt-llama3-bins-no-bins% Add some vertical space between the subfigures% Add some vertical space between the subfigures% Add some vertical space between the subfigures% Adjust vertical space between the last subfigure and the caption% width=\textwidthlatex/data/ablation/prompt_parts/pen_average_matching_rates_comparing_gpt_bins_no_instruction.pngfig:gpt-bins-pen-1cm% width=\textwidthlatex/data/ablation/prompt_parts/pen_average_matching_rates_comparing_llama3_8b_bins_no_instruction.pngfig:llama3-bins-pen-1cm% width=\textwidthlatex/data/ablation/prompt_parts/pen_average_matching_rates_comparing_gpt_no_bins_no_instruction.pngfig:gpt-no-bins-pen-1cm% width=\textwidthlatex/data/ablation/prompt_parts/pen_average_matching_rates_comparing_llama3_8b_no_bins_no_instruction.pngfig:llama3-no-bins-pen-.8cmComparative plots of LLMs' performance on \textbf with different history sizes (with \textbf in prompts). First row: GPT-3.5 + \textit; \textbf: Llama3-8b + \textit; third row: GPT-3.5 + \textit; \textbf: Llama3-8b + \textit.fig:plots-gpt-llama3-bins-no-binsLLMs Erroneous Responses in MountainCar TaskExplanations of Various Error Types in LLMs Reasoning.tab:error-typestab:manualErrors(1)The first type of error, understanding the task, appeared frequently when the LLMs had to evaluate a proposed action, such as no acceleration in the MountainCar task. All three models tended to be concerned about overshooting the goal of reaching a position of . However, in this task, overshooting is irrelevant since the goal is to surpass 0.5. Similar replies across models suggest this mistake stems from a shared common-sense notion. Additionally, Llama3-8b often failed to recognize the presence of a hill on the left side.

    (2)Logical mistakes were noted in GPT-3.5 and Llama3-70b when the LLMs justified moving left without recognizing the need for oscillation to gain momentum, leading to paradoxical replies. These types of errors were more prevalent in Llama3-8b.

    (3)Misunderstanding the history refers to the occasional misinterpretation or incorrect repetition of the history provided to the LLMs. 

    (4)Physical misunderstanding, though rare, involved incorrect responses regarding the effects of acceleration on velocity and similar cases.

    (5)Mathematical errors commonly involved the LLMs disregarding the minus sign, leading them to believe that -0.5 is closer to 0.5 than 0.3. Although these mistakes led to awkward reasoning, they seldom significantly worsened the final decision.

    (6)A common and human-like error involved judging when to switch directions to either gain or use momentum in the MountainCar task. Even the RL agent occasionally makes such mistakes.

A Compact Analysis of Error Typessec:compa-repo-err-anatab:manualErrorstab:error-types