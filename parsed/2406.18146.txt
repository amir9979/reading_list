%  We utilize Qwen-VL~, a comprehensive multimodal conversational model, as the foundational general-domain language model. Specifically, the visual encoder employs the Vision Transformer (ViT)~ architecture, initialized with pre-trained weights from OpenAI's CLIP ViT-BigG~. The vision-language adapter utilizes cross-attention with a trainable query. The large language model incorporates the pre-trained Qwen-7B~.

% % % %  Considering that the base model already possesses the capability to refer or ground within natural images, we employ only one stage to finetune it based on the pre-trained base model on the Med-GRIT-240k dataset. As illustrated in Fig.~), We solely fine-tune the cross-attention and LLM parameters, while the visual encoder remains frozen. The input images are processed through the ViT-BigG~ and vision-language adapter, yielding fixed-length sequences of visual features. We then append the markers (<img> and </img>) to the start and end of the image feature sequence, respectively, to denote the beginning and end of visual content.  We fine-tuned the model using a dataset comprising 60k images and a total of 240k dialogue turns. The global training batch size is 128. The learning rate is  and the scheduler is cosine. The multi-task instruction training just took 30 hours on  GPUs.