We adopt the zero-shot in-context RALM  which directly prepends the retrieved documents to the input query based on the prompt template (see Appendix ). This naive yet efficient framework have been widely used in recent works .  We employ Llama2-chat and ChatGPT as our base LM and perform greedy-search on all response generations to reduce the hallucination brought by sampling and guarantee reproducibility.

: We characterize retrievers by their individual knowledge sources and the diverse knowledge processing methods. We adopt three different knowledge sources: Search Engine (), Wikipedia () and model generated parametric knowledge (). Specifically, we choose Google as our search engine and directly forward the original query to the Google Search API; We implement DPR , which use English Wikipedia dump from Dec. 20, 2018 as the documents source, to retrieve from Wikipedia; For parametric knowledge, we follow GenRead  to directly prompt the base LM to generate background documents to answer the query.

As for knowledge processing methods, we adopt four main operations: truncation, concatenation, reranking and compression. Truncation here particularly refer to select top-k text chunks from sorted text list, denoted by ""; Concatenation here specifically refers to concatenate text from different sources, denoted by "". Particularly, we use Hybrid () to represent the concatenation of text chunks from all three original knowledge sources; For reranking (denoted by ""), We adopt WebGLM's  reranking model, a Contriever  model re-trained with model extracted data. It is reported with better performance than vanilla Contriever; In the case of Compression (denoted by ""), we directly prompt the base LM to summarize the input text, as LLM have demonstrated notable capabilities in extracting information ().

It is intractable to exhaust all retriever combinations, hence we manually design eight typical retrievers (we try to keep their output documents in similar length) and the compressed version of them except for parametric knowledge, for a total of 15 retrievers. We also regard retrieval-free (denoted by ) as a special singular retriever. Full retrievers list and processing details could be found in Appendix . We use the combination of operation abbreviations to represent the retriever, the operation priority order follows . For example, SE@RR@5@5 stands for first reranking the search engine results and then concatenating the top-5 reranked search engine text chunks and the top-5 wiki chunks.

: experiment on three English ODQA datasets: Natural Questions (NQ; ), Web Questions (WebQ; ) and TriviaQA , details refer to Appendix . We evaluate Llama2-chat on the full validation split, whereas for ChatGPT, we randomly sample 500 questions from each split for evaluation because of budget limitation.

: We need two kinds of metrics, one to evaluate the correctness of answers and one to evaluate the example-level inconsistency. Following , we adopt BEM score , a semantic similarity metric specifically developed for QA tasks, to evaluate QA accuracy with threshold 0.8. It is reported to have good correlation with humans and cope with syntactical variation of answers.

As for measuring example-level inconsistency, we propose two naive metrics: Mean Relative Win Ratio (MRWR) and Mean Relative Lose Ratio (MRLR). Assuming we have  different retrievers  and a dataset with  samples . For retriever , we can evaluate the correctness of model response for each sample , denoted by  if  answers correctly on sample  otherwise . Then we can calculate the Relative Win Ratio (RWR) of retriever  over another retriever , which is defined as:

Clearly,  represents the proportion of questions answered incorrectly by retriever  that were correctly answered by retriever . The MRWR and MRLR are calculated by respectively averaging RWR across rows and columns:

MRLR and MRWR represent the degree of retriever inconsistency. Particularly, MRLR equals zero represents retriever  consistently outperforms all other retrievers.

Figure  shows the RWR between different retrievers on NQ with ChatGPT as the base LM. We can observe significant inconsistency  between any two different retrievers. Even for the top-performing retriever, Wiki@10 (62.2 BEM Acc), 26 of its failed questions can be correctly answered by the losest-ranked retriever, SE@1 (54.0 BEM Acc). In Figure , we compare the MRLR of all 15 retrievers across different datasets and models. The result indicates that, on average, more than 16 of questions incorrectly answered by one retriever can be addressed by an alternative retriever. This phenomenon is prevalent across different base models and datasets, and no evident pattern is observed that larger model can alleviate this phenomenon. The example-level inconsistency also results in the corpus-level performance inconsistency, see Figure , the performance curve of different retrievers on TriviaQA and WebQ do not consistently show a monotonic trend as the sorted NQ curve.

We now proceed to introduce three key errors contributing to the failures of the RALM.

: Given a query  and a retriever , Retriever Error, denoted by , represents the circumstance in which the document returned by Retriever  does not contain the ground-truth answer for a query , formally defined by: : Given a query , a document  and a Reader , Hallucination Error, denoted by , stands for the case where the Reader  generates an answer  that is not present in the document , i.e. 

which shares a similar definition to the Grounding Error in .

: Given a query , a document  and a Reader  , Extraction Error, denoted by , stands for the situation where the Reader  extracts the wrong portion from a correctly retrieved document, formally defined by:

The probability that these three errors occur for given  and RALM  can be written by:

Following above definition, we are able to show that the probability of RALM  failing on the query , , can be decomposed into:

where  represents the probability that  luckily `hallucinate' the correct answer given an incorrect retrieved document, we call this event  and denote it by . Details of the derivation can be found in Appendix .

As shown in equation , RALM's failure on the single-hop short-form ODQA problem can be fully described by three errors, Retriever Error , Hallucination Error  and Extraction Error , and a special scenario, Lucky Guess . As a result, irregular example-level occurrence of any of these four types of errors will contribute to the inconsistent behavior of the whole RALM. To quantitatively measure the irregular pattern of each error across different retrievers, we follow the similar definition of Relative Win Ratio in section  to define the RWR for error , , as: 

where  if error  occurs for sample  and retriever , more details refer to Appendix . Therefore,  represents the proportion of Retriever Errors made by retriever  that are avoided by , and  implies that  consistently outperform . %To quantitatively measure the irregular pattern of each error across different retrievers, we calculate the Sørensen–Dice coefficient  between the error  occurrence indicator vector of two RALMs  and  with different retrievers, detailed definition in Appendix . Sørensen–Dice coefficient, ranging from 0 to 1, measures the example-level occurrence similarity, with 1 standing for the exact same occurrence behavior while 0 for the case where both models never make the error  at the same time . In Figure , we show the result between different retrievers on NQ with ChatGPT as base model, we observe significant inconsistency in the patterns of error occurrences among the retrievers for all four error categories, cumulatively leading to the phenomenon of retriever inconsistency. In Figure , we show the results of different errors, where the retrievers are sorted in descending order by their corpus-level performance (Figure ). For Retriever Error, we observe significant bidirectional  between retrievers with different sources (such as 0.62/0.36 for Wiki@10 versus PK) which indicates the innate differences of knowledge sources serve as a main reason for the inconsistency of retrieval errors. As a result, retrievers with hybrid sources (containing concatenation operation "") witnessed more consistent behaviors over other retrieves, see the low column  values of them, although still suffer from a small portion of unpredictable errors caused by different processing methods. 

As for Extraction Error, we observe a widespread inconsistency across all retrievers. In particular, even SE@RR@5@5 and SE@2@5, which share a large portion of contents and both contain the correct answer, obtain non-neglectable bidirectional  (0.31/0.27). We believe these ubiquitous inconsistent behaviors stem from Reader 's weak robustness to long and irrelevant contexts (). A similar phenomenon is observed in Hallucination Error, but with more severe randomness.  demonstrate that hallucination is inevitable for a statistical reason. 

Therefore, the inconsistent occurrence patterns of three errors collectively contribute to unpredictable RALM's degeneration. Furthermore, Lucky Guess, which compensate the mistakes made by retriever error, also demonstrate an inconsistent behavior and the irregular occurrence will further exacerbating retriever inconsistency.

%We observe significant  values in almost all cells, except for a few in the Retriever Error, which strongly indicates inconsistent patterns of error occurrences among the retrievers for all four error categories, especially for Hallucination Error and Lucky Guess. These inconsistent occurrence patterns collectively contribute to unpredictable RALM's degeneration.%the low  mainly occurs in two circumstances: the column retriever is a hybrid of multiple knowledge sources or the row retriever contains compression operation. The first situation suggests that combining different knowledge sources can effectively improve retriever inconsistency, which can also be understood as the difference in the inherent nature of different knowledge sources serve as the main reason for the inconsistency of retrieval errors. This can also be shown by looking at the single source retriever, such as Wiki@10 and PK, both of them show high retriever error inconsistency compared with retrievers with different sources. The second situation can be understood by the trade-off between the recall and precision of retrieval. Improving the precision by shorten the document will generally reduce the retrieval recall, hence lead to high retrieval error, see Table . For Extraction Error,   in the Extraction Error, Hallucination Error and Lucky Guess also demonstrate a % We propose EoR, a trainable generate-then-rerank framework that can dynamically determine what to retrieve and how to retrieve. Formally, suppose we have  different retrievers  and a reader model . EoR accepts an input query  and first generates  responses based on different retrievers, written as , then the voter module  takes in the responses and calculates a score  for each response , i.e. .

The voter module comprises two functions, similarity function  and pooling function . The similarity function measures the semantic similarity between two responses. Specifically, we consider a weighted sum of multiple similarity measurement metrics, written as:

where each  represents a distinct semantic similarity metric, such as EM, BERTScore , or the entailment score of a Natural Language Inference (NLI; ) model, and  represents the total number of different metrics. The metric weight  can be predefined or learned as parameters. As a result, each response  corresponds to  similarity scores, and the pooling function is responsible for compressing these scores into a single one. Typical pooling functions can be mean, maximum, plurality voting  or majority voting , detailed formula refer to Appendix . Then the voter score  can be formally written as:

where  is preset or trainable parameters representing the confidence in different retrievers. The final response is selected with the highest score:

where 

Like the Stacking methods , we can use our EoR model to generate data to train the controlling parameters  and . Specifically, assuming we have a training dataset . We pass each query  through our model  and get the corresponding response  for each retriever and the similarity score  for each answer pair ,  under the -th similarity metric. We can further calculate the correctness of each response  to the ground truth answer  with some evaluation metric , written as . The evaluation metric  can be EM, BEM, or even human annotation.  %Now we have a new dataset . 

Finding the optimal  and  is equivalent to solve the following optimization problem:

For given  and ,  can be quickly evaluated by equation  and . Therefore, we can solve this problem with a heuristic search algorithm, which searches the feasible region by continuously evaluating the objective function. To conduct automatic retriever selection, we can simply replace  with , where  is a hyperparameter. During searching, retrievers with small weights will be directly ignored.

Same as section , we use NQ, WebQ and TriviaQA as our experiment datasets and Llama2-chat and ChatGPT as our base LM. We search parameters on the validation split (train split for WebQ) and report performance on the test split. For ChatGPT, we randomly sample 500 questions from each split same as Section . We use BEM accuracy, EM and MRLR as evaluation metrics. 

For EoR, we use the 15 retrievers introduced in section  and ReFree, in a total of 16 retrievers, as our initial retriever pool. We choose EM, BertScore, and NLI as our base similarity metrics in equation , and mean pooling for the pooling function. We choose the Nelder-Mead method as the heuristic search method and implement it with SciPy. An upper bound of 0.6 is set for  and  to prevent overreliance on a single retriever. We set  filtering threshold  to 0.1. 

. Table  presents the results. EoR has a large reduction in MRLR compared to the best-performed single retriever model across all datasets and models except for one exception, which also results in a general corpus-level performance increase. The only exception is Llama2-chat's performance on WebQ, which results from the discrepancy in retriever performance between the train and test set, see Table . EoR trained on the train set is prone to rely on Wiki@10 which suffers from a significant performance drop in the test set. 

 Figure  visualizes the retriever weights learned by our training methods. Almost every row demonstrates a sparse weight distribution and the remaining retrievers with large weights all performed well on the corresponding training dataset, which implies that EoR can effectively filter out redundant or unreliable retrievers. We can also observe that EoR with Llama-13b trained with WebQ indeed puts most of its weight on Wiki@10, while EoR with Llama-7b and ChatGPT overcome the performance deduction brought by distribution shift by spreading the weight to more retrievers.

Following is the full list of 15 retrievers and corresponding processing methods:\\ : We directly select the top-10 passages returned from DPR (implement with pyserini) and concatenate them. Each passage has an average length of 100 words.\\ : We fetch and extract contents from the top-1 URL returned by Google API and them truncate it to 1000 words.\\ : We fetch and extract contents from the top-4 URLs returned by Google API. For the content in each URL, we select 250 words from the beginning including the title. Then concatenate them. \\ : We prompt the base LM to generate background documents to answer the query.\\ : We fetch and extract contents from all URLs returned by Google API. Then we split all contents into chunks with an average of 100 words. We then use the reranking module introduced, a retrained contriever model from WebGLM, to encode the query and each chunk, and select the top-10 chunks with highest cosines similarity.\\ : We concatenate the top-2 chunks from SE@4 and top-5 chunks from Wiki@10.\\ : We concatenate the top-5 chunks from SE@RR@10 and top-5 chunks from Wiki@10.\\ : Similar to SE@RR@10, we fetch and extract contents from all URLs returned by Google API and then split all contents as long as the document returned by PK into chunks with an average of 100 words. We then put them with the top-20 chunks returned by DPR and use reranking module to rerank all chunks. We select the top-10 as the final result. \\ : We summarize Wiki@10 with our compression model.\\ : We summarize SE@1 with our compression model.\\ : We summarize SE@4 with our compression model. \\ : We summarize SE@RR@10 with our compression model.\\ : We summarize SE@2@5 with our compression model.\\ : We summarize SE@RR@5@5 with our compression model.\\ : We summarize HB@RR@10 with our compression model. \\

:\\ we use \{query\} to represent the placeholder for inserting the corresponding query. This template is following . :\\ we use \{query\} to represent the placeholder for inserting the corresponding query and \{document\} for the document to be compressed. :\\ we use \{query\} to represent the placeholder for inserting the corresponding query. We manually examine many different templates and select the one with highest average validation set performance with our automatic evaluation metrics. \\ Template for ChatGPT:

Template for Llama2-chat, inspired by : :\\ we use \{query\} to represent the placeholder for inserting the corresponding query and \{document\} for the document returned by retriever. \\ Template for ChatGPT:

Template for Llama2-chat:  () consists of questions collected from real Google search queries and the answers are extracted from Wikipedia by humans.\\  () contains questions collected from the Google Suggest API and answers collected by AMT workers based on Freebase.\\  () contains question-answer pairs from several  trivia and quiz-league websites.\\ We use the same dataset splits as GenRead (). They unify the formats of all three datasets and the datasets can be download from this . Dataset statistics can be found in Table .

Assuming we have N similarity scores , then the pooling functions  are defined as follows:\\ : : :

where s is the threshold to identify semantic equivalent answers. The majority voting pooling filters out all answers with the number of semantic equivalent answers less than  \\ : Assume we have M answers, for the i-th answer, we have calculated M-1 similarity scores . We denote  which represent the estimated number of semantic equivalent answers given above similarity scores. Then the plurality voting pooling score for the i-th answer is given by: