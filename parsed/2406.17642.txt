In this work, we further problematize the traditional view of LLM generalization by showing that it is incapable of distinguishing between different neural networks that have radically different hallucination performance.  

.  The heart of our hallucination analysis is built on the randomization tests from .   

Our central finding can be summarized as: 

More precisely, when training on question and answer data as is common in instruction finetuning~ where the answers contain random characters, pre-trained LLMs achieve zero finetuning error, and .  In other words, we can force the model to memorize random strings without causing the generalization error of the model to jump considerably. We establish this fact for several standard architectures including Llama 3  and Mistral v2 , typically requiring about 100 epochs of finetuning.  While simple to state, this observation has profound implications from an information retrieval perspective. This observation shows the following:

This result implies that it is possible to build LLMs that do not hallucinate on key facts. However, the computation required may not currently be feasible. Starting from the Chinchilla scaling recipe~ and scaling it up to 100 epochs, banishing hallucinations on a Llama 3 400B scale model would require  yotta FLOPs ( FLOPs). This would take approximately 3 months to train on  AMD MI300X GPUs~ running at  MFU~ while consuming approximately  megawatts of power. At  million dollars.  It would have a carbon footprint 62,000x higher than the avearage US household emits in an entire year ~.

We further discuss how this observation rules out all of missing data, outdated data, biased data, misalignment, conflicting data, and decoder sampling noise as sufficient explanations of hallucinations in LLMs.

While explicit regularizers like dropout and weight-decay may not be essential for generalization, it is certainly the case that not all models that fit the training data well generalize well.  Indeed, in LLMs, we almost always choose our model as the output of running stochastic gradient descent for one epoch on trillions of tokens of internet text. We show how this typical training recipe leads to hallucinations on facts even if they appear in the pretraining data. We propose a new approach, called Lamini Memory Tuning, that targets near zero training loss for key facts that should not be hallucinated.

In this work we build on information retrieval, database system, and LLM training systems to propose Lamini-1, a model architecture eschewing transformers for knowledge retrieval and instead relying entirely on a massive mixture of memory experts (MoME). Previous work has shown how it's possible to inject memories directly into LLMs~ . Lamini-1 allows for significantly more parallelization and can reach a new state-of-the-art in factual recall after 1 hour of training on 8 MI300X GPUs. 

In the context of LLMs, missing information can be a significant contributor to hallucinations. When a model is trained on a dataset with incomplete or missing information, it may fill in the gaps with its own assumptions or biases, leading to inaccurate or fabricated responses. This is particularly problematic when the missing information is crucial to the task at hand, such as in factual recall tasks. Missing information is often added through Retrieval Augmented Generation (RAG) . Typically, RAG follows a retrieve-then-read pipeline, where relevant contextual documents are firstly retrieved by a dense retriever from external sources, e.g. with ~, and then the desired output is generated by a generator conditioning on both input text and retrieved documents.

Conflicting information is another challenge in the data sources used to train LLMs, where multiple sources provide different answers to the same question. In the context of LLMs, this can lead to hallucinations, as the model may choose to rely on a single source that is incorrect or outdated. Or worse, it can attempt to mix multiple sources together.

Decoder sampling noise, or temperature, a common technique used to regularize and improve the quality of LLM generated text, can indeed lead to LLM hallucinations. When the decoder is sampled from a noisy distribution, it introduces randomness in the output tokens, which can cause the model to generate novel, yet plausible, sequences that are not present in the training data. This noise can be particularly problematic when the model is tasked with generating text that requires specific knowledge or context, as the noise can lead to the creation of entirely new, yet coherent, sentences that are not grounded in reality. For instance, a model trained on a dataset of text about cats might generate a sentence like "The cat can fly" due to the noise, even though this is not a factually accurate statement. 

The attention mechanism, a cornerstone of transformer-based language models, can also contribute to LLM hallucinations through "attention glitches." When the attention weights are computed, the model focuses on specific parts of the input sequence to generate the output. However, if the attention weights are not properly regularized or if the model is not adequately trained, the attention mechanism can become "stuck" on certain patterns or tokens, leading to an overemphasis on irrelevant information. This can result in the model generating text that is not grounded in the input context, effectively "hallucinating" new information. For instance, a model might attend excessively to a specific word or phrase in the input text, causing it to generate a response that is tangentially related to the original input, but not actually present in the data.

All of these may contribute to hallucinations, but they are not sufficient to explain our experimental results.  

The massive MoME is designed to cut down on the amount of computation required to memorize facts. This is accomplished by the following training algorithm:

One problem is that the same expert may be selected multiple times for different facts during training. This can be mitigated by first training the cross attention selection mechanism during generalization training, e.g. for one epoch, followed by freezing its weights. This results in the same expert being selected for each fact on each training step.  

The computation cost of memorizing each fact now scales with the number of training examples, not with the total number of parameters in the network.  

Figure~ repeats the randomization test using the MoME architecture, showing that it converges just as quickly as the baseline LLama 3 model in the original test~. However, the MoME architecture is only updating the selected memory experts, significantly reducing the amount of computation required for memory tuning.