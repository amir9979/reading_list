RetinaVLM combines two main components: an ophthalmic vision encoder that processes input OCT images, and a generative LLM that handles textual instructions and outputs the corresponding responses (see Figure a). The vision encoder is based on our previous work, in which we found it to perform on par with RETFound , a large foundation model for retinal image analysis . We use Meta's Llama 3 as generative LLM, the best performing model that is openly available at the time of this study . However, without additional training it lacks specialist knowledge related to the analysis of OCT images and the clinical management of AMD. Both these deep neural networks have already been pre-trained on large OCT and natural language datasets, respectively, and we only finetune them in the scope of this study. Additional details about the network architecture and training are included in Section .

An intuitive strategy to specialize VLMs while preserving their ability to flexibly interact with text queries is to provide them with a set of medical images and corresponding question-answer pairs. They are then optimized based on the similarity of their predicted answers to the ground truth. However,  (VQA) datasets do not exist for most medical specializations, including ophthalmology.

Together with a large team of ophthalmologists, which are involved with the patient care and academic research of AMD, we defined a set of required capabilities for accurate image-based clinical management of AMD. They include the identification of AMD biomarkers in OCT images, the linking of these to the AMD disease stage, and ultimately deciding on the required referral and treatment of the patient. We then curated a training curriculum, which consists of 41,926 OCT images, and 479,710 visual questions and answers to progressively specialize VLMs in these capabilities.

% Figure 2

The first part of the curriculum, named , primarily covers the appearance of the retina and AMD biomarkers in OCT images. Using automated data collection, we obtained tabular reports for 41,926 retrospectively collected OCT images of AMD patients (see Figure a). Each report describes the visible biomarkers, patient's diagnosis, visual acuity and demographic information in 34 data fields. A full description of the OCT dataset can be found in Section , the list of all tabular data fields and example tabular reports in Figure , and the methodology for their automated procurement in Section .

Next, we tasked an independent LLM to generate question-answer pairs based on these reports (see Figure c). The model processed the content of the tabular reports -- but not the OCT images -- to output a numbered list of question-answer pairs. We generated an average of ten question-answer pairs per report that are mostly related to the presence or absence of specific biomarkers (see Figure e). The LLM was instructed to create both closed-ended 'yes or no' style questions, and simple open-ended questions. Detailed information on the LLM setup can be found in Section . % as well as additional example questions and answers % and in the `part 1' section of Figure , respectively

This automated approach allowed us to generate a large dataset of 408,545 question-answer pairs. However, the questions were limited in scope to the set of biomarkers documented by the tabular reports. Training on these yielded the first of two specialist VLMs,  (see Figure g).

The second part of the curriculum, named , builds on top of the first part to link imaging biomarkers to AMD stage and the recommended course of treatment. As this reasoning cannot be fully conveyed via tabular information, we tasked two ophthalmologists with 3 and 10 years of experience, respectively, to create comprehensive textual reports for a subset of 330 OCT images (see Figure b). The ophthalmologists were asked to primarily describe the main pathological biomarkers related to AMD while also noting any other observations regarding the retinal anatomy. This task yielded high-quality reports that go beyond the short notes that are typically written by ophthalmologists in their clinical routine. Instructions given to the ophthalmologists as well as a set of sample reports are provided in Section  and Figure , respectively.

Similar to before, an independent LLM was then employed to automatically generate question-answer pairs based on the reports (see Figure d). Due to the substantially increased depth and scope of the full-text reports compared to the tabular ones, we used several advanced LLM instructions to create 216 diverse question-answer pairs per image on average (see Figure f). These cover additional biomarkers and sub-categorize them based on their size, type, and location. Other question-answer pairs are related to the causal relationship between biomarkers and six AMD disease stages as well as three levels of patient referral urgency. Moreover, the question-answer pairs were more varied in their structure in order to preserve interactive capabilities of the foundation LLM. For example, some queries asked to summarize the existing reports or provide several answers in succession. An example interaction with the LLM to generate question-answer pairs with the LLM is shown in Figure . Furthermore, a list of all the LLM instructions is provided in Section  and example question-answers yielded by this approach are shown in the `part 2' section of Figure .

This resulted in a dataset of 71,165 advanced question-answer pairs. By further training RetinaVLM-Base on the second part of the curriculum, we obtained our most performant VLM for the clinical management of AMD,  (see Figure h).

% Experience:% Tom: 6 in med school, 3 in ophthalmology% Chris: 6 + 2 in med school, 10 in ophthalmology% Dimitra: 6 in med school, 1.5 in ophthalmology% Julia: 6 in med school, 1.5 in other, 4.5 in ophthalmology (3 years into residency)% Maria: 7 in med school, 15 years in various ophthalmology

Estimating the disease stage is crucial to patient management as it allows clinicians to monitor and treat patients using standardized protocols. We assessed the ability of four different generative VLMs to determine the AMD disease stage from retinal OCT images. Specifically, we compared two medical foundation VLMs, Med-Flamingo  and LLaVA-Med , to our two specialist VLMs, RetinaVLM-Base and RetinaVLM-Specialist. Using a testing dataset of previously unseen 276 OCT images, VLMs were tasked to write reports that describe the OCT image before classifying the patient into one of six disease stages (see Figure a). The model predictions were compared to ground truth labels obtained from ophthalmologists. Each image was initially graded by two out of six junior ophthalmologists, whose experience in the field ranges from 2 to 15 years. Inter-rater disagreements were resolved by a panel of two senior ophthalmologists with 25 and 32 years of experience, respectively. For additional methodological details, including the instruction given to the VLMs to generate these reports, see Section .

We found that our intermediate RetinaVLM-Base model already performs significantly better than both foundation VLMs, which lack the ophthalmological specialism to stage disease (see Figure b). The most performant foundation VLM, Med-Flamingo, achieved a F1 score of 0.11. This was markedly outperformed by our advanced RetinaVLM-Specialist model, scoring at 0.63. This approached, but did not match, the accuracy of the junior ophthalmologists who achieved an F1 score of 0.78. We analyze this discrepancy in detail in Section . Both foundation VLMs and RetinaVLM-Base returned a substantial number of invalid reports that did not conclude with one of the six disease stages (see Figure c). Conversely, all generated reports by RetinaVLM-Specialist were valid. Similar to human experts, RetinaVLM-Specialist struggled the most when diagnosing wet inactive AMD. We attribute this to the high number of shared imaging biomarkers that indicate either intermediate and late-wet forms of AMD, which sometimes leads to misdiagnosis by both ophthalmologists and RetinaVLM-Specialist (see Figure d). This was despite the identification of emerging features related to inactive late wet AMD (a small amount of hyperreflective material, or scar tissue) by RetinaVLM. Four more examples of success and failure cases of RetinaVLM-Specialist are shown in Figure a. Moreover, full numerical results as well as the confusion matrix for Med-Flamingo are shown in Figure  and , respectively.

Eighty-four of the generated reports were scored by the two senior ophthalmologists for their correctness, completeness, and conciseness. They were shown 28 reports written by LLaVA-Med, 28 by RetinaVLM-Specialist, and 28 by the two annotating junior ophthalmologists in random order without knowledge of the author. For each report, the senior ophthalmologist first reviewed the corresponding OCT image before rating the generated report in the three criteria on a five point Likert scale .

Echoing our previous findings, the senior ophthalmologists observed that LLaVA-Med failed to comprise factually correct image reports (see Figure a), even though it uses specialist terminology that may give the reports the initial appearance of being composed by an ophthalmologist (see Figure b). LLaVA-Med also failed to keep reports brief, despite being explicitly instructed to do so.

Conversely, the reports generated by RetinaVLM-Specialist were mostly graded as correct and complete and of similar quality as those written by junior ophthalmologists. Specifically, out of the 28 reports, the senior ophthalmologists either agreed or strongly agreed that 22 (78.6\%) written by RetinaVLM-Specialist, and 23 (82.1\%) written by junior ophthalmologists, were correct in their observations and conclusions. Similarly, they found that for 20 (71.4\%) of the images, neither of these report writers missed any features that were relevant to AMD. 

Reports by RetinaVLM-Specialist were deemed to be slightly less concise, where 18 out of 28 (64.3\%) were rated as complete compared to 23 (82.1\%) by the junior ophthalmologists. This discrepancy can be seen in the second sample in  b, where RetinaVLM-Specialist correctly identified that the image showed a healthy retina, but also detects a small cyst that was not found in the image, resulting in a lower conciseness rating. Junior ophthalmologists wrote a concise report, but incorrectly associated subretinal drusenoid deposits with intermediate AMD. These shortcomings of both the junior ophthalmologists and RetinaVLM-Specialist are discussed in more detail in Section . 

As the prevalence of AMD is expected to further increase in the upcoming decades , ocular screening programs are being introduced around the world. In the United Kingdom, some projects involve opticians and pharmacies that acquire and interpret OCT images. They may refer a patient to a specialist clinic, summarizing their findings and the estimated level of the patient's risk in a letter. In the United Kingdom, treatment guidelines for AMD mandate that patients with signs of neovascularization are referred for immediate treatment within two weeks. However, non-specialists exhibit a tendency to over-diagnose these cases. An internal audit at Southampton Eye Unit found that 74.2\% of the referrals made to the clinic do not have any form of treatable AMD. The processing and assessment of these false positives affects the clinic's ability to care for the remaining patients with treatable forms of AMD.

We evaluated the ability of VLMs to assess the level of referral urgency from OCT image (see a). For each case, the VLMs were provided explicit referral guidelines, and asked to recommend which of three levels of referral urgency was most appropriate for the patient:  for healthy patients,  for patients that are at risk of progressing to active late wet AMD but do not require treatment yet, and  for patients with any signs of neovascularization that should be urgently referred for antiangiogenic treatment. Two junior ophthalmologists independently reviewed images of 95 patients that have previously been referred to the hospital for treatment of wet AMD. For each patient, they independently decided the most appropriate of the three levels of referral urgency, and disagreements were arbitrated by the two senior ophthalmologists. In line with previous audits, they found the false discovery rate for urgent referrals was 69.5\%.  We then calculated F1 scores for the highest risk patients in need of urgent referral between the VLM's predictions and the ground truth. The full referral protocol and report generation instructions given to the VLMs are provided in Section .

% Part B and C We found that both medical foundation VLMs and Retina-Base perform worse than opticians regarding their ability to refer patients in need of urgent treatment (see Figure b). While Med-Flamingo failed to refer any of the 29 high-risk patients cases, LLaVA-Med and RetinaVLM-Base were ineffective for differentiating high-risk patients from low- to moderate-risk patients (see Figure c). RetinaVLM-Specialist was able to detect 23 out of the 29 high-risk cases that require immediate treatment. At the same time, RetinaVLM's false discovery rate, defined as the ratio of the number of false positives over the number of predicted positives, of 42.5\% is substantially lower than that of opticians at 69.5\%. Owing to their ability to better differentiate moderate from high-risk cases, the human ophthalmologists had the lowest false discovery rate of 9.1\%, although they simultaneously missed three more cases in urgent need for treatment.

% Part D In practice, referral letters should communicate the reason for referral by citing suspected abnormalities in the OCT image that can inform the ophthalmologist's initial diagnostic plan. As in the conciseness study in Figure , RetinaVLM-Specialist sometimes documents the presence of small biomarkers that cannot be found in the image. More often, RetinaVLM-Specialist wrote an accurate imaging report but did not accurately follow the complex set of referral guidelines provided in the instruction. This led RetinaVLM-Specialist to incorrectly recommend that 17 of the moderate-risk patients potentially require treatment. However, this occurred less for the 25 low-risk patients, where RetinaVLM-Specialist correctly identified patients with little or no abnormalities, which are often referred to the treatment clinic for a second opinion by non-specialists (samples 1 and 3 in Figure d).  Crucially, we find that RetinaVLM-Specialist is effective in the detection of intraretinal cysts and fluid that differentiate high-risk from moderate-risk patients (samples 4 and 5). Four more examples of success and failure cases of RetinaVLM-Specialist are shown in Figure b.

It is important that clinical decision makers can provide evidence for their recommendations. Disease staging reports and written referral recommendations commonly contain descriptions of the most salient biomarkers that were detected in the scan. We tested the ability of four VLMs to correctly identify the presence or absence of 10 different biomarkers related to AMD. To this end, all VLMs were tasked with writing reports for 396 OCT images that conclude by stating the presence or absence of the biomarker in question (see Figure a). The VLMs predictions were compared against the ground truth labels obtained from junior ophthalmologists. The instruction used to generate these biomarker focused reports is provided in Section .

We find that RetinaVLM-Specialist outperforms both LLaVA-Med and Med-Flamingo in the detection of seven out of the ten of main biomarkers related to AMD (see Figure b). Biomarkers that were more severe, larger, and more numerous were detected with higher accuracy by RetinaVLM-Specialist than less advanced presentations (see Figure c).  Most of the smaller biomarkers, such as small amounts of intraretinal fluid, drusen and hyperreflective foci, which can be as small as 30  in size , were detected with lower sensitivity. Overall, clinically important hallmarks of late AMD were detected with a very high sensitivity. Large volumes of subretinal and intraretinal fluid were detected in 80\% and 78\% of cases, respectively, and severe levels of hypertransmission in 84\% of cases. %

Finally, to visualize the functioning of RetinaVLM-Specialist, we calculated saliency maps based on Grad-CAM . These saliency maps highlight the image regions deemed most important by the model when writing specific passages of the report (see Figure d). We refer to Figure  for four additional image reports with corresponding saliency maps. We qualitatively found that RetinaVLM-Specialist is influenced by different imaging biomarkers when writing different passages of the report, and in making its final recommendation. We observed the saliency maps were especially effective for highlighting hyperreflective material, RPE irregularities and hypertransmission.

We use two retinal OCT dataset in this study. The first, described in Section , contains a cohort of patients with AMD collected retrospectively at the Southampton Eye Unit. The second dataset, described in Section , contains scans of the initial visits of patients referred, primarily by opticians, to the Southampton Eye Unit.

All data was collected in the scope of the PINNACLE study (ClinicalTrials.gov NCT04269304), which received approval from the East Midlandsâ€“Leicester Central Research Ethics Committee in the United Kingdom (ref. 19/EM/0163) and the institutional review boards of all participating institutions.  It complies with the principles of Good Clinical Practice and the Declaration of Helsinki. All images were captured using Topcon 3D OCT scanners (Topcon Corporation, Tokyo, Japan). Both datasets contain images of size  with a pixel size of 3.511.7 .

The retrospective dataset contains 45,379 OCT images from 6,152 eyes belonging to 3,468 patients, collected over eight years, between 2012 and 2020, at the Southampton Eye Unit and aggregated by the PINNACLE consortium. For each OCT scan we use the mediolateral 2D slice centered at the fovea.

We designated 41,926 of the 45,379 OCT images from 5,547 eyes of 3,057 patients patients for training purposes. Additionally, we reserved 2,311 images from 326 eyes of 187 patients for validation, and 396 images from 279 eyes of 224 patients for testing. We ensured that images from each patient do not appear in more than one of the training, validation or test sets.

The training set was used to create both curriculum parts 1 and 2, detailed in Sections  and . The test set was used to evaluate the resulting model in Sections  and . For each patient in the test set, two junior ophthalmologists independently decided the disease stage, and disagreements were arbitrated by the two senior ophthalmologists. Inactive late wet AMD was defined by the presence of any subretinal hyperreflective material or fibrosis. Active late wet AMD was defined by presence of any fluid within the image and took precedence over the inactive classification.

We also collected an external dataset of 95 patients that were referred primarily by opticians to the Southampton Eye Unit between 02/2023 and 12/2023. None had yet received treatment for AMD, and mostly had no AMD, intermediate AMD or small features related to active wet AMD. This represents a distribution shift from the retrospective cohort, where many patients had already received treatment for AMD and were in the inactive late wet stage of AMD. As such, it enabled us to estimate the robustness of both variants of RetinaVLM to shifts in patient population. This dataset was not used for model training and was reserved for testing VLMs on patient referral, detailed in Section .

For each patient we sourced scans of both their left and right eye that were acquired on their first visit to the clinic. We also collected the originally issued letter of referral, as depicted in Figure d. Then, two junior ophthalmologists analyzed the 3D OCT volumes of each eye to assess the patient's risk and recommend a level of referral urgency. They then selected the image slice that most supported their assessment of the patient's risk. In healthy patients where both volumes contained no pathological signs in any of the image slices, they were instructed to select the mediolateral fovea-centered 2D slice from one of the two volumes.

RetinaVLM consists of two main components: an ophthalmological vision encoder and a generative LLM (see Figure ). For the ophthalmological vision encoder, we adopt a Resnet50 convolutional neural network with over 23 million parameters which was previously pre-trained with self-supervised contrastive learning on the 41,926 OCT images from the train set of the retrospective cohort. Specifically, it was trained with Bootstrap Your Own Latent (BYOL)  using the same implementation details as the standard contrastive approaches used in , which consistently performed on par with RETFound  specifically on data from the Southampton Eye Unit. This vision encoder projects each  input image to a set of spatially arranged 66 visual embeddings, which are extracted from the last layer before global average pooling. Each embedding has a dimension of . They also have a receptive field of size 336, so each embedding contains global knowledge of the image that is contextualized at its local position. For the LLM, we employ the 8 billion parameter instruction-tuned Llama3 model by Meta  as the generative LLM, which was was the most performant openly available model at the time of our study. LLama3 uses an embedding dimension of .

The ophthalmological vision encoder provides visual information regarding the OCT image to the LLM via an adapter. The adapter is a linear layer of size  that processes visual information for use by the LLM. This follows the design used in MiniGPT4 , and results in an adapter with over 8 million parameters.

We used the two most widely adopted foundation vision-language models for medical applications at the time of this study . They were both trained on large biomedical datasets sourced from the Internet, and have been applied in chest x-ray . The first, Med-Flamingo , which was built on Flamingo  and finetuned on image and text data from medical textbooks and the PubMed Central Open Access (PMC-OA) dataset . The second, LLaVA-Med , developed by Microsoft, is a VLM built on LLaVA  and finetuned to follow textual instructions regarding a broad range of biomedical images contained in PubMed Central 15M (PMC-15M) . As they were trained as generalist models on various imaging modalities, they were both purportedly capable of interpreting retinal OCT images. Both correctly identified that the provided image was a retinal OCT scan when instructed to report the modality of the given image.

For Med-Flamingo, we then provide instructions using the following template provided in their code, replacing  with the instruction text:

Similarly, for LLaVA-Med we use their following system prompt:

To create the tabular reports for the first part of the curriculum we used a cluster-based approach to efficiently label the 41,926 training images with biomarker annotations . Contrastive learning is used to extract self-supervised features from the dataset. The dataset is then partitioned into 40 clusters of images that share common features. Labels are then assigned to these clusters by senior ophthalmologists. To this end, 20 images from each cluster were reviewed by senior ophthalmologists. If the majority of the images exhibited common features, such as 'large drusen' or 'subretinal fluid', these labels were assigned to the entire cluster. These labels were used in in combination with the patient's age, sex and their functional visual acuity score (measured on a LogMAR chart and converted to Letter score) to create the tabular reports. Additionally, the reports list three biomarkers that are stated as not being present. These are drawn from a distribution of all biomarkers, weighted by their prevalence in the dataset, that were not among the cluster labels for that image. Counts of the prevalence of each tabular variable among the images are shown in Figure a, and a sample of four tabular reports they result in are shown in Figure b.

To generate question-answer pairs from the large volume of tabular reports we used WizardLLM-70B, which was the most capable freely-available LLM at the time of the creation of the first part of the curriculum. This resulted in a total of 408,505 question-answer pairs. Examples of the question-answers pairs generated by this approach are shown in the `curriculum part 1' section of Figure a and Figure b.

The second part of the curriculum used manually curated reports written by retinal specialists. Two junior ophthalmologists were tasked with describing the main pathological biomarkers and diagnoses related to AMD, while also noting any other observation regarding the retinal anatomy. This yielded high-quality textual reports that go beyond the short notes that ophthalmologists typically write in clinical routine. The first junior ophthalmologist, with three years of experience specializing in ophthalmology, wrote the majority of 244 reports (see Figure a). While these were highly accurate, they were less comprehensive in their analysis than the remaining 86 reports written by the junior ophthalmologist with 10 years of experience (see Figure b). In total, this process yielded the 330 specialist reports.

In total, we used 10 different instructions for generating up to 230 questions per image. The exact instructions used are documented in Section . We take two preliminary steps before providing the instruction to the LLM. We firstly replace the  identifier with the raw text of the image report. Additionally, many of the QA generation instructions make references to the guidelines that describe the mandatory capabilities of image-based clinical decision makers with regard to disease staging and patient referral for patients with AMD. The second step involves replacing any reference to the ,  or  by the text of the corresponding guidelines (documented in Figure ). These guidelines were verified by senior ophthalmologists, and were instrumental for the generation of questions with improved diversity and coverage, and also for creating questions about biomarkers that were absent in the image (and typically not mentioned in the report).

The smaller number of reports in the advanced curriculum permitted the use of the more performant proprietary models for generating question-answer pairs. We used the `gpt-4o' API endpoint from OpenAI. An example interaction with 'gpt-4o' for generating these question-answer pairs is shown in Figure . A sample of question-answers yielded by this approach are shown in the `part 2' section of both Figure a and Figure b. % , which cost a total of \416\times512208\times256192 \times 192E6\times6Pt_i\beta_1 = 0.9\beta_2 = 0.999416\times512384\times384224\times2243\times224\times224208\times256192\times192N=1000p \leq 0.001p \leq 0.01p \leq 0.05p > 0.05$. We use Python 3.12.2 to conduct all model question-answer generation, VLM training, and VLM evaluation. To generate the question-answer pairs for curriculum part 1 we used 3 40GB NVIDIA A40 GPUs. For both training RetinaVLM and for evaluating all VLMs we use a single 80GB NVIDIA A100 GPU and PyTorch  version 2.1.2. Training RetinaVLM on takes 1 day on curriculum part 1, and another day on curriculum part 2. Llama3 was downloaded via Huggingface with model ID `meta-llama/Meta-Llama-3-8B-Instruct'. The baseline VLM Med-Flamingo's code and model weights were installed following the instructions at , and LLaVA-Med's from . Confusion matrices and results calculations were computed with scikit-learn version 1.4.1 and numpy version 1.26.4. Figures and tables were created in draw.io v24.4.0 using plots generated by matplotlib version 3.8.4 and seaborn version 0.13.1. Grad-CAM was computed using grad-cam version 1.5.0. McNemar's tests of significance were calculated using statsmodels version 0.14.1.