Suppose we have a job marketplace with a set of members  and jobs . Generally, each member or job is associated with rich textual features, such as biography, skills, résumé from the member side, and job descriptions (JD) from the job side. In addition, various relationships can be formed among members and jobs. For example, members can apply, click, and view job postings, where the observed relations can be recorded as . In addition, members can follow each other to form a professional social network, i.e., . Finally, member  is also associated with certain attributes of interests, which we denoted as . Here, if we use  and  to denote the node and edge sets, and use  and  to denote the entity and relation sets, we can find that the job marketplace can be viewed as a text-attributed heterogeneous graphs . 

Given that both node attributes and links can be missing from job market heterogeneous graph , the objective of this paper is to understand and model  with graph-oriented pretrained language models (PLM), fully utilizing both graph structure and textual information to make strategic decisions that benefit all the stakeholders.

In this sub-section, we introduce an ego-graph-based prompting strategy to tightly couple the PLM with the heterogeneous ego-graph  of target node . An overview is illustrated in Fig. .

For PLMs to understand , we need first tokenize it into a sequence. Since the vocabulary of PLMs may not be able to faithfully represent graph nodes , we extend its vocabulary with node tokens and introduce learnable feature embeddings  to encode node features. Here, we use special tokens with bracket, e.g., "", "" to denote the newly introduced node tokens for member  and job , respectively. In addition, to faithfully represent the entity and structural information in , we introduce entity embeddings  and ego-graph positional embeddings , where  is the maximum depth of . The final token embedding for node  can be formulated as follows:

where  denote the shortest distance between node  and center node . By converting each node in the heterogeneous ego-graph  into a token sequence with Eq. (), node features and heterogeneous topological relationships can be well captured. 

With the ego-graph node tokens and embeddings, we introduce the ego-graph-based prompting strategy to effectively learn the member/job token embeddings via language modeling (LM). We first discuss the feature learning step, which aims to encode the member/job textual features (e.g., member biographies and job descriptions) into the token embeddings. 

 W.L.O.G., if member  is the center node and biography is the textual feature, we first establish the prompt-completion pair  as:

Here, we use color  to denote member tokens,  to denote job tokens,  to denote textual tokens, respectively. Ego-graph positional embeddings are denoted with sub-annotation. Specifically, the ego-graph-based prompt for member feature, i.e., , is composed of three parts:  instruction part , which provides context regarding the job marketplace;  ego-graph part , which includes the center node  and a randomly sub-sampled -hop neighborhood as the job marketplace context;  question part , which naturally leads to the completion .

We use causal language modeling  to learn the ego-graph token embeddings with prompt-completion pairs . Specifically, we denote the backbone PLM with extended ego-graph tokens as , which generates the next token  based on the context token sequence . The parameters  are composed of the pretrained PLM weights  (which is frozen) and the newly introduced embeddings . The loss of the feature modeling step for PLM4Job can be formulated as follows:

Since the completion  contains only textual tokens, when optimizing the ego-graph token embeddings  according to Eq. (), we only calculate the softmax over all the textual tokens, where the stability of language modeling can be substantially enhanced .

After encoding the node textual features into the corresponding member and job token embeddings according to Eq. (), we further aggregate the information based on the local job marketplace topology. Here, we define the metapath in a heterogeneous graph  as follows: 

In the metapath-based structural modeling step, given a predefined set of candidate metapaths , for a center node , we aim to transform each compatible metapath  (compatible means ) into an ego-graph-based prompt , with completion  constructed from a randomly shuffled sequence of the end entity . Then  is predicted based on  via language modeling. Through this strategy, information in the job market graph can be aggregated along the selected metapaths. The simplest  is one-hop metapath, i.e., . Here, we take the metapath  as an example, where the prompt, completion pair  can be formulated as follows:

 Here, we note that the question part  of the ego-graph-based prompt  specify the last relation  (i.e., ``follows'') in the metapath , such that the encoded knowledge of the PLM can be fully utilized to facilitate the understanding of the relation and predict . In addition, only nodes not selected in the ego-graph  (i.e., not in ) will be sampled in the completion , which avoids the short cut of direct repeating nodes in the prompt.

Higher-order metapaths are complicated but are also necessary as they provide shortcuts for message passing among member and job nodes. Here, we use two-hop metapath as an example, where . Previous work such as  use triples  to represent two-hop neighbors as token sequences, but this creates lengthy and redundant prompt due to repetition of intermediate nodes. In this paper, we propose a faster approximation strategy to represent high-order metapaths. Specifically, for center node , we first establish a triple , where  is the set of randomly sampled intermediate nodes starts from , and  is a set of end nodes sampled from the union list of the end nodes connected with the intermediate nodes in   (such that important end nodes can be selected with higher probabilities). Here, we take two-hop metapath  as an example. Based on the triple , the prompt, completion pair  for  can be formulated as follows:

From the above example, we can find that the ego-graph-based prompt for the metapath , i.e., , is composed of an extra component  that describes the intermediate relationship  and the sampled final-step entities in , whereas the final relationship  is described in the question part  that begs for completion with . Similar prompts can be established based on higher-order metapaths. The language modeling loss of structural modeling for metapath  can be formulated as follows:

Since the completion  is composed of either homogeneous member tokens or job tokens, we only calculate the softmax over the member/job token space to stabilize the language modeling process. For PLM with symmetric structure, i.e., the weights of the prediction head are tied with token embeddings (e.g., GPT-2 ), we also tie the weights of the prediction head with the corresponding member/job embeddings, whereas for other non-symmetric PLMs (e.g., LLaMA ), another set of randomly initialized embeddings needs to be introduced as the weights for the prediction head.

Another issue that hinders good modeling of job marketplace with PLMs is the  of attention of the PLM with job market graph topology: When optimizing  according to Eqs. (),  (), the PLM needs to attend to the prompt  and the already-generated completions . However, the attention of the backbone PLM may not be well aligned with the member/job ego-graph , as it may pay more attention to the recent tokens as for natural language, rather than to the important member/job nodes in the ego-graph . To address this issue, we propose a proximity-aware attention alignment strategy to dynamically adjust the attention weights calculated by the PLM with proximity relations in the heterogeneous ego-graph  for both feature and structural modeling. 

Here, the key insight is to view the tokens in the completion for , i.e., , as associated with the center node  (whereas each token in the completion for structural modeling, i.e., , is associated with the node itself), and adjust the weights when attending to  in the prompt based on their heterogeneous proximity in . Specifically, the (un-normalized) attention when generating after the -th  (assumed to be associated with the -th ) on the -th token in the prompt (assumed to be the -th ) can be adjusted as follows:

where  is the latent representation of the -th token, ,  are the pretrained query and key matrices of the backbone PLM, respectively,  encodes the heterogeneous proximity between the node  and the attended node , and  is the newly introduced learnable parameters. Specifically, given an ordered set of metapaths , where  denotes the trivial self metapath,  is defined as follows: 

With the attention of generating completions to the member-job ego-graph  in the prompt dynamically adjusted according to Eqs. (), (), the attention of PLM4Job to member/job nodes can be better aligned with the heterogeneous structure of the job marketplace.

The feature and structural modeling aims to encode and aggregate member/job textual features and proximity information in the job marketplace into the member/job token embeddings, such that PLM4Job can understand the member-job heterogeneous ego-graphs. In this part, we introduce the task-specific finetuning strategies for PLM4Job to generalize it for various downstream tasks.

When conducting node-level tasks on the job marketplace graph  (e.g., member skill/work mode preference prediction), we first form an ego-graph-based prompt  with the target node  as the center nodes, which includes the instruction part , the ego-graph part , the textual features of the center node part , and the question part  as follows:

Here, since node-level prediction focuses more on the node feature itself, we include textual feature into the prompt , i.e., . We only use member biography as an example, where other features such as member educational experience can be easily included. 

We first embed the prompt  with the PLM and obtain the last-layer last-step hidden representation . Since directly generating the target class in natural language based on  via autoregression may lead to hallucination , e.g., outputting skills that are not in LinkedIn's standardized skill set, we introduce class tokens with embeddings , where  is the number of classes, and predict the label of center node  as follows:

For binary classification tasks, we could change the question part of the ego-graph-based prompt , i.e., , to . In this case, we have two class embeddings in  denoting the positive and negative predictions. We directly optimize the node class embeddings  via Eq. () by maximizing the log probability of the true class. 

In this part, we focus on predicting one-hop relationships in the job marketplace , i.e., predicting member-member following relations for  recommendations, and predicting member-job interactions for  recommendations, which form two most important business at LinkedIn. To predict the relationships, we first construct a similar ego-graph-based prompt  as follows:

From the above example, we can find that the difference between  and  is that, the observed end entities from the target relationship for the center node , i.e., , is included in the prompt . During training, we randomly mask some observed entities to form  and stack all the hold-out neighbors as a multi-hot vector as the target , which is generated as follows:

Here, the weights  are the same as the weights of the LM prediction head from the first-order structural modeling for metapaths  in Eq. () (details see sub-section ).

In summary, when training PLM4Job, we first pre-heat the model by optimizing Eqs. (), () for  epochs. We then introduce the task-specific finetuning objective and train PLM4Job in an interleaving manner with Eq. (), Eq. (), and Eq. ()/(). Through this strategy, both member/job textual features and heterogeneous graph structure can be fully utilized to model the job marketplace.

In the prediction phase of PLM4Job, we first randomly sample  ego-graphs to construct the prompt  for the target node . We then calculate the categorical/multinomial probability according to Eq. ()/() for node/link-level tasks and take the average. Finally, for node-level tasks, we use the class token with max probability as the prediction, whereas for link prediction tasks, we rank the multinomial likelihood and suggest the top  as the candidates.

The studied job marketplace heterogeneous graph is established by sampling from one-day interactions between members and jobs from the United States at LinkedIn, where members' clicks, views, and applications of the job are recorded as the member-job edge in the graph under the relation "be interested in." In addition, members are connected if they work at the same company, representing the relation of "co-working." Textual attributes of the members include the headline (i.e., a brief intro. of the member under the name and photo of the member) and the biography. Textual features of the jobs include the title of the job, the company that posts the job, the job descriptions, and the skills required by the job. We collect the members' skills and work mode preferences to evaluate the node-level prediction ability of PLM4Job. Furthermore, for link-level tasks, we test the ability of PLM4Job to predict both member-job and member-member relations. The statistics of the established job marketplace heterogeneous graph are summarized in Table .

%  Since the decisions on the job marketplace need to be fast at LinkedIn, we use a comparatively small PLM, i.e., GPT-2  with 768-dimensional token embeddings and vocabulary size of 50,257, as the PLM backbone for PLM4Job. For the metapath-based structural modeling (see Section ), we select six metapaths , where in each epoch, we randomly select one of the one-hop meta-paths and one of the two-hop meta-paths for the structural information aggregation. During the training stage, we first optimize the newly introduced ego-graph token embeddings (see Eq. ()) via self-supervised feature/structural modeling as with Eqs. () and () for ten epochs to warm up the model. Then, we add the task-specific finetuning objective to subsequent epochs, where we alternately train the PLM4Job model according to Eq. (), Eq. (),  Eq. ()/() for 100 epochs. For the node-level tasks, we randomly select 15\% nodes with labels as the validation set and another 15\% for testing, where accuracy and F1-score are used as the metrics. For the link-level tasks, we evaluate the PLM4Job on nodes with more than five target links, where for each of such nodes, 60\% of the links are included for training, 20\% are held out for validation, and another 20\% for testing, where ranking-based metrics such as Recall@ and NDCG@ are used to measure the performance.

We compare PLM4Job with various baselines on different downstream tasks on the job marketplace. Specifically, the baselines used in this paper can be categorized into three classes:  graph neural network (GNN)-based methods, such as GCN , GAT , as well as GNNs specifically designed for heterogeneous graphs, such as the heterogeneous GNN (HetGNN)  and heterogeneous graph attention network (HAN) ;  graph transformer-based methods such as the graphormer (GT) , the ego-graph-based transformer, Gophormer  and the heterogeneous graph transformer (HGT) ;  the PLM-based method, i.e., InstructGCL . In addition, we introduce two more baselines, i.e., SGL-Text  and JMMFR (graph-based)  for node-level tasks, and LightGCN (graph-based)  and P5 (PLM-based)  for link-level tasks.

In this subsection, we show the experiments of PLM4Job on node-level tasks on the LinkedIn job marketplace. Specifically, we are interested in two tasks, i.e., , which aims to predict whether a member has coding-related or management-related skills, and , which aims to predict whether a member is willing to take an online/onsite job. Since a member can have multiple skills and prefer multiple types of work modes, we model them as different binary classification problems. Both of these can significantly benefit the member-job matching at LinkedIn for better job recommendation results.

We first compare the proposed PLM4Job with the baselines introduced in Section , where the results are summarized in Table . From Table , we can find that heterogeneous GNNs generally show better performance than the normal GNN models due to their explicit consideration of different relations in the job marketplace graph. However, since these models use bag-of-word representations to model member/job textual features, their shallow understanding of important textual features leads to overall unsatisfactory results. For the graph transformer (GT)-based methods, HGT can distinguish heterogeneous relationships in the job market graph, but as a global model, it may not be able to fully utilize the local information for predictions. Gophormer is specifically designed for ego-graphs, but it does not consider the heterogeneous structure in the job marketplace. Most importantly, although GT-based methods have a similar underlying transformer structure as the PLM, these models are not pre-trained on large datasets and do not contain prior knowledge of the natural language. Therefore, their understanding of the member/job textual features as well as their relationship in the job marketplace is also shallow. As a PLM-based graph mining algorithm, InstructGCL performs the best among all the baselines as it utilizes the pretrained knowledge of PLMs. However, it does not consider the heterogeneous relationships in the job market graph. In addition, the proximity information is described via natural language such as "one-hop," etc., which may not faithfully direct the attention of the PLM according to the proximity information in the heterogeneous job marketplace ego-graph. In contrast, by tightly coupling the heterogeneous local structure of job marketplace graph with the pre-trained knowledge of the PLM, the proposed PLM4Job achieves the best results on the four datasets across all the metrics.

In this part,  we conduct ablation study to show the effectiveness of the ego-graph-based prompt (see Section ) and the proximity-aware attention alignment strategy (see Section ). Specifically, three ablation models are introduced on PLM4Job, where  removes the entity and graph positional embeddings,  removes the proximity-aware attention alignment module,  removes the second-order meta-paths in structural modeling. The results are summarized in Table . For Table , we can find that proximity-based attention alignment contributes significantly to the superior performance of PLM4Job, which demonstrates the misalignment of the attention original PLM with the proximity relations in the heterogeneous graph structure. In addition, the combination of entity and ego-graph positional embeddings facilitates PLM4Job to well distinguish different nodes in the heterogeneous job marketplace ego-graph. 

In this sub-section, we show the experimental results of link-level prediction tasks on the LinkedIn job marketplace. Specifically, we focus on predicting member-job interactions (i.e., JYMBII prediction) and member-member interactions (i.e., PYMK prediction).

Similarly, we first compare PLM4Job with various GNN-based, GT-based, and PLM-based baselines, where the results are summarized in Table . From Table , we can find that, generally, PLM4Job outperforms most of the GNN/GT/PLM-based baselines, which demonstrates its ability to generalize to link prediction tasks on the job marketplace. In addition, ablation studies are also conducted for the link prediction task, where the introduced ablation models are the same as the ones used in subsection . The results are summarized in Table . From the Table, we can find that all components of the proposed PLM4Job also contribute positively to its final superior results for link-level tasks.

PLM4Job intends to serve as the foundation model for the LinkedIn job retrieval system. At LinkedIn, the L1 retrieval model is evaluated from the user feedback on the L2 ranking model. Since PLM4Job is expensive to deploy directly, we extract the member/job token embeddings from the trained PLM4Job model (takes O(1) complexity), reduce their dimension, and deploy them on two of the most important systems at LinkedIn:  and . We name the two-tower model with PLM4Job embeddings as PLM4Job-Emb.

We compare the PLM4Job-Emb model with another model that adds the embeddings of M6-Rec , i.e., a PLM-based matching method for recommendations (where we denote the model as M6-Rec-Emb), as well as the original two tower model. Specifically, we randomly split the members into three folds and evaluate the three models on users' feedback on the L2 ranking model accumulated in a week. From Table , we can find that, adding PLM4Job member/job embeddings can improve the performance of the existing two-tower model at LinkedIn (which includes embeddings from internally trained BERT  and GNNs), which further demonstrates the ability of PLM to serve as foundation models for job marketplace and adapt to downstream tasks with effectiveness and efficiency.