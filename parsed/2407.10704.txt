In recent times, large-scale vision-language models have demonstrated remarkable performance across various tasks. Seminal works such as~. Classic works have focused on learning multimodal representations through self-supervised methods using extensive sets of image-text pairs. Among these, CLIP stands out as a milestone achievement, employing contrastive learning to align vision and language representations and achieving exceptional performance. A well-trained vision-language model is invaluable, offering substantial support to various fields. Successful applications of these robust models include few-shot recognition~, detection tasks~, and segmentation tasks~. Furthermore, for video data, research efforts have emerged in video classification~ and video understanding~.

Recently, a series of works~ have been proposed to help transfer the learnt knowledge, where one of the most popular field is parameter-efficient fine-tuning (PEFT). PEFT aims to transfer a pre-trained model to downstream tasks by a minimum number of parameters. Originating from natural language processing tasks, classic methods like adapter~, prompt tuning~ and LoRA~ follows a similar principle to add extra modules with a small number of parameters into the backbone model, freeze the original parameters and only tune and store the newly added parameters.  Inspired by the success of PEFT in language field, researchers have extended such kind of approaches to adapt visual models in a similar fashion~. In the field of vision-language modeling, several explorations have been made as well. Bahng et al. exclusively apply prompt tuning to the image encoder. CoOp replaces the fixed template in CLIP~ with tunable text prompts. CoCoOp~ leverages image features to guide the optimization of tunable text prompts in CoOp. Other works~ optimize both image and text prompts simultaneously and establish additional connections between different modalities. To mitigate overfitting and catastrophic forgetting, various works~ integrate regularization modules or losses into prompt tuning. 

Quantization is one of the most effective compression methods~ for deep learning models. Generally, parameters such as weights and activations are typically stored as 32-bit floating-point numbers, which consume a significant amount of memory and require intensive computation during inference. Quantization~ involves representing these parameters with reduced precision, such as 8-bit integers or even lower bit-widths. By doing so, quantization can significantly reduce the memory footprint and computational complexity of the model without significantly sacrificing accuracy. Quantization methods can be divided into two groups, Post-Training Quantization (PTQ)~ that consumes few resources but suffers higher accuracy loss, and Quantization-Aware Training (QAT)~ that relies on plenty of resources for training and shows better accuracy. Existing works aim to minimize quantization error to improve accuracy, while our work demonstrates that both excessive and insufficient errors are detrimental to model generalization. To achieve optimal generalization performance, a moderate error is required.

CLIP comprises a text encoder  and an image encoder . Typically,  is implemented as a language transformer, whereas  may be realized using either a convolutional neural network or a vision transformer. In this study, following the methodologies outlined by~, we employ a ViT-B/16 model~ as the image encoder , except where otherwise specified. The subsequent sections will provide a brief overview of the methods used to prompt CLIP for prediction tasks. \\  Consider a text encoder composed of  layers. For the -th layer, denoted as , the inputs consist of a sequence of prompt tokens  and a  token , while the outputs are represented by  and . The initial inputs,  and , correspond to the word embeddings of the prompts combined with the label, such as ``'' or alternatively, some randomly initialized vectors. Formally, we denote  and , where  signifies the length of the text prompts and  represents the dimension of the word embeddings. For each layer, , the relationship is given by . %%%    [P^{l}_{k},c^{l}_{k}]=_{k}([P^{l}_{k-1},c^{l}_{k-1}])%%\\ The output feature of the text encoder , where  is the dimension of the visual feature space, is calculted by projecting the  token of its last layer to the visual latent space through a linear transformation,  . \\  Suppose there are  layers in the image encoder. For -th layer , the inputs are a series of image tokens , a classification token  and prompt tokens , and the outputs are ,  and . The inputs of the first layer  and  are the patch embeddings of the input image and the pre-trained class token.  is randomly initialized. Formally, ,  and , where  denotes the number of image patches and  denotes the dimension of visual embedding. , . The output feature of the image encoder is . \\  For image classification, suppose there are  classes, and  are the text features. Label 's probability is  where sim denotes cosine similarity function and  is temperature. The final prediction is .

%%  p(y|f^v)=((f^v,f^{l}_{y})/\tau)}{\sum_{c=1}^{C}((f^v,f^{l}_{c})/\tau)}   %%%\\where sim denotes cosine similarity function and  is temperature. The final prediction is .

We have introduced shallow prompts. There are also different types of prompts. Several works~ use them for improve performance.They directly add and tune the prompt in each layer in the feature encoder, instead of inheriting the output prompt calculated by the last encoder. Now we have  and . Note that  and  are independent tunable parameters. They are no longer determined by the previous layer.

%%[\_,c^{l}_{k}]=_{k}([P^{l}_{k-1},c^{l}_{k-1}])%%and %%[\_,c^{v}_{k},I_{k}]=_{k}([P^{v}_{k-1},c^{v}_{k-1},I_{k-1}])%%\\Note that  and  are independent tunable parameters. They are no longer determined by the previous layer. Some existing work has explored how to use the addition of noise to suppress model overfitting, , techniques like Dropout~ that randomly drops connections, and random jittor that directly introduce perturbations in the input data. However, the impact of directly adding noise to model weights has been less explored, especially in the context of prompt structures, which have become popular only in recent years, in Transformer architectures.  The variations of the specialization capability represented by the test accuracy on base, seen classes and the generalization capability represented by the test accuracy on new, unseen classes. The curves of different colors represent the data under the influence of random Gaussian noise of different intensities,  ``Noise.01'' adds random noise with a distribution of  to the prompt. ``Noise'' denotes the baseline prompt tuning. As training progresses, the generalization capability of baseline continuously decreases while the specialization capability improves. Therefore, we expect that adding noise can achieve a better balance between generalization and specialization. However, excessive noise,  0.1, greatly diminishes the model's specialization capability, while insufficient noise,  0.001, fails to provide effective regularization. Only noise of moderate intensity outperforms baseline in specialization-generalization trade-off, effectively enhancing the unseen class accuracy without significantly compromising seen class accuracy.

Quantization, the technique of mapping parameter values from high precision to low precision, can also be viewed as introducing some form of noise into the parameters, and thus can possibly improve the genralizability as the noise did in . Compared to Gaussian noise, quantization error is more controllable. Better still, quantization also has another major advantage: it can significantly reduce the storage required for parameters. Therefore, using quantization algorithms to generalize vision-language models is a very promising direction.

% In this subsection, we will demonstrate the characteristics of prompts in the vision-language model to facilitate targeted design of quantization schemes. 

We start from analyzing the training procedure of CoOp~, which freezes all the parameters in backbone and tunes the added prompts only. The histogram about the frequency of weights of prompts for different training epochs is shown in . We observe that the shape of the prompt's weight distribution remains mostly consistent throughout the entire training process. However, the variance of the weight distribution increases rapidly at the initial stages of training. Refer to  for more details. Moreover, we notice minimal presence of outliers in the prompt's weights throughout the training process. Apart from the unstable initial training phase, the changes in weights between consecutive phases are not substantial, indicating a very gentle overall updating trend. 

Taking into account the observations from , we can draw the following conclusions: 

Suppose there are  parameters in total to be quantized, which are denoted by . Each  here is a high-precision float-type number. A -bit model quantization algorithm divides the value range of parameters into  intervals , aiming to find a mapping  from intervals to points, mapping all values within an interval  to the same quantized value ,  . We define the quantization error . Such error serves as the objective of K-Means algorithm as well,  so it can be ensured that each time clustering is redone using the K-Means algorithm, the quantization error will most likely decrease.

Based on the observation and analysis in , we build a simple yet quite proper quantization algorithm for generalizing vision-language models. As stated before, we choose a widely-used clustering method K-Means, to construct the mapping . Initially, K-Means algorithm is run to fit the pre-trained prompt weights, and record the codebook,  cluster centers. Normalization and denormalization are conducted beyond quantization to alleviate the influence of varying variance. Formally, given the original prompt  with  parameters, we first calculate  and , then normalize  and get . K-Means quantization is applied on , and the quantized prompt . Since  is not differentiable, we adopt a common practice to directly propagate the gradient across the quantization function by Straight-Through Estimator (STE),  . 

The overall framework is shown in . In training, we keep tuning the weight by the fake gradient propagated from quantization operation. For storage, we first convert the fp16 parameters in prompt to -bit indexes, which could be used to search for the corresponding cluster center in the codebook. In , we set  for a clear explanation. Compared with baseline method, our quantized method could save a lot more storage space. Specifically, for storage, an ordinary method needs  bits, while ours only needs . In experiments, we usually set  to ,  or , which is far more smaller than . For example, with , the storage space of our method is roughly  smaller than baseline.

Note that the codebook here is updated by a rule called ``constrained adaptive clustering''. We will give a detailed description of it in the following subsection.

In , re-clustering the prompt every iteration like classic QAT algorithms did to keep the codebook updated at all times is not a good choice. There are three reasons: 1. K-Means algorithm is not quite efficient, and thus if we run it every iteration then the training efficiency would decrease; 2. Only moderate noise promotes generalization, so keep updating the clusters to minimize the quantization error may be not helpful but harmful; 3. From , we can observe that the weight changes are very gentle for most of the training time, and the weight distributions of adjacent stages are highly similar. Therefore, even if we update the internal clustering state of K-Means with these already very similar data at each iteration, it's difficult to generate a better clustering solution, which is just futile effort.

We propose constrained adaptive clustering to instruct the update of parameters inside K-Means which would not updated by gradient at all. Intuitively, first, we do not want too often update, so we set a minimum cluster update interval . Second, to avoid K-Means meaninglessly handling similar weights with current cached state,  the weights of prompt that triggers the last re-clustering, we plan to only do re-clustering when current weight distribution is far different from the cached weight distribution. To reach the goal, Kullback-Leibler divergence is a good metric. However, if we want to compute the Kullback-Leibler divergence between two sets of discrete random variables, they must be defined on the same event space. However, that's not the case. So we  first project each of them into the same event space spanning by the K-Means clustering algorithm. Specifically, given a current weight , cached weight , quantization function , cluster centers , we will compute the  and  as in .  Then, we can obtain the probability distributions of the indices  and , respectively, implied by  and , thus successfully transforming two originally unrelated discrete random variables into the same event space. The Kullback-Leibler divergence could be computed as follows:

The update would continue only if Kullback-Leibler divergence exceed a certain threshold  which shows the distribution difference is significant.

The average results over 11 datasets are shown in . For complete results, please refer to the Appendix. Our proposed QCoOp reaches  average harmonic mean accuracy within merely  size. QCoOp outperforms all kinds of lightweight state-of-the-art methods with much more efficiency and higher accuracy. For heavier methods like MaPLe, our method can be fruitfully integrated into existing solutions. Besides prompts, we also perform a similar quantization operation on the other weights of MaPLe that are in the linear layers. As a result, QMaPLe not only shows stronger generalization and adaptation capability and gives 0.57\% higher accuracy, but also enjoys a much more smaller model size compared with the original MaPLe.  %Overall, RLP shows the strongest performance, and the superiority mainly relies on the improvement of new classes. In other words, RLP largely improves the generalization ability of CLIP. In particular, compared with our direct baseline CoOp, RLP gets \% accuracy gain on the new classes and \% accuracy drop on the base classes. Such a result tells us that our low-rank prompt and lightweight regularization design reach our initial purpose to make the model more generalizable, instead of overfitting every upcoming dataset. 

Notably, when comparing QCoOp and a lightweight method ProGrad, even if QCoOp is  smaller than ProGrad, QCoOp still outperforms it by a clear margin, demonstrating the outstanding efficiency and effectiveness.

%QCoOp, ImageNet:70.67, Target Average:60.2, V2: 63.87, S: 48.93, A: 51.1, R: 76.9 In this paragraph, ImageNet, ImageNet-A, ImageNet-R, ImageNet-v2, and ImageNet-S are used to construct domain generalization experiments. As shown in , on downstream target datasets, QCoOp gets better average accuracy compared with the other methods with significantly better efficiency. For CLIP, CoOp, CLIP-Adapter and ProGrad, there is a clear performance gap between our method and them. 

In this paragraph, we do cross-dataset transfer evaluation to further verify our QPrompt. Results are shown in . CoOp is good on source domain but fails on target domains. Probably because it focus too much on the dataset shown to its eyes and face overfitting and catastrophic forgetting problems, which leads to a severe performance drop on unseen objects. QCoOp wins on 5 of 10 datasets and its average accuracy is also slightly better than the best competitor CoCoOp, showing that QCoOp could maximally extract both general and data-agnostic knowledge from given images. 

% Here we will show the experiment results of QCoOp in the few-shot learning setting that is originated from CoOp. Seen from , QCoOp consistently outperforms zero-shot CLIP, CoOp, and CLIP-Adapter across all the shot numbers. Such results demonstrate the superiority of QCoOp in adaptation ability when there are few samples in downstream tasks.

Overall, in base-to-new generalization, domain generalization, cross-dataset transfer and few-shot learning, the proposed method can consistently accomplish state-of-the-art performance while enjoying extremely high parameter efficiency, fruitfully demonstrating the effectiveness and efficiency of the proposed method.

 In this subsection, we decompose the method into pieces and show the influence of each component. As in , we clearly show how much the K-Means algorithm, normalization/denormalization and constrained adaptive clustering influence the final performance. One interesting point is that K-Means+Norm+CAC only improves the new accuracy compared with K-Means+Norm, showing the superiority of our constrained adaptive clustering. 

 To verify the opinions we proposed at the end of , we show the KLD of prompt distributions between adjacent epochs during CoOp's training in . Clearly, this trend is consistent with what we summarized before.

 In , we study the choice of QAT or PTQ following the same strategy, K-Means clustering, in the paper. Results show that QAT consistently outperforms PTQ with the same hyper-parameters. The accuracy on unseen new classes of PTQ is significantly lower than QAT, again proving our opinion that quantization helps generalization by alleviating overfitting as well as catastrophic forgetting. Quantization error is not always undesirable.

In , we show the results across multiple quantization bits. In conclusions, more bits did not always lead to good results, and new accuracy continues decreasing as the training goes on.

In this paragraph, we explore the usage of QCoOp among other different backbones besides CLIP. We choose a self-supervised vision-language model, SLIP~ to further verify the universality and robustness of our proposed method. The experimental results are shown in . We could see that the base accuracies of QCoOp and CoOp are similar, but the new accuracies of QCoOp is much higher than CoOp. Such observation verifies our assumption that quantization is quite helpful for generalization again. %%