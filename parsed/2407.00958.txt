Before delving into transforming Linear and MHA into their matrix-vector format, we introduce the Matrix-Vector Method, which will subsequently be employed to cast both Linear and MHA operations into a unified matrix-vector format. This method constitutes a strategic realignment of input data and corresponding parameters of various transformations within the network, as illustrated in Figure . The underlying principle is as follows: both the input () and output () data are reconfigured uniformly through a transformation  into column vectors ( and , while parameter tensor () is reorganized into matrix form (). A critical requirement is that identical operations throughout the LLMs network adhere to the same restructuring scheme, thereby eliminating the need for additional transformations on intermediate feature data; they can directly be represented as column vectors. Additionally, by default, matrix variables in the original formulas are represented in bold, such as , and in the Matrix-Vector form, corresponding variables are denoted with a prime symbol () in the upper right corner, such as . Elements within matrices are represented by corresponding lowercase letters with subscripts, for example, . The Matrix-Vector Method can be succinctly encapsulated as follows:

It is obvious that  and  are not fixed, we could design various kinds of ways to do those. For convenience, we present a methodology tailored for Linear in Section  and an approach for MHA in Section , thereby illustrating the adaptability and application of the Matrix-Vector Method across different components of the Transformer architecture.

While our goal is to transform Linear and MHA modules into a matrix-vector format, this process is complex. In order to describe this transformation process clearly, we propose a new computational method called the Diamond Multiplication Method, denoted by the symbol . (The purpose of proposing this method is to clearly illustrate the calculation process in turning MHA into Matrix-Vector format. If we use direct matrix multiplication, some computational details cannot be visually distinguished.)

Let  and , the Diamond Matrix Multiplication is defined as , where . The computation procedure involves element-wise multiplication and summation of corresponding elements from left to right across the columns of matrix  with vector . The sum of the multiplication of the th column of  with  is assigned as the th element of . A detailed calculation process is provided in Eq. . The computation process for  is identical to that of , implying that .  %However, it should be noted that the Diamond Matrix Multiplication does not possess the properties of associativity and distributivity.

Furthermore, based on its computation method, it can be inferred that the Diamond Matrix Multiplication is related to conventional matrix multiplication, specifically: . Additionally, when  and  are square matrices, we can derive the following relationships:  and . Detailed derivations can be found in .

In this section, we present a way to transform Linear operation into matrix-vector format. Figure  illustrates this process: Figure . shows the linear transformation of multi-channel input: . Figure . provides a specific example of Figure ., Figure . converts the linear transformation in Figure . into the corresponding matrix-vector representation: . Thus, the linear transformation can be represented in matrix-vector form as follows:

Here,  and  represent the input and output of layer , respectively, while  represents the parameters of layer . , , and  are generated based on , , and  using the Matrix-Vector Method. For convenience, we use  to represent . Therefore, . So the  FFN in the Transformer could be written as:

We now employ the Matrix-Vector Method to elucidate the inner workings of the MHA. The mechanism is defined by the following equation:

Here,  represents the number of attention heads, and the input  is divided into  based on . The parameters , and  correspond to . The whole process of MHA can be represented in Figure~. Figure~. represents that the input  is split into  based on the number of heads. Figure~. represents . Figure~.~, ,  represent the process of  and we omit . Figure~. represents . Figure~. represents the whole process by a matrix multiplication of , where  is generated based ,  and . This means that we could represent the whole complex MHA in matrix multiplication. Next, we give the proof of this process.

Our objective is to express the MHA as . Prior to transforming the MHA into the matrix-vector form, we need to conduct a comprehensive analysis and clearly define the research object. In Eq.~,  describes an engineering process that requires mathematical representation. The learning process for the input  primarily consists of two parts:  and .  is derived based on the parameters ,  and . 

Figure . represents the computation process of  . In Figure ., we present a simple example of Figure .. Figure . depicts . In Figure . we convert Figure . into the matrix-vector form , where  is generated from  and . More details can be found in .

Figure  illustrates the parameter transformation scenario after incorporating . Figure . demonstrates an example of , while Figure . rewrites Figure . as . Consequently, based on Figures  and , the entire MHA can be expressed as , where  (Based on Figures , we can get , but the resulting expression is too complex and not easy to understand, so we did not compute the final result. However, it is easy to analyze that as long as the variables in  and  are non-zero, the result is essentially a dense matrix). So the whole MHA can be written as: Here,  and  are the input and output, respectively, while  denotes matrices generated in accordance with ,  and . In this manner, we have expressed MHA as a matrix-vector multiplication. This matrix multiplication representation provides a more concise way to express the MHA mechanism. 

For convenience, we use  to represent . The MHA operation could be simply written as:

The Transformer architecture is founded on two pivotal components: FFN and MHA. In Sections  and , we have showcased the matrix-vector representations for both FFN and MHA. In this section, we delve into why multi-layer Transformer is the implementation of UAT.

Based on Equations  and , we can demonstrate that -th layer () Transformer is a concrete realization of the UAT, which can be written as (see  for more details):

where  is the output of -th layer,   is the input of the network, , where . The term  is approximated by the  layer of UAT with  as input. This enhances the model's ability to dynamically adjust functions based on input. In the MHA mechanism, the parameters change dynamically with the input. Therefore, in the formula above, all , , and  parameters in layer , where , are dynamically adjusted based on the input.

Theoretically, we have established that Transformer networks are concrete implementations of the UAT, enabling them to approximate any Borel measurable function. While UAT provides powerful function approximation capabilities, it inherently lacks the ability to approximate multiple functions simultaneously. However, language tasks are inherently diverse, often requiring the approximation of different functions based on the input. For instance, when summarizing, translating, or continuing the same text, the input functions are nearly identical, with only minor variations in the prompt. Without the ability to dynamically approximate functions based on the input, simply fitting a general function trend based on input will result in identical or similar outputs.

To address this, LLMs must distinguish and adapt to these nearly identical functions, dynamically generating response functions based on the input. The MHA and residual mechanisms in Transformers equip LLMs with the ability to dynamically approximate relevant functions according to the input. Specifically, MHA allows for the dynamic adjustment of the weight parameters in UAT in response to the input, while the residual mechanism supports the dynamic approximation of bias terms, as illustrated in Figure . This theoretical foundation enables Transformer-based LLMs to handle a wide range of tasks, including translation, continuation, summarization, code generation, and solving mathematical problems. 

Furthermore, the MHA mechanism can capture global information (as shown in Figure , where each element in the output  contains global information), which helps generate content that is consistent with the context. This is crucial for language understanding that requires extensive contextual information. In contrast, 1D convolution employs a sliding convolution learning strategy, where the learned content is somewhat influenced by the size of the convolution kernel, leading to a more fragmented learning approach (for more information, please refer to ).

Contextual interaction, as the core capability of LLMs, permeates every phase from training and fine-tuning to prediction. ICL, multi-step reasoning, and instruction following are intuitive manifestations of this contextual interaction. Leveraging their context-sensitive interaction capabilities, LLMs can exhibit behaviors consistent with ICL, multi-step inference, and instruction following, which are tailored based on contextual cues. 

So, how does this contextual interaction capability arise within LLMs? The formula  in Figure  reveals this mode of contextual interaction. Since  represents a dense matrix (almost devoid of zero elements and whose internal elements are highly correlated), each element in  encapsulates comprehensive information from both preceding and subsequent contexts. This learning of holistic contextual information constitutes the foundation of contextual interaction within LLMs. (See  for more details)

Due to the massive size of parameters in LLMs and the subsequent high demand for computational resources, pruning LLMs is pivotal for their deployment. A legitimate question to ask is why LLMs are amenable to pruning. The rationale lies in the presence of excessively low-weight parameters in certain layers of LLMs. To understand this, we can directly analyze it from the perspective of the formula underlying the UAT:

for all . Let's assume  and . Then we have:

Since , we have the following inequality:

Therefore, we have:

Hence, when parameters in certain layers are small enough, we can directly remove those layers since their impact on the final result is minimal.

Given the substantial computational resources required to train LLMs and their powerful generalization abilities, we believe that more efficient use of pre-trained models is essential. Re-training models from scratch incurs significant computational costs, so reusing well-trained models for new tasks is both practical and resource-efficient. A prominent solution to this challenge is the LoRA~, which can be expressed as follows:

According to Eq. , we use LoRA to fine-tune the Linear operation in FFN can be written as:

From Eq. , it can be seen that LoRA essentially fine-tunes the UAT parameters layer by layer for a specific task.

The capabilities of LLMs have become so advanced that their language processing abilities are approaching human levels, raising a core question: How do LLMs differ from humans in language processing? Figure  illustrates the comparison between the language processing processes of LLMs and humans. Both start with language encoding—humans encode language through a character-based system, while LLMs use numerical arrays. At this level, there is almost no difference. Given the ambiguity of words, determining context is crucial: humans understand context through the activation and transmission of neurons in the brain, while LLMs approximate the corresponding functions using UAT. Here, the input and output of network layers are analogous to the transmission of neural signals in the brain, and the function approximation corresponds to the final output of humans. From this perspective, the differences between humans and LLMs in language processing seem minimal.

So how do we explain human understanding, analysis of language, and memory retrieval? Are these also capabilities of LLMs? First, we can consider the human brain as a combination of one or more UAT models, which are randomly initialized at birth. What supports such an assumption? The original neural network, the perceptron , was designed based on the human neuron. Over time, as deep learning networks evolved, the difference in form between deep networks and the perceptron grew, leading people to no longer associate neural networks with human neurons. However, through our derivation, the Transformer can also be understood as an implementation of UAT, and the perceptron can be seen as a layer of UAT. The human brain can be considered as a cluster of multi-layer perception. Learning language in a social environment is akin to training the UATs in the brain. Therefore, what we call understanding, analysis, and memory retrieval are essentially processes of fitting outputs based on inputs. Understanding and analysis: When we respond to a question posed by someone, it can be seen as a simple example of understanding and analysis, which both humans and LLMs can accomplish. Humans may guess the answer to unfamiliar questions based on their experiences, which can be either correct or incorrect, and LLMs exhibit similar behavior. Memory retrieval: This is essentially fitting specific results based on particular words, as there is no actual database in the brain. For instance, recalling the experience of eating an apple for the first time in childhood is the brain fitting specific results based on those words; without those words, the memory would not surface. This is because the brain dynamically outputs results based on inputs and fills in the details of the event based on learned natural rules (We provide some examples about understanding, analysis of language, and memory retrieval in ). However, as humans grow, the weights in the brain are constantly updated, leading to potential memory distortions.

Therefore, it is entirely reasonable for LLMs to make errors or generate hallucinations—these are just outputs produced based on existing weights and inputs, a problem humans also face. We believe the greatest advantage humans have over LLMs is their powerful multimodal and multitask processing abilities (which can be understood as the coordination of multiple UAT models to produce reasonable results). Another advantage is the interaction with the real world, which allows us to verify the knowledge we've learned in reality, enabling the brain to continuously optimize its parameters based on inputs. LLMs, on the other hand, are limited to function approximation within the corpora data.

Before expressing Transformers in the UAT format, we present two lemma regarding UAT. There are two cases for UAT-approximated functions:  and . The proof for the case where  can be inferred from . Therefore, we will only provide the proof for approximating  using UAT.

Lemma 1. The mathematical form of UAT remains unchanged when multiplied by a matrix (constant).

Eq.  shows the representation of UAT multiplying a matrix. Let , and the result remains consistent with the original UAT mathematical form. Thus, it is proven that the mathematical form of UAT remains unchanged when multiplied by a matrix (constant).

Lemma 2. Let the general term of the network be written as: , . The multi-layer network of this form still corresponds to the mathematical form of UAT, and we refer to such a network as a residual network.

To prove that a multi-layer network with the general term written as  corresponds to the UAT mathematical form, we first provide the forms of single-layer and two-layer networks, as shown in Eq.  and .  %, where , % In Equation , let , thus Eq.  can be written as:% % % '_{1} =('_{1,1}'_{0}+'_{1,3})+'_{1,3}\sigma ('_{1,2}'_{0}+'_{1,2})% % % In Eq. , let , , , , and . Thus, Eq.  can be written as:

Why can we combine the above terms? Once the network training is complete, we know these parameters, so they can be directly calculated and thus combined. And  clearly has the same mathematical form as UAT. We can understand it as dynamically fitting the bias term  using UAT based on the input. Therefore, we have proven that the mathematical form of one and two-layer residual networks is consistent with the UAT mathematical form.

Assume that the mathematical form of the first  layers of the residual network is consistent with UAT. Our goal is to prove that the mathematical form of the -th layer of the residual network is still consistent with UAT. For the convenience of expression, we make the following definition: since the mathematical form of the first  layers of the residual network is consistent with UAT, we write  as , where the first term is written separately and the rest is written as the remainder term . Since , we divide  into two parts:  and . 

First, consider the part . Substitute  into it, and we get Eq. . Let  and , we can simplify the first part to . Since the mathematical form of  is consistent with UAT, and we have already proven that the mathematical form of UAT remains unchanged when multiplied by a matrix, we have proven that the mathematical form of the first part is consistent with UAT.

Next, we prove that the mathematical form of the second part, , is consistent with UAT. By substituting  into it, we obtain Eq. . Let  and . Thus, the second term can be written as , which can be considered as a term within UAT. Here,  is a bias term approximated using UAT.

Since the first term of  has the same mathematical form as UAT, and the second term can be considered as a component within UAT, the sum of both terms still corresponds to the UAT mathematical form. Therefore, we have proven that the mathematical form of a multi-layer residual network is consistent with the UAT mathematical form. The difference lies in that some bias terms are approximated using UAT rather than being directly defined.

Similarly, To illustrate relationship between multi-layer Transformer and UAT, we will first derive the general form based on Figure  and then compare it with the general form given in Section : 

By examining the relationship between these forms, we can demonstrate that the mathematical form of multi-layer Transformer networks is consistent with the UAT. The Transformer involves two key operations: MHA and FFN. These operations, when expressed in matrix-vector form, correspond to Eq. and Eq., respectively. Consequently, the general term for a Transformer-based network can be written as Eq. .

% [htbp!]% \centering% % MHA('_{i}) = '_{i,1}'_{i}% % % % % % [htbp!]% % FFN('_{i}) = '_{i,3}\sigma ('_{i,2}'_{i}+'_{i,2})+'_{i,3}\\% % % 

Let , so we have . It is evident that, when compared to the general form given in Section : , the mathematical forms are the same. Therefore, multi-layer Transformers are also specific implementations of the UAT.  Similarly, to clearly present the UAT format, we use the method mentioned in Section . Assuming the mathematical form of  is consistent with UAT, we decompose  into a primary term plus a remainder term, written as . Substituting this into , we obtain Eq. . By setting , , , and , , we get . Since the mathematical form of  is consistent with multi-layer UAT, it follows that multi-layer Transformer networks are also specific implementations of UAT.  In addition, . So, , where , , . To sum up, the -layer Transformer can be written as:

Other parameters will also vary with the number of layers. And, these changes involve only matrix multiplications or additions. Thus, they do not fundamentally alter the mathematical form of the UAT. Therefore, we will not detail the specific forms for each layer here.

The parameters in the MHA mechanism dynamically change with the input. Therefore, in the corresponding UAT mathematical form for Transformers, all ,  and  parameters for the -th layer, where , are dynamically changing with the input. We give some examples in Figure .

%