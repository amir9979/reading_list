We gathered 32 participants, all of whom are Korean students. Each participant was asked to provide a free-form text that they had manually written without using LLMs, along with instructions for writing that text. This includes writings from the past, for which we labeled the age at which the text was written. We excluded texts that were part of classification tasks or were short answers. After this process, the main authors categorized the text into 15 types of writing, including Book Report, Descriptive Essay, Process Essay, Reflective Essay, Story Writing, Play Script, Linguistic Report, Scientific Report, Presentation Script, Problem Creation, Argumentative Essay, Presentation Report, Diary, Self Introduction Essay, and Letter.

The main authors manually typed and revised the texts based on the instructions, as these documents are typically long and in PDF format. To maintain the quality of the text, we did not revise the content or grammar. Instead, we manually corrected line break issues using an online JSON formatting tool~ to ensure compatibility of the dataset. Each instance consists of six components: student id (ranging between 0 and 31 to distinguish the writer of each text), age (ranging between 11 and 19, based on the age when the writer composed the text, not their current age), language (either Korean or English), type of writing (among the 15 categories listed above), input (the instruction to write the corresponding text), and output (the text that each student wrote). The statistics of the age, type of writing, and language are shown in Table~, Table~, and Table~.

We mainly follow the evaluation pipeline of Prometheus~. Specifically, we utilize the  library~, following the default hyper-parameters, and add five custom score rubrics to evaluate texts written by students. The code and prompts are provided in supplementary material. We use GPT-4-Turbo-April as the evaluator for conducting 500 judgments. The judgments consist of verbal feedback, which highlights the strengths and weaknesses of the student's text with respect to each evaluation criterion, and a scoring decision, which is an integer between 1 and 5. The criteria are as follows: the fluency criterion assesses if the text fluent and easy to read, considering it is written by a Korean student. The coherence criterion evaluates whether the text is coherent and logically organized, considering it is written by a Korean student. The consistency criterion examines if the text is consistent in terms of style, tone, and tense. The relevance criterion checks if the text is relevant to the given instruction or topic. Lastly, the grammaticality criterion assesses whether the text demonstrates proper grammatical usage.

To verify if the evaluation results from Subsection~ are valid, we distribute the verbal feedback and scoring decisions back to the 32 student participants who provided their writing. We ask if the scoring decisions and the verbal feedback were valid or not (, the scoring as well as the verbal feedback is overly critical or overly optimistic). Each participant were paid \$ 10 for providing each writing piece and verifying the judgments (15 minutes per instance). Although we were unable to confirm whether students' writing improved through direct revisions due to cost limitations, the goal was to indirectly verify if they could improve their writing by reviewing whether the feedback was valid. Future work could involve experimenting with students directly revising their writing.